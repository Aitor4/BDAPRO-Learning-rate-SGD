objc[2380]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x103a0e4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x103a924e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 20:49:43 INFO SparkContext: Running Spark version 2.0.0
18/02/26 20:49:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 20:49:43 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 20:49:43 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 20:49:43 INFO SecurityManager: Changing view acls groups to: 
18/02/26 20:49:43 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 20:49:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 20:49:44 INFO Utils: Successfully started service 'sparkDriver' on port 51291.
18/02/26 20:49:44 INFO SparkEnv: Registering MapOutputTracker
18/02/26 20:49:44 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 20:49:44 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-c12b378d-e501-48fb-9d99-5ac848bfe8c9
18/02/26 20:49:44 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 20:49:44 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 20:49:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 20:49:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 20:49:45 INFO Executor: Starting executor ID driver on host localhost
18/02/26 20:49:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51292.
18/02/26 20:49:45 INFO NettyBlockTransferService: Server created on 192.168.2.140:51292
18/02/26 20:49:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 51292)
18/02/26 20:49:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:51292 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 51292)
18/02/26 20:49:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 51292)
Loss in iteration 1 : 1.0
Loss in iteration 2 : 7.061600602025958
Loss in iteration 3 : 2.935446769222878
Loss in iteration 4 : 1.5141555336887036
Loss in iteration 5 : 2.667328781414312
Loss in iteration 6 : 1.5293072060501407
Loss in iteration 7 : 2.981625671978102
Loss in iteration 8 : 1.108984160408452
Loss in iteration 9 : 2.002526445224543
Loss in iteration 10 : 1.733316219461027
Loss in iteration 11 : 3.240038518946684
Loss in iteration 12 : 0.91593821876377
Loss in iteration 13 : 0.9901029907364393
Loss in iteration 14 : 1.142822587217551
Loss in iteration 15 : 2.5341922434635915
Loss in iteration 16 : 1.2585822752476654
Loss in iteration 17 : 2.6016681639339816
Loss in iteration 18 : 1.1173292719092764
Loss in iteration 19 : 1.9714070678060294
Loss in iteration 20 : 1.4473552417310105
Loss in iteration 21 : 2.7607688168959625
Loss in iteration 22 : 0.9831609023447154
Loss in iteration 23 : 1.1978515754698198
Loss in iteration 24 : 1.3574141614950859
Loss in iteration 25 : 2.6731649640203066
Loss in iteration 26 : 1.0002443796823408
Loss in iteration 27 : 1.3746129711770037
Loss in iteration 28 : 1.5753348665702787
Loss in iteration 29 : 2.8743039121878184
Loss in iteration 30 : 0.9121033216318837
Loss in iteration 31 : 1.0059660411472473
Loss in iteration 32 : 1.101742247538469
Loss in iteration 33 : 2.0085616838918434
Loss in iteration 34 : 1.3515703101890144
Loss in iteration 35 : 2.5487118564706095
Loss in iteration 36 : 1.0119806601609427
Loss in iteration 37 : 1.4179608973643085
Loss in iteration 38 : 1.51181621229225
Loss in iteration 39 : 2.664794629759309
Loss in iteration 40 : 0.9526465315969306
Loss in iteration 41 : 1.1224777617130215
Loss in iteration 42 : 1.2107308412667765
Loss in iteration 43 : 2.1303949729548677
Loss in iteration 44 : 1.1995351711299165
Loss in iteration 45 : 2.0026781402242086
Loss in iteration 46 : 1.2525770510990053
Loss in iteration 47 : 2.0389176813623955
Loss in iteration 48 : 1.2050385151736458
Loss in iteration 49 : 1.8598838790907284
Loss in iteration 50 : 1.293271050534564
Loss in iteration 51 : 2.0207429093293636
Loss in iteration 52 : 1.186719660245458
Loss in iteration 53 : 1.7129766762099399
Loss in iteration 54 : 1.3164405893636553
Loss in iteration 55 : 2.003164635765985
Loss in iteration 56 : 1.1798847733007756
Loss in iteration 57 : 1.6435600935562533
Loss in iteration 58 : 1.3005856132545333
Loss in iteration 59 : 1.9581225588443039
Loss in iteration 60 : 1.202332812905601
Loss in iteration 61 : 1.6799292008252371
Loss in iteration 62 : 1.2829699017199043
Loss in iteration 63 : 1.8892024419857625
Loss in iteration 64 : 1.221822035976672
Loss in iteration 65 : 1.7143289896860237
Loss in iteration 66 : 1.2678532903156643
Loss in iteration 67 : 1.8305785784248614
Loss in iteration 68 : 1.2299300874892105
Loss in iteration 69 : 1.707184732476503
Loss in iteration 70 : 1.256421314798157
Loss in iteration 71 : 1.7895326677190941
Loss in iteration 72 : 1.240169924433591
Loss in iteration 73 : 1.7368859663354406
Loss in iteration 74 : 1.245913120513855
Loss in iteration 75 : 1.7546546085397443
Loss in iteration 76 : 1.239688220650289
Loss in iteration 77 : 1.7257589277629217
Loss in iteration 78 : 1.2461365975647283
Loss in iteration 79 : 1.7490647958937233
Loss in iteration 80 : 1.2397508342187404
Loss in iteration 81 : 1.7142324470718207
Loss in iteration 82 : 1.2449652389993315
Loss in iteration 83 : 1.7410677660293772
Loss in iteration 84 : 1.2363656172086779
Loss in iteration 85 : 1.7039802817252114
Loss in iteration 86 : 1.245467864279291
Loss in iteration 87 : 1.733380644087803
Loss in iteration 88 : 1.2386111314435955
Loss in iteration 89 : 1.7053087967630327
Loss in iteration 90 : 1.242935250137339
Loss in iteration 91 : 1.7180115975647303
Loss in iteration 92 : 1.239745127498506
Loss in iteration 93 : 1.707376336787422
Loss in iteration 94 : 1.240910024736034
Loss in iteration 95 : 1.7118742038889443
Loss in iteration 96 : 1.2398989071017645
Loss in iteration 97 : 1.7020526836865881
Loss in iteration 98 : 1.2401909025258213
Loss in iteration 99 : 1.7072077102789633
Loss in iteration 100 : 1.2396379449166608
Loss in iteration 101 : 1.7040884830273675
Loss in iteration 102 : 1.2386420420588127
Loss in iteration 103 : 1.6999700703747063
Loss in iteration 104 : 1.239029533078982
Loss in iteration 105 : 1.7012223040133017
Loss in iteration 106 : 1.2383191265869407
Loss in iteration 107 : 1.696112204872953
Loss in iteration 108 : 1.2394511417968166
Loss in iteration 109 : 1.6992672665545807
Loss in iteration 110 : 1.237474409368605
Loss in iteration 111 : 1.6936902067775839
Loss in iteration 112 : 1.2353714122180013
Loss in iteration 113 : 1.6830578135092864
Loss in iteration 114 : 1.235472520208396
Loss in iteration 115 : 1.6907637817765269
Loss in iteration 116 : 1.235721078530508
Loss in iteration 117 : 1.6898598655892878
Loss in iteration 118 : 1.2342415476398885
Loss in iteration 119 : 1.6834402580909067
Loss in iteration 120 : 1.2338434732476469
Loss in iteration 121 : 1.687331533845661
Loss in iteration 122 : 1.2353179482671228
Loss in iteration 123 : 1.692469534603288
Loss in iteration 124 : 1.2329504611407267
Loss in iteration 125 : 1.677362167142572
Loss in iteration 126 : 1.2346976324940089
Loss in iteration 127 : 1.6828844141075407
Loss in iteration 128 : 1.2338429261571147
Loss in iteration 129 : 1.682773269835615
Loss in iteration 130 : 1.2336837794070608
Loss in iteration 131 : 1.681896104489013
Loss in iteration 132 : 1.2336081016939415
Loss in iteration 133 : 1.6814514047775706
Loss in iteration 134 : 1.2333276989296644
Loss in iteration 135 : 1.6776866484252821
Loss in iteration 136 : 1.2325601309093304
Loss in iteration 137 : 1.6746773675059932
Loss in iteration 138 : 1.2340028935429725
Loss in iteration 139 : 1.6778082817433166
Loss in iteration 140 : 1.2337722948825505
Loss in iteration 141 : 1.6750148374726082
Loss in iteration 142 : 1.233303277940105
Loss in iteration 143 : 1.6715839289702914
Loss in iteration 144 : 1.2347496909881746
Loss in iteration 145 : 1.67632411944835
Loss in iteration 146 : 1.232146398408684
Loss in iteration 147 : 1.6669567126876685
Loss in iteration 148 : 1.2348185677999866
Loss in iteration 149 : 1.6743818159934545
Loss in iteration 150 : 1.2331846347548128
Loss in iteration 151 : 1.6665194175636435
Loss in iteration 152 : 1.2339557777288122
Loss in iteration 153 : 1.6686912726608665
Loss in iteration 154 : 1.2352587870285952
Loss in iteration 155 : 1.673296256316067
Loss in iteration 156 : 1.2338810810056222
Loss in iteration 157 : 1.666242410533718
Loss in iteration 158 : 1.232402182098899
Loss in iteration 159 : 1.6621877999565342
Loss in iteration 160 : 1.234637377074417
Loss in iteration 161 : 1.6754529626650296
Loss in iteration 162 : 1.2357421415160994
Loss in iteration 163 : 1.671167819455597
Loss in iteration 164 : 1.230688939791368
Loss in iteration 165 : 1.651972034995684
Loss in iteration 166 : 1.2343887810219187
Loss in iteration 167 : 1.6700991818544018
Loss in iteration 168 : 1.2304981844140337
Loss in iteration 169 : 1.6541279584392292
Loss in iteration 170 : 1.2345832056788737
Loss in iteration 171 : 1.6760981427596893
Loss in iteration 172 : 1.2351005741054883
Loss in iteration 173 : 1.666674281915382
Loss in iteration 174 : 1.228868552632376
Loss in iteration 175 : 1.645314660064353
Loss in iteration 176 : 1.2370368501319058
Loss in iteration 177 : 1.6787762829839008
Loss in iteration 178 : 1.2311334980138733
Loss in iteration 179 : 1.6523532722050844
Loss in iteration 180 : 1.2347401734993848
Loss in iteration 181 : 1.6660016624006224
Loss in iteration 182 : 1.2297185332389573
Loss in iteration 183 : 1.6504234386413457
Loss in iteration 184 : 1.2356591724067163
Loss in iteration 185 : 1.6666530397104682
Loss in iteration 186 : 1.2324564289552011
Loss in iteration 187 : 1.6553579783155985
Loss in iteration 188 : 1.2339510048355247
Loss in iteration 189 : 1.6625506058744688
Loss in iteration 190 : 1.2298761236107696
Loss in iteration 191 : 1.6524314778537756
Loss in iteration 192 : 1.2352696533785306
Loss in iteration 193 : 1.6663489516990755
Loss in iteration 194 : 1.2351730730339445
Loss in iteration 195 : 1.662448073562168
Loss in iteration 196 : 1.2297829107027496
Loss in iteration 197 : 1.6484526298830644
Loss in iteration 198 : 1.2350164825172512
Loss in iteration 199 : 1.6637711365598369
Loss in iteration 200 : 1.2311034740626303
Testing accuracy  of updater 0 on alg 1 with rate 10.0 = 0.8119280142497389, training accuracy 0.807985257985258, time elapsed: 6457 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 5.015365507413249
Loss in iteration 3 : 2.127023616200521
Loss in iteration 4 : 1.1491092036097403
Loss in iteration 5 : 1.9668304017455542
Loss in iteration 6 : 1.1413066144377588
Loss in iteration 7 : 2.1759199780107332
Loss in iteration 8 : 0.8550536733545043
Loss in iteration 9 : 1.5720709961273511
Loss in iteration 10 : 1.2561853338987865
Loss in iteration 11 : 2.3149374713249133
Loss in iteration 12 : 0.7265556047727431
Loss in iteration 13 : 0.8113708928593009
Loss in iteration 14 : 1.0101160048883486
Loss in iteration 15 : 2.0858963257776972
Loss in iteration 16 : 0.7918337654090873
Loss in iteration 17 : 1.2834526044150574
Loss in iteration 18 : 1.2608676940775396
Loss in iteration 19 : 2.313934340269181
Loss in iteration 20 : 0.6829233480884274
Loss in iteration 21 : 0.713268354698792
Loss in iteration 22 : 0.7831764293400497
Loss in iteration 23 : 1.4498221031367555
Loss in iteration 24 : 1.150160284321063
Loss in iteration 25 : 2.182917864920701
Loss in iteration 26 : 0.6985495243330768
Loss in iteration 27 : 0.8001104604087556
Loss in iteration 28 : 0.9823519742799537
Loss in iteration 29 : 1.8837064232956446
Loss in iteration 30 : 0.8116575994271029
Loss in iteration 31 : 1.2959252989378134
Loss in iteration 32 : 1.1287656571651519
Loss in iteration 33 : 1.9841142557078502
Loss in iteration 34 : 0.7374677990057309
Loss in iteration 35 : 0.8832940056987969
Loss in iteration 36 : 0.9844734442254426
Loss in iteration 37 : 1.726755574852547
Loss in iteration 38 : 0.8353011337225105
Loss in iteration 39 : 1.257685039224506
Loss in iteration 40 : 1.0831856581914197
Loss in iteration 41 : 1.8011155364656584
Loss in iteration 42 : 0.781099829307752
Loss in iteration 43 : 0.9824906975743887
Loss in iteration 44 : 1.020112471438101
Loss in iteration 45 : 1.6773347465876642
Loss in iteration 46 : 0.832872019677148
Loss in iteration 47 : 1.165375655942687
Loss in iteration 48 : 1.0541745969640637
Loss in iteration 49 : 1.6725024760562999
Loss in iteration 50 : 0.8199506741287297
Loss in iteration 51 : 1.0730644823542537
Loss in iteration 52 : 1.0125424240411969
Loss in iteration 53 : 1.5656659591591247
Loss in iteration 54 : 0.8679737624812107
Loss in iteration 55 : 1.1983397283186132
Loss in iteration 56 : 1.0084895877352098
Loss in iteration 57 : 1.4952177552384849
Loss in iteration 58 : 0.8924272897814044
Loss in iteration 59 : 1.2232649976380783
Loss in iteration 60 : 0.9840313431940432
Loss in iteration 61 : 1.4108635861822292
Loss in iteration 62 : 0.9228244171410639
Loss in iteration 63 : 1.2707836279573073
Loss in iteration 64 : 0.9601806577839282
Loss in iteration 65 : 1.3362511837907254
Loss in iteration 66 : 0.9472821296838474
Loss in iteration 67 : 1.300058822608346
Loss in iteration 68 : 0.947014217562436
Loss in iteration 69 : 1.2944467905405423
Loss in iteration 70 : 0.9453265102339892
Loss in iteration 71 : 1.2834048934041244
Loss in iteration 72 : 0.9442561143970085
Loss in iteration 73 : 1.280749278217795
Loss in iteration 74 : 0.9445605655633272
Loss in iteration 75 : 1.2795505849341078
Loss in iteration 76 : 0.9449468218567583
Loss in iteration 77 : 1.2732480963135908
Loss in iteration 78 : 0.9409392648386055
Loss in iteration 79 : 1.2659329676378968
Loss in iteration 80 : 0.9415806634057556
Loss in iteration 81 : 1.2642457630667876
Loss in iteration 82 : 0.9416386134990248
Loss in iteration 83 : 1.2642838622629784
Loss in iteration 84 : 0.9426796966024537
Loss in iteration 85 : 1.2627879469541046
Loss in iteration 86 : 0.9402827873244013
Loss in iteration 87 : 1.2553020458544897
Loss in iteration 88 : 0.9423491633665162
Loss in iteration 89 : 1.2641963636212714
Loss in iteration 90 : 0.9418520184244988
Loss in iteration 91 : 1.2563022697843045
Loss in iteration 92 : 0.9411686957437112
Loss in iteration 93 : 1.256188856029918
Loss in iteration 94 : 0.9410732925870349
Loss in iteration 95 : 1.25065681425786
Loss in iteration 96 : 0.9439278497002683
Loss in iteration 97 : 1.2560551018267516
Loss in iteration 98 : 0.941090746661617
Loss in iteration 99 : 1.2465720269213836
Loss in iteration 100 : 0.9414391093894321
Loss in iteration 101 : 1.2459291502665282
Loss in iteration 102 : 0.9409233577096754
Loss in iteration 103 : 1.2480434155956277
Loss in iteration 104 : 0.9433961965511407
Loss in iteration 105 : 1.2519732253892268
Loss in iteration 106 : 0.9407839222542841
Loss in iteration 107 : 1.240411865790011
Loss in iteration 108 : 0.9405089375728177
Loss in iteration 109 : 1.2377161809383093
Loss in iteration 110 : 0.9417963500760643
Loss in iteration 111 : 1.2481864156288278
Loss in iteration 112 : 0.9410010662605862
Loss in iteration 113 : 1.2372412940917223
Loss in iteration 114 : 0.9385514051926063
Loss in iteration 115 : 1.2324906815389758
Loss in iteration 116 : 0.9407287113867294
Loss in iteration 117 : 1.245680741922678
Loss in iteration 118 : 0.9414943702497421
Loss in iteration 119 : 1.2430230628844772
Loss in iteration 120 : 0.9386244729065697
Loss in iteration 121 : 1.2262747001943872
Loss in iteration 122 : 0.9363212321308939
Loss in iteration 123 : 1.2211491542357646
Loss in iteration 124 : 0.9381276099991533
Loss in iteration 125 : 1.2389487344098058
Loss in iteration 126 : 0.9418375233551075
Loss in iteration 127 : 1.2512797919093985
Loss in iteration 128 : 0.9438231195932372
Loss in iteration 129 : 1.2447581237284837
Loss in iteration 130 : 0.9353819937035543
Loss in iteration 131 : 1.2103317509840084
Loss in iteration 132 : 0.936538484612043
Loss in iteration 133 : 1.2223406164917994
Loss in iteration 134 : 0.9389257254797803
Loss in iteration 135 : 1.2353379670643945
Loss in iteration 136 : 0.938493566403965
Loss in iteration 137 : 1.2254441328124508
Loss in iteration 138 : 0.9373753774924702
Loss in iteration 139 : 1.215287713025738
Loss in iteration 140 : 0.9396943112771589
Loss in iteration 141 : 1.232817320999521
Loss in iteration 142 : 0.940122262472156
Loss in iteration 143 : 1.2321421329890314
Loss in iteration 144 : 0.939216365439876
Loss in iteration 145 : 1.2159208901427123
Loss in iteration 146 : 0.9384635462257518
Loss in iteration 147 : 1.2242149448910047
Loss in iteration 148 : 0.9387547238437909
Loss in iteration 149 : 1.2242964028985976
Loss in iteration 150 : 0.941833030609902
Loss in iteration 151 : 1.2356887652883497
Loss in iteration 152 : 0.9391349621413334
Loss in iteration 153 : 1.2131536675440233
Loss in iteration 154 : 0.9371810367856138
Loss in iteration 155 : 1.209848700226079
Loss in iteration 156 : 0.9362613021358411
Loss in iteration 157 : 1.2191186154534617
Loss in iteration 158 : 0.9411698672720633
Loss in iteration 159 : 1.2315795079128162
Loss in iteration 160 : 0.9410649343642291
Loss in iteration 161 : 1.2297923225464684
Loss in iteration 162 : 0.9374887903035957
Loss in iteration 163 : 1.2097300919413931
Loss in iteration 164 : 0.9355482677227153
Loss in iteration 165 : 1.2050545336072043
Loss in iteration 166 : 0.9374338105346253
Loss in iteration 167 : 1.2252706673825402
Loss in iteration 168 : 0.9408547638757265
Loss in iteration 169 : 1.2293128731534746
Loss in iteration 170 : 0.942384013033584
Loss in iteration 171 : 1.2300925479100355
Loss in iteration 172 : 0.9359669655415971
Loss in iteration 173 : 1.2042773896159933
Loss in iteration 174 : 0.9347685325975402
Loss in iteration 175 : 1.205193575723368
Loss in iteration 176 : 0.9383132529097672
Loss in iteration 177 : 1.224621536916918
Loss in iteration 178 : 0.9373997475460175
Loss in iteration 179 : 1.2166979945170218
Loss in iteration 180 : 0.9362427463341166
Loss in iteration 181 : 1.210528402676747
Loss in iteration 182 : 0.939687605645069
Loss in iteration 183 : 1.224154081069006
Loss in iteration 184 : 0.9372310795341344
Loss in iteration 185 : 1.2080483346941417
Loss in iteration 186 : 0.9385369827541963
Loss in iteration 187 : 1.221187922202972
Loss in iteration 188 : 0.9374201634555612
Loss in iteration 189 : 1.2104431810635734
Loss in iteration 190 : 0.9378338091763317
Loss in iteration 191 : 1.2157610585862306
Loss in iteration 192 : 0.9391872498475676
Loss in iteration 193 : 1.2227665377922603
Loss in iteration 194 : 0.9344330161214375
Loss in iteration 195 : 1.1987134854798418
Loss in iteration 196 : 0.9370604599182597
Loss in iteration 197 : 1.2137976591316577
Loss in iteration 198 : 0.9396469502155155
Loss in iteration 199 : 1.2225137829084398
Loss in iteration 200 : 0.9350276771215007
Testing accuracy  of updater 0 on alg 1 with rate 7.0 = 0.8148762361034334, training accuracy 0.8108108108108109, time elapsed: 4842 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.969130412800565
Loss in iteration 3 : 1.3187170010081564
Loss in iteration 4 : 0.7916646493489221
Loss in iteration 5 : 1.262897821598683
Loss in iteration 6 : 0.7635981955822249
Loss in iteration 7 : 1.3034492096239647
Loss in iteration 8 : 0.6715270277816354
Loss in iteration 9 : 1.1890052233940451
Loss in iteration 10 : 0.7302067247613913
Loss in iteration 11 : 1.2546726776195438
Loss in iteration 12 : 0.633552421083134
Loss in iteration 13 : 1.0175483703493495
Loss in iteration 14 : 0.7474944989103472
Loss in iteration 15 : 1.219997743723175
Loss in iteration 16 : 0.6020265252129497
Loss in iteration 17 : 0.850450142319
Loss in iteration 18 : 0.7480371070758054
Loss in iteration 19 : 1.1713771061099094
Loss in iteration 20 : 0.5934622266056552
Loss in iteration 21 : 0.7783603508019957
Loss in iteration 22 : 0.7375232683264028
Loss in iteration 23 : 1.1259114981074445
Loss in iteration 24 : 0.5970567736599671
Loss in iteration 25 : 0.7854440564084297
Loss in iteration 26 : 0.7167762143146049
Loss in iteration 27 : 1.0543011427778015
Loss in iteration 28 : 0.611733462019088
Loss in iteration 29 : 0.8084259186593354
Loss in iteration 30 : 0.6976511780934378
Loss in iteration 31 : 0.9861358693683641
Loss in iteration 32 : 0.6298997428297194
Loss in iteration 33 : 0.8463500760644491
Loss in iteration 34 : 0.6783751863880858
Loss in iteration 35 : 0.9387962687670913
Loss in iteration 36 : 0.6339629238631076
Loss in iteration 37 : 0.8334495680625903
Loss in iteration 38 : 0.6715708289817635
Loss in iteration 39 : 0.9070252574721229
Loss in iteration 40 : 0.6434471193608176
Loss in iteration 41 : 0.8453156561464302
Loss in iteration 42 : 0.6633837625038504
Loss in iteration 43 : 0.8758449341680309
Loss in iteration 44 : 0.6479648609409044
Loss in iteration 45 : 0.8406234190970058
Loss in iteration 46 : 0.6567764595620852
Loss in iteration 47 : 0.854538576296869
Loss in iteration 48 : 0.6541657925794908
Loss in iteration 49 : 0.8430712530712541
Loss in iteration 50 : 0.6526876860107819
Loss in iteration 51 : 0.8380324700118933
Loss in iteration 52 : 0.6497244210650227
Loss in iteration 53 : 0.8311297495004504
Loss in iteration 54 : 0.6515580346093249
Loss in iteration 55 : 0.8320572566088512
Loss in iteration 56 : 0.6497564277478297
Loss in iteration 57 : 0.8270479409776108
Loss in iteration 58 : 0.6504557075503016
Loss in iteration 59 : 0.8260758554232153
Loss in iteration 60 : 0.6499883224468609
Loss in iteration 61 : 0.8231228342760906
Loss in iteration 62 : 0.6505555195020789
Loss in iteration 63 : 0.8221418330324956
Loss in iteration 64 : 0.6492544929338548
Loss in iteration 65 : 0.815605168911373
Loss in iteration 66 : 0.650306117905932
Loss in iteration 67 : 0.8171878678712201
Loss in iteration 68 : 0.6482861766445919
Loss in iteration 69 : 0.8112968950914289
Loss in iteration 70 : 0.6495072449878954
Loss in iteration 71 : 0.8123604617293195
Loss in iteration 72 : 0.6490819518982912
Loss in iteration 73 : 0.8100328027938611
Loss in iteration 74 : 0.6492206601005748
Loss in iteration 75 : 0.8104824659370118
Loss in iteration 76 : 0.6464705114428712
Loss in iteration 77 : 0.8055849907032342
Loss in iteration 78 : 0.6470261479091343
Loss in iteration 79 : 0.8064297588575847
Loss in iteration 80 : 0.6470113991029225
Loss in iteration 81 : 0.8065189119765278
Loss in iteration 82 : 0.6467163890213651
Loss in iteration 83 : 0.8041611102391187
Loss in iteration 84 : 0.647001925004073
Loss in iteration 85 : 0.8043754942680006
Loss in iteration 86 : 0.6461708907086667
Loss in iteration 87 : 0.8011043116770993
Loss in iteration 88 : 0.645951590411051
Loss in iteration 89 : 0.8014747561108132
Loss in iteration 90 : 0.6457272229231682
Loss in iteration 91 : 0.8023706338402289
Loss in iteration 92 : 0.6461104541832425
Loss in iteration 93 : 0.8030450455481172
Loss in iteration 94 : 0.6451093992417716
Loss in iteration 95 : 0.8004217766180302
Loss in iteration 96 : 0.6455369863385844
Loss in iteration 97 : 0.8011513614630934
Loss in iteration 98 : 0.6454280436344326
Loss in iteration 99 : 0.8009032766572701
Loss in iteration 100 : 0.644754103556314
Loss in iteration 101 : 0.7984984590912101
Loss in iteration 102 : 0.6451688283660004
Loss in iteration 103 : 0.7969955334774139
Loss in iteration 104 : 0.644808552270162
Loss in iteration 105 : 0.7962080702268036
Loss in iteration 106 : 0.6442744485327422
Loss in iteration 107 : 0.7952019631872204
Loss in iteration 108 : 0.6441823524440243
Loss in iteration 109 : 0.794370491219386
Loss in iteration 110 : 0.6438588936546537
Loss in iteration 111 : 0.7926531513924011
Loss in iteration 112 : 0.643752267595943
Loss in iteration 113 : 0.7919928998967684
Loss in iteration 114 : 0.6440846194362793
Loss in iteration 115 : 0.7935671510241519
Loss in iteration 116 : 0.6435720220164316
Loss in iteration 117 : 0.7926278569445036
Loss in iteration 118 : 0.643315047932676
Loss in iteration 119 : 0.7924387975478263
Loss in iteration 120 : 0.6436920046302737
Loss in iteration 121 : 0.7910791794698425
Loss in iteration 122 : 0.6435309109925201
Loss in iteration 123 : 0.7904083332830252
Loss in iteration 124 : 0.6435388004153345
Loss in iteration 125 : 0.7908195001177175
Loss in iteration 126 : 0.6421797332009239
Loss in iteration 127 : 0.7883576870370462
Loss in iteration 128 : 0.6416430864357776
Loss in iteration 129 : 0.7872436258293128
Loss in iteration 130 : 0.6409623813002207
Loss in iteration 131 : 0.786794815392789
Loss in iteration 132 : 0.6410328616532546
Loss in iteration 133 : 0.7867813456465175
Loss in iteration 134 : 0.640899590398977
Loss in iteration 135 : 0.7835897666149504
Loss in iteration 136 : 0.6412490341022287
Loss in iteration 137 : 0.7863309128035793
Loss in iteration 138 : 0.6408665499037142
Loss in iteration 139 : 0.7836426872483389
Loss in iteration 140 : 0.6409987043386919
Loss in iteration 141 : 0.784924184570991
Loss in iteration 142 : 0.6406244755476944
Loss in iteration 143 : 0.7827296195570148
Loss in iteration 144 : 0.641108763862141
Loss in iteration 145 : 0.7851045508877185
Loss in iteration 146 : 0.6402630675404013
Loss in iteration 147 : 0.7818814028457768
Loss in iteration 148 : 0.6415238508834944
Loss in iteration 149 : 0.7851620142590654
Loss in iteration 150 : 0.6416247796545708
Loss in iteration 151 : 0.7841697919999527
Loss in iteration 152 : 0.6401611389142092
Loss in iteration 153 : 0.7811407064032987
Loss in iteration 154 : 0.6409637999625717
Loss in iteration 155 : 0.7821740736436676
Loss in iteration 156 : 0.6403746362791206
Loss in iteration 157 : 0.7807849730454163
Loss in iteration 158 : 0.6409538089574947
Loss in iteration 159 : 0.7823339334979381
Loss in iteration 160 : 0.6416013302223362
Loss in iteration 161 : 0.7819832560112046
Loss in iteration 162 : 0.6406073685926255
Loss in iteration 163 : 0.7792168342398696
Loss in iteration 164 : 0.639640037066326
Loss in iteration 165 : 0.7758154931511816
Loss in iteration 166 : 0.6393101037434579
Loss in iteration 167 : 0.7764976056299782
Loss in iteration 168 : 0.6401331203327515
Loss in iteration 169 : 0.7800920168549156
Loss in iteration 170 : 0.6420957340521238
Loss in iteration 171 : 0.7834393355830686
Loss in iteration 172 : 0.6413597425882434
Loss in iteration 173 : 0.7792296889809172
Loss in iteration 174 : 0.6389397460896232
Loss in iteration 175 : 0.7709624492149066
Loss in iteration 176 : 0.6412925032749962
Loss in iteration 177 : 0.7799841192823386
Loss in iteration 178 : 0.6403996326570034
Loss in iteration 179 : 0.7746471077398601
Loss in iteration 180 : 0.6388198502556643
Loss in iteration 181 : 0.7707140625660281
Loss in iteration 182 : 0.6408876374140491
Loss in iteration 183 : 0.7793464796044661
Loss in iteration 184 : 0.6415396825516598
Loss in iteration 185 : 0.7795766575711284
Loss in iteration 186 : 0.6411992828209033
Loss in iteration 187 : 0.7765146824007397
Loss in iteration 188 : 0.6403463762232193
Loss in iteration 189 : 0.7726598145778131
Loss in iteration 190 : 0.6390825555843989
Loss in iteration 191 : 0.771876331882475
Loss in iteration 192 : 0.6394603800807724
Loss in iteration 193 : 0.7735442939589131
Loss in iteration 194 : 0.6414129952489916
Loss in iteration 195 : 0.7796157726578491
Loss in iteration 196 : 0.6419182767478216
Loss in iteration 197 : 0.7792442151778787
Loss in iteration 198 : 0.6415851853920022
Loss in iteration 199 : 0.7757273436603898
Loss in iteration 200 : 0.6397973161624887
Testing accuracy  of updater 0 on alg 1 with rate 4.0 = 0.8197285179043057, training accuracy 0.8153562653562654, time elapsed: 3588 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9228953181878563
Loss in iteration 3 : 0.5186005339226321
Loss in iteration 4 : 0.5205571937877698
Loss in iteration 5 : 0.533586530291461
Loss in iteration 6 : 0.45756471892374945
Loss in iteration 7 : 0.42655479639931426
Loss in iteration 8 : 0.4217564558569615
Loss in iteration 9 : 0.41063035299790585
Loss in iteration 10 : 0.4161213658096335
Loss in iteration 11 : 0.4212176801550269
Loss in iteration 12 : 0.42899869415148834
Loss in iteration 13 : 0.43661601960319857
Loss in iteration 14 : 0.42269560278812474
Loss in iteration 15 : 0.4325841661057419
Loss in iteration 16 : 0.4126988692959211
Loss in iteration 17 : 0.4247361203131318
Loss in iteration 18 : 0.4054098736258599
Loss in iteration 19 : 0.4163248882048791
Loss in iteration 20 : 0.4000434040878613
Loss in iteration 21 : 0.41039301286153224
Loss in iteration 22 : 0.3951581714196891
Loss in iteration 23 : 0.40435457277134257
Loss in iteration 24 : 0.39058158742280413
Loss in iteration 25 : 0.4001525750608217
Loss in iteration 26 : 0.38681662355341695
Loss in iteration 27 : 0.3962961263349004
Loss in iteration 28 : 0.3839974745168991
Loss in iteration 29 : 0.39257815565593523
Loss in iteration 30 : 0.38156850196047104
Loss in iteration 31 : 0.3897928526583316
Loss in iteration 32 : 0.3792036182304132
Loss in iteration 33 : 0.38711858545780586
Loss in iteration 34 : 0.37716810941358025
Loss in iteration 35 : 0.3839519971445631
Loss in iteration 36 : 0.3760939075243446
Loss in iteration 37 : 0.38193547897210384
Loss in iteration 38 : 0.3750817211770064
Loss in iteration 39 : 0.38055177381843497
Loss in iteration 40 : 0.3743071183270045
Loss in iteration 41 : 0.3789357617613139
Loss in iteration 42 : 0.3732454429244972
Loss in iteration 43 : 0.3773737682916897
Loss in iteration 44 : 0.37201098406872346
Loss in iteration 45 : 0.3756882568714571
Loss in iteration 46 : 0.3711043522372605
Loss in iteration 47 : 0.3745985713768276
Loss in iteration 48 : 0.37064465941539043
Loss in iteration 49 : 0.37400746646523625
Loss in iteration 50 : 0.36988531850478895
Loss in iteration 51 : 0.3730754232216924
Loss in iteration 52 : 0.3696902401689714
Loss in iteration 53 : 0.37273308603583477
Loss in iteration 54 : 0.3692768755395446
Loss in iteration 55 : 0.3722569465405767
Loss in iteration 56 : 0.3687691104383355
Loss in iteration 57 : 0.3711286892767238
Loss in iteration 58 : 0.3680344668923448
Loss in iteration 59 : 0.3695716026432402
Loss in iteration 60 : 0.36711858923084406
Loss in iteration 61 : 0.36860157339464783
Loss in iteration 62 : 0.36680602131615625
Loss in iteration 63 : 0.3682510430564029
Loss in iteration 64 : 0.3666386295344373
Loss in iteration 65 : 0.3678981809805676
Loss in iteration 66 : 0.3664195452432555
Loss in iteration 67 : 0.36742292626276063
Loss in iteration 68 : 0.36592957284680255
Loss in iteration 69 : 0.3670402769938245
Loss in iteration 70 : 0.365845028493984
Loss in iteration 71 : 0.36704812208344106
Loss in iteration 72 : 0.36574329323600013
Loss in iteration 73 : 0.36676065430518767
Loss in iteration 74 : 0.36548219522152253
Loss in iteration 75 : 0.3662473786817295
Loss in iteration 76 : 0.36484813144057443
Loss in iteration 77 : 0.3655370759482393
Loss in iteration 78 : 0.3645034653469084
Loss in iteration 79 : 0.3650633181833274
Loss in iteration 80 : 0.36420013665944195
Loss in iteration 81 : 0.3644443673067756
Loss in iteration 82 : 0.36374283405725255
Loss in iteration 83 : 0.363999608169986
Loss in iteration 84 : 0.3634198808549998
Loss in iteration 85 : 0.36357387174839606
Loss in iteration 86 : 0.3630487591609371
Loss in iteration 87 : 0.3631650838142704
Loss in iteration 88 : 0.36247220308453426
Loss in iteration 89 : 0.3623674088358514
Loss in iteration 90 : 0.3619420714130488
Loss in iteration 91 : 0.36177754336730156
Loss in iteration 92 : 0.361091004171472
Loss in iteration 93 : 0.3611571898638079
Loss in iteration 94 : 0.3607038272567291
Loss in iteration 95 : 0.3607522683505482
Loss in iteration 96 : 0.36010561488448356
Loss in iteration 97 : 0.3605989745259557
Loss in iteration 98 : 0.3599817262328767
Loss in iteration 99 : 0.36053378397243524
Loss in iteration 100 : 0.3599853115624
Loss in iteration 101 : 0.36054332881574835
Loss in iteration 102 : 0.36008472168561223
Loss in iteration 103 : 0.36058745449716045
Loss in iteration 104 : 0.36027039572378894
Loss in iteration 105 : 0.3606242840660067
Loss in iteration 106 : 0.36021091094724295
Loss in iteration 107 : 0.3605187116282022
Loss in iteration 108 : 0.3600411506408128
Loss in iteration 109 : 0.3603445038530267
Loss in iteration 110 : 0.35985454937850564
Loss in iteration 111 : 0.36002530954005063
Loss in iteration 112 : 0.35956123244782673
Loss in iteration 113 : 0.3597899728869472
Loss in iteration 114 : 0.35950799582249227
Loss in iteration 115 : 0.359803902943874
Loss in iteration 116 : 0.35944530679327996
Loss in iteration 117 : 0.35971919635041655
Loss in iteration 118 : 0.3593927455417784
Loss in iteration 119 : 0.35966522586915683
Loss in iteration 120 : 0.3593093104621814
Loss in iteration 121 : 0.3596550198009049
Loss in iteration 122 : 0.3593083096638066
Loss in iteration 123 : 0.3595644734876159
Loss in iteration 124 : 0.35913314730997464
Loss in iteration 125 : 0.35930250201480146
Loss in iteration 126 : 0.358818307083048
Loss in iteration 127 : 0.35888844597613084
Loss in iteration 128 : 0.35827321814499263
Loss in iteration 129 : 0.3580703098117098
Loss in iteration 130 : 0.35757948093559244
Loss in iteration 131 : 0.35752479640686086
Loss in iteration 132 : 0.3568582288453294
Loss in iteration 133 : 0.35682166338915494
Loss in iteration 134 : 0.3560823441056092
Loss in iteration 135 : 0.35618145238576826
Loss in iteration 136 : 0.35560943527428446
Loss in iteration 137 : 0.35589705774710434
Loss in iteration 138 : 0.35549133351997175
Loss in iteration 139 : 0.3558275329159852
Loss in iteration 140 : 0.3553872712406956
Loss in iteration 141 : 0.35575280317870944
Loss in iteration 142 : 0.3553610797529714
Loss in iteration 143 : 0.35570283117465235
Loss in iteration 144 : 0.35528018675784334
Loss in iteration 145 : 0.3555975652961992
Loss in iteration 146 : 0.35516433562080074
Loss in iteration 147 : 0.3555153196668859
Loss in iteration 148 : 0.35509493812972015
Loss in iteration 149 : 0.3554818556480872
Loss in iteration 150 : 0.3551982684395927
Loss in iteration 151 : 0.35558533499296735
Loss in iteration 152 : 0.355258096562611
Loss in iteration 153 : 0.3556399761317609
Loss in iteration 154 : 0.3554278427201492
Loss in iteration 155 : 0.3558702154102951
Loss in iteration 156 : 0.3556570019665069
Loss in iteration 157 : 0.35619470989864127
Loss in iteration 158 : 0.3563137313310075
Loss in iteration 159 : 0.3566953930072036
Loss in iteration 160 : 0.3565787966951207
Loss in iteration 161 : 0.35696963722992575
Loss in iteration 162 : 0.3569244220837428
Loss in iteration 163 : 0.35719958504126226
Loss in iteration 164 : 0.35703260357744465
Loss in iteration 165 : 0.3572112135449066
Loss in iteration 166 : 0.3569804375516899
Loss in iteration 167 : 0.35716190182252877
Loss in iteration 168 : 0.35696590663843375
Loss in iteration 169 : 0.35713003757191397
Loss in iteration 170 : 0.35693386316850656
Loss in iteration 171 : 0.3571097018394317
Loss in iteration 172 : 0.35683231467591103
Loss in iteration 173 : 0.3570047095062449
Loss in iteration 174 : 0.356536309455536
Loss in iteration 175 : 0.35667444226949707
Loss in iteration 176 : 0.3562888613499031
Loss in iteration 177 : 0.3564999826440239
Loss in iteration 178 : 0.3561904133514246
Loss in iteration 179 : 0.35632857351991243
Loss in iteration 180 : 0.3556127904484781
Loss in iteration 181 : 0.35567633501409635
Loss in iteration 182 : 0.35506018373186665
Loss in iteration 183 : 0.35514880768220675
Loss in iteration 184 : 0.35461280987962507
Loss in iteration 185 : 0.3547596027594484
Loss in iteration 186 : 0.354360525682315
Loss in iteration 187 : 0.35454347087063665
Loss in iteration 188 : 0.3542268827837776
Loss in iteration 189 : 0.35443896525937535
Loss in iteration 190 : 0.35420640650562446
Loss in iteration 191 : 0.35439526876105476
Loss in iteration 192 : 0.35417471392824573
Loss in iteration 193 : 0.3543783174438124
Loss in iteration 194 : 0.35413301431037936
Loss in iteration 195 : 0.35435619234797683
Loss in iteration 196 : 0.3541507560791194
Loss in iteration 197 : 0.35433521991530326
Loss in iteration 198 : 0.354095637650998
Loss in iteration 199 : 0.35426913892477524
Loss in iteration 200 : 0.35399883186738235
Testing accuracy  of updater 0 on alg 1 with rate 1.0 = 0.8493335790184878, training accuracy 0.8461302211302212, time elapsed: 3305 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.718290390656598
Loss in iteration 3 : 0.4638791649625111
Loss in iteration 4 : 0.46785883149385116
Loss in iteration 5 : 0.47434590534805526
Loss in iteration 6 : 0.47737514271516807
Loss in iteration 7 : 0.4627539276384106
Loss in iteration 8 : 0.4475408426665714
Loss in iteration 9 : 0.42397339668515965
Loss in iteration 10 : 0.4191960040507349
Loss in iteration 11 : 0.4120180550252342
Loss in iteration 12 : 0.4094870023551301
Loss in iteration 13 : 0.4047023076656062
Loss in iteration 14 : 0.40246979541455
Loss in iteration 15 : 0.3992531107756462
Loss in iteration 16 : 0.3964143343945627
Loss in iteration 17 : 0.39372674195512797
Loss in iteration 18 : 0.3907842488107369
Loss in iteration 19 : 0.38944036675440147
Loss in iteration 20 : 0.38612795551512535
Loss in iteration 21 : 0.3856822439691747
Loss in iteration 22 : 0.38229543973627816
Loss in iteration 23 : 0.3823341026243754
Loss in iteration 24 : 0.378916191389624
Loss in iteration 25 : 0.3787963901645957
Loss in iteration 26 : 0.3762100475817839
Loss in iteration 27 : 0.3762496986285765
Loss in iteration 28 : 0.3741744888665181
Loss in iteration 29 : 0.3740032470766493
Loss in iteration 30 : 0.37233944543507613
Loss in iteration 31 : 0.37220117301870187
Loss in iteration 32 : 0.37077620393874916
Loss in iteration 33 : 0.37065341993839374
Loss in iteration 34 : 0.3695123979204524
Loss in iteration 35 : 0.3692287104434685
Loss in iteration 36 : 0.36835026226388273
Loss in iteration 37 : 0.3682019896928154
Loss in iteration 38 : 0.367280208641465
Loss in iteration 39 : 0.3668703210553643
Loss in iteration 40 : 0.3660702586870426
Loss in iteration 41 : 0.3656071614528299
Loss in iteration 42 : 0.36517709811106597
Loss in iteration 43 : 0.3648038090895504
Loss in iteration 44 : 0.3644580037270079
Loss in iteration 45 : 0.3640751810869678
Loss in iteration 46 : 0.36376700291353997
Loss in iteration 47 : 0.36348070043812486
Loss in iteration 48 : 0.36321078973839244
Loss in iteration 49 : 0.36296076785477094
Loss in iteration 50 : 0.3627282740916034
Loss in iteration 51 : 0.36251099321834157
Loss in iteration 52 : 0.36230545441717066
Loss in iteration 53 : 0.3621049986530254
Loss in iteration 54 : 0.36191815695386004
Loss in iteration 55 : 0.36171449608811373
Loss in iteration 56 : 0.36155038609499623
Loss in iteration 57 : 0.3613544366589899
Loss in iteration 58 : 0.361191076557208
Loss in iteration 59 : 0.3610135111553648
Loss in iteration 60 : 0.36085919559203455
Loss in iteration 61 : 0.36069932234347934
Loss in iteration 62 : 0.3605595525969066
Loss in iteration 63 : 0.36041522700106915
Loss in iteration 64 : 0.3602842759766128
Loss in iteration 65 : 0.36015842874089155
Loss in iteration 66 : 0.36003442755842135
Loss in iteration 67 : 0.3599125105079113
Loss in iteration 68 : 0.3597952963608301
Loss in iteration 69 : 0.35968361386501063
Loss in iteration 70 : 0.35957362140731286
Loss in iteration 71 : 0.3594536353977383
Loss in iteration 72 : 0.35935953252434294
Loss in iteration 73 : 0.35924512419709775
Loss in iteration 74 : 0.35914994449860804
Loss in iteration 75 : 0.359049022141698
Loss in iteration 76 : 0.3589556757624561
Loss in iteration 77 : 0.35885884734061263
Loss in iteration 78 : 0.3587693725701636
Loss in iteration 79 : 0.3586799290216061
Loss in iteration 80 : 0.35859633839851124
Loss in iteration 81 : 0.3585186464489672
Loss in iteration 82 : 0.3584323919665982
Loss in iteration 83 : 0.3583605274745087
Loss in iteration 84 : 0.35827369015206895
Loss in iteration 85 : 0.3582020310075816
Loss in iteration 86 : 0.35812084164030555
Loss in iteration 87 : 0.35804846542991514
Loss in iteration 88 : 0.3579767880805198
Loss in iteration 89 : 0.35790772827635614
Loss in iteration 90 : 0.35784017740448226
Loss in iteration 91 : 0.3577727494393274
Loss in iteration 92 : 0.3577066276056603
Loss in iteration 93 : 0.35764169116626116
Loss in iteration 94 : 0.35757867416570543
Loss in iteration 95 : 0.35751444752911243
Loss in iteration 96 : 0.35745409665165456
Loss in iteration 97 : 0.35739221127957166
Loss in iteration 98 : 0.35733114418514417
Loss in iteration 99 : 0.3572734510357743
Loss in iteration 100 : 0.3572157691111931
Loss in iteration 101 : 0.3571601656590135
Loss in iteration 102 : 0.3571082350708425
Loss in iteration 103 : 0.35705342942758467
Loss in iteration 104 : 0.357007409115056
Loss in iteration 105 : 0.35695304331372385
Loss in iteration 106 : 0.3569110070465258
Loss in iteration 107 : 0.3568539619164617
Loss in iteration 108 : 0.3568129492745207
Loss in iteration 109 : 0.35675677543329637
Loss in iteration 110 : 0.35671397305069735
Loss in iteration 111 : 0.35666193040329214
Loss in iteration 112 : 0.35661557259249915
Loss in iteration 113 : 0.3565677205642648
Loss in iteration 114 : 0.35652441476404917
Loss in iteration 115 : 0.3564820041171391
Loss in iteration 116 : 0.35644322086912644
Loss in iteration 117 : 0.35639913791737277
Loss in iteration 118 : 0.35636221072116325
Loss in iteration 119 : 0.35631934561180695
Loss in iteration 120 : 0.3562792316132297
Loss in iteration 121 : 0.3562390396070901
Loss in iteration 122 : 0.3561977413461591
Loss in iteration 123 : 0.3561573864390968
Loss in iteration 124 : 0.35611796271785484
Loss in iteration 125 : 0.3560775827200891
Loss in iteration 126 : 0.3560396599134309
Loss in iteration 127 : 0.3560027713908629
Loss in iteration 128 : 0.3559667429323447
Loss in iteration 129 : 0.3559308880335825
Loss in iteration 130 : 0.3558956867193579
Loss in iteration 131 : 0.35586087129261257
Loss in iteration 132 : 0.35582653457008523
Loss in iteration 133 : 0.35579285341293904
Loss in iteration 134 : 0.3557594800413827
Loss in iteration 135 : 0.3557263952129201
Loss in iteration 136 : 0.3556938326672667
Loss in iteration 137 : 0.3556619026714618
Loss in iteration 138 : 0.3556304985428526
Loss in iteration 139 : 0.3555999655144314
Loss in iteration 140 : 0.3555739993714115
Loss in iteration 141 : 0.35555233449190826
Loss in iteration 142 : 0.35553535355631455
Loss in iteration 143 : 0.35551132647193706
Loss in iteration 144 : 0.3554723091069067
Loss in iteration 145 : 0.3554532756385486
Loss in iteration 146 : 0.35541800556070374
Loss in iteration 147 : 0.3553969431410685
Loss in iteration 148 : 0.35536321699044326
Loss in iteration 149 : 0.35534616389549756
Loss in iteration 150 : 0.35533837370358407
Loss in iteration 151 : 0.35529889782008944
Loss in iteration 152 : 0.3552812774488518
Loss in iteration 153 : 0.3552368189854753
Loss in iteration 154 : 0.3552071740359131
Loss in iteration 155 : 0.3551816271717618
Loss in iteration 156 : 0.3551528871854039
Loss in iteration 157 : 0.35513189749108054
Loss in iteration 158 : 0.35510134880454997
Loss in iteration 159 : 0.3550845827434819
Loss in iteration 160 : 0.35507667370086704
Loss in iteration 161 : 0.3550420030644624
Loss in iteration 162 : 0.3550338350027771
Loss in iteration 163 : 0.35499438439863795
Loss in iteration 164 : 0.3549613960656263
Loss in iteration 165 : 0.35494008160704255
Loss in iteration 166 : 0.35491330529840076
Loss in iteration 167 : 0.35489621041705605
Loss in iteration 168 : 0.3548692851677345
Loss in iteration 169 : 0.3548517356352894
Loss in iteration 170 : 0.354847982122592
Loss in iteration 171 : 0.3548231057273212
Loss in iteration 172 : 0.35480195596185427
Loss in iteration 173 : 0.3547786968793958
Loss in iteration 174 : 0.35475866119777455
Loss in iteration 175 : 0.35473673711054726
Loss in iteration 176 : 0.35472247049861394
Loss in iteration 177 : 0.3546943152388488
Loss in iteration 178 : 0.3546591739159325
Loss in iteration 179 : 0.3546478464064067
Loss in iteration 180 : 0.35461000273922605
Loss in iteration 181 : 0.35460216264888533
Loss in iteration 182 : 0.35457220217900537
Loss in iteration 183 : 0.3545719010905589
Loss in iteration 184 : 0.3545583414516538
Loss in iteration 185 : 0.35454985975617087
Loss in iteration 186 : 0.3545689213336636
Loss in iteration 187 : 0.35453999741169545
Loss in iteration 188 : 0.35453641500627875
Loss in iteration 189 : 0.35449996094905606
Loss in iteration 190 : 0.35449864010271687
Loss in iteration 191 : 0.3544397453350154
Loss in iteration 192 : 0.35441089989224095
Loss in iteration 193 : 0.35439763615385594
Loss in iteration 194 : 0.35437013230913134
Loss in iteration 195 : 0.35436433164024495
Loss in iteration 196 : 0.35436819126964236
Loss in iteration 197 : 0.3543434625888173
Loss in iteration 198 : 0.3543725137564976
Loss in iteration 199 : 0.3543183724509335
Loss in iteration 200 : 0.354350018525618
Testing accuracy  of updater 0 on alg 1 with rate 0.7 = 0.8487193661323015, training accuracy 0.8454545454545455, time elapsed: 3082 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5170319327312575
Loss in iteration 3 : 0.5239613252268333
Loss in iteration 4 : 0.5678150007696989
Loss in iteration 5 : 0.4581526665569966
Loss in iteration 6 : 0.45304268589004454
Loss in iteration 7 : 0.4486657748462112
Loss in iteration 8 : 0.4447371837439403
Loss in iteration 9 : 0.44113408094826995
Loss in iteration 10 : 0.4377603181274863
Loss in iteration 11 : 0.43454919324897795
Loss in iteration 12 : 0.43146221604114776
Loss in iteration 13 : 0.4284819848746448
Loss in iteration 14 : 0.4255802725189997
Loss in iteration 15 : 0.4227346320382253
Loss in iteration 16 : 0.4199301720052653
Loss in iteration 17 : 0.4171539044908215
Loss in iteration 18 : 0.41440967950304525
Loss in iteration 19 : 0.41168593418915966
Loss in iteration 20 : 0.4089854790852949
Loss in iteration 21 : 0.4063180267463127
Loss in iteration 22 : 0.40370363162469985
Loss in iteration 23 : 0.4012344241438232
Loss in iteration 24 : 0.39899905938158314
Loss in iteration 25 : 0.39698282701676463
Loss in iteration 26 : 0.39510248967696815
Loss in iteration 27 : 0.39334831669675097
Loss in iteration 28 : 0.3917082022831407
Loss in iteration 29 : 0.3901925045578288
Loss in iteration 30 : 0.3887848206146744
Loss in iteration 31 : 0.387485075747515
Loss in iteration 32 : 0.3862933484355475
Loss in iteration 33 : 0.38518963553960406
Loss in iteration 34 : 0.38417265837704945
Loss in iteration 35 : 0.38321216321861407
Loss in iteration 36 : 0.3822993090812498
Loss in iteration 37 : 0.38142497539979087
Loss in iteration 38 : 0.38059096606378684
Loss in iteration 39 : 0.37980021046006934
Loss in iteration 40 : 0.37906300860554637
Loss in iteration 41 : 0.3783537660957814
Loss in iteration 42 : 0.37767188565883314
Loss in iteration 43 : 0.37701385006851934
Loss in iteration 44 : 0.3763886949966495
Loss in iteration 45 : 0.3757962449969509
Loss in iteration 46 : 0.3752269018376206
Loss in iteration 47 : 0.37467326545889107
Loss in iteration 48 : 0.3741417832283937
Loss in iteration 49 : 0.37362513620667814
Loss in iteration 50 : 0.3731264726167973
Loss in iteration 51 : 0.3726428094947744
Loss in iteration 52 : 0.37217275685334544
Loss in iteration 53 : 0.3717166037072371
Loss in iteration 54 : 0.3712852918671404
Loss in iteration 55 : 0.3708707504271086
Loss in iteration 56 : 0.3704697353289172
Loss in iteration 57 : 0.3700770378933768
Loss in iteration 58 : 0.3696984553936335
Loss in iteration 59 : 0.3693429223840773
Loss in iteration 60 : 0.3690013926283888
Loss in iteration 61 : 0.3686757873576065
Loss in iteration 62 : 0.3683642985318351
Loss in iteration 63 : 0.36806233323171317
Loss in iteration 64 : 0.3677766610423244
Loss in iteration 65 : 0.36750011734148724
Loss in iteration 66 : 0.3672344822486097
Loss in iteration 67 : 0.3669770663420846
Loss in iteration 68 : 0.3667259646149381
Loss in iteration 69 : 0.3664848584205153
Loss in iteration 70 : 0.3662535806132245
Loss in iteration 71 : 0.3660308857131648
Loss in iteration 72 : 0.3658145140477758
Loss in iteration 73 : 0.36560425319500933
Loss in iteration 74 : 0.36539923950642644
Loss in iteration 75 : 0.36519725292636757
Loss in iteration 76 : 0.3650007791323836
Loss in iteration 77 : 0.36481247132490935
Loss in iteration 78 : 0.3646308029327069
Loss in iteration 79 : 0.36445451640215193
Loss in iteration 80 : 0.36428099098092975
Loss in iteration 81 : 0.3641109148712042
Loss in iteration 82 : 0.36394599688196044
Loss in iteration 83 : 0.36378580915067277
Loss in iteration 84 : 0.36362995286720706
Loss in iteration 85 : 0.36347715576308964
Loss in iteration 86 : 0.36332858974397686
Loss in iteration 87 : 0.36318380808214956
Loss in iteration 88 : 0.36304402984624173
Loss in iteration 89 : 0.36290837478644683
Loss in iteration 90 : 0.3627750276941003
Loss in iteration 91 : 0.36264470280532946
Loss in iteration 92 : 0.3625183324378652
Loss in iteration 93 : 0.362396571213831
Loss in iteration 94 : 0.3622765154030504
Loss in iteration 95 : 0.362158661537347
Loss in iteration 96 : 0.3620450976160424
Loss in iteration 97 : 0.3619320955001239
Loss in iteration 98 : 0.3618229675397987
Loss in iteration 99 : 0.36171468172159166
Loss in iteration 100 : 0.3616096305290103
Loss in iteration 101 : 0.3615071774505135
Loss in iteration 102 : 0.3614082691413773
Loss in iteration 103 : 0.3613108232618366
Loss in iteration 104 : 0.3612156619569086
Loss in iteration 105 : 0.3611226327958512
Loss in iteration 106 : 0.3610342188150847
Loss in iteration 107 : 0.3609467462073423
Loss in iteration 108 : 0.3608610439091087
Loss in iteration 109 : 0.3607774213095152
Loss in iteration 110 : 0.3606942669439589
Loss in iteration 111 : 0.3606144366250321
Loss in iteration 112 : 0.36053647735573363
Loss in iteration 113 : 0.3604581717215313
Loss in iteration 114 : 0.3603825328707073
Loss in iteration 115 : 0.3603092125518406
Loss in iteration 116 : 0.3602366072237083
Loss in iteration 117 : 0.36016585030697423
Loss in iteration 118 : 0.360096618829574
Loss in iteration 119 : 0.36002824571835684
Loss in iteration 120 : 0.3599610977126333
Loss in iteration 121 : 0.359894589991489
Loss in iteration 122 : 0.35982892101672814
Loss in iteration 123 : 0.35976423416380476
Loss in iteration 124 : 0.3597016522888758
Loss in iteration 125 : 0.35963941564694035
Loss in iteration 126 : 0.3595775381982376
Loss in iteration 127 : 0.3595165964630026
Loss in iteration 128 : 0.3594570190583706
Loss in iteration 129 : 0.3593978359362266
Loss in iteration 130 : 0.3593406502604902
Loss in iteration 131 : 0.3592816278697735
Loss in iteration 132 : 0.35922501494123177
Loss in iteration 133 : 0.3591700327197871
Loss in iteration 134 : 0.3591161846736165
Loss in iteration 135 : 0.3590605282102514
Loss in iteration 136 : 0.359007716994971
Loss in iteration 137 : 0.3589536169098514
Loss in iteration 138 : 0.35890145616635155
Loss in iteration 139 : 0.35885014171531254
Loss in iteration 140 : 0.35879947018997993
Loss in iteration 141 : 0.3587490401390895
Loss in iteration 142 : 0.3586991734027969
Loss in iteration 143 : 0.35864979112460693
Loss in iteration 144 : 0.35860114307964425
Loss in iteration 145 : 0.35855289309020927
Loss in iteration 146 : 0.3585056655941173
Loss in iteration 147 : 0.3584602525520819
Loss in iteration 148 : 0.35841619931300545
Loss in iteration 149 : 0.3583714733412216
Loss in iteration 150 : 0.35832730087413794
Loss in iteration 151 : 0.3582845810267487
Loss in iteration 152 : 0.3582412763584445
Loss in iteration 153 : 0.35819883496127464
Loss in iteration 154 : 0.35815721872754963
Loss in iteration 155 : 0.3581165061364691
Loss in iteration 156 : 0.3580754543492574
Loss in iteration 157 : 0.3580355876733329
Loss in iteration 158 : 0.35799540783524125
Loss in iteration 159 : 0.357956258036571
Loss in iteration 160 : 0.35791754704224016
Loss in iteration 161 : 0.3578794895381189
Loss in iteration 162 : 0.35784125356929325
Loss in iteration 163 : 0.35780390275522267
Loss in iteration 164 : 0.3577669454690337
Loss in iteration 165 : 0.35773034737607845
Loss in iteration 166 : 0.35769429297792327
Loss in iteration 167 : 0.357658661160042
Loss in iteration 168 : 0.3576233013782154
Loss in iteration 169 : 0.35758837511847374
Loss in iteration 170 : 0.35755414800270546
Loss in iteration 171 : 0.357519457935152
Loss in iteration 172 : 0.3574853232588174
Loss in iteration 173 : 0.357451489293627
Loss in iteration 174 : 0.3574180269726955
Loss in iteration 175 : 0.35738493780523956
Loss in iteration 176 : 0.35735242553531954
Loss in iteration 177 : 0.3573196144407746
Loss in iteration 178 : 0.3572872968596244
Loss in iteration 179 : 0.35725540751830787
Loss in iteration 180 : 0.3572238007775481
Loss in iteration 181 : 0.35719255511654047
Loss in iteration 182 : 0.3571617988185857
Loss in iteration 183 : 0.3571323378046344
Loss in iteration 184 : 0.3571017105445842
Loss in iteration 185 : 0.357072147658603
Loss in iteration 186 : 0.3570422244474771
Loss in iteration 187 : 0.3570129618953327
Loss in iteration 188 : 0.35698393666728995
Loss in iteration 189 : 0.35695522497871973
Loss in iteration 190 : 0.3569268079644314
Loss in iteration 191 : 0.3568988354140364
Loss in iteration 192 : 0.3568712688425527
Loss in iteration 193 : 0.3568439365767382
Loss in iteration 194 : 0.35681668580854714
Loss in iteration 195 : 0.3567895316301345
Loss in iteration 196 : 0.356762592514895
Loss in iteration 197 : 0.35673596505261074
Loss in iteration 198 : 0.35670943757583795
Loss in iteration 199 : 0.35668314176360877
Loss in iteration 200 : 0.3566570240387809
Testing accuracy  of updater 0 on alg 1 with rate 0.4 = 0.8470609913395983, training accuracy 0.8430589680589681, time elapsed: 3489 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.818419298486557
Loss in iteration 3 : 0.6368385969731185
Loss in iteration 4 : 0.49049345057169136
Loss in iteration 5 : 0.4836623891104091
Loss in iteration 6 : 0.48032932727861266
Loss in iteration 7 : 0.47777336840623763
Loss in iteration 8 : 0.4755645573433889
Loss in iteration 9 : 0.4736408209150074
Loss in iteration 10 : 0.4719577298044359
Loss in iteration 11 : 0.470429599824025
Loss in iteration 12 : 0.4690005338849005
Loss in iteration 13 : 0.467649555875677
Loss in iteration 14 : 0.4663728580462296
Loss in iteration 15 : 0.4651561651066404
Loss in iteration 16 : 0.46399955864886017
Loss in iteration 17 : 0.4629014510727501
Loss in iteration 18 : 0.46184967668835797
Loss in iteration 19 : 0.4608304661701865
Loss in iteration 20 : 0.45984045000648843
Loss in iteration 21 : 0.45888326295359544
Loss in iteration 22 : 0.45794869564183993
Loss in iteration 23 : 0.45703172964747807
Loss in iteration 24 : 0.45613292922836823
Loss in iteration 25 : 0.45525312454723554
Loss in iteration 26 : 0.45438907616783064
Loss in iteration 27 : 0.4535387986231421
Loss in iteration 28 : 0.4527021997567134
Loss in iteration 29 : 0.4518774508712708
Loss in iteration 30 : 0.451063288659304
Loss in iteration 31 : 0.45025646698742616
Loss in iteration 32 : 0.449459473985657
Loss in iteration 33 : 0.448672317388727
Loss in iteration 34 : 0.4478935194862335
Loss in iteration 35 : 0.44712352172364417
Loss in iteration 36 : 0.44636146714966773
Loss in iteration 37 : 0.44560888874970567
Loss in iteration 38 : 0.4448632537775646
Loss in iteration 39 : 0.4441231798863859
Loss in iteration 40 : 0.44338973220484246
Loss in iteration 41 : 0.4426621444288825
Loss in iteration 42 : 0.4419386367220756
Loss in iteration 43 : 0.44121892157891157
Loss in iteration 44 : 0.4405032332107329
Loss in iteration 45 : 0.4397912726985977
Loss in iteration 46 : 0.43908319473706514
Loss in iteration 47 : 0.4383783905464261
Loss in iteration 48 : 0.43767618880886877
Loss in iteration 49 : 0.4369763429563414
Loss in iteration 50 : 0.4362796460814735
Loss in iteration 51 : 0.4355852041628683
Loss in iteration 52 : 0.4348933604526736
Loss in iteration 53 : 0.43420352796047695
Loss in iteration 54 : 0.4335164514840109
Loss in iteration 55 : 0.4328312540748808
Loss in iteration 56 : 0.4321477694364277
Loss in iteration 57 : 0.43146572223119284
Loss in iteration 58 : 0.43078560087896706
Loss in iteration 59 : 0.43010736198980876
Loss in iteration 60 : 0.4294305831720983
Loss in iteration 61 : 0.4287551658552115
Loss in iteration 62 : 0.428081127395125
Loss in iteration 63 : 0.4274084155352587
Loss in iteration 64 : 0.4267369045391159
Loss in iteration 65 : 0.42606711074697007
Loss in iteration 66 : 0.42539952606867554
Loss in iteration 67 : 0.42473318073909255
Loss in iteration 68 : 0.42406794939903153
Loss in iteration 69 : 0.42340395731335356
Loss in iteration 70 : 0.4227408290647706
Loss in iteration 71 : 0.42207840307291433
Loss in iteration 72 : 0.42141646512354525
Loss in iteration 73 : 0.4207554560961741
Loss in iteration 74 : 0.4200952442174413
Loss in iteration 75 : 0.4194362633867381
Loss in iteration 76 : 0.4187779343483809
Loss in iteration 77 : 0.4181203503907361
Loss in iteration 78 : 0.41746364724885276
Loss in iteration 79 : 0.41680792104087566
Loss in iteration 80 : 0.4161530112240333
Loss in iteration 81 : 0.4154986587792557
Loss in iteration 82 : 0.41484516554959505
Loss in iteration 83 : 0.4141924459814125
Loss in iteration 84 : 0.4135406210005792
Loss in iteration 85 : 0.4128893588627167
Loss in iteration 86 : 0.4122384987420683
Loss in iteration 87 : 0.4115883074867645
Loss in iteration 88 : 0.4109389958209827
Loss in iteration 89 : 0.41028999948686595
Loss in iteration 90 : 0.40964154364046845
Loss in iteration 91 : 0.4089933924669031
Loss in iteration 92 : 0.408345483145085
Loss in iteration 93 : 0.4076980177212057
Loss in iteration 94 : 0.4070509291295147
Loss in iteration 95 : 0.40640526136967225
Loss in iteration 96 : 0.40576088059315296
Loss in iteration 97 : 0.4051192476260038
Loss in iteration 98 : 0.4044799340397464
Loss in iteration 99 : 0.40384220371463114
Loss in iteration 100 : 0.4032069770833212
Loss in iteration 101 : 0.40258181852139097
Loss in iteration 102 : 0.40196528531337455
Loss in iteration 103 : 0.401358315153578
Loss in iteration 104 : 0.4007655069982323
Loss in iteration 105 : 0.4001987133751187
Loss in iteration 106 : 0.3996395153494742
Loss in iteration 107 : 0.3990927497675802
Loss in iteration 108 : 0.3985648029681726
Loss in iteration 109 : 0.39805052475415076
Loss in iteration 110 : 0.39754664456169536
Loss in iteration 111 : 0.3970521776278447
Loss in iteration 112 : 0.39656573632349157
Loss in iteration 113 : 0.39608604017380267
Loss in iteration 114 : 0.3956157858634826
Loss in iteration 115 : 0.39515547897964937
Loss in iteration 116 : 0.3947040580723411
Loss in iteration 117 : 0.3942603620833822
Loss in iteration 118 : 0.3938226641875285
Loss in iteration 119 : 0.393394818165972
Loss in iteration 120 : 0.39297345592185917
Loss in iteration 121 : 0.39256080232147483
Loss in iteration 122 : 0.39215239404931485
Loss in iteration 123 : 0.3917495978884587
Loss in iteration 124 : 0.3913581371605016
Loss in iteration 125 : 0.3909763922888163
Loss in iteration 126 : 0.3906018724079226
Loss in iteration 127 : 0.39023160304318033
Loss in iteration 128 : 0.38986459518696126
Loss in iteration 129 : 0.389503394036487
Loss in iteration 130 : 0.38915045027437517
Loss in iteration 131 : 0.38880632334786214
Loss in iteration 132 : 0.3884672790810689
Loss in iteration 133 : 0.3881343519467377
Loss in iteration 134 : 0.38780577154102985
Loss in iteration 135 : 0.38748356662657263
Loss in iteration 136 : 0.3871686007574759
Loss in iteration 137 : 0.38686328632002126
Loss in iteration 138 : 0.38656715319365226
Loss in iteration 139 : 0.38627664868562434
Loss in iteration 140 : 0.3859936081903607
Loss in iteration 141 : 0.3857148903706022
Loss in iteration 142 : 0.38544056106586927
Loss in iteration 143 : 0.38517032220614134
Loss in iteration 144 : 0.384904755499581
Loss in iteration 145 : 0.38464530781199996
Loss in iteration 146 : 0.3843906290560161
Loss in iteration 147 : 0.3841397747458494
Loss in iteration 148 : 0.38389198753840825
Loss in iteration 149 : 0.3836507252345315
Loss in iteration 150 : 0.3834135629025824
Loss in iteration 151 : 0.38317931637453284
Loss in iteration 152 : 0.38294727358375236
Loss in iteration 153 : 0.38271781824822393
Loss in iteration 154 : 0.382491802131314
Loss in iteration 155 : 0.3822679509837059
Loss in iteration 156 : 0.38204500329009017
Loss in iteration 157 : 0.3818242048133407
Loss in iteration 158 : 0.3816068900770609
Loss in iteration 159 : 0.3813914958172082
Loss in iteration 160 : 0.38117848423096334
Loss in iteration 161 : 0.38096856087646125
Loss in iteration 162 : 0.3807617200941448
Loss in iteration 163 : 0.3805567009346558
Loss in iteration 164 : 0.38035533501937874
Loss in iteration 165 : 0.38015656023806405
Loss in iteration 166 : 0.37996031254339024
Loss in iteration 167 : 0.3797672330688692
Loss in iteration 168 : 0.37957772807638385
Loss in iteration 169 : 0.3793920207411445
Loss in iteration 170 : 0.3792082429376264
Loss in iteration 171 : 0.3790266855670729
Loss in iteration 172 : 0.37884691350385435
Loss in iteration 173 : 0.3786697188973366
Loss in iteration 174 : 0.3784952803065515
Loss in iteration 175 : 0.3783219402358296
Loss in iteration 176 : 0.37815012135977294
Loss in iteration 177 : 0.3779802111694003
Loss in iteration 178 : 0.3778107832676324
Loss in iteration 179 : 0.3776424354923663
Loss in iteration 180 : 0.37747548609937326
Loss in iteration 181 : 0.37731000564446454
Loss in iteration 182 : 0.37714657847692995
Loss in iteration 183 : 0.37698535504666425
Loss in iteration 184 : 0.37682558631126684
Loss in iteration 185 : 0.37666724001125795
Loss in iteration 186 : 0.3765099535576731
Loss in iteration 187 : 0.37635463342292974
Loss in iteration 188 : 0.37620157220613976
Loss in iteration 189 : 0.37605029507045085
Loss in iteration 190 : 0.37590262118621404
Loss in iteration 191 : 0.37575600894059025
Loss in iteration 192 : 0.3756114266838315
Loss in iteration 193 : 0.3754687068930381
Loss in iteration 194 : 0.37532721645240863
Loss in iteration 195 : 0.3751870056376734
Loss in iteration 196 : 0.3750483114899581
Loss in iteration 197 : 0.3749104281681455
Loss in iteration 198 : 0.3747738037205169
Loss in iteration 199 : 0.3746388512004306
Loss in iteration 200 : 0.3745053970481263
Testing accuracy  of updater 0 on alg 1 with rate 0.09999999999999998 = 0.8405503347460229, training accuracy 0.837960687960688, time elapsed: 3357 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9228953181878563
Loss in iteration 3 : 1.124116251645044
Loss in iteration 4 : 0.8934454726843334
Loss in iteration 5 : 0.4598038313108597
Loss in iteration 6 : 0.7863018230269629
Loss in iteration 7 : 0.8921878329650065
Loss in iteration 8 : 0.5975582052958756
Loss in iteration 9 : 0.568318746472627
Loss in iteration 10 : 0.7502507815894734
Loss in iteration 11 : 0.7670481597507046
Loss in iteration 12 : 0.6302220793300619
Loss in iteration 13 : 0.5328163537995121
Loss in iteration 14 : 0.6134497680303688
Loss in iteration 15 : 0.6875785080117517
Loss in iteration 16 : 0.5649136062292041
Loss in iteration 17 : 0.5027838170832951
Loss in iteration 18 : 0.5512452498467216
Loss in iteration 19 : 0.5819051516750852
Loss in iteration 20 : 0.5186315037331464
Loss in iteration 21 : 0.44851374103053016
Loss in iteration 22 : 0.49618202269182365
Loss in iteration 23 : 0.5176750248118818
Loss in iteration 24 : 0.4410321893717481
Loss in iteration 25 : 0.4564474917598284
Loss in iteration 26 : 0.49741735626891503
Loss in iteration 27 : 0.4447989070341919
Loss in iteration 28 : 0.43565061089727697
Loss in iteration 29 : 0.475016883050182
Loss in iteration 30 : 0.4253561674444015
Loss in iteration 31 : 0.415876611135544
Loss in iteration 32 : 0.43885829018642564
Loss in iteration 33 : 0.39915078862503683
Loss in iteration 34 : 0.39879318117001405
Loss in iteration 35 : 0.415719935689984
Loss in iteration 36 : 0.38415879601422515
Loss in iteration 37 : 0.39767658385246185
Loss in iteration 38 : 0.40157028750651835
Loss in iteration 39 : 0.3793573794478339
Loss in iteration 40 : 0.39539845388702727
Loss in iteration 41 : 0.3856871221554202
Loss in iteration 42 : 0.3758601875824867
Loss in iteration 43 : 0.3873919305182204
Loss in iteration 44 : 0.37033965087709614
Loss in iteration 45 : 0.3782742627852155
Loss in iteration 46 : 0.3735295561526366
Loss in iteration 47 : 0.36784113569969906
Loss in iteration 48 : 0.37400601384752946
Loss in iteration 49 : 0.36209913532133553
Loss in iteration 50 : 0.37012494043145727
Loss in iteration 51 : 0.3611855943230945
Loss in iteration 52 : 0.36595526579836685
Loss in iteration 53 : 0.3623555814336951
Loss in iteration 54 : 0.3616956304089781
Loss in iteration 55 : 0.36255056526544616
Loss in iteration 56 : 0.3579487042861314
Loss in iteration 57 : 0.3622383465153564
Loss in iteration 58 : 0.35626824569480514
Loss in iteration 59 : 0.3624483503848219
Loss in iteration 60 : 0.35610451951079997
Loss in iteration 61 : 0.3611494674393612
Loss in iteration 62 : 0.3560236300376297
Loss in iteration 63 : 0.3591121407739177
Loss in iteration 64 : 0.3557154788611596
Loss in iteration 65 : 0.35732031289071325
Loss in iteration 66 : 0.35542719177119264
Loss in iteration 67 : 0.3560409228381729
Loss in iteration 68 : 0.35530880183360375
Loss in iteration 69 : 0.3553521784155715
Loss in iteration 70 : 0.3547802299843473
Loss in iteration 71 : 0.3545893372372422
Loss in iteration 72 : 0.3543716592485372
Loss in iteration 73 : 0.35421771719777134
Loss in iteration 74 : 0.35390817474144004
Loss in iteration 75 : 0.3538308283103409
Loss in iteration 76 : 0.3535220108850041
Loss in iteration 77 : 0.35357047572693806
Loss in iteration 78 : 0.35322534950670326
Loss in iteration 79 : 0.3532263042658507
Loss in iteration 80 : 0.3527954918380711
Loss in iteration 81 : 0.3530627202036337
Loss in iteration 82 : 0.3526407878352139
Loss in iteration 83 : 0.3528688579962066
Loss in iteration 84 : 0.3523306473718715
Loss in iteration 85 : 0.3529582119208665
Loss in iteration 86 : 0.35219659509070217
Loss in iteration 87 : 0.3529799182889709
Loss in iteration 88 : 0.3521267418691719
Loss in iteration 89 : 0.3528393048332931
Loss in iteration 90 : 0.3520969967995228
Loss in iteration 91 : 0.3525798675772129
Loss in iteration 92 : 0.3521226306607102
Loss in iteration 93 : 0.3524283354667848
Loss in iteration 94 : 0.35210447902265124
Loss in iteration 95 : 0.35221148324950763
Loss in iteration 96 : 0.35213798611222147
Loss in iteration 97 : 0.35211085967715106
Loss in iteration 98 : 0.352077697038593
Loss in iteration 99 : 0.3519415325433094
Loss in iteration 100 : 0.35204170946015545
Loss in iteration 101 : 0.3519033370339026
Loss in iteration 102 : 0.35208519209792194
Loss in iteration 103 : 0.3518538493199126
Loss in iteration 104 : 0.35194189754409483
Loss in iteration 105 : 0.3518389735610392
Loss in iteration 106 : 0.3518301903532427
Loss in iteration 107 : 0.35187904436955014
Loss in iteration 108 : 0.35178930137183345
Loss in iteration 109 : 0.35186780022807584
Loss in iteration 110 : 0.3518168963405014
Loss in iteration 111 : 0.3517938177707204
Loss in iteration 112 : 0.35186154971556266
Loss in iteration 113 : 0.35177360635076704
Loss in iteration 114 : 0.35176184492582135
Loss in iteration 115 : 0.35183890477486524
Loss in iteration 116 : 0.35176674332742974
Loss in iteration 117 : 0.35172227606838935
Loss in iteration 118 : 0.3517576081838775
Loss in iteration 119 : 0.35179216821764375
Loss in iteration 120 : 0.3517171214446219
Loss in iteration 121 : 0.3517040264858223
Loss in iteration 122 : 0.35174808529380586
Loss in iteration 123 : 0.35169595976085616
Loss in iteration 124 : 0.3516762480739403
Loss in iteration 125 : 0.35169515276538976
Loss in iteration 126 : 0.351716872264418
Loss in iteration 127 : 0.3517307466688787
Loss in iteration 128 : 0.35165493049626595
Loss in iteration 129 : 0.3516830766987263
Loss in iteration 130 : 0.3517800948369704
Loss in iteration 131 : 0.35164668754681006
Loss in iteration 132 : 0.35167984033685057
Loss in iteration 133 : 0.35185465458899395
Loss in iteration 134 : 0.35164349621462876
Loss in iteration 135 : 0.3517171906442546
Loss in iteration 136 : 0.3518810669414725
Loss in iteration 137 : 0.35162912498694454
Loss in iteration 138 : 0.35199727377658674
Loss in iteration 139 : 0.3517376764729677
Loss in iteration 140 : 0.3517774523640408
Loss in iteration 141 : 0.35186154640802986
Loss in iteration 142 : 0.3516101468555529
Loss in iteration 143 : 0.3518255662125281
Loss in iteration 144 : 0.35163697448658776
Loss in iteration 145 : 0.3516495904046787
Loss in iteration 146 : 0.35176761642970017
Loss in iteration 147 : 0.35161787025179886
Loss in iteration 148 : 0.3518666279195171
Loss in iteration 149 : 0.3516419060832571
Loss in iteration 150 : 0.3516782521980144
Loss in iteration 151 : 0.351735211758495
Loss in iteration 152 : 0.35156927606116406
Loss in iteration 153 : 0.35170236371866626
Loss in iteration 154 : 0.3516230367659534
Loss in iteration 155 : 0.3515542138464774
Loss in iteration 156 : 0.3515976511888353
Loss in iteration 157 : 0.3515655039522155
Loss in iteration 158 : 0.35154628911304936
Loss in iteration 159 : 0.35156288320871587
Loss in iteration 160 : 0.35157163573082506
Loss in iteration 161 : 0.35157680199752267
Loss in iteration 162 : 0.3515539967526528
Loss in iteration 163 : 0.35152704728632134
Loss in iteration 164 : 0.3515247724873828
Loss in iteration 165 : 0.3515383746388542
Loss in iteration 166 : 0.35153970507938936
Loss in iteration 167 : 0.3515574407630374
Loss in iteration 168 : 0.3515143119569117
Loss in iteration 169 : 0.3515083847193834
Loss in iteration 170 : 0.35151259494830966
Loss in iteration 171 : 0.3515549734313479
Loss in iteration 172 : 0.3515853904740902
Loss in iteration 173 : 0.3515442616247622
Loss in iteration 174 : 0.35152802465720295
Loss in iteration 175 : 0.35149589977227785
Loss in iteration 176 : 0.3514971892332358
Loss in iteration 177 : 0.351535258128225
Loss in iteration 178 : 0.35149322059873506
Loss in iteration 179 : 0.35147507018608604
Loss in iteration 180 : 0.3514813817042708
Loss in iteration 181 : 0.35157025519627455
Loss in iteration 182 : 0.35155393728282
Loss in iteration 183 : 0.3514693692857416
Loss in iteration 184 : 0.35146181492664547
Loss in iteration 185 : 0.35147077658594766
Loss in iteration 186 : 0.3514745878456915
Loss in iteration 187 : 0.35151973016573024
Loss in iteration 188 : 0.3514647865551833
Loss in iteration 189 : 0.35144891575554854
Loss in iteration 190 : 0.3514499641956308
Loss in iteration 191 : 0.3514624854948744
Loss in iteration 192 : 0.35151524539675394
Loss in iteration 193 : 0.3514768048718903
Loss in iteration 194 : 0.3514908499594429
Loss in iteration 195 : 0.3514439658460645
Loss in iteration 196 : 0.35144416315699
Loss in iteration 197 : 0.3514628877032309
Loss in iteration 198 : 0.3514335156734384
Loss in iteration 199 : 0.35142487482101586
Loss in iteration 200 : 0.35142074186539807
Testing accuracy  of updater 1 on alg 1 with rate 1.0 = 0.8493950003071065, training accuracy 0.8495085995085995, time elapsed: 3215 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7182903906565983
Loss in iteration 3 : 0.8592746190740679
Loss in iteration 4 : 0.6992489190792726
Loss in iteration 5 : 0.42752910015812867
Loss in iteration 6 : 0.6209730588413983
Loss in iteration 7 : 0.6825664588140472
Loss in iteration 8 : 0.5032090426789423
Loss in iteration 9 : 0.4753632158016867
Loss in iteration 10 : 0.5910011923337681
Loss in iteration 11 : 0.5941618610933995
Loss in iteration 12 : 0.5023653229183175
Loss in iteration 13 : 0.45319219931567917
Loss in iteration 14 : 0.5124671748504356
Loss in iteration 15 : 0.5331348555431183
Loss in iteration 16 : 0.45990980626698097
Loss in iteration 17 : 0.4360730095552597
Loss in iteration 18 : 0.4661320047208923
Loss in iteration 19 : 0.474498755913257
Loss in iteration 20 : 0.432535359528061
Loss in iteration 21 : 0.4017506357401641
Loss in iteration 22 : 0.431071196279698
Loss in iteration 23 : 0.43484261103270927
Loss in iteration 24 : 0.3971982659716647
Loss in iteration 25 : 0.4038766528023423
Loss in iteration 26 : 0.42632637858428746
Loss in iteration 27 : 0.4037274135827678
Loss in iteration 28 : 0.39274812014359106
Loss in iteration 29 : 0.4115429932122535
Loss in iteration 30 : 0.39899238544833127
Loss in iteration 31 : 0.3795938534225567
Loss in iteration 32 : 0.39091643923819347
Loss in iteration 33 : 0.3854852542582757
Loss in iteration 34 : 0.3686544325405877
Loss in iteration 35 : 0.3787046703012428
Loss in iteration 36 : 0.3778463291764113
Loss in iteration 37 : 0.3659933075025024
Loss in iteration 38 : 0.3737705892755194
Loss in iteration 39 : 0.3738515101427239
Loss in iteration 40 : 0.3650142028347174
Loss in iteration 41 : 0.37033354255068807
Loss in iteration 42 : 0.36883783837780043
Loss in iteration 43 : 0.362266426602997
Loss in iteration 44 : 0.3665758985264145
Loss in iteration 45 : 0.36397728414580555
Loss in iteration 46 : 0.3598194158841564
Loss in iteration 47 : 0.36324711475217447
Loss in iteration 48 : 0.3600238402602968
Loss in iteration 49 : 0.35822126026425993
Loss in iteration 50 : 0.3603040638086198
Loss in iteration 51 : 0.3568539445015761
Loss in iteration 52 : 0.35761785983556277
Loss in iteration 53 : 0.3577017919027407
Loss in iteration 54 : 0.35555878803183666
Loss in iteration 55 : 0.3570989963585516
Loss in iteration 56 : 0.35547467282744066
Loss in iteration 57 : 0.3552645861027564
Loss in iteration 58 : 0.3556170925817848
Loss in iteration 59 : 0.35418728349276885
Loss in iteration 60 : 0.3549502948557469
Loss in iteration 61 : 0.3545123858812674
Loss in iteration 62 : 0.35399146903480844
Loss in iteration 63 : 0.3544909710194693
Loss in iteration 64 : 0.3536966740878502
Loss in iteration 65 : 0.3540836729103253
Loss in iteration 66 : 0.35378119316229745
Loss in iteration 67 : 0.3533527755754437
Loss in iteration 68 : 0.35358729953049095
Loss in iteration 69 : 0.35311027901513137
Loss in iteration 70 : 0.3533458545861285
Loss in iteration 71 : 0.3531227918221697
Loss in iteration 72 : 0.35308409438200883
Loss in iteration 73 : 0.35312147600169497
Loss in iteration 74 : 0.35286372020052326
Loss in iteration 75 : 0.35309310825675994
Loss in iteration 76 : 0.3527363202967626
Loss in iteration 77 : 0.35289393099645183
Loss in iteration 78 : 0.35271040222133043
Loss in iteration 79 : 0.35270132319577396
Loss in iteration 80 : 0.3526085912084661
Loss in iteration 81 : 0.35252850195261065
Loss in iteration 82 : 0.3525745231658625
Loss in iteration 83 : 0.35244140319705014
Loss in iteration 84 : 0.35253217881943244
Loss in iteration 85 : 0.3523693845616261
Loss in iteration 86 : 0.3524402210977789
Loss in iteration 87 : 0.3523074495364456
Loss in iteration 88 : 0.35231424678301515
Loss in iteration 89 : 0.35229123015800506
Loss in iteration 90 : 0.3522409668377543
Loss in iteration 91 : 0.3522430895377334
Loss in iteration 92 : 0.35219615817799166
Loss in iteration 93 : 0.35220448245753544
Loss in iteration 94 : 0.3521624952310531
Loss in iteration 95 : 0.35217344463786054
Loss in iteration 96 : 0.35214701155926004
Loss in iteration 97 : 0.35215552206319983
Loss in iteration 98 : 0.35210447651298354
Loss in iteration 99 : 0.35211740168154054
Loss in iteration 100 : 0.35207387807692225
Loss in iteration 101 : 0.3520980530492717
Loss in iteration 102 : 0.3520562371512612
Loss in iteration 103 : 0.3520690989714584
Loss in iteration 104 : 0.3520382664191266
Loss in iteration 105 : 0.35205379676537524
Loss in iteration 106 : 0.3520195871355971
Loss in iteration 107 : 0.35204749642418204
Loss in iteration 108 : 0.35202402275389605
Loss in iteration 109 : 0.35203134781051476
Loss in iteration 110 : 0.35199059963529383
Loss in iteration 111 : 0.35203092450816453
Loss in iteration 112 : 0.3519857422208474
Loss in iteration 113 : 0.3520043935691207
Loss in iteration 114 : 0.35195859890757336
Loss in iteration 115 : 0.3519732675403965
Loss in iteration 116 : 0.3519715442092613
Loss in iteration 117 : 0.3519502478637464
Loss in iteration 118 : 0.35195322568228493
Loss in iteration 119 : 0.35191046157618977
Loss in iteration 120 : 0.3519278079911462
Loss in iteration 121 : 0.35189943203684754
Loss in iteration 122 : 0.3519056743634481
Loss in iteration 123 : 0.35188249763257734
Loss in iteration 124 : 0.35188467275738083
Loss in iteration 125 : 0.3518656837013218
Loss in iteration 126 : 0.3518634315851592
Loss in iteration 127 : 0.3518516036956369
Loss in iteration 128 : 0.351853127505642
Loss in iteration 129 : 0.35183881686341256
Loss in iteration 130 : 0.3518359664173908
Loss in iteration 131 : 0.351827848626935
Loss in iteration 132 : 0.35182036211512463
Loss in iteration 133 : 0.35181500753794315
Loss in iteration 134 : 0.3518095573141693
Loss in iteration 135 : 0.35180432267205014
Loss in iteration 136 : 0.35180198960648357
Loss in iteration 137 : 0.35181176064810143
Loss in iteration 138 : 0.35182265145633884
Loss in iteration 139 : 0.35178557962809986
Loss in iteration 140 : 0.3518085730809867
Loss in iteration 141 : 0.3518142609643535
Loss in iteration 142 : 0.3517731923448704
Loss in iteration 143 : 0.35180460992317003
Loss in iteration 144 : 0.35179488520502766
Loss in iteration 145 : 0.35176206783541863
Loss in iteration 146 : 0.3517935779207656
Loss in iteration 147 : 0.3517668040669087
Loss in iteration 148 : 0.35175158261413847
Loss in iteration 149 : 0.35176301689561107
Loss in iteration 150 : 0.35173945805303525
Loss in iteration 151 : 0.35177116205435577
Loss in iteration 152 : 0.35174738713622383
Loss in iteration 153 : 0.3517299397353511
Loss in iteration 154 : 0.35173885217011264
Loss in iteration 155 : 0.3517211906311175
Loss in iteration 156 : 0.3517292042657336
Loss in iteration 157 : 0.35176252845528183
Loss in iteration 158 : 0.35170657245559833
Loss in iteration 159 : 0.3517494552439398
Loss in iteration 160 : 0.35175901237795754
Loss in iteration 161 : 0.35169877193709864
Loss in iteration 162 : 0.3517604803497973
Loss in iteration 163 : 0.35171964633870567
Loss in iteration 164 : 0.3517029187696726
Loss in iteration 165 : 0.3517381445367904
Loss in iteration 166 : 0.3516945128633092
Loss in iteration 167 : 0.35168873628638647
Loss in iteration 168 : 0.3517197207226114
Loss in iteration 169 : 0.3516870086920556
Loss in iteration 170 : 0.3516710454940544
Loss in iteration 171 : 0.35170114745385417
Loss in iteration 172 : 0.3516779712185131
Loss in iteration 173 : 0.35165967492499683
Loss in iteration 174 : 0.3516664144930887
Loss in iteration 175 : 0.351653427645298
Loss in iteration 176 : 0.35166105224365807
Loss in iteration 177 : 0.35165100415263806
Loss in iteration 178 : 0.35163919504696944
Loss in iteration 179 : 0.3516430942559159
Loss in iteration 180 : 0.35164158444189875
Loss in iteration 181 : 0.3516365751005289
Loss in iteration 182 : 0.3516265384548731
Loss in iteration 183 : 0.3516290593798128
Loss in iteration 184 : 0.3516581786897288
Loss in iteration 185 : 0.35163421109524173
Loss in iteration 186 : 0.35163331309913737
Loss in iteration 187 : 0.3516139174348816
Loss in iteration 188 : 0.3516111435823961
Loss in iteration 189 : 0.3516241415435457
Loss in iteration 190 : 0.35162291578171107
Loss in iteration 191 : 0.3516399270260401
Loss in iteration 192 : 0.35160729326750084
Loss in iteration 193 : 0.35161081428802216
Loss in iteration 194 : 0.3516623454600991
Loss in iteration 195 : 0.351596315284335
Loss in iteration 196 : 0.3516503213789252
Loss in iteration 197 : 0.3516677382788166
Loss in iteration 198 : 0.3515961958145646
Loss in iteration 199 : 0.35171791225906285
Loss in iteration 200 : 0.3515862137659273
Testing accuracy  of updater 1 on alg 1 with rate 0.7000000000000001 = 0.8504391622136233, training accuracy 0.8488636363636364, time elapsed: 3398 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5170319327312575
Loss in iteration 3 : 0.6121113865009756
Loss in iteration 4 : 0.542141524752639
Loss in iteration 5 : 0.4341890481002002
Loss in iteration 6 : 0.4662181800054625
Loss in iteration 7 : 0.5020467110240774
Loss in iteration 8 : 0.44696010035444234
Loss in iteration 9 : 0.39686767848088145
Loss in iteration 10 : 0.4363481617284292
Loss in iteration 11 : 0.44993777471054863
Loss in iteration 12 : 0.4173164447199581
Loss in iteration 13 : 0.39223262550198146
Loss in iteration 14 : 0.40899738065651087
Loss in iteration 15 : 0.4227826894821178
Loss in iteration 16 : 0.4006576997752091
Loss in iteration 17 : 0.3842153998947152
Loss in iteration 18 : 0.39073875551752485
Loss in iteration 19 : 0.3984919781448598
Loss in iteration 20 : 0.3881568424630705
Loss in iteration 21 : 0.3726634479123832
Loss in iteration 22 : 0.37398813042819035
Loss in iteration 23 : 0.3822794556353476
Loss in iteration 24 : 0.3760887357076293
Loss in iteration 25 : 0.366653915243018
Loss in iteration 26 : 0.37133505548639506
Loss in iteration 27 : 0.37682906559865836
Loss in iteration 28 : 0.3709787038969074
Loss in iteration 29 : 0.36679046662292214
Loss in iteration 30 : 0.3704518163377174
Loss in iteration 31 : 0.37146697653677285
Loss in iteration 32 : 0.3658798661105253
Loss in iteration 33 : 0.3626041598166725
Loss in iteration 34 : 0.36466144423165137
Loss in iteration 35 : 0.3635522405999754
Loss in iteration 36 : 0.3591415785356663
Loss in iteration 37 : 0.3588576159474962
Loss in iteration 38 : 0.3607709019189492
Loss in iteration 39 : 0.35896499211096977
Loss in iteration 40 : 0.35694840727706145
Loss in iteration 41 : 0.35830219595021445
Loss in iteration 42 : 0.3588949447895711
Loss in iteration 43 : 0.3571186470318475
Loss in iteration 44 : 0.3564862964660062
Loss in iteration 45 : 0.3574568792819982
Loss in iteration 46 : 0.3569200656831135
Loss in iteration 47 : 0.35560678684105235
Loss in iteration 48 : 0.35592810481734277
Loss in iteration 49 : 0.35615188876708137
Loss in iteration 50 : 0.35510701459656174
Loss in iteration 51 : 0.35472930194057284
Loss in iteration 52 : 0.35521882829952883
Loss in iteration 53 : 0.35468415270204134
Loss in iteration 54 : 0.354057070893376
Loss in iteration 55 : 0.3543925026634744
Loss in iteration 56 : 0.35422762152966364
Loss in iteration 57 : 0.35366634478779696
Loss in iteration 58 : 0.35385282407376945
Loss in iteration 59 : 0.3538569812970032
Loss in iteration 60 : 0.35345959617681205
Loss in iteration 61 : 0.35350790869517223
Loss in iteration 62 : 0.3535701240998626
Loss in iteration 63 : 0.35327182924967127
Loss in iteration 64 : 0.3532073830728681
Loss in iteration 65 : 0.35327251814572447
Loss in iteration 66 : 0.35307677613712524
Loss in iteration 67 : 0.35302693668569757
Loss in iteration 68 : 0.35309332942393123
Loss in iteration 69 : 0.35303119403548555
Loss in iteration 70 : 0.3529254160475741
Loss in iteration 71 : 0.35299056622528413
Loss in iteration 72 : 0.35295277402772224
Loss in iteration 73 : 0.35284951061728764
Loss in iteration 74 : 0.3528701071600026
Loss in iteration 75 : 0.3528382353750053
Loss in iteration 76 : 0.3527550107652884
Loss in iteration 77 : 0.35275619641315176
Loss in iteration 78 : 0.3527487253212314
Loss in iteration 79 : 0.35267976809354173
Loss in iteration 80 : 0.35270618441920665
Loss in iteration 81 : 0.35269165067501135
Loss in iteration 82 : 0.35263450151495723
Loss in iteration 83 : 0.3526493661537246
Loss in iteration 84 : 0.3526159673444592
Loss in iteration 85 : 0.3525970611886219
Loss in iteration 86 : 0.3526093142099628
Loss in iteration 87 : 0.35255654619600413
Loss in iteration 88 : 0.35255680271856726
Loss in iteration 89 : 0.3525321988231188
Loss in iteration 90 : 0.3525104066539194
Loss in iteration 91 : 0.3525103246220343
Loss in iteration 92 : 0.35248964144739325
Loss in iteration 93 : 0.3524785591155971
Loss in iteration 94 : 0.3524735505544148
Loss in iteration 95 : 0.35245438888033276
Loss in iteration 96 : 0.3524465929022545
Loss in iteration 97 : 0.35243444635813975
Loss in iteration 98 : 0.352419892054264
Loss in iteration 99 : 0.3524145152320652
Loss in iteration 100 : 0.3524005524483281
Loss in iteration 101 : 0.35239696254859393
Loss in iteration 102 : 0.3523811063123733
Loss in iteration 103 : 0.352382728817931
Loss in iteration 104 : 0.35236581363734404
Loss in iteration 105 : 0.3523621358600899
Loss in iteration 106 : 0.3523481480121869
Loss in iteration 107 : 0.3523440170558558
Loss in iteration 108 : 0.3523320062311733
Loss in iteration 109 : 0.35232695676512815
Loss in iteration 110 : 0.3523181654529616
Loss in iteration 111 : 0.35232003860469424
Loss in iteration 112 : 0.35230046660331304
Loss in iteration 113 : 0.3523081012198512
Loss in iteration 114 : 0.3522861112695759
Loss in iteration 115 : 0.35228637618420805
Loss in iteration 116 : 0.3522751919538117
Loss in iteration 117 : 0.35226587166427603
Loss in iteration 118 : 0.35226550853296035
Loss in iteration 119 : 0.35225207010054316
Loss in iteration 120 : 0.352248888798078
Loss in iteration 121 : 0.35224175824992665
Loss in iteration 122 : 0.35223309684276716
Loss in iteration 123 : 0.35223301806094787
Loss in iteration 124 : 0.3522208094234742
Loss in iteration 125 : 0.35222305884894894
Loss in iteration 126 : 0.3522091838735568
Loss in iteration 127 : 0.35220737716589845
Loss in iteration 128 : 0.352200345303487
Loss in iteration 129 : 0.3521923342677823
Loss in iteration 130 : 0.3521945649704415
Loss in iteration 131 : 0.35218036325723234
Loss in iteration 132 : 0.3521822135315267
Loss in iteration 133 : 0.3521691861825539
Loss in iteration 134 : 0.35217256875676894
Loss in iteration 135 : 0.35215814578996535
Loss in iteration 136 : 0.35216280935018673
Loss in iteration 137 : 0.3521478100481613
Loss in iteration 138 : 0.3521467992051149
Loss in iteration 139 : 0.3521411455206967
Loss in iteration 140 : 0.35213212242207464
Loss in iteration 141 : 0.3521323119985978
Loss in iteration 142 : 0.3521219321126427
Loss in iteration 143 : 0.3521196553368097
Loss in iteration 144 : 0.35211162442246147
Loss in iteration 145 : 0.3521110006208421
Loss in iteration 146 : 0.352101361418311
Loss in iteration 147 : 0.3520985031436643
Loss in iteration 148 : 0.352093013387829
Loss in iteration 149 : 0.3520869370407432
Loss in iteration 150 : 0.3520825966288853
Loss in iteration 151 : 0.3520779958185398
Loss in iteration 152 : 0.35207519874734633
Loss in iteration 153 : 0.3520688573448476
Loss in iteration 154 : 0.3520682304744589
Loss in iteration 155 : 0.3520591892897438
Loss in iteration 156 : 0.3520588340593144
Loss in iteration 157 : 0.3520520406661374
Loss in iteration 158 : 0.3520456376609725
Loss in iteration 159 : 0.35204381494172265
Loss in iteration 160 : 0.3520395135699662
Loss in iteration 161 : 0.35203233610225193
Loss in iteration 162 : 0.35203147993392725
Loss in iteration 163 : 0.35202435413227273
Loss in iteration 164 : 0.35202240926302136
Loss in iteration 165 : 0.35201679738421693
Loss in iteration 166 : 0.3520117825366354
Loss in iteration 167 : 0.3520095864882413
Loss in iteration 168 : 0.35200370690133004
Loss in iteration 169 : 0.3520038288917135
Loss in iteration 170 : 0.35199848741249085
Loss in iteration 171 : 0.3519947282061182
Loss in iteration 172 : 0.3519890846031236
Loss in iteration 173 : 0.3519891634697905
Loss in iteration 174 : 0.35198502819881133
Loss in iteration 175 : 0.3519773857471249
Loss in iteration 176 : 0.3519757661215213
Loss in iteration 177 : 0.35196763638721595
Loss in iteration 178 : 0.3519663181137351
Loss in iteration 179 : 0.35196006943567915
Loss in iteration 180 : 0.35195768508255226
Loss in iteration 181 : 0.3519540632287796
Loss in iteration 182 : 0.35194866862071805
Loss in iteration 183 : 0.35194689759921116
Loss in iteration 184 : 0.35194126061842596
Loss in iteration 185 : 0.35193855787205564
Loss in iteration 186 : 0.3519348335594244
Loss in iteration 187 : 0.3519297075473975
Loss in iteration 188 : 0.35192863506654415
Loss in iteration 189 : 0.35192270831739436
Loss in iteration 190 : 0.35192022869814055
Loss in iteration 191 : 0.35191741738017157
Loss in iteration 192 : 0.35191272224479786
Loss in iteration 193 : 0.3519135563857559
Loss in iteration 194 : 0.35190511717783346
Loss in iteration 195 : 0.3519078594477349
Loss in iteration 196 : 0.3518996479066734
Loss in iteration 197 : 0.35190078807512515
Loss in iteration 198 : 0.35189202525600205
Loss in iteration 199 : 0.3518933343871227
Loss in iteration 200 : 0.351886863941268
Testing accuracy  of updater 1 on alg 1 with rate 0.4 = 0.8505620047908605, training accuracy 0.8486179361179361, time elapsed: 2857 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8184192984865569
Loss in iteration 3 : 0.496287718270258
Loss in iteration 4 : 0.5809905680837186
Loss in iteration 5 : 0.6676089985707726
Loss in iteration 6 : 0.7046963152472893
Loss in iteration 7 : 0.6969300935392079
Loss in iteration 8 : 0.6492147446514221
Loss in iteration 9 : 0.5688885538348197
Loss in iteration 10 : 0.47858668683408895
Loss in iteration 11 : 0.4289081890705452
Loss in iteration 12 : 0.4377458104959909
Loss in iteration 13 : 0.47576345192607444
Loss in iteration 14 : 0.5122129247760911
Loss in iteration 15 : 0.5194924132347669
Loss in iteration 16 : 0.4964283151654851
Loss in iteration 17 : 0.4565781453290681
Loss in iteration 18 : 0.41588467093532167
Loss in iteration 19 : 0.39393871480003073
Loss in iteration 20 : 0.4049020381305836
Loss in iteration 21 : 0.4261013730258327
Loss in iteration 22 : 0.43853598515764564
Loss in iteration 23 : 0.4365568753263578
Loss in iteration 24 : 0.4219793817148346
Loss in iteration 25 : 0.40055544864206843
Loss in iteration 26 : 0.38361118598408117
Loss in iteration 27 : 0.37862839154256145
Loss in iteration 28 : 0.3863572269334802
Loss in iteration 29 : 0.3966098748439268
Loss in iteration 30 : 0.39855670509453306
Loss in iteration 31 : 0.39073575059477234
Loss in iteration 32 : 0.37880918048644224
Loss in iteration 33 : 0.37144173394192637
Loss in iteration 34 : 0.37070188138042426
Loss in iteration 35 : 0.37399593570022244
Loss in iteration 36 : 0.37741968744115795
Loss in iteration 37 : 0.3778600266384372
Loss in iteration 38 : 0.37469241891612604
Loss in iteration 39 : 0.3695048639985948
Loss in iteration 40 : 0.3649563024981431
Loss in iteration 41 : 0.36317951468761384
Loss in iteration 42 : 0.36439795423013366
Loss in iteration 43 : 0.36676162518768324
Loss in iteration 44 : 0.3671089774195422
Loss in iteration 45 : 0.3649448852655619
Loss in iteration 46 : 0.36189414326178343
Loss in iteration 47 : 0.3601716417223707
Loss in iteration 48 : 0.36042910101468656
Loss in iteration 49 : 0.36156331955425536
Loss in iteration 50 : 0.36235791881489077
Loss in iteration 51 : 0.36202635326611865
Loss in iteration 52 : 0.36079009916248234
Loss in iteration 53 : 0.35954641806532245
Loss in iteration 54 : 0.3590336291682706
Loss in iteration 55 : 0.3593792312844664
Loss in iteration 56 : 0.35996519909066793
Loss in iteration 57 : 0.36007837077938615
Loss in iteration 58 : 0.35954119768092685
Loss in iteration 59 : 0.35870927502858607
Loss in iteration 60 : 0.3581637966423144
Loss in iteration 61 : 0.3580299814942352
Loss in iteration 62 : 0.358217525282122
Loss in iteration 63 : 0.35831138681500824
Loss in iteration 64 : 0.3580742067860366
Loss in iteration 65 : 0.3576104683299285
Loss in iteration 66 : 0.35723386827456816
Loss in iteration 67 : 0.3570495390648324
Loss in iteration 68 : 0.3570477750683502
Loss in iteration 69 : 0.35709733958191053
Loss in iteration 70 : 0.3570468763240507
Loss in iteration 71 : 0.3568531842742561
Loss in iteration 72 : 0.3566122575046623
Loss in iteration 73 : 0.35645423863952086
Loss in iteration 74 : 0.35641455963109
Loss in iteration 75 : 0.3564121269841481
Loss in iteration 76 : 0.356387577945932
Loss in iteration 77 : 0.3563052339858032
Loss in iteration 78 : 0.3561764149194648
Loss in iteration 79 : 0.35604101352005907
Loss in iteration 80 : 0.3559344241380751
Loss in iteration 81 : 0.3558951463912475
Loss in iteration 82 : 0.3558817123039063
Loss in iteration 83 : 0.3558269073660539
Loss in iteration 84 : 0.35573220562721275
Loss in iteration 85 : 0.35563540607563326
Loss in iteration 86 : 0.35557104276569024
Loss in iteration 87 : 0.3555389891713563
Loss in iteration 88 : 0.3555092887558076
Loss in iteration 89 : 0.3554626263267261
Loss in iteration 90 : 0.3553962194385857
Loss in iteration 91 : 0.3553309158591593
Loss in iteration 92 : 0.3552810315013821
Loss in iteration 93 : 0.35524394748595384
Loss in iteration 94 : 0.3552077026151433
Loss in iteration 95 : 0.3551638597600287
Loss in iteration 96 : 0.35511057837517285
Loss in iteration 97 : 0.35506107543291043
Loss in iteration 98 : 0.3550190919232744
Loss in iteration 99 : 0.3549827847724289
Loss in iteration 100 : 0.3549487076480332
Loss in iteration 101 : 0.3549104400073732
Loss in iteration 102 : 0.35486873675124536
Loss in iteration 103 : 0.354828393397093
Loss in iteration 104 : 0.3547933060126772
Loss in iteration 105 : 0.35476207811918753
Loss in iteration 106 : 0.3547309487446091
Loss in iteration 107 : 0.3546970355978206
Loss in iteration 108 : 0.3546625386352207
Loss in iteration 109 : 0.35463000883150253
Loss in iteration 110 : 0.3545990565431636
Loss in iteration 111 : 0.35456993075681564
Loss in iteration 112 : 0.35454023696308595
Loss in iteration 113 : 0.3545103402926679
Loss in iteration 114 : 0.35448048779058866
Loss in iteration 115 : 0.3544519330644985
Loss in iteration 116 : 0.3544245630821271
Loss in iteration 117 : 0.3543978843155994
Loss in iteration 118 : 0.3543716691142191
Loss in iteration 119 : 0.35434558422825363
Loss in iteration 120 : 0.35431995602002914
Loss in iteration 121 : 0.3542949756740004
Loss in iteration 122 : 0.35427094310868945
Loss in iteration 123 : 0.35424732580062085
Loss in iteration 124 : 0.35422371444071593
Loss in iteration 125 : 0.3542006383923689
Loss in iteration 126 : 0.35417833539868265
Loss in iteration 127 : 0.35415630969260287
Loss in iteration 128 : 0.3541347551310368
Loss in iteration 129 : 0.35411359842611884
Loss in iteration 130 : 0.3540930381094586
Loss in iteration 131 : 0.35407271116381345
Loss in iteration 132 : 0.35405281183901277
Loss in iteration 133 : 0.3540335261681046
Loss in iteration 134 : 0.3540145030584706
Loss in iteration 135 : 0.35399566372130004
Loss in iteration 136 : 0.35397714837876965
Loss in iteration 137 : 0.3539591030803212
Loss in iteration 138 : 0.35394125522989855
Loss in iteration 139 : 0.3539237301785552
Loss in iteration 140 : 0.35390653994720467
Loss in iteration 141 : 0.3538896573258352
Loss in iteration 142 : 0.3538731050942188
Loss in iteration 143 : 0.35385681273259906
Loss in iteration 144 : 0.35384102785281485
Loss in iteration 145 : 0.3538253000632682
Loss in iteration 146 : 0.3538095597833229
Loss in iteration 147 : 0.3537943157669324
Loss in iteration 148 : 0.35377919379924455
Loss in iteration 149 : 0.3537643108050668
Loss in iteration 150 : 0.3537498679905938
Loss in iteration 151 : 0.35373536885924084
Loss in iteration 152 : 0.3537213503610545
Loss in iteration 153 : 0.3537072226379534
Loss in iteration 154 : 0.3536933875749465
Loss in iteration 155 : 0.35368003185202673
Loss in iteration 156 : 0.35366664179985674
Loss in iteration 157 : 0.35365371819225067
Loss in iteration 158 : 0.3536406393793775
Loss in iteration 159 : 0.3536284065490841
Loss in iteration 160 : 0.3536161765596321
Loss in iteration 161 : 0.35360368007322707
Loss in iteration 162 : 0.3535910348126396
Loss in iteration 163 : 0.3535790601722689
Loss in iteration 164 : 0.3535671666491827
Loss in iteration 165 : 0.353555253830031
Loss in iteration 166 : 0.3535437246074305
Loss in iteration 167 : 0.35353230019171716
Loss in iteration 168 : 0.3535207113237755
Loss in iteration 169 : 0.3535093998285941
Loss in iteration 170 : 0.35349826713162663
Loss in iteration 171 : 0.35348716358061666
Loss in iteration 172 : 0.3534762343808143
Loss in iteration 173 : 0.3534655537434418
Loss in iteration 174 : 0.35345478708882205
Loss in iteration 175 : 0.35344432449109
Loss in iteration 176 : 0.3534339072829544
Loss in iteration 177 : 0.3534234105576486
Loss in iteration 178 : 0.35341307087256346
Loss in iteration 179 : 0.35340287606222753
Loss in iteration 180 : 0.3533926056748055
Loss in iteration 181 : 0.35338234559059906
Loss in iteration 182 : 0.3533722684725535
Loss in iteration 183 : 0.3533622615813459
Loss in iteration 184 : 0.35335240664841017
Loss in iteration 185 : 0.3533427369731409
Loss in iteration 186 : 0.353333130492726
Loss in iteration 187 : 0.3533236056876629
Loss in iteration 188 : 0.3533141932784406
Loss in iteration 189 : 0.3533048605199636
Loss in iteration 190 : 0.3532957105182635
Loss in iteration 191 : 0.35328673519001186
Loss in iteration 192 : 0.3532777181469136
Loss in iteration 193 : 0.35326908321757355
Loss in iteration 194 : 0.353260321153113
Loss in iteration 195 : 0.35325138349037355
Loss in iteration 196 : 0.3532428694267191
Loss in iteration 197 : 0.35323428873327856
Loss in iteration 198 : 0.3532258734770103
Loss in iteration 199 : 0.35321754782218273
Loss in iteration 200 : 0.35320937703017935
Testing accuracy  of updater 1 on alg 1 with rate 0.1 = 0.8497635280388183, training accuracy 0.846529484029484, time elapsed: 2858 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8728935089405899
Loss in iteration 3 : 0.6313911759277164
Loss in iteration 4 : 0.5126866444941564
Loss in iteration 5 : 0.6003375971220685
Loss in iteration 6 : 0.6537526985059309
Loss in iteration 7 : 0.6732183631211027
Loss in iteration 8 : 0.6620685920665091
Loss in iteration 9 : 0.6238841356003721
Loss in iteration 10 : 0.5639151733606074
Loss in iteration 11 : 0.4954745417066678
Loss in iteration 12 : 0.44410627922444285
Loss in iteration 13 : 0.4320102441285468
Loss in iteration 14 : 0.4425907288113248
Loss in iteration 15 : 0.46637058658375175
Loss in iteration 16 : 0.48904858977629606
Loss in iteration 17 : 0.49561834485076556
Loss in iteration 18 : 0.48365900128963973
Loss in iteration 19 : 0.4592576858303102
Loss in iteration 20 : 0.4309359583181327
Loss in iteration 21 : 0.40679884438571895
Loss in iteration 22 : 0.39615817507482454
Loss in iteration 23 : 0.4020735221802175
Loss in iteration 24 : 0.4140766019956117
Loss in iteration 25 : 0.42255722403437435
Loss in iteration 26 : 0.42329725479603325
Loss in iteration 27 : 0.416114558000457
Loss in iteration 28 : 0.4037447212671613
Loss in iteration 29 : 0.39051063488072385
Loss in iteration 30 : 0.3813528406695605
Loss in iteration 31 : 0.37892169417517413
Loss in iteration 32 : 0.3831272527705149
Loss in iteration 33 : 0.38891070711090014
Loss in iteration 34 : 0.390758162697044
Loss in iteration 35 : 0.38714398026594476
Loss in iteration 36 : 0.37988888996069164
Loss in iteration 37 : 0.373622791625078
Loss in iteration 38 : 0.37093728182179414
Loss in iteration 39 : 0.3711285159262421
Loss in iteration 40 : 0.37268649983307967
Loss in iteration 41 : 0.3741056906202605
Loss in iteration 42 : 0.3741047130364758
Loss in iteration 43 : 0.3723633070235374
Loss in iteration 44 : 0.36957135036981786
Loss in iteration 45 : 0.3666878382936216
Loss in iteration 46 : 0.36476839848775966
Loss in iteration 47 : 0.36440099471604553
Loss in iteration 48 : 0.36499239165618025
Loss in iteration 49 : 0.36587676451890566
Loss in iteration 50 : 0.36581232079635745
Loss in iteration 51 : 0.36461215895811605
Loss in iteration 52 : 0.36292261739139503
Loss in iteration 53 : 0.36172111403994023
Loss in iteration 54 : 0.36137425761025027
Loss in iteration 55 : 0.36167372421859595
Loss in iteration 56 : 0.36210109299705084
Loss in iteration 57 : 0.36221251063400633
Loss in iteration 58 : 0.36183824416159
Loss in iteration 59 : 0.36111262736542155
Loss in iteration 60 : 0.3604478603904201
Loss in iteration 61 : 0.3601222248852871
Loss in iteration 62 : 0.360161676594775
Loss in iteration 63 : 0.3603686120392928
Loss in iteration 64 : 0.36045176632311443
Loss in iteration 65 : 0.36027450671306366
Loss in iteration 66 : 0.3598897968116052
Loss in iteration 67 : 0.35948532433832653
Loss in iteration 68 : 0.3592102023291462
Loss in iteration 69 : 0.35912324358763975
Loss in iteration 70 : 0.35913649367161193
Loss in iteration 71 : 0.359133599604339
Loss in iteration 72 : 0.3590327958793257
Loss in iteration 73 : 0.35883243938192166
Loss in iteration 74 : 0.358602222766713
Loss in iteration 75 : 0.35840964177725065
Loss in iteration 76 : 0.358285759207925
Loss in iteration 77 : 0.3582243673541639
Loss in iteration 78 : 0.35820027771907337
Loss in iteration 79 : 0.3581564765593356
Loss in iteration 80 : 0.358057165084436
Loss in iteration 81 : 0.3579234213540229
Loss in iteration 82 : 0.3578028345908798
Loss in iteration 83 : 0.35770569775762767
Loss in iteration 84 : 0.35763495165477904
Loss in iteration 85 : 0.35757895186552746
Loss in iteration 86 : 0.35752794506053515
Loss in iteration 87 : 0.3574703423603539
Loss in iteration 88 : 0.35740136553915486
Loss in iteration 89 : 0.3573227191609695
Loss in iteration 90 : 0.35723987870952173
Loss in iteration 91 : 0.3571596751289654
Loss in iteration 92 : 0.35708898697924246
Loss in iteration 93 : 0.3570330801707638
Loss in iteration 94 : 0.35698483656570107
Loss in iteration 95 : 0.35693509736605905
Loss in iteration 96 : 0.3568783172442453
Loss in iteration 97 : 0.3568151806577092
Loss in iteration 98 : 0.3567498636000438
Loss in iteration 99 : 0.3566897071822613
Loss in iteration 100 : 0.3566373573542685
Loss in iteration 101 : 0.35659165060177295
Loss in iteration 102 : 0.3565456980315777
Loss in iteration 103 : 0.35649462198882764
Loss in iteration 104 : 0.35644138931122177
Loss in iteration 105 : 0.35639005797545426
Loss in iteration 106 : 0.35634143059942425
Loss in iteration 107 : 0.3562969348924988
Loss in iteration 108 : 0.35625387943798664
Loss in iteration 109 : 0.35621123448403924
Loss in iteration 110 : 0.35616715838082996
Loss in iteration 111 : 0.3561229226264401
Loss in iteration 112 : 0.3560791699097278
Loss in iteration 113 : 0.35603709707945147
Loss in iteration 114 : 0.3559980238798596
Loss in iteration 115 : 0.35595983978799645
Loss in iteration 116 : 0.3559215762717714
Loss in iteration 117 : 0.35588317690643095
Loss in iteration 118 : 0.3558450287261657
Loss in iteration 119 : 0.3558073435697038
Loss in iteration 120 : 0.35577033347251175
Loss in iteration 121 : 0.3557344680358385
Loss in iteration 122 : 0.35569932396396226
Loss in iteration 123 : 0.35566480526728006
Loss in iteration 124 : 0.3556304510703121
Loss in iteration 125 : 0.35559623176518035
Loss in iteration 126 : 0.35556263022262935
Loss in iteration 127 : 0.35553029716219614
Loss in iteration 128 : 0.3554987420548829
Loss in iteration 129 : 0.3554671033601251
Loss in iteration 130 : 0.35543573207399476
Loss in iteration 131 : 0.35540486512784314
Loss in iteration 132 : 0.3553749060576796
Loss in iteration 133 : 0.35534491900963977
Loss in iteration 134 : 0.3553156649124311
Loss in iteration 135 : 0.3552870373935893
Loss in iteration 136 : 0.35525890983550584
Loss in iteration 137 : 0.35523108087161087
Loss in iteration 138 : 0.35520372332169864
Loss in iteration 139 : 0.35517694834175967
Loss in iteration 140 : 0.3551506696679239
Loss in iteration 141 : 0.3551247575934152
Loss in iteration 142 : 0.3550992088846852
Loss in iteration 143 : 0.35507406739626407
Loss in iteration 144 : 0.3550493556975597
Loss in iteration 145 : 0.3550248616459752
Loss in iteration 146 : 0.3550008144400224
Loss in iteration 147 : 0.3549768837261493
Loss in iteration 148 : 0.3549529925104701
Loss in iteration 149 : 0.35492941999038574
Loss in iteration 150 : 0.3549061103280109
Loss in iteration 151 : 0.3548825824937402
Loss in iteration 152 : 0.35485957928215855
Loss in iteration 153 : 0.3548370834266867
Loss in iteration 154 : 0.35481412649705546
Loss in iteration 155 : 0.354791596153005
Loss in iteration 156 : 0.35476957736079073
Loss in iteration 157 : 0.354747350512811
Loss in iteration 158 : 0.3547252382927204
Loss in iteration 159 : 0.35470355972752654
Loss in iteration 160 : 0.3546821559972986
Loss in iteration 161 : 0.35466121040892445
Loss in iteration 162 : 0.35464060631364847
Loss in iteration 163 : 0.3546199315797138
Loss in iteration 164 : 0.3545997447983483
Loss in iteration 165 : 0.3545797476971971
Loss in iteration 166 : 0.35455971336135117
Loss in iteration 167 : 0.35454006182927456
Loss in iteration 168 : 0.35452059169639405
Loss in iteration 169 : 0.3545013409150271
Loss in iteration 170 : 0.3544825750677971
Loss in iteration 171 : 0.35446385601598523
Loss in iteration 172 : 0.35444548862272
Loss in iteration 173 : 0.35442734660744296
Loss in iteration 174 : 0.35440952418563876
Loss in iteration 175 : 0.3543919702560153
Loss in iteration 176 : 0.35437446944242074
Loss in iteration 177 : 0.3543571027167019
Loss in iteration 178 : 0.3543398068573704
Loss in iteration 179 : 0.3543227591309395
Loss in iteration 180 : 0.35430573225865797
Loss in iteration 181 : 0.35428928373845303
Loss in iteration 182 : 0.35427245646318656
Loss in iteration 183 : 0.35425660667731745
Loss in iteration 184 : 0.35424057425805716
Loss in iteration 185 : 0.354224384626633
Loss in iteration 186 : 0.35420875444355043
Loss in iteration 187 : 0.35419358423815017
Loss in iteration 188 : 0.35417805780608597
Loss in iteration 189 : 0.35416313383883674
Loss in iteration 190 : 0.3541484463807511
Loss in iteration 191 : 0.35413400347172064
Loss in iteration 192 : 0.35411968723268855
Loss in iteration 193 : 0.3541057698991868
Loss in iteration 194 : 0.3540920193992625
Loss in iteration 195 : 0.35407827120955127
Loss in iteration 196 : 0.35406489122089146
Loss in iteration 197 : 0.35405164824464996
Loss in iteration 198 : 0.3540383742668402
Loss in iteration 199 : 0.35402536531901013
Loss in iteration 200 : 0.3540127520832132
Testing accuracy  of updater 1 on alg 1 with rate 0.07 = 0.8490878938640133, training accuracy 0.8461609336609337, time elapsed: 2775 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9273677193946211
Loss in iteration 3 : 0.789366386244408
Loss in iteration 4 : 0.5925329058038384
Loss in iteration 5 : 0.49649318511955304
Loss in iteration 6 : 0.5614372802244841
Loss in iteration 7 : 0.6103971188566686
Loss in iteration 8 : 0.6384790070062897
Loss in iteration 9 : 0.6474217018878136
Loss in iteration 10 : 0.639149227193942
Loss in iteration 11 : 0.6156357897749086
Loss in iteration 12 : 0.5790633367764879
Loss in iteration 13 : 0.5338122918778103
Loss in iteration 14 : 0.4876663326940268
Loss in iteration 15 : 0.452509458490613
Loss in iteration 16 : 0.438857530561619
Loss in iteration 17 : 0.439471229489738
Loss in iteration 18 : 0.44528366394259833
Loss in iteration 19 : 0.45458610746353817
Loss in iteration 20 : 0.4643680098858618
Loss in iteration 21 : 0.47020849040167234
Loss in iteration 22 : 0.4688880325391712
Loss in iteration 23 : 0.46052511750282427
Loss in iteration 24 : 0.44758070548732587
Loss in iteration 25 : 0.43298891802476197
Loss in iteration 26 : 0.41915345639455004
Loss in iteration 27 : 0.4078643102880997
Loss in iteration 28 : 0.40072741519911753
Loss in iteration 29 : 0.3995516366514527
Loss in iteration 30 : 0.4024422212662154
Loss in iteration 31 : 0.4063190016078658
Loss in iteration 32 : 0.4087632779252246
Loss in iteration 33 : 0.40848842332178154
Loss in iteration 34 : 0.40537538658266237
Loss in iteration 35 : 0.4001460368293898
Loss in iteration 36 : 0.3939788933173508
Loss in iteration 37 : 0.38827047664794045
Loss in iteration 38 : 0.3838678806391611
Loss in iteration 39 : 0.3815409218196658
Loss in iteration 40 : 0.3817048791641516
Loss in iteration 41 : 0.3830578776795263
Loss in iteration 42 : 0.38419394373192306
Loss in iteration 43 : 0.38415443136529853
Loss in iteration 44 : 0.38265045931930225
Loss in iteration 45 : 0.38006732812008664
Loss in iteration 46 : 0.3772742120276041
Loss in iteration 47 : 0.3751776133197156
Loss in iteration 48 : 0.37411343090041105
Loss in iteration 49 : 0.37382798321448063
Loss in iteration 50 : 0.3739155651459404
Loss in iteration 51 : 0.3740364816273529
Loss in iteration 52 : 0.37391461096940193
Loss in iteration 53 : 0.37341615355376123
Loss in iteration 54 : 0.3725568771287105
Loss in iteration 55 : 0.37147655718121375
Loss in iteration 56 : 0.3703695829379234
Loss in iteration 57 : 0.36943392658394286
Loss in iteration 58 : 0.3687632974153451
Loss in iteration 59 : 0.3683951099350052
Loss in iteration 60 : 0.36820057947669094
Loss in iteration 61 : 0.3680801613409959
Loss in iteration 62 : 0.367894644805851
Loss in iteration 63 : 0.3675363376018054
Loss in iteration 64 : 0.36701340298618446
Loss in iteration 65 : 0.3664281099540793
Loss in iteration 66 : 0.3659111624317377
Loss in iteration 67 : 0.36551635283527895
Loss in iteration 68 : 0.3652595079243895
Loss in iteration 69 : 0.3650769612059806
Loss in iteration 70 : 0.36492741137438117
Loss in iteration 71 : 0.36476801810475445
Loss in iteration 72 : 0.3645708689196546
Loss in iteration 73 : 0.36432392348266046
Loss in iteration 74 : 0.36404066101175075
Loss in iteration 75 : 0.36376772346765396
Loss in iteration 76 : 0.3635561727007816
Loss in iteration 77 : 0.36338580567514556
Loss in iteration 78 : 0.36323714630036463
Loss in iteration 79 : 0.3631104958309455
Loss in iteration 80 : 0.36299445066046887
Loss in iteration 81 : 0.3628594797069555
Loss in iteration 82 : 0.36270015294350094
Loss in iteration 83 : 0.36252609428044935
Loss in iteration 84 : 0.36235691250288365
Loss in iteration 85 : 0.36220904166299484
Loss in iteration 86 : 0.3620744186044994
Loss in iteration 87 : 0.3619539725392082
Loss in iteration 88 : 0.36184482208221436
Loss in iteration 89 : 0.36173677738379045
Loss in iteration 90 : 0.36162634531284343
Loss in iteration 91 : 0.36151280989917817
Loss in iteration 92 : 0.36139800507165776
Loss in iteration 93 : 0.361283676068849
Loss in iteration 94 : 0.361172828345279
Loss in iteration 95 : 0.3610685722092852
Loss in iteration 96 : 0.36096960439767134
Loss in iteration 97 : 0.3608744263852602
Loss in iteration 98 : 0.36078326002082356
Loss in iteration 99 : 0.3606938958766858
Loss in iteration 100 : 0.36060595025939723
Loss in iteration 101 : 0.36051905135723583
Loss in iteration 102 : 0.360432514300554
Loss in iteration 103 : 0.3603457779000582
Loss in iteration 104 : 0.3602604128736464
Loss in iteration 105 : 0.36017787217667185
Loss in iteration 106 : 0.3600991821830504
Loss in iteration 107 : 0.36002411291482816
Loss in iteration 108 : 0.3599506246703084
Loss in iteration 109 : 0.3598790128749446
Loss in iteration 110 : 0.3598089756002779
Loss in iteration 111 : 0.3597400995251518
Loss in iteration 112 : 0.35967153663321366
Loss in iteration 113 : 0.3596035626549482
Loss in iteration 114 : 0.3595362951716529
Loss in iteration 115 : 0.35947021768754156
Loss in iteration 116 : 0.3594062716243158
Loss in iteration 117 : 0.35934483361382047
Loss in iteration 118 : 0.35928454905892115
Loss in iteration 119 : 0.3592256929125829
Loss in iteration 120 : 0.35916729836247413
Loss in iteration 121 : 0.3591095339669161
Loss in iteration 122 : 0.35905234234551847
Loss in iteration 123 : 0.3589960950542624
Loss in iteration 124 : 0.3589408407948453
Loss in iteration 125 : 0.3588866939173334
Loss in iteration 126 : 0.35883354139326024
Loss in iteration 127 : 0.3587815895665045
Loss in iteration 128 : 0.3587305605054554
Loss in iteration 129 : 0.3586801071778318
Loss in iteration 130 : 0.35863014499781704
Loss in iteration 131 : 0.35858058300722906
Loss in iteration 132 : 0.35853175639656926
Loss in iteration 133 : 0.35848376881020627
Loss in iteration 134 : 0.3584364696875931
Loss in iteration 135 : 0.35838971370630673
Loss in iteration 136 : 0.35834343862704277
Loss in iteration 137 : 0.35829768905480824
Loss in iteration 138 : 0.3582524099268591
Loss in iteration 139 : 0.3582075296041478
Loss in iteration 140 : 0.35816315066911064
Loss in iteration 141 : 0.3581191965723437
Loss in iteration 142 : 0.3580758250558371
Loss in iteration 143 : 0.3580331999505174
Loss in iteration 144 : 0.3579911444749929
Loss in iteration 145 : 0.35794950029541867
Loss in iteration 146 : 0.35790822954437823
Loss in iteration 147 : 0.3578676374433535
Loss in iteration 148 : 0.35782745273258726
Loss in iteration 149 : 0.3577878660096055
Loss in iteration 150 : 0.3577489175668293
Loss in iteration 151 : 0.3577103855711397
Loss in iteration 152 : 0.35767219728792593
Loss in iteration 153 : 0.3576344698334572
Loss in iteration 154 : 0.3575972620281716
Loss in iteration 155 : 0.35756041315003556
Loss in iteration 156 : 0.3575238321249731
Loss in iteration 157 : 0.35748772269627843
Loss in iteration 158 : 0.3574522119084275
Loss in iteration 159 : 0.3574170657568732
Loss in iteration 160 : 0.35738222906674116
Loss in iteration 161 : 0.35734790037986375
Loss in iteration 162 : 0.35731389013402226
Loss in iteration 163 : 0.35728014194887736
Loss in iteration 164 : 0.35724689815513366
Loss in iteration 165 : 0.3572140312197114
Loss in iteration 166 : 0.35718145666541085
Loss in iteration 167 : 0.3571493055436842
Loss in iteration 168 : 0.35711740907135225
Loss in iteration 169 : 0.35708586471355014
Loss in iteration 170 : 0.3570546391808017
Loss in iteration 171 : 0.35702377835194693
Loss in iteration 172 : 0.3569932755484101
Loss in iteration 173 : 0.35696312163400457
Loss in iteration 174 : 0.3569332931817728
Loss in iteration 175 : 0.35690376614565783
Loss in iteration 176 : 0.35687460618782435
Loss in iteration 177 : 0.3568457275544798
Loss in iteration 178 : 0.3568171714360955
Loss in iteration 179 : 0.3567887775559926
Loss in iteration 180 : 0.3567606560345899
Loss in iteration 181 : 0.3567328012121954
Loss in iteration 182 : 0.35670518281865715
Loss in iteration 183 : 0.3566777872761618
Loss in iteration 184 : 0.3566506459363683
Loss in iteration 185 : 0.35662380644294234
Loss in iteration 186 : 0.3565973415907793
Loss in iteration 187 : 0.35657109574753215
Loss in iteration 188 : 0.35654501607797534
Loss in iteration 189 : 0.35651927896016256
Loss in iteration 190 : 0.35649382287510184
Loss in iteration 191 : 0.35646862064319046
Loss in iteration 192 : 0.3564435703648125
Loss in iteration 193 : 0.3564187027775713
Loss in iteration 194 : 0.35639396207258717
Loss in iteration 195 : 0.35636939206602375
Loss in iteration 196 : 0.3563450192517233
Loss in iteration 197 : 0.3563208645787456
Loss in iteration 198 : 0.35629696173282255
Loss in iteration 199 : 0.35627324930546916
Loss in iteration 200 : 0.356249682832591
Testing accuracy  of updater 1 on alg 1 with rate 0.04000000000000001 = 0.8471838339168356, training accuracy 0.8435196560196561, time elapsed: 3106 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9818419298486529
Loss in iteration 3 : 0.9473415965611012
Loss in iteration 4 : 0.8981332264509566
Loss in iteration 5 : 0.8356876232004858
Loss in iteration 6 : 0.7613285101237147
Loss in iteration 7 : 0.6762472382032809
Loss in iteration 8 : 0.5815160233235447
Loss in iteration 9 : 0.49822076660741865
Loss in iteration 10 : 0.4891830962625987
Loss in iteration 11 : 0.5108312100960828
Loss in iteration 12 : 0.533630361077988
Loss in iteration 13 : 0.5516540704630928
Loss in iteration 14 : 0.5642889420006931
Loss in iteration 15 : 0.5717693991754159
Loss in iteration 16 : 0.5745431462620392
Loss in iteration 17 : 0.5730694200058631
Loss in iteration 18 : 0.5677907897545923
Loss in iteration 19 : 0.5591414505825149
Loss in iteration 20 : 0.5475676692098562
Loss in iteration 21 : 0.5337882293245061
Loss in iteration 22 : 0.5184386578999627
Loss in iteration 23 : 0.502522015412298
Loss in iteration 24 : 0.4871268768074204
Loss in iteration 25 : 0.4736199761491194
Loss in iteration 26 : 0.46343132345433685
Loss in iteration 27 : 0.4570321413054033
Loss in iteration 28 : 0.45431382279396876
Loss in iteration 29 : 0.45432824431226004
Loss in iteration 30 : 0.4555324736529433
Loss in iteration 31 : 0.45705967812707166
Loss in iteration 32 : 0.45846805715933614
Loss in iteration 33 : 0.4594954251996459
Loss in iteration 34 : 0.4599730608559662
Loss in iteration 35 : 0.45979337244798746
Loss in iteration 36 : 0.4589348898229434
Loss in iteration 37 : 0.45744419863669067
Loss in iteration 38 : 0.45541733246940214
Loss in iteration 39 : 0.4529775111090072
Loss in iteration 40 : 0.4502456352948796
Loss in iteration 41 : 0.4473408266513286
Loss in iteration 42 : 0.44440312792101794
Loss in iteration 43 : 0.44153840357313096
Loss in iteration 44 : 0.4388370665368809
Loss in iteration 45 : 0.43639058498643796
Loss in iteration 46 : 0.4341994460723359
Loss in iteration 47 : 0.4325204514235385
Loss in iteration 48 : 0.43133109177285633
Loss in iteration 49 : 0.43048986909223097
Loss in iteration 50 : 0.4298095894474368
Loss in iteration 51 : 0.4292707349683504
Loss in iteration 52 : 0.4287729690622168
Loss in iteration 53 : 0.428219903935652
Loss in iteration 54 : 0.4275502922894034
Loss in iteration 55 : 0.42673493522769745
Loss in iteration 56 : 0.42577136662461673
Loss in iteration 57 : 0.42468496320130344
Loss in iteration 58 : 0.42351260262444446
Loss in iteration 59 : 0.42229749341776585
Loss in iteration 60 : 0.42108471054427477
Loss in iteration 61 : 0.4199123739408954
Loss in iteration 62 : 0.4188119288826602
Loss in iteration 63 : 0.4177871477740054
Loss in iteration 64 : 0.4168439118993449
Loss in iteration 65 : 0.415977988855112
Loss in iteration 66 : 0.41518370879728744
Loss in iteration 67 : 0.41444209145258526
Loss in iteration 68 : 0.41372583849248457
Loss in iteration 69 : 0.4130200817141072
Loss in iteration 70 : 0.4123097549599785
Loss in iteration 71 : 0.41158712014694954
Loss in iteration 72 : 0.41084598286869645
Loss in iteration 73 : 0.41008850401534125
Loss in iteration 74 : 0.4093173887891677
Loss in iteration 75 : 0.408536544589793
Loss in iteration 76 : 0.4077537902767856
Loss in iteration 77 : 0.4069784462703447
Loss in iteration 78 : 0.40621556786893515
Loss in iteration 79 : 0.40547246385740926
Loss in iteration 80 : 0.4047567465202478
Loss in iteration 81 : 0.40407221117908976
Loss in iteration 82 : 0.4034121020314503
Loss in iteration 83 : 0.40277555230898815
Loss in iteration 84 : 0.40216078251906373
Loss in iteration 85 : 0.4015615688835626
Loss in iteration 86 : 0.40097681153936937
Loss in iteration 87 : 0.4004035750716705
Loss in iteration 88 : 0.39983953142399753
Loss in iteration 89 : 0.39928063221829846
Loss in iteration 90 : 0.3987297480899461
Loss in iteration 91 : 0.3981858558813882
Loss in iteration 92 : 0.3976493902460616
Loss in iteration 93 : 0.3971214984548573
Loss in iteration 94 : 0.3966040070382565
Loss in iteration 95 : 0.396096724474586
Loss in iteration 96 : 0.3956032474229346
Loss in iteration 97 : 0.3951213194617009
Loss in iteration 98 : 0.39464894784544735
Loss in iteration 99 : 0.39418843719984276
Loss in iteration 100 : 0.39373511511668424
Loss in iteration 101 : 0.3932902793785024
Loss in iteration 102 : 0.39285314623130924
Loss in iteration 103 : 0.392425260931616
Loss in iteration 104 : 0.39200590049752204
Loss in iteration 105 : 0.3915944912858389
Loss in iteration 106 : 0.3911907446081512
Loss in iteration 107 : 0.3907942251207006
Loss in iteration 108 : 0.39040526661116964
Loss in iteration 109 : 0.3900232194595108
Loss in iteration 110 : 0.38964879700680166
Loss in iteration 111 : 0.3892824413399212
Loss in iteration 112 : 0.38892301418651226
Loss in iteration 113 : 0.38857160841504135
Loss in iteration 114 : 0.38822930047279786
Loss in iteration 115 : 0.3878941797916246
Loss in iteration 116 : 0.3875655082866941
Loss in iteration 117 : 0.38724270919865733
Loss in iteration 118 : 0.3869259992549315
Loss in iteration 119 : 0.3866153234716664
Loss in iteration 120 : 0.3863118042229861
Loss in iteration 121 : 0.386015018157764
Loss in iteration 122 : 0.38572368204003926
Loss in iteration 123 : 0.3854373025586159
Loss in iteration 124 : 0.3851559130633001
Loss in iteration 125 : 0.3848784060757646
Loss in iteration 126 : 0.38460511411587633
Loss in iteration 127 : 0.38433526012025665
Loss in iteration 128 : 0.38406987151704547
Loss in iteration 129 : 0.38380870129913297
Loss in iteration 130 : 0.38355171818347267
Loss in iteration 131 : 0.38329852955123567
Loss in iteration 132 : 0.38304883000773
Loss in iteration 133 : 0.3828028889069332
Loss in iteration 134 : 0.3825611813812966
Loss in iteration 135 : 0.3823229830503291
Loss in iteration 136 : 0.3820883609005678
Loss in iteration 137 : 0.3818567165449435
Loss in iteration 138 : 0.38162867406817774
Loss in iteration 139 : 0.38140402969891923
Loss in iteration 140 : 0.3811826442158774
Loss in iteration 141 : 0.3809640090252857
Loss in iteration 142 : 0.380748033808476
Loss in iteration 143 : 0.380534902427959
Loss in iteration 144 : 0.3803244893541189
Loss in iteration 145 : 0.38011757811931807
Loss in iteration 146 : 0.3799136150455383
Loss in iteration 147 : 0.3797122923360365
Loss in iteration 148 : 0.3795139111563812
Loss in iteration 149 : 0.37931790874867566
Loss in iteration 150 : 0.3791242716966724
Loss in iteration 151 : 0.37893343288908765
Loss in iteration 152 : 0.3787444896557668
Loss in iteration 153 : 0.37855823211039225
Loss in iteration 154 : 0.3783745255575765
Loss in iteration 155 : 0.3781928909357658
Loss in iteration 156 : 0.37801297023958563
Loss in iteration 157 : 0.3778349795202152
Loss in iteration 158 : 0.3776590874012765
Loss in iteration 159 : 0.3774849546599013
Loss in iteration 160 : 0.37731290031131437
Loss in iteration 161 : 0.37714264028308625
Loss in iteration 162 : 0.3769743771884619
Loss in iteration 163 : 0.3768081578281489
Loss in iteration 164 : 0.3766445099873322
Loss in iteration 165 : 0.37648279091553893
Loss in iteration 166 : 0.3763228417024539
Loss in iteration 167 : 0.376164413189253
Loss in iteration 168 : 0.37600736338606333
Loss in iteration 169 : 0.375852174783546
Loss in iteration 170 : 0.37569843778178563
Loss in iteration 171 : 0.37554653234636065
Loss in iteration 172 : 0.3753962880108335
Loss in iteration 173 : 0.37524786427703494
Loss in iteration 174 : 0.37510132690218173
Loss in iteration 175 : 0.37495631823606856
Loss in iteration 176 : 0.3748129223286558
Loss in iteration 177 : 0.3746708878736325
Loss in iteration 178 : 0.37453006912095865
Loss in iteration 179 : 0.37439036533459363
Loss in iteration 180 : 0.37425195341983336
Loss in iteration 181 : 0.374114766771351
Loss in iteration 182 : 0.3739789916963879
Loss in iteration 183 : 0.3738440841879753
Loss in iteration 184 : 0.37371006610800345
Loss in iteration 185 : 0.37357717857383715
Loss in iteration 186 : 0.37344539563977985
Loss in iteration 187 : 0.3733145927737526
Loss in iteration 188 : 0.37318481146445265
Loss in iteration 189 : 0.3730564663884567
Loss in iteration 190 : 0.3729297642351297
Loss in iteration 191 : 0.37280458196622784
Loss in iteration 192 : 0.37268130349276196
Loss in iteration 193 : 0.3725594107060508
Loss in iteration 194 : 0.3724386170060089
Loss in iteration 195 : 0.3723189530072814
Loss in iteration 196 : 0.3722006799529735
Loss in iteration 197 : 0.3720836002468927
Loss in iteration 198 : 0.37196746885385773
Loss in iteration 199 : 0.3718521771143256
Loss in iteration 200 : 0.3717380720804042
Testing accuracy  of updater 1 on alg 1 with rate 0.009999999999999995 = 0.8415330753639212, training accuracy 0.8374078624078624, time elapsed: 4009 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.5367658465716663
Loss in iteration 3 : 1.3052209952406877
Loss in iteration 4 : 0.7001803862902508
Loss in iteration 5 : 0.6662779575579437
Loss in iteration 6 : 0.5416840751894345
Loss in iteration 7 : 0.5018309433006266
Loss in iteration 8 : 0.50039202179943
Loss in iteration 9 : 0.4956551817453353
Loss in iteration 10 : 0.48724031374941895
Loss in iteration 11 : 0.4753620422560926
Loss in iteration 12 : 0.4613268298651334
Loss in iteration 13 : 0.44640801368372096
Loss in iteration 14 : 0.4314647404751176
Loss in iteration 15 : 0.41796477528273934
Loss in iteration 16 : 0.40745891501918735
Loss in iteration 17 : 0.40067168852828466
Loss in iteration 18 : 0.39789173769181707
Loss in iteration 19 : 0.397573036726638
Loss in iteration 20 : 0.3976736837831974
Loss in iteration 21 : 0.39629947485164424
Loss in iteration 22 : 0.3930556292102105
Loss in iteration 23 : 0.3881881919852257
Loss in iteration 24 : 0.38246338066664975
Loss in iteration 25 : 0.3770072214114962
Loss in iteration 26 : 0.3726918719345201
Loss in iteration 27 : 0.3695511119562059
Loss in iteration 28 : 0.3675618958682814
Loss in iteration 29 : 0.36642282881992877
Loss in iteration 30 : 0.3657288026459315
Loss in iteration 31 : 0.36512149790733367
Loss in iteration 32 : 0.36442211608094477
Loss in iteration 33 : 0.36370328668360863
Loss in iteration 34 : 0.3634141626045892
Loss in iteration 35 : 0.3634390163791024
Loss in iteration 36 : 0.3655130083438551
Loss in iteration 37 : 0.37049056597487795
Loss in iteration 38 : 0.3839232416566327
Loss in iteration 39 : 0.39502976173199517
Loss in iteration 40 : 0.42433108174105594
Loss in iteration 41 : 0.41425789308019034
Loss in iteration 42 : 0.44795121980546465
Loss in iteration 43 : 0.40644281456974046
Loss in iteration 44 : 0.41961715479472395
Loss in iteration 45 : 0.39881637141522114
Loss in iteration 46 : 0.40514725460873974
Loss in iteration 47 : 0.391411539943375
Loss in iteration 48 : 0.3929794610082012
Loss in iteration 49 : 0.38582282251030137
Loss in iteration 50 : 0.3866200681932886
Loss in iteration 51 : 0.3810155342920084
Loss in iteration 52 : 0.38277344985798156
Loss in iteration 53 : 0.3788250968983759
Loss in iteration 54 : 0.3815719100399092
Loss in iteration 55 : 0.3808758758140461
Loss in iteration 56 : 0.38799427818890087
Loss in iteration 57 : 0.38653678130717883
Loss in iteration 58 : 0.3993839106343889
Loss in iteration 59 : 0.3922800417686821
Loss in iteration 60 : 0.4048691386279197
Loss in iteration 61 : 0.39427368215517306
Loss in iteration 62 : 0.4067640700849414
Loss in iteration 63 : 0.39340019081476696
Loss in iteration 64 : 0.40301677469486885
Loss in iteration 65 : 0.3916229721419091
Loss in iteration 66 : 0.3997972566474395
Loss in iteration 67 : 0.38760577258929046
Loss in iteration 68 : 0.3936368882556763
Loss in iteration 69 : 0.38668147984780143
Loss in iteration 70 : 0.3930202356237739
Loss in iteration 71 : 0.3852885152368971
Loss in iteration 72 : 0.392059655797881
Loss in iteration 73 : 0.3847853071365314
Loss in iteration 74 : 0.391849072635758
Loss in iteration 75 : 0.3860434925081052
Loss in iteration 76 : 0.39622535252687613
Loss in iteration 77 : 0.38912262166702305
Loss in iteration 78 : 0.4007935879627193
Loss in iteration 79 : 0.39093207226097887
Loss in iteration 80 : 0.4009168604633106
Loss in iteration 81 : 0.390791101850111
Loss in iteration 82 : 0.4006054029223111
Loss in iteration 83 : 0.38958612498908857
Loss in iteration 84 : 0.3966218072612366
Loss in iteration 85 : 0.38605296736128586
Loss in iteration 86 : 0.39279963606923146
Loss in iteration 87 : 0.38420001603679316
Loss in iteration 88 : 0.39043284807353856
Loss in iteration 89 : 0.384677988529593
Loss in iteration 90 : 0.39252143838507414
Loss in iteration 91 : 0.38592893331207495
Loss in iteration 92 : 0.3964221973312553
Loss in iteration 93 : 0.38989233378050614
Loss in iteration 94 : 0.4021848560347344
Loss in iteration 95 : 0.3919508665162947
Loss in iteration 96 : 0.4029860225904877
Loss in iteration 97 : 0.3914045566381871
Loss in iteration 98 : 0.40129296398710784
Loss in iteration 99 : 0.3904235102642759
Loss in iteration 100 : 0.3989304361388281
Loss in iteration 101 : 0.38725061569013025
Loss in iteration 102 : 0.39386078888726916
Loss in iteration 103 : 0.38519027756367813
Loss in iteration 104 : 0.3914222969015767
Loss in iteration 105 : 0.3844938444675685
Loss in iteration 106 : 0.39076793495513024
Loss in iteration 107 : 0.38503689491182264
Loss in iteration 108 : 0.3938245569846483
Loss in iteration 109 : 0.38703669585319517
Loss in iteration 110 : 0.3976546517625939
Loss in iteration 111 : 0.3897592403339366
Loss in iteration 112 : 0.4014556246471403
Loss in iteration 113 : 0.39127500496040596
Loss in iteration 114 : 0.40330813381097375
Loss in iteration 115 : 0.39144736113399264
Loss in iteration 116 : 0.40082362046423786
Loss in iteration 117 : 0.3904529210385237
Loss in iteration 118 : 0.3985002789256787
Loss in iteration 119 : 0.38673474228374455
Loss in iteration 120 : 0.3922797194752515
Loss in iteration 121 : 0.3848444079278447
Loss in iteration 122 : 0.3916186379937717
Loss in iteration 123 : 0.3842590118806302
Loss in iteration 124 : 0.39082058188867297
Loss in iteration 125 : 0.3847073172585083
Loss in iteration 126 : 0.3937106618024683
Loss in iteration 127 : 0.3866535707701626
Loss in iteration 128 : 0.3970026044615865
Loss in iteration 129 : 0.3898145084358557
Loss in iteration 130 : 0.4016837680997841
Loss in iteration 131 : 0.3913318826541563
Loss in iteration 132 : 0.4031339459679
Loss in iteration 133 : 0.3909886458097138
Loss in iteration 134 : 0.400122095951602
Loss in iteration 135 : 0.38978004683624246
Loss in iteration 136 : 0.3973070265160431
Loss in iteration 137 : 0.38508616070663015
Loss in iteration 138 : 0.3913178192859762
Loss in iteration 139 : 0.3843813838818633
Loss in iteration 140 : 0.3911869612032406
Loss in iteration 141 : 0.3841400422747701
Loss in iteration 142 : 0.3910765091024948
Loss in iteration 143 : 0.38485071098537155
Loss in iteration 144 : 0.3943914670740173
Loss in iteration 145 : 0.3872627176200075
Loss in iteration 146 : 0.3989004033158411
Loss in iteration 147 : 0.39094782257405986
Loss in iteration 148 : 0.4037184273143244
Loss in iteration 149 : 0.3916672818733157
Loss in iteration 150 : 0.4028599857610453
Loss in iteration 151 : 0.39074350475938996
Loss in iteration 152 : 0.3988287728481685
Loss in iteration 153 : 0.3878262288394053
Loss in iteration 154 : 0.3945797073227723
Loss in iteration 155 : 0.38484290376677555
Loss in iteration 156 : 0.3914080236182409
Loss in iteration 157 : 0.38441507167881644
Loss in iteration 158 : 0.3915287140847159
Loss in iteration 159 : 0.38455310067831955
Loss in iteration 160 : 0.39203314553151547
Loss in iteration 161 : 0.3858115456802952
Loss in iteration 162 : 0.396019400509559
Loss in iteration 163 : 0.388739760049722
Loss in iteration 164 : 0.4005456084699823
Loss in iteration 165 : 0.3913852175928235
Loss in iteration 166 : 0.4033553226549049
Loss in iteration 167 : 0.3910352609234099
Loss in iteration 168 : 0.4006042854429464
Loss in iteration 169 : 0.3901151407216028
Loss in iteration 170 : 0.3986538290280693
Loss in iteration 171 : 0.38696969507164763
Loss in iteration 172 : 0.39286006632444576
Loss in iteration 173 : 0.3848008127094088
Loss in iteration 174 : 0.3915639596709888
Loss in iteration 175 : 0.38420061932517935
Loss in iteration 176 : 0.3907805099232211
Loss in iteration 177 : 0.3847245863821145
Loss in iteration 178 : 0.39340869830589953
Loss in iteration 179 : 0.38592410516908837
Loss in iteration 180 : 0.39589516787108575
Loss in iteration 181 : 0.3887868106937787
Loss in iteration 182 : 0.3999460036934893
Loss in iteration 183 : 0.390703288064944
Loss in iteration 184 : 0.40254568801600427
Loss in iteration 185 : 0.3909742935651439
Loss in iteration 186 : 0.4007560932059946
Loss in iteration 187 : 0.3897418154863734
Loss in iteration 188 : 0.397369928878786
Loss in iteration 189 : 0.3853606221411219
Loss in iteration 190 : 0.3911558030217329
Loss in iteration 191 : 0.3843831848707616
Loss in iteration 192 : 0.39138816873374743
Loss in iteration 193 : 0.38492863762980245
Loss in iteration 194 : 0.3921175537110019
Loss in iteration 195 : 0.38564956002834183
Loss in iteration 196 : 0.39528953247130655
Loss in iteration 197 : 0.3869623040389777
Loss in iteration 198 : 0.3974886243529656
Loss in iteration 199 : 0.3893819411511943
Loss in iteration 200 : 0.4007854779596879
Testing accuracy  of updater 2 on alg 1 with rate 1.0 = 0.8469381487623611, training accuracy 0.8493243243243244, time elapsed: 3549 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.1479811785952547
Loss in iteration 3 : 0.9859298886878286
Loss in iteration 4 : 0.571334651480975
Loss in iteration 5 : 0.5544796737836005
Loss in iteration 6 : 0.48906172715010177
Loss in iteration 7 : 0.43525495642943673
Loss in iteration 8 : 0.43450084869279504
Loss in iteration 9 : 0.42979431751354247
Loss in iteration 10 : 0.4241313177731186
Loss in iteration 11 : 0.4169566148948253
Loss in iteration 12 : 0.40879105547364664
Loss in iteration 13 : 0.4004283889356299
Loss in iteration 14 : 0.3922891856585973
Loss in iteration 15 : 0.38503825083130594
Loss in iteration 16 : 0.37952372363322173
Loss in iteration 17 : 0.3763292486780964
Loss in iteration 18 : 0.3751005813432428
Loss in iteration 19 : 0.37501521046810216
Loss in iteration 20 : 0.37510210315016035
Loss in iteration 21 : 0.3746706528688982
Loss in iteration 22 : 0.37339669309630213
Loss in iteration 23 : 0.3712740455495561
Loss in iteration 24 : 0.3686330639207578
Loss in iteration 25 : 0.3659736652565456
Loss in iteration 26 : 0.3636422711464847
Loss in iteration 27 : 0.36179251761110215
Loss in iteration 28 : 0.36048528723346923
Loss in iteration 29 : 0.3596102163787526
Loss in iteration 30 : 0.35910200073718074
Loss in iteration 31 : 0.35875914584427
Loss in iteration 32 : 0.35845055654893715
Loss in iteration 33 : 0.3581202420048442
Loss in iteration 34 : 0.35773875914260805
Loss in iteration 35 : 0.35731951099693665
Loss in iteration 36 : 0.3568669163774121
Loss in iteration 37 : 0.3564255510016307
Loss in iteration 38 : 0.3560080752174618
Loss in iteration 39 : 0.3556034020669124
Loss in iteration 40 : 0.3552151783584453
Loss in iteration 41 : 0.35489076067652564
Loss in iteration 42 : 0.35472405231058773
Loss in iteration 43 : 0.35491991263184314
Loss in iteration 44 : 0.35487046874797307
Loss in iteration 45 : 0.354721171794082
Loss in iteration 46 : 0.35456908090212064
Loss in iteration 47 : 0.35458239317599577
Loss in iteration 48 : 0.3546314518268792
Loss in iteration 49 : 0.35449811986342444
Loss in iteration 50 : 0.3544690502341966
Loss in iteration 51 : 0.3541842281885013
Loss in iteration 52 : 0.35412953659158214
Loss in iteration 53 : 0.35375290212061744
Loss in iteration 54 : 0.3537091784055418
Loss in iteration 55 : 0.3533628807855001
Loss in iteration 56 : 0.35323707371146046
Loss in iteration 57 : 0.35312550138619125
Loss in iteration 58 : 0.3530750479385449
Loss in iteration 59 : 0.35300244962304833
Loss in iteration 60 : 0.35297972509093006
Loss in iteration 61 : 0.35298188248737716
Loss in iteration 62 : 0.3530188322519378
Loss in iteration 63 : 0.35303020734689994
Loss in iteration 64 : 0.3531896455302656
Loss in iteration 65 : 0.3532766661719569
Loss in iteration 66 : 0.3534401958580174
Loss in iteration 67 : 0.3534313355414455
Loss in iteration 68 : 0.3535901958864539
Loss in iteration 69 : 0.3534430086748815
Loss in iteration 70 : 0.35356764905523286
Loss in iteration 71 : 0.353406489937389
Loss in iteration 72 : 0.35348905849667206
Loss in iteration 73 : 0.3533533137349574
Loss in iteration 74 : 0.3534693936491414
Loss in iteration 75 : 0.35336226742481236
Loss in iteration 76 : 0.3534167192796978
Loss in iteration 77 : 0.3533481757733001
Loss in iteration 78 : 0.35345863609933886
Loss in iteration 79 : 0.35332079724743887
Loss in iteration 80 : 0.35343944090321566
Loss in iteration 81 : 0.3533070886500411
Loss in iteration 82 : 0.3534347875374014
Loss in iteration 83 : 0.35331609842907413
Loss in iteration 84 : 0.3534150658981418
Loss in iteration 85 : 0.35334557000905015
Loss in iteration 86 : 0.3535410781500147
Loss in iteration 87 : 0.3535910902208921
Loss in iteration 88 : 0.3538429086922216
Loss in iteration 89 : 0.3539516832398181
Loss in iteration 90 : 0.3544942165713846
Loss in iteration 91 : 0.3548969257428515
Loss in iteration 92 : 0.3549581172539812
Loss in iteration 93 : 0.35507027882562386
Loss in iteration 94 : 0.35513516088407804
Loss in iteration 95 : 0.3549190752359112
Loss in iteration 96 : 0.3549877698854851
Loss in iteration 97 : 0.35443734349033723
Loss in iteration 98 : 0.3543486302322584
Loss in iteration 99 : 0.3537586749236103
Loss in iteration 100 : 0.3534174505707651
Loss in iteration 101 : 0.35336313571637895
Loss in iteration 102 : 0.3533970779166148
Loss in iteration 103 : 0.3534168787387431
Loss in iteration 104 : 0.35339200666530973
Loss in iteration 105 : 0.35334219565362895
Loss in iteration 106 : 0.3533431083295091
Loss in iteration 107 : 0.3533187279667142
Loss in iteration 108 : 0.3532625186534912
Loss in iteration 109 : 0.3531829095258982
Loss in iteration 110 : 0.35319107492209784
Loss in iteration 111 : 0.3531498005002855
Loss in iteration 112 : 0.3531947467895838
Loss in iteration 113 : 0.3531974452775388
Loss in iteration 114 : 0.35337679337558225
Loss in iteration 115 : 0.3534592210019638
Loss in iteration 116 : 0.3538821777103918
Loss in iteration 117 : 0.35387341312786785
Loss in iteration 118 : 0.3540978614275074
Loss in iteration 119 : 0.35417131907472177
Loss in iteration 120 : 0.35443536898189654
Loss in iteration 121 : 0.35467138449546387
Loss in iteration 122 : 0.3549818414836107
Loss in iteration 123 : 0.355035751764254
Loss in iteration 124 : 0.3548987689001728
Loss in iteration 125 : 0.354407107696129
Loss in iteration 126 : 0.35452882390206414
Loss in iteration 127 : 0.3536431738361114
Loss in iteration 128 : 0.35338629788383646
Loss in iteration 129 : 0.35321799986715446
Loss in iteration 130 : 0.35322000048077373
Loss in iteration 131 : 0.35311583261602547
Loss in iteration 132 : 0.3530586247941906
Loss in iteration 133 : 0.35287837209283124
Loss in iteration 134 : 0.352920030592073
Loss in iteration 135 : 0.35279837699173294
Loss in iteration 136 : 0.3528302377267558
Loss in iteration 137 : 0.35283156266882193
Loss in iteration 138 : 0.3529260619997649
Loss in iteration 139 : 0.35300866487141647
Loss in iteration 140 : 0.3532141927948162
Loss in iteration 141 : 0.3534042292190565
Loss in iteration 142 : 0.35365917123893914
Loss in iteration 143 : 0.3536395047772908
Loss in iteration 144 : 0.353779603263572
Loss in iteration 145 : 0.35366440652769854
Loss in iteration 146 : 0.3538696850470401
Loss in iteration 147 : 0.3540298080185531
Loss in iteration 148 : 0.35436226096958245
Loss in iteration 149 : 0.35453784631334423
Loss in iteration 150 : 0.35479838389911234
Loss in iteration 151 : 0.35480667632304336
Loss in iteration 152 : 0.35478240620094464
Loss in iteration 153 : 0.35446229433837134
Loss in iteration 154 : 0.3543628831121005
Loss in iteration 155 : 0.3535507540812231
Loss in iteration 156 : 0.35361245550519416
Loss in iteration 157 : 0.3532472985658632
Loss in iteration 158 : 0.3531383338746241
Loss in iteration 159 : 0.3529811153533451
Loss in iteration 160 : 0.3530075931007217
Loss in iteration 161 : 0.3528124996409159
Loss in iteration 162 : 0.35286511071647825
Loss in iteration 163 : 0.35279076039331103
Loss in iteration 164 : 0.352844416746036
Loss in iteration 165 : 0.3527942328644346
Loss in iteration 166 : 0.3528966828904735
Loss in iteration 167 : 0.3528386573662896
Loss in iteration 168 : 0.35296945758804193
Loss in iteration 169 : 0.3530227192094291
Loss in iteration 170 : 0.35332595705576536
Loss in iteration 171 : 0.3533363105258228
Loss in iteration 172 : 0.35357746405561713
Loss in iteration 173 : 0.35357342942795245
Loss in iteration 174 : 0.3538298578741157
Loss in iteration 175 : 0.35402366026077897
Loss in iteration 176 : 0.3542332449228792
Loss in iteration 177 : 0.3544564149179138
Loss in iteration 178 : 0.35464198802540314
Loss in iteration 179 : 0.3545894479779258
Loss in iteration 180 : 0.3546962237251515
Loss in iteration 181 : 0.3544295035347124
Loss in iteration 182 : 0.35442019828601207
Loss in iteration 183 : 0.35352586649791173
Loss in iteration 184 : 0.3534392699961241
Loss in iteration 185 : 0.3532097617367518
Loss in iteration 186 : 0.35312284407804145
Loss in iteration 187 : 0.35287698903870895
Loss in iteration 188 : 0.3529298293065292
Loss in iteration 189 : 0.35274673120676564
Loss in iteration 190 : 0.3527967240543152
Loss in iteration 191 : 0.35271527510015377
Loss in iteration 192 : 0.3527548051396295
Loss in iteration 193 : 0.352693884306467
Loss in iteration 194 : 0.3528287809747436
Loss in iteration 195 : 0.3527344692093772
Loss in iteration 196 : 0.35287467380694526
Loss in iteration 197 : 0.352891545898013
Loss in iteration 198 : 0.35320821211020426
Loss in iteration 199 : 0.35319106973949527
Loss in iteration 200 : 0.35350708021287075
Testing accuracy  of updater 2 on alg 1 with rate 0.7000000000000001 = 0.848965051286776, training accuracy 0.8490479115479116, time elapsed: 2673 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7592039750466353
Loss in iteration 3 : 0.6670423908120474
Loss in iteration 4 : 0.4621898825109873
Loss in iteration 5 : 0.4423247296165374
Loss in iteration 6 : 0.4344867353472321
Loss in iteration 7 : 0.40021832691517817
Loss in iteration 8 : 0.3931652975703656
Loss in iteration 9 : 0.38803938423199036
Loss in iteration 10 : 0.3846766456325086
Loss in iteration 11 : 0.38214090014703106
Loss in iteration 12 : 0.3793598797730281
Loss in iteration 13 : 0.3763175864458631
Loss in iteration 14 : 0.3731231062971221
Loss in iteration 15 : 0.36987611040224655
Loss in iteration 16 : 0.3670112221218685
Loss in iteration 17 : 0.36480875124621787
Loss in iteration 18 : 0.36332052965731326
Loss in iteration 19 : 0.36252893968072636
Loss in iteration 20 : 0.3622918785399518
Loss in iteration 21 : 0.36229449307220596
Loss in iteration 22 : 0.3622287545359063
Loss in iteration 23 : 0.3619966724554871
Loss in iteration 24 : 0.3615564961253375
Loss in iteration 25 : 0.360958049134428
Loss in iteration 26 : 0.36024300725332853
Loss in iteration 27 : 0.3594795199547812
Loss in iteration 28 : 0.35871887164483063
Loss in iteration 29 : 0.35803321812100025
Loss in iteration 30 : 0.3574530546489563
Loss in iteration 31 : 0.3569457966157211
Loss in iteration 32 : 0.35649256529194656
Loss in iteration 33 : 0.3560958623108841
Loss in iteration 34 : 0.3557581213117004
Loss in iteration 35 : 0.35547264534102285
Loss in iteration 36 : 0.355240636268969
Loss in iteration 37 : 0.3550415253784945
Loss in iteration 38 : 0.3548667865344701
Loss in iteration 39 : 0.35469653957374697
Loss in iteration 40 : 0.3545414355602369
Loss in iteration 41 : 0.35440334943037105
Loss in iteration 42 : 0.35427243704167316
Loss in iteration 43 : 0.3541459585890363
Loss in iteration 44 : 0.35403384209364286
Loss in iteration 45 : 0.3539214038401941
Loss in iteration 46 : 0.3538113777855047
Loss in iteration 47 : 0.353718701439743
Loss in iteration 48 : 0.3536225682486503
Loss in iteration 49 : 0.35353952846281944
Loss in iteration 50 : 0.3534788469970708
Loss in iteration 51 : 0.3534252108460822
Loss in iteration 52 : 0.3533556385672571
Loss in iteration 53 : 0.35328910288151083
Loss in iteration 54 : 0.3532643309277497
Loss in iteration 55 : 0.3531636555233738
Loss in iteration 56 : 0.3531252942209013
Loss in iteration 57 : 0.3530687211083338
Loss in iteration 58 : 0.3530264977913478
Loss in iteration 59 : 0.3529927840544126
Loss in iteration 60 : 0.35296175931918244
Loss in iteration 61 : 0.3529358924621628
Loss in iteration 62 : 0.35291090552845494
Loss in iteration 63 : 0.3528884324572767
Loss in iteration 64 : 0.35286317653938115
Loss in iteration 65 : 0.35283919595186297
Loss in iteration 66 : 0.3528162282241306
Loss in iteration 67 : 0.3527955033525683
Loss in iteration 68 : 0.3527723993863141
Loss in iteration 69 : 0.35275261751413955
Loss in iteration 70 : 0.35273444889438554
Loss in iteration 71 : 0.3527174367596967
Loss in iteration 72 : 0.35269811284421315
Loss in iteration 73 : 0.3526816633057647
Loss in iteration 74 : 0.3526656538331317
Loss in iteration 75 : 0.3526501278258509
Loss in iteration 76 : 0.3526352615269658
Loss in iteration 77 : 0.3526235943830071
Loss in iteration 78 : 0.3526161936878423
Loss in iteration 79 : 0.35261680286101615
Loss in iteration 80 : 0.35263973770973156
Loss in iteration 81 : 0.35258469286455585
Loss in iteration 82 : 0.35258358495076997
Loss in iteration 83 : 0.3525557424397029
Loss in iteration 84 : 0.3525363569321641
Loss in iteration 85 : 0.3525295996164815
Loss in iteration 86 : 0.3525164383389517
Loss in iteration 87 : 0.35250828030539333
Loss in iteration 88 : 0.3524996061202044
Loss in iteration 89 : 0.35250802386464913
Loss in iteration 90 : 0.3525238847006044
Loss in iteration 91 : 0.35248728256908113
Loss in iteration 92 : 0.3524949927321569
Loss in iteration 93 : 0.35246323097857085
Loss in iteration 94 : 0.3524532037729324
Loss in iteration 95 : 0.3524325626011478
Loss in iteration 96 : 0.3524151913261234
Loss in iteration 97 : 0.35240632450356246
Loss in iteration 98 : 0.35239535241739495
Loss in iteration 99 : 0.3523857360410467
Loss in iteration 100 : 0.3523768215723929
Loss in iteration 101 : 0.3523681041318561
Loss in iteration 102 : 0.35235961357600953
Loss in iteration 103 : 0.3523514316517875
Loss in iteration 104 : 0.3523444789365134
Loss in iteration 105 : 0.35233679947768803
Loss in iteration 106 : 0.35233084984084845
Loss in iteration 107 : 0.35234704543775264
Loss in iteration 108 : 0.35235104212507645
Loss in iteration 109 : 0.3523817050968882
Loss in iteration 110 : 0.3523257611604964
Loss in iteration 111 : 0.35233778593796566
Loss in iteration 112 : 0.3523051356502727
Loss in iteration 113 : 0.3522867816741793
Loss in iteration 114 : 0.3522849443677918
Loss in iteration 115 : 0.3522854538633239
Loss in iteration 116 : 0.35228815641971584
Loss in iteration 117 : 0.3523218271751673
Loss in iteration 118 : 0.35227547047880897
Loss in iteration 119 : 0.35228015279143216
Loss in iteration 120 : 0.3522568684323477
Loss in iteration 121 : 0.3522396153643406
Loss in iteration 122 : 0.3522322819088757
Loss in iteration 123 : 0.35222077917435396
Loss in iteration 124 : 0.3522151745640344
Loss in iteration 125 : 0.3522077088381155
Loss in iteration 126 : 0.3522006915165664
Loss in iteration 127 : 0.3521945301981738
Loss in iteration 128 : 0.35218905958016844
Loss in iteration 129 : 0.35218242165536817
Loss in iteration 130 : 0.35217732225477116
Loss in iteration 131 : 0.3521710297734331
Loss in iteration 132 : 0.35216732651658467
Loss in iteration 133 : 0.352175159584396
Loss in iteration 134 : 0.3521960659662597
Loss in iteration 135 : 0.35220291247014784
Loss in iteration 136 : 0.35218056277984494
Loss in iteration 137 : 0.3521952475389912
Loss in iteration 138 : 0.3521617794210727
Loss in iteration 139 : 0.352156787542802
Loss in iteration 140 : 0.35213653092276886
Loss in iteration 141 : 0.35212422603206495
Loss in iteration 142 : 0.3521199509628769
Loss in iteration 143 : 0.35211261756093326
Loss in iteration 144 : 0.3521049114955882
Loss in iteration 145 : 0.3520998360097882
Loss in iteration 146 : 0.3520945581218916
Loss in iteration 147 : 0.3520892744747521
Loss in iteration 148 : 0.35208398710228656
Loss in iteration 149 : 0.35207868127692415
Loss in iteration 150 : 0.3520736025380409
Loss in iteration 151 : 0.35206906019911166
Loss in iteration 152 : 0.35207009839204323
Loss in iteration 153 : 0.35208652102312116
Loss in iteration 154 : 0.3520861239941055
Loss in iteration 155 : 0.35211247233412074
Loss in iteration 156 : 0.35209040370024725
Loss in iteration 157 : 0.35210300756570595
Loss in iteration 158 : 0.3520664917813479
Loss in iteration 159 : 0.3520464478152935
Loss in iteration 160 : 0.35204292498279643
Loss in iteration 161 : 0.35202973125564124
Loss in iteration 162 : 0.35202473720894584
Loss in iteration 163 : 0.3520188892956827
Loss in iteration 164 : 0.35201376408113516
Loss in iteration 165 : 0.3520091366656592
Loss in iteration 166 : 0.35200457129601176
Loss in iteration 167 : 0.35200003388632195
Loss in iteration 168 : 0.3519954877404512
Loss in iteration 169 : 0.3519911015395137
Loss in iteration 170 : 0.3519869879373285
Loss in iteration 171 : 0.35198422212618424
Loss in iteration 172 : 0.3519846733578716
Loss in iteration 173 : 0.3520050922385533
Loss in iteration 174 : 0.35200813608546294
Loss in iteration 175 : 0.35203721580329267
Loss in iteration 176 : 0.3520044803537516
Loss in iteration 177 : 0.35202077376277535
Loss in iteration 178 : 0.3519789670875323
Loss in iteration 179 : 0.35196585304612027
Loss in iteration 180 : 0.3519603374888402
Loss in iteration 181 : 0.35195008802092537
Loss in iteration 182 : 0.35194641312198105
Loss in iteration 183 : 0.3519420543616883
Loss in iteration 184 : 0.35194049179607817
Loss in iteration 185 : 0.3519380967138346
Loss in iteration 186 : 0.3519379671864757
Loss in iteration 187 : 0.35195002498845074
Loss in iteration 188 : 0.3519607580863902
Loss in iteration 189 : 0.3519668719611558
Loss in iteration 190 : 0.3519499336869362
Loss in iteration 191 : 0.3519534254546412
Loss in iteration 192 : 0.3519290207647677
Loss in iteration 193 : 0.35190967907550114
Loss in iteration 194 : 0.3519054618828674
Loss in iteration 195 : 0.35190369269977423
Loss in iteration 196 : 0.35189803615874393
Loss in iteration 197 : 0.35189317894114774
Loss in iteration 198 : 0.35188832177504187
Loss in iteration 199 : 0.35188471221150375
Loss in iteration 200 : 0.35188078361893105
Testing accuracy  of updater 2 on alg 1 with rate 0.4 = 0.8506234260794792, training accuracy 0.8489250614250614, time elapsed: 2770 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6549966671244581
Loss in iteration 3 : 0.5561764172418024
Loss in iteration 4 : 0.5844209549164371
Loss in iteration 5 : 0.5698267485349855
Loss in iteration 6 : 0.5212280733852648
Loss in iteration 7 : 0.4665887576763199
Loss in iteration 8 : 0.44280054279347786
Loss in iteration 9 : 0.4379839840855104
Loss in iteration 10 : 0.4338660901106495
Loss in iteration 11 : 0.4289986780735409
Loss in iteration 12 : 0.42149252752869215
Loss in iteration 13 : 0.4114871355472184
Loss in iteration 14 : 0.40107029901055685
Loss in iteration 15 : 0.39390853602617526
Loss in iteration 16 : 0.3902804372748307
Loss in iteration 17 : 0.38800923603438253
Loss in iteration 18 : 0.38555737725386796
Loss in iteration 19 : 0.38306719389387445
Loss in iteration 20 : 0.38095804533147415
Loss in iteration 21 : 0.3792855395587801
Loss in iteration 22 : 0.377878344088944
Loss in iteration 23 : 0.37655656449004477
Loss in iteration 24 : 0.3751928422488333
Loss in iteration 25 : 0.3737601139745703
Loss in iteration 26 : 0.3722828523210692
Loss in iteration 27 : 0.37082224060975133
Loss in iteration 28 : 0.3694257307032381
Loss in iteration 29 : 0.36812384827417666
Loss in iteration 30 : 0.3669060498911252
Loss in iteration 31 : 0.3658048428368854
Loss in iteration 32 : 0.36482722662566386
Loss in iteration 33 : 0.36396882583375545
Loss in iteration 34 : 0.36321808217609863
Loss in iteration 35 : 0.3625613715539966
Loss in iteration 36 : 0.362064998471041
Loss in iteration 37 : 0.36168066912424574
Loss in iteration 38 : 0.3613431282347676
Loss in iteration 39 : 0.3610546331579231
Loss in iteration 40 : 0.36082387680139216
Loss in iteration 41 : 0.3606220192997414
Loss in iteration 42 : 0.36043315661626
Loss in iteration 43 : 0.3602453025846249
Loss in iteration 44 : 0.3600558984527535
Loss in iteration 45 : 0.3598650572361107
Loss in iteration 46 : 0.35967123319938904
Loss in iteration 47 : 0.35947674075177716
Loss in iteration 48 : 0.35928527181943803
Loss in iteration 49 : 0.3591031586210934
Loss in iteration 50 : 0.35893048170188746
Loss in iteration 51 : 0.3587699628700221
Loss in iteration 52 : 0.35862012590999576
Loss in iteration 53 : 0.35847757870616753
Loss in iteration 54 : 0.35834150992737834
Loss in iteration 55 : 0.3582114256226036
Loss in iteration 56 : 0.35808861657084223
Loss in iteration 57 : 0.35796981540225065
Loss in iteration 58 : 0.3578553102467731
Loss in iteration 59 : 0.3577444981348473
Loss in iteration 60 : 0.35763658729042197
Loss in iteration 61 : 0.3575311896942958
Loss in iteration 62 : 0.35742808031960854
Loss in iteration 63 : 0.35732847835526105
Loss in iteration 64 : 0.35723313906501003
Loss in iteration 65 : 0.35714043786417604
Loss in iteration 66 : 0.35705039188777143
Loss in iteration 67 : 0.35696292402569174
Loss in iteration 68 : 0.3568789490046743
Loss in iteration 69 : 0.35679711653410917
Loss in iteration 70 : 0.35671703422526774
Loss in iteration 71 : 0.3566389676041092
Loss in iteration 72 : 0.35656384695359
Loss in iteration 73 : 0.3564918540441919
Loss in iteration 74 : 0.3564207901989665
Loss in iteration 75 : 0.356351457735952
Loss in iteration 76 : 0.356284892146956
Loss in iteration 77 : 0.3562203356135862
Loss in iteration 78 : 0.3561574783918616
Loss in iteration 79 : 0.3560961171817815
Loss in iteration 80 : 0.35603615860345317
Loss in iteration 81 : 0.3559776998968307
Loss in iteration 82 : 0.3559204960206295
Loss in iteration 83 : 0.3558649176629581
Loss in iteration 84 : 0.35581095422072684
Loss in iteration 85 : 0.3557580382960207
Loss in iteration 86 : 0.35570667952348073
Loss in iteration 87 : 0.3556566642335437
Loss in iteration 88 : 0.3556076087643847
Loss in iteration 89 : 0.35556002217071814
Loss in iteration 90 : 0.3555132355731325
Loss in iteration 91 : 0.35546761166633656
Loss in iteration 92 : 0.35542348752243647
Loss in iteration 93 : 0.35538013425936527
Loss in iteration 94 : 0.3553383901803475
Loss in iteration 95 : 0.35529722588041646
Loss in iteration 96 : 0.355257890452581
Loss in iteration 97 : 0.35521898570454463
Loss in iteration 98 : 0.3551802890566306
Loss in iteration 99 : 0.35514317988317906
Loss in iteration 100 : 0.35510615212195096
Loss in iteration 101 : 0.35506957315631965
Loss in iteration 102 : 0.35503390355906456
Loss in iteration 103 : 0.35499861826068735
Loss in iteration 104 : 0.3549637584771494
Loss in iteration 105 : 0.35492968267918235
Loss in iteration 106 : 0.3548962204476767
Loss in iteration 107 : 0.3548629144858358
Loss in iteration 108 : 0.35482984975543147
Loss in iteration 109 : 0.3547976535771815
Loss in iteration 110 : 0.35476586462748366
Loss in iteration 111 : 0.3547346189431848
Loss in iteration 112 : 0.3547036392941842
Loss in iteration 113 : 0.35467333453954203
Loss in iteration 114 : 0.3546440907374719
Loss in iteration 115 : 0.35461534199367084
Loss in iteration 116 : 0.35458645525815097
Loss in iteration 117 : 0.354558685326571
Loss in iteration 118 : 0.3545311850222774
Loss in iteration 119 : 0.3545040317448068
Loss in iteration 120 : 0.35447767615190245
Loss in iteration 121 : 0.35445147583043013
Loss in iteration 122 : 0.3544256774639743
Loss in iteration 123 : 0.35440043429112295
Loss in iteration 124 : 0.3543756204932978
Loss in iteration 125 : 0.35435092970497556
Loss in iteration 126 : 0.3543261874632886
Loss in iteration 127 : 0.35430253463896877
Loss in iteration 128 : 0.35427852173302565
Loss in iteration 129 : 0.35425509508535513
Loss in iteration 130 : 0.3542322668263097
Loss in iteration 131 : 0.35420992744278745
Loss in iteration 132 : 0.35418800189422894
Loss in iteration 133 : 0.3541666422072593
Loss in iteration 134 : 0.3541453346437442
Loss in iteration 135 : 0.3541244816060018
Loss in iteration 136 : 0.3541040374952029
Loss in iteration 137 : 0.3540842357266726
Loss in iteration 138 : 0.35406514372402875
Loss in iteration 139 : 0.3540466468705044
Loss in iteration 140 : 0.35402750477248435
Loss in iteration 141 : 0.35400925468237837
Loss in iteration 142 : 0.3539913781800067
Loss in iteration 143 : 0.3539741452060159
Loss in iteration 144 : 0.353957054758405
Loss in iteration 145 : 0.3539395009307845
Loss in iteration 146 : 0.353922948515968
Loss in iteration 147 : 0.3539069755374499
Loss in iteration 148 : 0.35389044045972456
Loss in iteration 149 : 0.35387501062838195
Loss in iteration 150 : 0.3538592524033873
Loss in iteration 151 : 0.3538441400963939
Loss in iteration 152 : 0.3538293992754305
Loss in iteration 153 : 0.35381486297822456
Loss in iteration 154 : 0.3538001085082894
Loss in iteration 155 : 0.3537857286113251
Loss in iteration 156 : 0.3537714971035114
Loss in iteration 157 : 0.3537574422458454
Loss in iteration 158 : 0.3537435351934416
Loss in iteration 159 : 0.3537298326888877
Loss in iteration 160 : 0.35371692750254213
Loss in iteration 161 : 0.3537037147050529
Loss in iteration 162 : 0.35369017043002526
Loss in iteration 163 : 0.3536769187365291
Loss in iteration 164 : 0.35366427781762255
Loss in iteration 165 : 0.3536510837880535
Loss in iteration 166 : 0.3536389744688139
Loss in iteration 167 : 0.35362588069974316
Loss in iteration 168 : 0.3536132984927262
Loss in iteration 169 : 0.3536009769011624
Loss in iteration 170 : 0.3535887284607627
Loss in iteration 171 : 0.35357673248885324
Loss in iteration 172 : 0.3535646988079865
Loss in iteration 173 : 0.3535527662633833
Loss in iteration 174 : 0.3535409162446996
Loss in iteration 175 : 0.3535293703946848
Loss in iteration 176 : 0.3535178546076862
Loss in iteration 177 : 0.3535061268282586
Loss in iteration 178 : 0.3534947164972076
Loss in iteration 179 : 0.353483530353514
Loss in iteration 180 : 0.3534723591839551
Loss in iteration 181 : 0.35346126491120633
Loss in iteration 182 : 0.35345025560366605
Loss in iteration 183 : 0.3534394191666784
Loss in iteration 184 : 0.35342872390117813
Loss in iteration 185 : 0.35341821468485085
Loss in iteration 186 : 0.3534078536924669
Loss in iteration 187 : 0.3533970451695088
Loss in iteration 188 : 0.35338698070055446
Loss in iteration 189 : 0.35337649367887564
Loss in iteration 190 : 0.35336645363359726
Loss in iteration 191 : 0.3533563853654723
Loss in iteration 192 : 0.35334648259290474
Loss in iteration 193 : 0.35333668136331003
Loss in iteration 194 : 0.3533269411857458
Loss in iteration 195 : 0.3533174832837292
Loss in iteration 196 : 0.35330772390843895
Loss in iteration 197 : 0.35329829536805724
Loss in iteration 198 : 0.3532890976542383
Loss in iteration 199 : 0.3532801566725357
Loss in iteration 200 : 0.353271546116445
Testing accuracy  of updater 2 on alg 1 with rate 0.1 = 0.8493950003071065, training accuracy 0.8466523341523342, time elapsed: 2635 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7584976669871213
Loss in iteration 3 : 0.4861135890811485
Loss in iteration 4 : 0.526620247372032
Loss in iteration 5 : 0.5367869017747847
Loss in iteration 6 : 0.5201838732152804
Loss in iteration 7 : 0.487376574492125
Loss in iteration 8 : 0.45886250979414994
Loss in iteration 9 : 0.4487332285809374
Loss in iteration 10 : 0.4451501362651023
Loss in iteration 11 : 0.4410510109132461
Loss in iteration 12 : 0.4354858492784428
Loss in iteration 13 : 0.42839525061787265
Loss in iteration 14 : 0.42034203291296546
Loss in iteration 15 : 0.41240584237575617
Loss in iteration 16 : 0.40561028062972526
Loss in iteration 17 : 0.40062565900006203
Loss in iteration 18 : 0.3966820198174252
Loss in iteration 19 : 0.3932661726882885
Loss in iteration 20 : 0.39009459439111893
Loss in iteration 21 : 0.38724955014266754
Loss in iteration 22 : 0.3847333292071802
Loss in iteration 23 : 0.38256306880806085
Loss in iteration 24 : 0.3807971994086859
Loss in iteration 25 : 0.3792818575849902
Loss in iteration 26 : 0.3778963829897813
Loss in iteration 27 : 0.37657145660725305
Loss in iteration 28 : 0.3752979781322999
Loss in iteration 29 : 0.37410535057335575
Loss in iteration 30 : 0.37298721580587474
Loss in iteration 31 : 0.37193259935624573
Loss in iteration 32 : 0.3709302526093353
Loss in iteration 33 : 0.36997969751254495
Loss in iteration 34 : 0.3690926641766715
Loss in iteration 35 : 0.3682678382495439
Loss in iteration 36 : 0.36750171195361825
Loss in iteration 37 : 0.3667900544503655
Loss in iteration 38 : 0.3661336547619063
Loss in iteration 39 : 0.3655321060858104
Loss in iteration 40 : 0.3650067816515464
Loss in iteration 41 : 0.3645453549539956
Loss in iteration 42 : 0.3641200262729906
Loss in iteration 43 : 0.3637347962266631
Loss in iteration 44 : 0.36338495189952996
Loss in iteration 45 : 0.36307007633992516
Loss in iteration 46 : 0.36278323350889663
Loss in iteration 47 : 0.36251541073275545
Loss in iteration 48 : 0.36226014196116957
Loss in iteration 49 : 0.3620166041248505
Loss in iteration 50 : 0.36178872007125834
Loss in iteration 51 : 0.36157352259321257
Loss in iteration 52 : 0.3613701505638603
Loss in iteration 53 : 0.36118188195030815
Loss in iteration 54 : 0.3610051437619232
Loss in iteration 55 : 0.3608349666439734
Loss in iteration 56 : 0.36067265172162455
Loss in iteration 57 : 0.3605167433711258
Loss in iteration 58 : 0.36036818234123924
Loss in iteration 59 : 0.36022495819949185
Loss in iteration 60 : 0.36008835954703733
Loss in iteration 61 : 0.35995760376278546
Loss in iteration 62 : 0.3598322731183704
Loss in iteration 63 : 0.3597113024539533
Loss in iteration 64 : 0.35959345720169594
Loss in iteration 65 : 0.35947879287438445
Loss in iteration 66 : 0.3593693025318989
Loss in iteration 67 : 0.35926345062596887
Loss in iteration 68 : 0.3591601172092565
Loss in iteration 69 : 0.35905962134139485
Loss in iteration 70 : 0.3589618605496377
Loss in iteration 71 : 0.35886687500136294
Loss in iteration 72 : 0.3587746355652732
Loss in iteration 73 : 0.3586851436138556
Loss in iteration 74 : 0.35859809594027287
Loss in iteration 75 : 0.35851310229555694
Loss in iteration 76 : 0.35842966329316683
Loss in iteration 77 : 0.3583482248346628
Loss in iteration 78 : 0.3582685640494361
Loss in iteration 79 : 0.35819066283745643
Loss in iteration 80 : 0.3581144382092145
Loss in iteration 81 : 0.3580396590471357
Loss in iteration 82 : 0.3579666518393453
Loss in iteration 83 : 0.35789544681856544
Loss in iteration 84 : 0.3578261099352535
Loss in iteration 85 : 0.3577583013502027
Loss in iteration 86 : 0.35769187415931325
Loss in iteration 87 : 0.3576264539002514
Loss in iteration 88 : 0.3575627765847904
Loss in iteration 89 : 0.35750051339890543
Loss in iteration 90 : 0.3574395837680423
Loss in iteration 91 : 0.3573800181108056
Loss in iteration 92 : 0.35732130243769195
Loss in iteration 93 : 0.3572633079314487
Loss in iteration 94 : 0.35720618052065195
Loss in iteration 95 : 0.35715002644610694
Loss in iteration 96 : 0.3570949203170031
Loss in iteration 97 : 0.35704105681096493
Loss in iteration 98 : 0.35698821680388326
Loss in iteration 99 : 0.35693622055115887
Loss in iteration 100 : 0.3568850184950729
Loss in iteration 101 : 0.35683440750169704
Loss in iteration 102 : 0.3567847212630359
Loss in iteration 103 : 0.3567358585330989
Loss in iteration 104 : 0.35668825323243974
Loss in iteration 105 : 0.3566412633521274
Loss in iteration 106 : 0.3565951230226494
Loss in iteration 107 : 0.356549442773688
Loss in iteration 108 : 0.35650446788591966
Loss in iteration 109 : 0.35646042072949674
Loss in iteration 110 : 0.35641694574540966
Loss in iteration 111 : 0.35637417286549267
Loss in iteration 112 : 0.3563316922715312
Loss in iteration 113 : 0.35628975943301877
Loss in iteration 114 : 0.3562482910787831
Loss in iteration 115 : 0.35620756496337636
Loss in iteration 116 : 0.35616744651766186
Loss in iteration 117 : 0.35612787656796807
Loss in iteration 118 : 0.3560889622890932
Loss in iteration 119 : 0.3560505724851462
Loss in iteration 120 : 0.35601275181891995
Loss in iteration 121 : 0.3559755390120922
Loss in iteration 122 : 0.35593900472988743
Loss in iteration 123 : 0.3559031032589769
Loss in iteration 124 : 0.35586769552278874
Loss in iteration 125 : 0.3558328496057384
Loss in iteration 126 : 0.3557983230151046
Loss in iteration 127 : 0.35576431645378187
Loss in iteration 128 : 0.3557306827061805
Loss in iteration 129 : 0.3556976898956452
Loss in iteration 130 : 0.35566506827418254
Loss in iteration 131 : 0.3556329403543196
Loss in iteration 132 : 0.3556011566512193
Loss in iteration 133 : 0.3555697594812207
Loss in iteration 134 : 0.3555388215272278
Loss in iteration 135 : 0.3555083828855285
Loss in iteration 136 : 0.35547874664623574
Loss in iteration 137 : 0.35544953873096913
Loss in iteration 138 : 0.35542044740358
Loss in iteration 139 : 0.3553922141063693
Loss in iteration 140 : 0.3553642614482438
Loss in iteration 141 : 0.3553365271392404
Loss in iteration 142 : 0.3553090308848061
Loss in iteration 143 : 0.35528194938923646
Loss in iteration 144 : 0.3552551645903935
Loss in iteration 145 : 0.35522864096699674
Loss in iteration 146 : 0.3552024175227112
Loss in iteration 147 : 0.35517649419405095
Loss in iteration 148 : 0.35515106146555997
Loss in iteration 149 : 0.3551257019867487
Loss in iteration 150 : 0.3551008131798855
Loss in iteration 151 : 0.35507643323189186
Loss in iteration 152 : 0.3550515230168537
Loss in iteration 153 : 0.3550272627674114
Loss in iteration 154 : 0.35500402123093866
Loss in iteration 155 : 0.3549797151155892
Loss in iteration 156 : 0.35495671532930584
Loss in iteration 157 : 0.35493315425087635
Loss in iteration 158 : 0.35490991722530046
Loss in iteration 159 : 0.35488703891359685
Loss in iteration 160 : 0.3548642900219259
Loss in iteration 161 : 0.35484191678589094
Loss in iteration 162 : 0.3548197064676519
Loss in iteration 163 : 0.3547977707882033
Loss in iteration 164 : 0.35477612131275615
Loss in iteration 165 : 0.354755145742906
Loss in iteration 166 : 0.3547338918501839
Loss in iteration 167 : 0.35471299718251004
Loss in iteration 168 : 0.35469242210180074
Loss in iteration 169 : 0.35467213003302517
Loss in iteration 170 : 0.35465220060612845
Loss in iteration 171 : 0.35463220578144444
Loss in iteration 172 : 0.35461218019687973
Loss in iteration 173 : 0.35459276252938376
Loss in iteration 174 : 0.3545737779412363
Loss in iteration 175 : 0.3545541164653599
Loss in iteration 176 : 0.3545356917638954
Loss in iteration 177 : 0.3545163051127313
Loss in iteration 178 : 0.3544981271779454
Loss in iteration 179 : 0.35447941736897576
Loss in iteration 180 : 0.3544611023033786
Loss in iteration 181 : 0.3544427511184229
Loss in iteration 182 : 0.3544247911076018
Loss in iteration 183 : 0.35440727938863353
Loss in iteration 184 : 0.3543896052175032
Loss in iteration 185 : 0.3543720839264311
Loss in iteration 186 : 0.35435488000530413
Loss in iteration 187 : 0.3543378753518677
Loss in iteration 188 : 0.35432119773806975
Loss in iteration 189 : 0.35430493035344746
Loss in iteration 190 : 0.3542884689641288
Loss in iteration 191 : 0.35427261310310965
Loss in iteration 192 : 0.3542566183378392
Loss in iteration 193 : 0.3542408447573231
Loss in iteration 194 : 0.3542253596262868
Loss in iteration 195 : 0.35420986951820904
Loss in iteration 196 : 0.3541947070196433
Loss in iteration 197 : 0.35417954232221016
Loss in iteration 198 : 0.35416490164464276
Loss in iteration 199 : 0.3541503064688977
Loss in iteration 200 : 0.3541364501664474
Testing accuracy  of updater 2 on alg 1 with rate 0.07 = 0.848965051286776, training accuracy 0.8454545454545455, time elapsed: 2692 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8619986668497831
Loss in iteration 3 : 0.6651651864092164
Loss in iteration 4 : 0.4861047047372384
Loss in iteration 5 : 0.5179907279960597
Loss in iteration 6 : 0.5397252889131355
Loss in iteration 7 : 0.5440765332878483
Loss in iteration 8 : 0.5329919537320444
Loss in iteration 9 : 0.510586513771245
Loss in iteration 10 : 0.4834757909267414
Loss in iteration 11 : 0.4613502468642399
Loss in iteration 12 : 0.45092821602444494
Loss in iteration 13 : 0.44834738914495265
Loss in iteration 14 : 0.44720828163635984
Loss in iteration 15 : 0.44560877894903267
Loss in iteration 16 : 0.4428447526661448
Loss in iteration 17 : 0.43881469424165814
Loss in iteration 18 : 0.433696882015721
Loss in iteration 19 : 0.42787219621078354
Loss in iteration 20 : 0.42178599147480667
Loss in iteration 21 : 0.41599290593233135
Loss in iteration 22 : 0.4109926808153832
Loss in iteration 23 : 0.4072957920880995
Loss in iteration 24 : 0.404386727700341
Loss in iteration 25 : 0.40188018781172397
Loss in iteration 26 : 0.3995378657511801
Loss in iteration 27 : 0.3971806395097358
Loss in iteration 28 : 0.3948057627774335
Loss in iteration 29 : 0.3925458954819938
Loss in iteration 30 : 0.39049450374499267
Loss in iteration 31 : 0.3887008462738777
Loss in iteration 32 : 0.38717456970282843
Loss in iteration 33 : 0.3858253194494719
Loss in iteration 34 : 0.3845716849685414
Loss in iteration 35 : 0.38336928163275924
Loss in iteration 36 : 0.38221955952544145
Loss in iteration 37 : 0.3811176464891799
Loss in iteration 38 : 0.3800715726766818
Loss in iteration 39 : 0.3790873367747555
Loss in iteration 40 : 0.3781592361065618
Loss in iteration 41 : 0.37728770858782334
Loss in iteration 42 : 0.3764683977034014
Loss in iteration 43 : 0.37569576898748536
Loss in iteration 44 : 0.37496134928389735
Loss in iteration 45 : 0.3742620258816057
Loss in iteration 46 : 0.37359804099380156
Loss in iteration 47 : 0.3729653836948201
Loss in iteration 48 : 0.3723655343146077
Loss in iteration 49 : 0.3717982651334316
Loss in iteration 50 : 0.37125978459959813
Loss in iteration 51 : 0.3707503301852888
Loss in iteration 52 : 0.37026730250431994
Loss in iteration 53 : 0.36980789641427003
Loss in iteration 54 : 0.369369489925988
Loss in iteration 55 : 0.3689521506869947
Loss in iteration 56 : 0.3685554387124928
Loss in iteration 57 : 0.3681761035268462
Loss in iteration 58 : 0.3678154549770026
Loss in iteration 59 : 0.36747574070564165
Loss in iteration 60 : 0.36715238702649117
Loss in iteration 61 : 0.3668442230630647
Loss in iteration 62 : 0.3665525872305235
Loss in iteration 63 : 0.3662754013305414
Loss in iteration 64 : 0.3660133552938292
Loss in iteration 65 : 0.36576568961886713
Loss in iteration 66 : 0.36552709101746783
Loss in iteration 67 : 0.36529727684043345
Loss in iteration 68 : 0.36507507466696215
Loss in iteration 69 : 0.36486102153064975
Loss in iteration 70 : 0.3646542957635451
Loss in iteration 71 : 0.3644532982981427
Loss in iteration 72 : 0.3642595255526002
Loss in iteration 73 : 0.3640724153435428
Loss in iteration 74 : 0.36389117930872666
Loss in iteration 75 : 0.3637172376266295
Loss in iteration 76 : 0.36355014486647735
Loss in iteration 77 : 0.36338812621243005
Loss in iteration 78 : 0.36323055194312204
Loss in iteration 79 : 0.3630776048532098
Loss in iteration 80 : 0.36293112318250637
Loss in iteration 81 : 0.36278948539734984
Loss in iteration 82 : 0.36265244703055766
Loss in iteration 83 : 0.3625204739454205
Loss in iteration 84 : 0.36239357678238165
Loss in iteration 85 : 0.36227071222798957
Loss in iteration 86 : 0.3621508279298839
Loss in iteration 87 : 0.36203381356247016
Loss in iteration 88 : 0.3619194704685868
Loss in iteration 89 : 0.3618075285924396
Loss in iteration 90 : 0.3616977108678923
Loss in iteration 91 : 0.3615898908620824
Loss in iteration 92 : 0.36148463677483
Loss in iteration 93 : 0.3613817061496413
Loss in iteration 94 : 0.3612812735926832
Loss in iteration 95 : 0.36118357170904847
Loss in iteration 96 : 0.36108851371315825
Loss in iteration 97 : 0.3609966206015173
Loss in iteration 98 : 0.36090678154384614
Loss in iteration 99 : 0.36081927797804986
Loss in iteration 100 : 0.36073343332134616
Loss in iteration 101 : 0.36064961449311267
Loss in iteration 102 : 0.36056736246070215
Loss in iteration 103 : 0.36048646366147097
Loss in iteration 104 : 0.36040718069075767
Loss in iteration 105 : 0.36032908143720077
Loss in iteration 106 : 0.360252606448524
Loss in iteration 107 : 0.36017788954795377
Loss in iteration 108 : 0.36010484932136005
Loss in iteration 109 : 0.36003294962213833
Loss in iteration 110 : 0.3599621384722216
Loss in iteration 111 : 0.3598925805283696
Loss in iteration 112 : 0.35982434835526833
Loss in iteration 113 : 0.35975737101852695
Loss in iteration 114 : 0.35969157982266425
Loss in iteration 115 : 0.3596268865467096
Loss in iteration 116 : 0.35956320283343857
Loss in iteration 117 : 0.35950072258816507
Loss in iteration 118 : 0.35943959621972077
Loss in iteration 119 : 0.3593798643226854
Loss in iteration 120 : 0.3593212702317363
Loss in iteration 121 : 0.3592635495548373
Loss in iteration 122 : 0.35920673780502405
Loss in iteration 123 : 0.35915061343028765
Loss in iteration 124 : 0.3590953202766678
Loss in iteration 125 : 0.3590408168150659
Loss in iteration 126 : 0.35898721313162446
Loss in iteration 127 : 0.35893459536672817
Loss in iteration 128 : 0.35888268439096394
Loss in iteration 129 : 0.3588313849577977
Loss in iteration 130 : 0.35878076955597077
Loss in iteration 131 : 0.3587308633978312
Loss in iteration 132 : 0.35868167346576174
Loss in iteration 133 : 0.35863293939332924
Loss in iteration 134 : 0.35858480385042446
Loss in iteration 135 : 0.3585372847162733
Loss in iteration 136 : 0.35849039622323614
Loss in iteration 137 : 0.35844430290444873
Loss in iteration 138 : 0.358399045816357
Loss in iteration 139 : 0.3583544163987368
Loss in iteration 140 : 0.35831022961978803
Loss in iteration 141 : 0.35826643401023595
Loss in iteration 142 : 0.35822307657652613
Loss in iteration 143 : 0.35818035927390496
Loss in iteration 144 : 0.358137922100383
Loss in iteration 145 : 0.3580959577271394
Loss in iteration 146 : 0.3580546207198493
Loss in iteration 147 : 0.3580137038414106
Loss in iteration 148 : 0.3579732611653224
Loss in iteration 149 : 0.35793345471159105
Loss in iteration 150 : 0.3578941796384885
Loss in iteration 151 : 0.3578555679813384
Loss in iteration 152 : 0.3578174968805787
Loss in iteration 153 : 0.3577799727260252
Loss in iteration 154 : 0.3577429428652555
Loss in iteration 155 : 0.3577063411843514
Loss in iteration 156 : 0.3576699670222082
Loss in iteration 157 : 0.35763377496527043
Loss in iteration 158 : 0.3575979529882556
Loss in iteration 159 : 0.35756239580353627
Loss in iteration 160 : 0.35752715707357063
Loss in iteration 161 : 0.35749241054820813
Loss in iteration 162 : 0.35745811525827165
Loss in iteration 163 : 0.357424236734861
Loss in iteration 164 : 0.35739058645786276
Loss in iteration 165 : 0.35735724832185745
Loss in iteration 166 : 0.35732428652699705
Loss in iteration 167 : 0.3572916168146364
Loss in iteration 168 : 0.35725918558462144
Loss in iteration 169 : 0.357227007273131
Loss in iteration 170 : 0.3571950495103924
Loss in iteration 171 : 0.35716332346057883
Loss in iteration 172 : 0.35713190727370686
Loss in iteration 173 : 0.3571008609846221
Loss in iteration 174 : 0.35707011761283014
Loss in iteration 175 : 0.35703972065695705
Loss in iteration 176 : 0.3570097063644596
Loss in iteration 177 : 0.35697992250365873
Loss in iteration 178 : 0.3569503891013138
Loss in iteration 179 : 0.3569211445257014
Loss in iteration 180 : 0.35689217513551774
Loss in iteration 181 : 0.35686345387565616
Loss in iteration 182 : 0.35683495783717917
Loss in iteration 183 : 0.3568067173061767
Loss in iteration 184 : 0.35677865744095466
Loss in iteration 185 : 0.35675084161339105
Loss in iteration 186 : 0.3567232507158817
Loss in iteration 187 : 0.35669595360254686
Loss in iteration 188 : 0.35666889047215267
Loss in iteration 189 : 0.3566420835574905
Loss in iteration 190 : 0.35661569448179953
Loss in iteration 191 : 0.3565896006273879
Loss in iteration 192 : 0.3565636866248378
Loss in iteration 193 : 0.3565379989241732
Loss in iteration 194 : 0.35651258070510433
Loss in iteration 195 : 0.356487415713995
Loss in iteration 196 : 0.35646252496633596
Loss in iteration 197 : 0.3564378064857312
Loss in iteration 198 : 0.3564133223359536
Loss in iteration 199 : 0.3563890049410915
Loss in iteration 200 : 0.3563648533655262
Testing accuracy  of updater 2 on alg 1 with rate 0.04000000000000001 = 0.8468153061851238, training accuracy 0.8435196560196561, time elapsed: 2998 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9654996667124428
Loss in iteration 3 : 0.9162912966023015
Loss in iteration 4 : 0.8538456933518306
Loss in iteration 5 : 0.7794865802750626
Loss in iteration 6 : 0.6944053083546261
Loss in iteration 7 : 0.5996740934748898
Loss in iteration 8 : 0.5072642717271971
Loss in iteration 9 : 0.4864999696877185
Loss in iteration 10 : 0.49988297029210255
Loss in iteration 11 : 0.5154842029668749
Loss in iteration 12 : 0.5274012946175352
Loss in iteration 13 : 0.5347364016300463
Loss in iteration 14 : 0.5376618479708718
Loss in iteration 15 : 0.5365471629484408
Loss in iteration 16 : 0.5318508445203052
Loss in iteration 17 : 0.5242257021320169
Loss in iteration 18 : 0.5143174675309258
Loss in iteration 19 : 0.5028567759461762
Loss in iteration 20 : 0.49098680950905416
Loss in iteration 21 : 0.4796280638972843
Loss in iteration 22 : 0.4703104616445889
Loss in iteration 23 : 0.4634123478294857
Loss in iteration 24 : 0.4592663984187398
Loss in iteration 25 : 0.4573429595329814
Loss in iteration 26 : 0.4568910598753665
Loss in iteration 27 : 0.45704540279658395
Loss in iteration 28 : 0.45726067302287066
Loss in iteration 29 : 0.45721440346713166
Loss in iteration 30 : 0.45681896564374935
Loss in iteration 31 : 0.45603631444008347
Loss in iteration 32 : 0.454866380654974
Loss in iteration 33 : 0.4533638997174019
Loss in iteration 34 : 0.4515925137708099
Loss in iteration 35 : 0.4496306582383191
Loss in iteration 36 : 0.4475877613582092
Loss in iteration 37 : 0.44557919060471707
Loss in iteration 38 : 0.4436186797139571
Loss in iteration 39 : 0.4418045841757405
Loss in iteration 40 : 0.4403201312516437
Loss in iteration 41 : 0.43908890280996354
Loss in iteration 42 : 0.4380517204429875
Loss in iteration 43 : 0.43712010432541204
Loss in iteration 44 : 0.4362576219225702
Loss in iteration 45 : 0.43544768311170945
Loss in iteration 46 : 0.43464830394930676
Loss in iteration 47 : 0.43382549196533754
Loss in iteration 48 : 0.4329635879543681
Loss in iteration 49 : 0.43206308693791
Loss in iteration 50 : 0.4311316491282777
Loss in iteration 51 : 0.4301833365370069
Loss in iteration 52 : 0.429236128361786
Loss in iteration 53 : 0.428299225879151
Loss in iteration 54 : 0.42738238081734026
Loss in iteration 55 : 0.4264908055770373
Loss in iteration 56 : 0.425624104689613
Loss in iteration 57 : 0.4247867565693496
Loss in iteration 58 : 0.4239750476078815
Loss in iteration 59 : 0.42318817814229287
Loss in iteration 60 : 0.4224168582375562
Loss in iteration 61 : 0.42165699078444474
Loss in iteration 62 : 0.42090004383271284
Loss in iteration 63 : 0.42014504578808487
Loss in iteration 64 : 0.41939069355304054
Loss in iteration 65 : 0.41863535213436737
Loss in iteration 66 : 0.4178803232869053
Loss in iteration 67 : 0.4171288366476532
Loss in iteration 68 : 0.4163814715140581
Loss in iteration 69 : 0.4156402011490621
Loss in iteration 70 : 0.41490546842195253
Loss in iteration 71 : 0.4141763223452477
Loss in iteration 72 : 0.41345198561277224
Loss in iteration 73 : 0.4127330822952564
Loss in iteration 74 : 0.41201990734986305
Loss in iteration 75 : 0.41131041350129055
Loss in iteration 76 : 0.41060469833634594
Loss in iteration 77 : 0.4099032187652056
Loss in iteration 78 : 0.4092053661511418
Loss in iteration 79 : 0.4085103867763154
Loss in iteration 80 : 0.407817233016856
Loss in iteration 81 : 0.40712712775803517
Loss in iteration 82 : 0.4064421029060446
Loss in iteration 83 : 0.40576393948986544
Loss in iteration 84 : 0.4050906704375648
Loss in iteration 85 : 0.40442508570851965
Loss in iteration 86 : 0.4037714794117665
Loss in iteration 87 : 0.4031306105250746
Loss in iteration 88 : 0.40250166188225583
Loss in iteration 89 : 0.40188710062244903
Loss in iteration 90 : 0.40128548491228005
Loss in iteration 91 : 0.4006946059877319
Loss in iteration 92 : 0.40011589108106727
Loss in iteration 93 : 0.3995513828312586
Loss in iteration 94 : 0.3989971039470858
Loss in iteration 95 : 0.3984519652099362
Loss in iteration 96 : 0.39791830741058315
Loss in iteration 97 : 0.39739552711726983
Loss in iteration 98 : 0.3968826049349472
Loss in iteration 99 : 0.3963783951024677
Loss in iteration 100 : 0.39588330018220363
Loss in iteration 101 : 0.39539612645649613
Loss in iteration 102 : 0.39491727340132615
Loss in iteration 103 : 0.39444611660448364
Loss in iteration 104 : 0.3939830567875818
Loss in iteration 105 : 0.3935289614759698
Loss in iteration 106 : 0.3930846402549668
Loss in iteration 107 : 0.39264835165427825
Loss in iteration 108 : 0.39221902267428427
Loss in iteration 109 : 0.39179953424459146
Loss in iteration 110 : 0.39138793056549986
Loss in iteration 111 : 0.39098366733176865
Loss in iteration 112 : 0.3905862623599185
Loss in iteration 113 : 0.39019781663250397
Loss in iteration 114 : 0.38981827994543167
Loss in iteration 115 : 0.3894466615500797
Loss in iteration 116 : 0.3890819503282251
Loss in iteration 117 : 0.38872491800828307
Loss in iteration 118 : 0.3883745988798695
Loss in iteration 119 : 0.38803064694437345
Loss in iteration 120 : 0.38769477587812623
Loss in iteration 121 : 0.38736722363502646
Loss in iteration 122 : 0.38704708644515007
Loss in iteration 123 : 0.3867332744427514
Loss in iteration 124 : 0.38642509437925626
Loss in iteration 125 : 0.3861239893695397
Loss in iteration 126 : 0.38582986865615543
Loss in iteration 127 : 0.38554234910324603
Loss in iteration 128 : 0.3852595350058054
Loss in iteration 129 : 0.38498106775302876
Loss in iteration 130 : 0.38470746446267967
Loss in iteration 131 : 0.3844385672432095
Loss in iteration 132 : 0.3841734274777112
Loss in iteration 133 : 0.3839114529147254
Loss in iteration 134 : 0.38365322728363227
Loss in iteration 135 : 0.38339862313603273
Loss in iteration 136 : 0.38314758297058005
Loss in iteration 137 : 0.38290010004127234
Loss in iteration 138 : 0.38265619299807596
Loss in iteration 139 : 0.38241592100246186
Loss in iteration 140 : 0.382178421086231
Loss in iteration 141 : 0.38194444101658725
Loss in iteration 142 : 0.38171337368649394
Loss in iteration 143 : 0.3814865254918027
Loss in iteration 144 : 0.3812627330461238
Loss in iteration 145 : 0.3810422089520235
Loss in iteration 146 : 0.3808255279835637
Loss in iteration 147 : 0.38061158088966013
Loss in iteration 148 : 0.3804001639808711
Loss in iteration 149 : 0.3801921888665069
Loss in iteration 150 : 0.3799871418558418
Loss in iteration 151 : 0.3797853245270312
Loss in iteration 152 : 0.37958631937784393
Loss in iteration 153 : 0.3793906216871304
Loss in iteration 154 : 0.37919829265356025
Loss in iteration 155 : 0.3790086955784915
Loss in iteration 156 : 0.3788209613217768
Loss in iteration 157 : 0.37863486370335936
Loss in iteration 158 : 0.3784505931351515
Loss in iteration 159 : 0.3782681809316765
Loss in iteration 160 : 0.378088095380782
Loss in iteration 161 : 0.37791025377994114
Loss in iteration 162 : 0.37773436404936067
Loss in iteration 163 : 0.37756096131165934
Loss in iteration 164 : 0.377389532255587
Loss in iteration 165 : 0.37722053889161694
Loss in iteration 166 : 0.3770531204015971
Loss in iteration 167 : 0.3768872893001823
Loss in iteration 168 : 0.37672349799117333
Loss in iteration 169 : 0.37656165890280674
Loss in iteration 170 : 0.3764017221445736
Loss in iteration 171 : 0.3762432960047789
Loss in iteration 172 : 0.3760866407173456
Loss in iteration 173 : 0.37593214672134
Loss in iteration 174 : 0.3757790452091189
Loss in iteration 175 : 0.37562747043730316
Loss in iteration 176 : 0.37547749567527033
Loss in iteration 177 : 0.37532873675833933
Loss in iteration 178 : 0.37518171108463094
Loss in iteration 179 : 0.3750361708509927
Loss in iteration 180 : 0.3748920314844738
Loss in iteration 181 : 0.3747494257827411
Loss in iteration 182 : 0.3746085832477108
Loss in iteration 183 : 0.37446933615562333
Loss in iteration 184 : 0.3743314480475033
Loss in iteration 185 : 0.3741948159876304
Loss in iteration 186 : 0.3740593390543397
Loss in iteration 187 : 0.37392484176835683
Loss in iteration 188 : 0.3737915401320887
Loss in iteration 189 : 0.3736594235781671
Loss in iteration 190 : 0.3735282280303549
Loss in iteration 191 : 0.37339772710300245
Loss in iteration 192 : 0.37326849516491284
Loss in iteration 193 : 0.3731404220288438
Loss in iteration 194 : 0.3730134322184782
Loss in iteration 195 : 0.37288748283872397
Loss in iteration 196 : 0.3727627092475264
Loss in iteration 197 : 0.3726391086628104
Loss in iteration 198 : 0.37251688977771047
Loss in iteration 199 : 0.3723968214514924
Loss in iteration 200 : 0.3722783700883284
Testing accuracy  of updater 2 on alg 1 with rate 0.009999999999999995 = 0.8413488114980652, training accuracy 0.8375307125307125, time elapsed: 2686 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 30.86598923888144
Loss in iteration 3 : 11.507581994223433
Loss in iteration 4 : 3.0906953443737972
Loss in iteration 5 : 2.5697024908525328
Loss in iteration 6 : 2.238908326264294
Loss in iteration 7 : 2.1753021611177084
Loss in iteration 8 : 2.997934543515922
Loss in iteration 9 : 5.580084439376735
Loss in iteration 10 : 3.861731624636468
Loss in iteration 11 : 5.008985747500675
Loss in iteration 12 : 2.7129528873832376
Loss in iteration 13 : 2.8334923494185635
Loss in iteration 14 : 2.417676048748685
Loss in iteration 15 : 2.8578276638054936
Loss in iteration 16 : 2.571077473171058
Loss in iteration 17 : 3.299448705157477
Loss in iteration 18 : 2.6604221264162815
Loss in iteration 19 : 3.2762494591612783
Loss in iteration 20 : 2.4546900144131527
Loss in iteration 21 : 2.82688524140031
Loss in iteration 22 : 2.2356876229048117
Loss in iteration 23 : 2.594508050064673
Loss in iteration 24 : 2.1795448666854353
Loss in iteration 25 : 2.6654589267740434
Loss in iteration 26 : 2.2690296814743895
Loss in iteration 27 : 2.8630509714917753
Loss in iteration 28 : 2.253119769024434
Loss in iteration 29 : 2.674268031914993
Loss in iteration 30 : 2.0817352531213364
Loss in iteration 31 : 2.4157980996659525
Loss in iteration 32 : 1.9852202021354428
Loss in iteration 33 : 2.3983690647186875
Loss in iteration 34 : 2.025743143654805
Loss in iteration 35 : 2.5168642895217994
Loss in iteration 36 : 2.0294694740298853
Loss in iteration 37 : 2.46058950055332
Loss in iteration 38 : 1.9450386530889618
Loss in iteration 39 : 2.3080063919715252
Loss in iteration 40 : 1.8603477193204736
Loss in iteration 41 : 2.229148316746885
Loss in iteration 42 : 1.853661736098946
Loss in iteration 43 : 2.2843242525616065
Loss in iteration 44 : 1.8586329810881745
Loss in iteration 45 : 2.290457470331978
Loss in iteration 46 : 1.8140826952481588
Loss in iteration 47 : 2.1771238780704896
Loss in iteration 48 : 1.750817158309278
Loss in iteration 49 : 2.1002397116395075
Loss in iteration 50 : 1.7263987728003931
Loss in iteration 51 : 2.137779941583005
Loss in iteration 52 : 1.736732123212775
Loss in iteration 53 : 2.1415295415030977
Loss in iteration 54 : 1.7038169605637445
Loss in iteration 55 : 2.05746532917414
Loss in iteration 56 : 1.6518960243013596
Loss in iteration 57 : 1.993123774233357
Loss in iteration 58 : 1.6331991798033496
Loss in iteration 59 : 2.0260201843577073
Loss in iteration 60 : 1.6420929078077406
Loss in iteration 61 : 2.0416236900546325
Loss in iteration 62 : 1.613721136823431
Loss in iteration 63 : 1.9588905651588746
Loss in iteration 64 : 1.566127708081844
Loss in iteration 65 : 1.895099303445857
Loss in iteration 66 : 1.544912325363142
Loss in iteration 67 : 1.921632593249256
Loss in iteration 68 : 1.562046040622386
Loss in iteration 69 : 1.9549346735275284
Loss in iteration 70 : 1.5579505666493345
Loss in iteration 71 : 1.9083404732473614
Loss in iteration 72 : 1.491950400112465
Loss in iteration 73 : 1.782427818262157
Loss in iteration 74 : 1.4588265708600745
Loss in iteration 75 : 1.8053892821036983
Loss in iteration 76 : 1.493999435275899
Loss in iteration 77 : 1.9012036939297543
Loss in iteration 78 : 1.49903543345182
Loss in iteration 79 : 1.8511438641299793
Loss in iteration 80 : 1.4322274135897843
Loss in iteration 81 : 1.7081113700765667
Loss in iteration 82 : 1.3901613863042637
Loss in iteration 83 : 1.7150676325980656
Loss in iteration 84 : 1.4260304462078368
Loss in iteration 85 : 1.834244933229677
Loss in iteration 86 : 1.4465602040076966
Loss in iteration 87 : 1.7848625049501827
Loss in iteration 88 : 1.3771354424902655
Loss in iteration 89 : 1.6388060181210122
Loss in iteration 90 : 1.339557960528106
Loss in iteration 91 : 1.6613381219283654
Loss in iteration 92 : 1.3792412657034638
Loss in iteration 93 : 1.780837024896291
Loss in iteration 94 : 1.3900294506533972
Loss in iteration 95 : 1.7117739049160658
Loss in iteration 96 : 1.3205738558711433
Loss in iteration 97 : 1.5697533213998724
Loss in iteration 98 : 1.3019005144185205
Loss in iteration 99 : 1.6331517576479468
Loss in iteration 100 : 1.3351226983764044
Loss in iteration 101 : 1.70222349293225
Loss in iteration 102 : 1.3437243726098496
Loss in iteration 103 : 1.6678888620365324
Loss in iteration 104 : 1.2958657656385053
Loss in iteration 105 : 1.53730794398541
Loss in iteration 106 : 1.2654323688196367
Loss in iteration 107 : 1.573760017356632
Loss in iteration 108 : 1.2860221830202279
Loss in iteration 109 : 1.6358486249269568
Loss in iteration 110 : 1.2956094363354531
Loss in iteration 111 : 1.60970110293435
Loss in iteration 112 : 1.257690842203163
Loss in iteration 113 : 1.4986070764210027
Loss in iteration 114 : 1.2265221922329455
Loss in iteration 115 : 1.5340509012175574
Loss in iteration 116 : 1.2463263312242372
Loss in iteration 117 : 1.6026554941615327
Loss in iteration 118 : 1.2609176753625007
Loss in iteration 119 : 1.5747383310558651
Loss in iteration 120 : 1.2228083773487157
Loss in iteration 121 : 1.4532040743743313
Loss in iteration 122 : 1.1909740343106416
Loss in iteration 123 : 1.4822042965192643
Loss in iteration 124 : 1.2118585877575696
Loss in iteration 125 : 1.5690131310995514
Loss in iteration 126 : 1.2252897945546317
Loss in iteration 127 : 1.527312887940186
Loss in iteration 128 : 1.1833673782948804
Loss in iteration 129 : 1.414027439310208
Loss in iteration 130 : 1.1615135594463566
Loss in iteration 131 : 1.463735566486619
Loss in iteration 132 : 1.2031891779517023
Loss in iteration 133 : 1.5549785925917063
Loss in iteration 134 : 1.180918247214315
Loss in iteration 135 : 1.4132988593019513
Loss in iteration 136 : 1.1414038244587188
Loss in iteration 137 : 1.390798809681545
Loss in iteration 138 : 1.1589770045574819
Loss in iteration 139 : 1.499892407571041
Loss in iteration 140 : 1.1733402124381576
Loss in iteration 141 : 1.4827622393740363
Loss in iteration 142 : 1.1405593524683966
Loss in iteration 143 : 1.3696842859162441
Loss in iteration 144 : 1.1172386399474878
Loss in iteration 145 : 1.379142082063716
Loss in iteration 146 : 1.1362251646161168
Loss in iteration 147 : 1.4623306829806262
Loss in iteration 148 : 1.144123825569624
Loss in iteration 149 : 1.4405702288357018
Loss in iteration 150 : 1.110879358249559
Loss in iteration 151 : 1.3323881263671333
Loss in iteration 152 : 1.0968040928283236
Loss in iteration 153 : 1.3702108441480858
Loss in iteration 154 : 1.1230042494020707
Loss in iteration 155 : 1.4496348040838685
Loss in iteration 156 : 1.1151444654208333
Loss in iteration 157 : 1.357703784528441
Loss in iteration 158 : 1.0800411228162796
Loss in iteration 159 : 1.3156291950368249
Loss in iteration 160 : 1.0846140969208673
Loss in iteration 161 : 1.3934614907097642
Loss in iteration 162 : 1.1033993477706405
Loss in iteration 163 : 1.3895688880678085
Loss in iteration 164 : 1.0770118633145132
Loss in iteration 165 : 1.3068693262590574
Loss in iteration 166 : 1.0623351430794685
Loss in iteration 167 : 1.323126314335365
Loss in iteration 168 : 1.0807549709038042
Loss in iteration 169 : 1.3884030158138443
Loss in iteration 170 : 1.076912583609709
Loss in iteration 171 : 1.3185870737418843
Loss in iteration 172 : 1.0432515775611062
Loss in iteration 173 : 1.2734083441299178
Loss in iteration 174 : 1.0489388070848364
Loss in iteration 175 : 1.3395185222934503
Loss in iteration 176 : 1.0658630905230209
Loss in iteration 177 : 1.3554489803031362
Loss in iteration 178 : 1.04597960486866
Loss in iteration 179 : 1.2703928704375116
Loss in iteration 180 : 1.023782356279961
Loss in iteration 181 : 1.27192338836053
Loss in iteration 182 : 1.0382198013647854
Loss in iteration 183 : 1.3277421046387476
Loss in iteration 184 : 1.0443503324792727
Loss in iteration 185 : 1.2927841486868559
Loss in iteration 186 : 1.0238670933159824
Loss in iteration 187 : 1.2608128043172657
Loss in iteration 188 : 1.0141031880590936
Loss in iteration 189 : 1.2677949820973353
Loss in iteration 190 : 1.0320650265791655
Loss in iteration 191 : 1.3233381913219093
Loss in iteration 192 : 1.0194824070883466
Loss in iteration 193 : 1.240476374638852
Loss in iteration 194 : 0.9892315393358065
Loss in iteration 195 : 1.212341642233098
Loss in iteration 196 : 1.0032660927026216
Loss in iteration 197 : 1.2887882999764138
Loss in iteration 198 : 1.021057674541611
Loss in iteration 199 : 1.2924851403634134
Loss in iteration 200 : 0.9981624730330925
Testing accuracy  of updater 3 on alg 1 with rate 10.0 = 0.8200356243473989, training accuracy 0.8162776412776412, time elapsed: 2742 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.9451447551221612
Loss in iteration 3 : 1.7143134556484052
Loss in iteration 4 : 0.7839102231264014
Loss in iteration 5 : 0.7179680829180562
Loss in iteration 6 : 0.6606274266916778
Loss in iteration 7 : 0.6141648643545161
Loss in iteration 8 : 0.5777347864892854
Loss in iteration 9 : 0.5668303744030714
Loss in iteration 10 : 0.5956813071372933
Loss in iteration 11 : 0.7490989723376452
Loss in iteration 12 : 0.9510707979965289
Loss in iteration 13 : 1.4034792842732982
Loss in iteration 14 : 0.7006564019425598
Loss in iteration 15 : 0.7782369454961967
Loss in iteration 16 : 0.7198371196715105
Loss in iteration 17 : 0.8710405538206396
Loss in iteration 18 : 0.7888104409272777
Loss in iteration 19 : 0.9910048297146072
Loss in iteration 20 : 0.7619782569439093
Loss in iteration 21 : 0.876039764877444
Loss in iteration 22 : 0.7426502954357569
Loss in iteration 23 : 0.8579071465632012
Loss in iteration 24 : 0.741457802809813
Loss in iteration 25 : 0.872100271648392
Loss in iteration 26 : 0.7494119725287236
Loss in iteration 27 : 0.8917099962391266
Loss in iteration 28 : 0.7426655318266506
Loss in iteration 29 : 0.8697343658088846
Loss in iteration 30 : 0.7366079381787404
Loss in iteration 31 : 0.8629917853752124
Loss in iteration 32 : 0.7348184980773899
Loss in iteration 33 : 0.8652807221668658
Loss in iteration 34 : 0.7351643795609157
Loss in iteration 35 : 0.8638573078506298
Loss in iteration 36 : 0.7320992911761132
Loss in iteration 37 : 0.8574625156666338
Loss in iteration 38 : 0.7282088459350178
Loss in iteration 39 : 0.8542986135550572
Loss in iteration 40 : 0.7281345930911807
Loss in iteration 41 : 0.8581227344636987
Loss in iteration 42 : 0.7267941872189749
Loss in iteration 43 : 0.8533352062055878
Loss in iteration 44 : 0.7223864561117402
Loss in iteration 45 : 0.8443354499294646
Loss in iteration 46 : 0.7214265899258064
Loss in iteration 47 : 0.8557585382472808
Loss in iteration 48 : 0.7205955537921994
Loss in iteration 49 : 0.8495451305940327
Loss in iteration 50 : 0.7154819961818508
Loss in iteration 51 : 0.8427603981414074
Loss in iteration 52 : 0.7148101854936946
Loss in iteration 53 : 0.8462775472974308
Loss in iteration 54 : 0.7135418560194335
Loss in iteration 55 : 0.844338799599388
Loss in iteration 56 : 0.7105200376563459
Loss in iteration 57 : 0.8368063039574334
Loss in iteration 58 : 0.7075092875686003
Loss in iteration 59 : 0.8383521091545186
Loss in iteration 60 : 0.7083170367273474
Loss in iteration 61 : 0.8421165499939848
Loss in iteration 62 : 0.7047459230571178
Loss in iteration 63 : 0.8291167594138565
Loss in iteration 64 : 0.7009976259046627
Loss in iteration 65 : 0.8282043506622542
Loss in iteration 66 : 0.7011106561911566
Loss in iteration 67 : 0.8337898956305093
Loss in iteration 68 : 0.6994963574405684
Loss in iteration 69 : 0.8262258895553046
Loss in iteration 70 : 0.6961783962766448
Loss in iteration 71 : 0.820086169203209
Loss in iteration 72 : 0.6950103190174505
Loss in iteration 73 : 0.8296286981411378
Loss in iteration 74 : 0.6966510170278725
Loss in iteration 75 : 0.8284330080107236
Loss in iteration 76 : 0.692134953752572
Loss in iteration 77 : 0.8117461597510239
Loss in iteration 78 : 0.689113775929912
Loss in iteration 79 : 0.8118340944271916
Loss in iteration 80 : 0.6896480423271477
Loss in iteration 81 : 0.8232905852503816
Loss in iteration 82 : 0.6892056769044028
Loss in iteration 83 : 0.8157139396660439
Loss in iteration 84 : 0.6851023939457118
Loss in iteration 85 : 0.8036663185024276
Loss in iteration 86 : 0.6828885641835479
Loss in iteration 87 : 0.8079409797987542
Loss in iteration 88 : 0.6846739449886178
Loss in iteration 89 : 0.8144888803386612
Loss in iteration 90 : 0.6808155134113721
Loss in iteration 91 : 0.7986466580271118
Loss in iteration 92 : 0.6789734132820353
Loss in iteration 93 : 0.8023053729032564
Loss in iteration 94 : 0.6785928910875029
Loss in iteration 95 : 0.8051682932818022
Loss in iteration 96 : 0.6762511646440537
Loss in iteration 97 : 0.7967707989002987
Loss in iteration 98 : 0.6733606206085226
Loss in iteration 99 : 0.7927200713704567
Loss in iteration 100 : 0.6737758690036042
Loss in iteration 101 : 0.8019513536309744
Loss in iteration 102 : 0.6723013794561673
Loss in iteration 103 : 0.7932297550664797
Loss in iteration 104 : 0.6700566120953024
Loss in iteration 105 : 0.7917001622445989
Loss in iteration 106 : 0.6693684137913363
Loss in iteration 107 : 0.7908759239340832
Loss in iteration 108 : 0.6681618207064685
Loss in iteration 109 : 0.7902475352274831
Loss in iteration 110 : 0.6662690477383767
Loss in iteration 111 : 0.783359630003817
Loss in iteration 112 : 0.6661869022054304
Loss in iteration 113 : 0.7902088997557892
Loss in iteration 114 : 0.6619062434269809
Loss in iteration 115 : 0.772265814941832
Loss in iteration 116 : 0.6645045069799065
Loss in iteration 117 : 0.7907558512455499
Loss in iteration 118 : 0.6611993485450625
Loss in iteration 119 : 0.7771813877458159
Loss in iteration 120 : 0.6604293759815772
Loss in iteration 121 : 0.7815437922374634
Loss in iteration 122 : 0.6576874346769783
Loss in iteration 123 : 0.7706625510298138
Loss in iteration 124 : 0.6595564195216801
Loss in iteration 125 : 0.7822829927711404
Loss in iteration 126 : 0.6541709507618814
Loss in iteration 127 : 0.7602538788708224
Loss in iteration 128 : 0.6551865498948192
Loss in iteration 129 : 0.7782528241410145
Loss in iteration 130 : 0.6538325360303641
Loss in iteration 131 : 0.7698951361634301
Loss in iteration 132 : 0.6536074405596095
Loss in iteration 133 : 0.7706069371701244
Loss in iteration 134 : 0.6513593470283455
Loss in iteration 135 : 0.7611322345861706
Loss in iteration 136 : 0.651238458541619
Loss in iteration 137 : 0.7690584214716089
Loss in iteration 138 : 0.6490245126527227
Loss in iteration 139 : 0.759640806583143
Loss in iteration 140 : 0.6486538178352458
Loss in iteration 141 : 0.7634882249320679
Loss in iteration 142 : 0.6470168185963717
Loss in iteration 143 : 0.755789009799277
Loss in iteration 144 : 0.6464266437303523
Loss in iteration 145 : 0.7605846410996395
Loss in iteration 146 : 0.6454548454880796
Loss in iteration 147 : 0.7568972409500293
Loss in iteration 148 : 0.643012712344302
Loss in iteration 149 : 0.7506483097772173
Loss in iteration 150 : 0.6418979165085903
Loss in iteration 151 : 0.7539605397009835
Loss in iteration 152 : 0.6421479467940935
Loss in iteration 153 : 0.7590394525926898
Loss in iteration 154 : 0.6410610462199704
Loss in iteration 155 : 0.7463797943491159
Loss in iteration 156 : 0.6381347242882526
Loss in iteration 157 : 0.7455044308572545
Loss in iteration 158 : 0.6383594740303867
Loss in iteration 159 : 0.7513768393041714
Loss in iteration 160 : 0.6375967210104911
Loss in iteration 161 : 0.747831689039465
Loss in iteration 162 : 0.6342947088079652
Loss in iteration 163 : 0.7364630788495051
Loss in iteration 164 : 0.6361393796776423
Loss in iteration 165 : 0.7541694889313434
Loss in iteration 166 : 0.633115950409104
Loss in iteration 167 : 0.7359064707610374
Loss in iteration 168 : 0.6333334878418461
Loss in iteration 169 : 0.74455926066096
Loss in iteration 170 : 0.6305686207000074
Loss in iteration 171 : 0.7324713870911931
Loss in iteration 172 : 0.6320246720475704
Loss in iteration 173 : 0.7462511925550349
Loss in iteration 174 : 0.6289776364580444
Loss in iteration 175 : 0.7298631691826456
Loss in iteration 176 : 0.6293116183821322
Loss in iteration 177 : 0.7389008582420411
Loss in iteration 178 : 0.626907671016313
Loss in iteration 179 : 0.7299164969609275
Loss in iteration 180 : 0.6265939825594329
Loss in iteration 181 : 0.7371097642077314
Loss in iteration 182 : 0.6252415539802026
Loss in iteration 183 : 0.7274529897609112
Loss in iteration 184 : 0.6243468085930806
Loss in iteration 185 : 0.7309870440185051
Loss in iteration 186 : 0.6230691072681236
Loss in iteration 187 : 0.7257813925643137
Loss in iteration 188 : 0.6227384143003089
Loss in iteration 189 : 0.7311295832144769
Loss in iteration 190 : 0.6220041073982547
Loss in iteration 191 : 0.7245465924587249
Loss in iteration 192 : 0.6198207839941162
Loss in iteration 193 : 0.7221874470649566
Loss in iteration 194 : 0.620023700359479
Loss in iteration 195 : 0.7251022939598445
Loss in iteration 196 : 0.6195244082554495
Loss in iteration 197 : 0.7252985121158998
Loss in iteration 198 : 0.6172862516064279
Loss in iteration 199 : 0.715438479522846
Loss in iteration 200 : 0.617977631149233
Testing accuracy  of updater 3 on alg 1 with rate 7.0 = 0.8212026288311529, training accuracy 0.8180282555282555, time elapsed: 2919 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.7729788432054616
Loss in iteration 3 : 0.8583248665223476
Loss in iteration 4 : 0.47459666825157215
Loss in iteration 5 : 0.4444468704796758
Loss in iteration 6 : 0.42405601528027664
Loss in iteration 7 : 0.4075602297489982
Loss in iteration 8 : 0.3955675834461463
Loss in iteration 9 : 0.38729139520642925
Loss in iteration 10 : 0.3840075737029339
Loss in iteration 11 : 0.3880043667917837
Loss in iteration 12 : 0.40761857098980947
Loss in iteration 13 : 0.4558985896284769
Loss in iteration 14 : 0.4987344506832771
Loss in iteration 15 : 0.6161841362222151
Loss in iteration 16 : 0.4756202119163366
Loss in iteration 17 : 0.5148118792255515
Loss in iteration 18 : 0.45981822143210377
Loss in iteration 19 : 0.48723682163469123
Loss in iteration 20 : 0.45349000874818696
Loss in iteration 21 : 0.48142138602779294
Loss in iteration 22 : 0.4516003688472642
Loss in iteration 23 : 0.48274043114121123
Loss in iteration 24 : 0.45169478213446174
Loss in iteration 25 : 0.48539670555322095
Loss in iteration 26 : 0.4522580105718572
Loss in iteration 27 : 0.4853750059301853
Loss in iteration 28 : 0.450003699754089
Loss in iteration 29 : 0.48305130581461964
Loss in iteration 30 : 0.449144875892202
Loss in iteration 31 : 0.48192812946267166
Loss in iteration 32 : 0.4491708552290067
Loss in iteration 33 : 0.4820858236646049
Loss in iteration 34 : 0.4486565765643928
Loss in iteration 35 : 0.48233701394912326
Loss in iteration 36 : 0.44844806487094724
Loss in iteration 37 : 0.48071869698954417
Loss in iteration 38 : 0.44744324513033246
Loss in iteration 39 : 0.4794009254321205
Loss in iteration 40 : 0.44661985296919426
Loss in iteration 41 : 0.4781965406612916
Loss in iteration 42 : 0.4466220782024293
Loss in iteration 43 : 0.480404816096183
Loss in iteration 44 : 0.4478797731651532
Loss in iteration 45 : 0.4817060826420485
Loss in iteration 46 : 0.4470723576731504
Loss in iteration 47 : 0.478669945432035
Loss in iteration 48 : 0.44496844451773065
Loss in iteration 49 : 0.4757917150818424
Loss in iteration 50 : 0.4442284672687249
Loss in iteration 51 : 0.4763943083549795
Loss in iteration 52 : 0.44510633543880795
Loss in iteration 53 : 0.47965644208409314
Loss in iteration 54 : 0.4462098228545634
Loss in iteration 55 : 0.4794946962181462
Loss in iteration 56 : 0.4446149367383524
Loss in iteration 57 : 0.4748759903592823
Loss in iteration 58 : 0.4422566432641669
Loss in iteration 59 : 0.4702035168277816
Loss in iteration 60 : 0.44174810040879914
Loss in iteration 61 : 0.4728526023020542
Loss in iteration 62 : 0.4428084258657802
Loss in iteration 63 : 0.4758356959577475
Loss in iteration 64 : 0.44447694519494874
Loss in iteration 65 : 0.4797962869321994
Loss in iteration 66 : 0.444326759803499
Loss in iteration 67 : 0.47574373279101795
Loss in iteration 68 : 0.4412798914695901
Loss in iteration 69 : 0.46877026746150574
Loss in iteration 70 : 0.43962408849637424
Loss in iteration 71 : 0.46748607713638457
Loss in iteration 72 : 0.44022418220961257
Loss in iteration 73 : 0.4729022004781073
Loss in iteration 74 : 0.44253337211932425
Loss in iteration 75 : 0.47662119310350765
Loss in iteration 76 : 0.442724191544179
Loss in iteration 77 : 0.47492673960272624
Loss in iteration 78 : 0.4404655606825721
Loss in iteration 79 : 0.4677842178460308
Loss in iteration 80 : 0.437943001669979
Loss in iteration 81 : 0.4663156396973784
Loss in iteration 82 : 0.438302264860503
Loss in iteration 83 : 0.4701208194249578
Loss in iteration 84 : 0.44048413348137133
Loss in iteration 85 : 0.4734074453572046
Loss in iteration 86 : 0.4406722215646656
Loss in iteration 87 : 0.4717254609811624
Loss in iteration 88 : 0.4388147847145421
Loss in iteration 89 : 0.4663215469899975
Loss in iteration 90 : 0.4365969491854129
Loss in iteration 91 : 0.4644084416439064
Loss in iteration 92 : 0.43721818250732625
Loss in iteration 93 : 0.46909257837344226
Loss in iteration 94 : 0.43941010429305516
Loss in iteration 95 : 0.4718685447320238
Loss in iteration 96 : 0.43889656688436124
Loss in iteration 97 : 0.46856821769087825
Loss in iteration 98 : 0.43594720690613203
Loss in iteration 99 : 0.4631100008562264
Loss in iteration 100 : 0.43528243276832573
Loss in iteration 101 : 0.46401496720089724
Loss in iteration 102 : 0.43617481604769615
Loss in iteration 103 : 0.4680873503274395
Loss in iteration 104 : 0.4380075971579835
Loss in iteration 105 : 0.47046798093867487
Loss in iteration 106 : 0.43729619887159266
Loss in iteration 107 : 0.4651536535344139
Loss in iteration 108 : 0.4342508086112471
Loss in iteration 109 : 0.46060019410368136
Loss in iteration 110 : 0.43353026867945643
Loss in iteration 111 : 0.4630018960614008
Loss in iteration 112 : 0.4352292129340495
Loss in iteration 113 : 0.467159631794002
Loss in iteration 114 : 0.4363770045203252
Loss in iteration 115 : 0.46752121538051106
Loss in iteration 116 : 0.43493891467251933
Loss in iteration 117 : 0.4623605182456513
Loss in iteration 118 : 0.43306865810198236
Loss in iteration 119 : 0.45997765912630495
Loss in iteration 120 : 0.43254761137850817
Loss in iteration 121 : 0.4620549270075357
Loss in iteration 122 : 0.43389904316534694
Loss in iteration 123 : 0.4643483115251742
Loss in iteration 124 : 0.43483159047838965
Loss in iteration 125 : 0.46525172857476776
Loss in iteration 126 : 0.43383079837312377
Loss in iteration 127 : 0.46036470153826287
Loss in iteration 128 : 0.4313050508777224
Loss in iteration 129 : 0.4571147521061533
Loss in iteration 130 : 0.4308638004735935
Loss in iteration 131 : 0.46001097364285326
Loss in iteration 132 : 0.4325076907406359
Loss in iteration 133 : 0.46382384668291293
Loss in iteration 134 : 0.43424236546939393
Loss in iteration 135 : 0.4648687107018919
Loss in iteration 136 : 0.4328379428577168
Loss in iteration 137 : 0.4589684204038742
Loss in iteration 138 : 0.4299526383626719
Loss in iteration 139 : 0.45535910212514674
Loss in iteration 140 : 0.4285036661413106
Loss in iteration 141 : 0.4553091560369283
Loss in iteration 142 : 0.42979337558194297
Loss in iteration 143 : 0.4602857283939561
Loss in iteration 144 : 0.4326345968765988
Loss in iteration 145 : 0.46401786248285776
Loss in iteration 146 : 0.432801204502139
Loss in iteration 147 : 0.4610099012665909
Loss in iteration 148 : 0.43032336110233826
Loss in iteration 149 : 0.45554580867403927
Loss in iteration 150 : 0.4277913194668414
Loss in iteration 151 : 0.45268903921865006
Loss in iteration 152 : 0.42694568680071604
Loss in iteration 153 : 0.45468706934353975
Loss in iteration 154 : 0.4293251591073694
Loss in iteration 155 : 0.4608819020941096
Loss in iteration 156 : 0.432160306862548
Loss in iteration 157 : 0.4632939383615672
Loss in iteration 158 : 0.43091041729475144
Loss in iteration 159 : 0.45722777046713836
Loss in iteration 160 : 0.4272798692754086
Loss in iteration 161 : 0.4507446810152003
Loss in iteration 162 : 0.42437810327859243
Loss in iteration 163 : 0.4481655843490155
Loss in iteration 164 : 0.4252497108438908
Loss in iteration 165 : 0.45420817680863546
Loss in iteration 166 : 0.4290735763551752
Loss in iteration 167 : 0.4606133519895584
Loss in iteration 168 : 0.4309893123110592
Loss in iteration 169 : 0.4615808993149329
Loss in iteration 170 : 0.4288815908629978
Loss in iteration 171 : 0.4538187728053719
Loss in iteration 172 : 0.42478051323856836
Loss in iteration 173 : 0.44717597874247156
Loss in iteration 174 : 0.4230394632958399
Loss in iteration 175 : 0.4468121763664139
Loss in iteration 176 : 0.42361165795051653
Loss in iteration 177 : 0.4522658226488554
Loss in iteration 178 : 0.4276945117103269
Loss in iteration 179 : 0.45882120298638707
Loss in iteration 180 : 0.4298111720471759
Loss in iteration 181 : 0.46020291525350615
Loss in iteration 182 : 0.4276833913849104
Loss in iteration 183 : 0.45195872083067556
Loss in iteration 184 : 0.4235220957236026
Loss in iteration 185 : 0.4457332152562617
Loss in iteration 186 : 0.42177035560444864
Loss in iteration 187 : 0.44476755689589165
Loss in iteration 188 : 0.42208436698668955
Loss in iteration 189 : 0.44867069373164725
Loss in iteration 190 : 0.42489949992209375
Loss in iteration 191 : 0.45494027495609773
Loss in iteration 192 : 0.4275639303981926
Loss in iteration 193 : 0.45738911014383044
Loss in iteration 194 : 0.4265387056374341
Loss in iteration 195 : 0.4530354826898612
Loss in iteration 196 : 0.4234838246984079
Loss in iteration 197 : 0.44602539683281067
Loss in iteration 198 : 0.4203231282608972
Loss in iteration 199 : 0.4408548996459074
Loss in iteration 200 : 0.42014960807983953
Testing accuracy  of updater 3 on alg 1 with rate 4.0 = 0.8326884098028376, training accuracy 0.8304668304668305, time elapsed: 2949 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5699982938332996
Loss in iteration 3 : 0.43515360515635576
Loss in iteration 4 : 0.4126185839918025
Loss in iteration 5 : 0.39990410187963676
Loss in iteration 6 : 0.38992980151007167
Loss in iteration 7 : 0.3825363377809734
Loss in iteration 8 : 0.37706988776627837
Loss in iteration 9 : 0.3730578523550685
Loss in iteration 10 : 0.3700502116363469
Loss in iteration 11 : 0.367686726770065
Loss in iteration 12 : 0.3657422629886139
Loss in iteration 13 : 0.36413631627423826
Loss in iteration 14 : 0.3627659743514368
Loss in iteration 15 : 0.3615922262467451
Loss in iteration 16 : 0.360598549439299
Loss in iteration 17 : 0.35973464255493504
Loss in iteration 18 : 0.3589883200558365
Loss in iteration 19 : 0.35835869471485704
Loss in iteration 20 : 0.3578040611216269
Loss in iteration 21 : 0.3573135723707695
Loss in iteration 22 : 0.3568693534235666
Loss in iteration 23 : 0.35647252827891046
Loss in iteration 24 : 0.35612194978738054
Loss in iteration 25 : 0.35581440557591393
Loss in iteration 26 : 0.35553613838339165
Loss in iteration 27 : 0.3552860229858234
Loss in iteration 28 : 0.3550568558307196
Loss in iteration 29 : 0.35484330676293613
Loss in iteration 30 : 0.354650046640249
Loss in iteration 31 : 0.3544746687900814
Loss in iteration 32 : 0.3543175641261686
Loss in iteration 33 : 0.354172462933006
Loss in iteration 34 : 0.35404008621069033
Loss in iteration 35 : 0.3539192207351034
Loss in iteration 36 : 0.3538090914251948
Loss in iteration 37 : 0.3537078536256294
Loss in iteration 38 : 0.3536134044307359
Loss in iteration 39 : 0.3535249757815993
Loss in iteration 40 : 0.353443376776349
Loss in iteration 41 : 0.35336768398029483
Loss in iteration 42 : 0.35329587628019565
Loss in iteration 43 : 0.3532293935402358
Loss in iteration 44 : 0.3531686535731463
Loss in iteration 45 : 0.35310749554848847
Loss in iteration 46 : 0.35305047279141677
Loss in iteration 47 : 0.35299673904008977
Loss in iteration 48 : 0.3529467400717306
Loss in iteration 49 : 0.35289615185336515
Loss in iteration 50 : 0.3528526468071893
Loss in iteration 51 : 0.35280523869959685
Loss in iteration 52 : 0.3527631595981688
Loss in iteration 53 : 0.3527236910111346
Loss in iteration 54 : 0.352687701551044
Loss in iteration 55 : 0.35264856337859996
Loss in iteration 56 : 0.35261261984152487
Loss in iteration 57 : 0.35257878327086134
Loss in iteration 58 : 0.35254763686553947
Loss in iteration 59 : 0.3525184808038652
Loss in iteration 60 : 0.35249244257392387
Loss in iteration 61 : 0.352468841096528
Loss in iteration 62 : 0.35244056665719287
Loss in iteration 63 : 0.35241628938157593
Loss in iteration 64 : 0.3523932023180525
Loss in iteration 65 : 0.35237154857210634
Loss in iteration 66 : 0.3523514700163064
Loss in iteration 67 : 0.3523320140002989
Loss in iteration 68 : 0.35231381522384914
Loss in iteration 69 : 0.35229721297659655
Loss in iteration 70 : 0.3522797446871984
Loss in iteration 71 : 0.3522639235908106
Loss in iteration 72 : 0.35224744027481086
Loss in iteration 73 : 0.3522314388962485
Loss in iteration 74 : 0.35221682098771656
Loss in iteration 75 : 0.3522044609487434
Loss in iteration 76 : 0.3521904904531652
Loss in iteration 77 : 0.3521788246036276
Loss in iteration 78 : 0.3521624907427789
Loss in iteration 79 : 0.3521487341630922
Loss in iteration 80 : 0.35213611474955875
Loss in iteration 81 : 0.3521237764553041
Loss in iteration 82 : 0.35211246921313394
Loss in iteration 83 : 0.35210266747344765
Loss in iteration 84 : 0.3520907175997
Loss in iteration 85 : 0.3520820868260975
Loss in iteration 86 : 0.3520724808845151
Loss in iteration 87 : 0.3520601524444581
Loss in iteration 88 : 0.3520479206633562
Loss in iteration 89 : 0.3520380393077422
Loss in iteration 90 : 0.35202731027163
Loss in iteration 91 : 0.3520182746459093
Loss in iteration 92 : 0.35201073279822165
Loss in iteration 93 : 0.3520016302119402
Loss in iteration 94 : 0.3519967881052081
Loss in iteration 95 : 0.35198897540077095
Loss in iteration 96 : 0.351979305813102
Loss in iteration 97 : 0.3519679008944723
Loss in iteration 98 : 0.3519605456879019
Loss in iteration 99 : 0.35194985526919054
Loss in iteration 100 : 0.3519415919497096
Loss in iteration 101 : 0.3519339972509687
Loss in iteration 102 : 0.3519259826937252
Loss in iteration 103 : 0.3519190237006359
Loss in iteration 104 : 0.35191175031666827
Loss in iteration 105 : 0.3519109424727425
Loss in iteration 106 : 0.351905591642546
Loss in iteration 107 : 0.35189579061118986
Loss in iteration 108 : 0.35188587661280046
Loss in iteration 109 : 0.35188192943313357
Loss in iteration 110 : 0.3518778844850503
Loss in iteration 111 : 0.3518684225550068
Loss in iteration 112 : 0.35185994527585357
Loss in iteration 113 : 0.3518515221965781
Loss in iteration 114 : 0.3518445538412594
Loss in iteration 115 : 0.3518377589417818
Loss in iteration 116 : 0.35183202597011237
Loss in iteration 117 : 0.3518311670563911
Loss in iteration 118 : 0.3518299541605152
Loss in iteration 119 : 0.351817405425895
Loss in iteration 120 : 0.351810028851958
Loss in iteration 121 : 0.3518032750994589
Loss in iteration 122 : 0.35179818936606505
Loss in iteration 123 : 0.35179625145368293
Loss in iteration 124 : 0.3517960246003822
Loss in iteration 125 : 0.3517845545159816
Loss in iteration 126 : 0.35177710495334585
Loss in iteration 127 : 0.3517746069267307
Loss in iteration 128 : 0.35176989076941345
Loss in iteration 129 : 0.35176451000022285
Loss in iteration 130 : 0.35175613892361485
Loss in iteration 131 : 0.35174997578793765
Loss in iteration 132 : 0.3517454082687564
Loss in iteration 133 : 0.35174387497911647
Loss in iteration 134 : 0.35174081463486356
Loss in iteration 135 : 0.35173707302656987
Loss in iteration 136 : 0.3517349838792138
Loss in iteration 137 : 0.3517343357731276
Loss in iteration 138 : 0.3517261581532059
Loss in iteration 139 : 0.3517221416581549
Loss in iteration 140 : 0.351713750173642
Loss in iteration 141 : 0.35171055026275944
Loss in iteration 142 : 0.35170140168012093
Loss in iteration 143 : 0.3516964028262964
Loss in iteration 144 : 0.35169112390508095
Loss in iteration 145 : 0.3516863106052803
Loss in iteration 146 : 0.3516820666197769
Loss in iteration 147 : 0.3516778740674721
Loss in iteration 148 : 0.35167677488390003
Loss in iteration 149 : 0.3516751372378008
Loss in iteration 150 : 0.3516722586416736
Loss in iteration 151 : 0.35167032996955694
Loss in iteration 152 : 0.3516701228018424
Loss in iteration 153 : 0.35166286448290984
Loss in iteration 154 : 0.3516627941324199
Loss in iteration 155 : 0.35165907445052397
Loss in iteration 156 : 0.35165346776640927
Loss in iteration 157 : 0.3516449191591069
Loss in iteration 158 : 0.3516402743868685
Loss in iteration 159 : 0.3516329457290944
Loss in iteration 160 : 0.3516281346269064
Loss in iteration 161 : 0.3516248351232863
Loss in iteration 162 : 0.3516211011464402
Loss in iteration 163 : 0.3516183864113765
Loss in iteration 164 : 0.35161862590878706
Loss in iteration 165 : 0.3516181509804919
Loss in iteration 166 : 0.35161536028810664
Loss in iteration 167 : 0.351612868708428
Loss in iteration 168 : 0.35160999965427364
Loss in iteration 169 : 0.35160472425577294
Loss in iteration 170 : 0.351603784744708
Loss in iteration 171 : 0.35159694644537737
Loss in iteration 172 : 0.3515956584169002
Loss in iteration 173 : 0.3515882221070756
Loss in iteration 174 : 0.35158458040339335
Loss in iteration 175 : 0.35157931906063256
Loss in iteration 176 : 0.3515754680728524
Loss in iteration 177 : 0.3515727156725502
Loss in iteration 178 : 0.35156956022283564
Loss in iteration 179 : 0.35157293255970185
Loss in iteration 180 : 0.3515707834623877
Loss in iteration 181 : 0.35157202737501747
Loss in iteration 182 : 0.35157715967929737
Loss in iteration 183 : 0.3515681574034005
Loss in iteration 184 : 0.35156587866532485
Loss in iteration 185 : 0.35155873929630543
Loss in iteration 186 : 0.35155054081526493
Loss in iteration 187 : 0.3515473872276702
Loss in iteration 188 : 0.3515416406840367
Loss in iteration 189 : 0.35153807529063713
Loss in iteration 190 : 0.3515354713780713
Loss in iteration 191 : 0.3515322176186233
Loss in iteration 192 : 0.3515307132379522
Loss in iteration 193 : 0.3515317140472775
Loss in iteration 194 : 0.3515310685535147
Loss in iteration 195 : 0.3515313849915361
Loss in iteration 196 : 0.35153029517825773
Loss in iteration 197 : 0.35152780838279085
Loss in iteration 198 : 0.35152441120342404
Loss in iteration 199 : 0.3515282760254245
Loss in iteration 200 : 0.3515227046984507
Testing accuracy  of updater 3 on alg 1 with rate 1.0 = 0.8501934770591487, training accuracy 0.8488943488943489, time elapsed: 2916 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4774902133052874
Loss in iteration 3 : 0.4439768389751117
Loss in iteration 4 : 0.43392685420864313
Loss in iteration 5 : 0.4255195415459319
Loss in iteration 6 : 0.41796801447998555
Loss in iteration 7 : 0.4108685933337114
Loss in iteration 8 : 0.4040851687371383
Loss in iteration 9 : 0.3978565315096912
Loss in iteration 10 : 0.3923850805543794
Loss in iteration 11 : 0.38759220886757534
Loss in iteration 12 : 0.38343238752605685
Loss in iteration 13 : 0.37989984074696137
Loss in iteration 14 : 0.37690263839092425
Loss in iteration 15 : 0.3743632007477711
Loss in iteration 16 : 0.3722387471318859
Loss in iteration 17 : 0.370421739484391
Loss in iteration 18 : 0.36887304925225106
Loss in iteration 19 : 0.3675079473483384
Loss in iteration 20 : 0.36629390677194423
Loss in iteration 21 : 0.36521773059141566
Loss in iteration 22 : 0.3642633591942527
Loss in iteration 23 : 0.36341341256575993
Loss in iteration 24 : 0.36263514936985575
Loss in iteration 25 : 0.36191895859216355
Loss in iteration 26 : 0.36127730470280384
Loss in iteration 27 : 0.36069486238129134
Loss in iteration 28 : 0.36015326410034
Loss in iteration 29 : 0.35965769275370807
Loss in iteration 30 : 0.3592008626645514
Loss in iteration 31 : 0.3587806749959116
Loss in iteration 32 : 0.35838371271078223
Loss in iteration 33 : 0.35801358009067746
Loss in iteration 34 : 0.3576830333852793
Loss in iteration 35 : 0.3573758497582246
Loss in iteration 36 : 0.3570868093855895
Loss in iteration 37 : 0.356820089923237
Loss in iteration 38 : 0.3565721575327672
Loss in iteration 39 : 0.35634004503486816
Loss in iteration 40 : 0.3561273635008555
Loss in iteration 41 : 0.3559297423364642
Loss in iteration 42 : 0.3557417847693929
Loss in iteration 43 : 0.35556454506199636
Loss in iteration 44 : 0.3554004729910505
Loss in iteration 45 : 0.3552452954476511
Loss in iteration 46 : 0.35510157132873704
Loss in iteration 47 : 0.3549672176721246
Loss in iteration 48 : 0.3548401873839975
Loss in iteration 49 : 0.3547191717825232
Loss in iteration 50 : 0.35460475805039987
Loss in iteration 51 : 0.3544973011070128
Loss in iteration 52 : 0.3543961660490581
Loss in iteration 53 : 0.3543020241219961
Loss in iteration 54 : 0.35421231226087574
Loss in iteration 55 : 0.3541270402057134
Loss in iteration 56 : 0.3540446449912686
Loss in iteration 57 : 0.3539659674177198
Loss in iteration 58 : 0.353892180436836
Loss in iteration 59 : 0.3538208018840396
Loss in iteration 60 : 0.3537532471155448
Loss in iteration 61 : 0.3536884856112435
Loss in iteration 62 : 0.35362508833377354
Loss in iteration 63 : 0.3535651493634336
Loss in iteration 64 : 0.3535075368500816
Loss in iteration 65 : 0.3534525636206525
Loss in iteration 66 : 0.3533988658227602
Loss in iteration 67 : 0.3533487114552669
Loss in iteration 68 : 0.3533012054919605
Loss in iteration 69 : 0.3532572159000058
Loss in iteration 70 : 0.3532115545097098
Loss in iteration 71 : 0.3531733047032916
Loss in iteration 72 : 0.35313310560923133
Loss in iteration 73 : 0.353093820101022
Loss in iteration 74 : 0.35305806251309857
Loss in iteration 75 : 0.3530224937655947
Loss in iteration 76 : 0.3529878659001703
Loss in iteration 77 : 0.35295561655915075
Loss in iteration 78 : 0.3529242491613907
Loss in iteration 79 : 0.35289202369804207
Loss in iteration 80 : 0.3528620848333549
Loss in iteration 81 : 0.3528317663276133
Loss in iteration 82 : 0.3528024877572881
Loss in iteration 83 : 0.35277540246415845
Loss in iteration 84 : 0.3527484366439228
Loss in iteration 85 : 0.3527232501839724
Loss in iteration 86 : 0.35269841892136033
Loss in iteration 87 : 0.35267404678358666
Loss in iteration 88 : 0.35265045944175455
Loss in iteration 89 : 0.35262769503329716
Loss in iteration 90 : 0.3526055238923267
Loss in iteration 91 : 0.35258441062870194
Loss in iteration 92 : 0.3525635831934757
Loss in iteration 93 : 0.35254363155596274
Loss in iteration 94 : 0.35252373360678535
Loss in iteration 95 : 0.3525051032560995
Loss in iteration 96 : 0.3524892075968928
Loss in iteration 97 : 0.3524709183019819
Loss in iteration 98 : 0.35245435391889235
Loss in iteration 99 : 0.352437080785703
Loss in iteration 100 : 0.3524212811734105
Loss in iteration 101 : 0.35240570442164487
Loss in iteration 102 : 0.352390726893158
Loss in iteration 103 : 0.35237607832363627
Loss in iteration 104 : 0.3523620976211922
Loss in iteration 105 : 0.3523486104132369
Loss in iteration 106 : 0.35233487891374105
Loss in iteration 107 : 0.352322242355324
Loss in iteration 108 : 0.3523114256733636
Loss in iteration 109 : 0.35229856810196547
Loss in iteration 110 : 0.3522861063407584
Loss in iteration 111 : 0.3522737418707538
Loss in iteration 112 : 0.35226276356284525
Loss in iteration 113 : 0.35225172668683
Loss in iteration 114 : 0.35224095862263355
Loss in iteration 115 : 0.3522304557802415
Loss in iteration 116 : 0.3522205313743523
Loss in iteration 117 : 0.3522117866766728
Loss in iteration 118 : 0.3522010259705712
Loss in iteration 119 : 0.35219151065190807
Loss in iteration 120 : 0.35218178620860985
Loss in iteration 121 : 0.35217337703354823
Loss in iteration 122 : 0.352164451587125
Loss in iteration 123 : 0.3521570404147139
Loss in iteration 124 : 0.35214660719781393
Loss in iteration 125 : 0.35213842112787896
Loss in iteration 126 : 0.35212927604886346
Loss in iteration 127 : 0.35212125515663434
Loss in iteration 128 : 0.3521140384262187
Loss in iteration 129 : 0.35210547312513607
Loss in iteration 130 : 0.3520999965507925
Loss in iteration 131 : 0.35209238823649813
Loss in iteration 132 : 0.35208382237679803
Loss in iteration 133 : 0.3520748268046681
Loss in iteration 134 : 0.35206691550888175
Loss in iteration 135 : 0.35205999329120236
Loss in iteration 136 : 0.3520537164690947
Loss in iteration 137 : 0.3520466801866216
Loss in iteration 138 : 0.3520398816986596
Loss in iteration 139 : 0.35203195300081114
Loss in iteration 140 : 0.3520254279550619
Loss in iteration 141 : 0.35201912926131523
Loss in iteration 142 : 0.3520151119810716
Loss in iteration 143 : 0.35200961531088226
Loss in iteration 144 : 0.352001393213016
Loss in iteration 145 : 0.3519939149978782
Loss in iteration 146 : 0.35198731071755796
Loss in iteration 147 : 0.35198151344278505
Loss in iteration 148 : 0.35197657052171194
Loss in iteration 149 : 0.351971651981699
Loss in iteration 150 : 0.35196663537913825
Loss in iteration 151 : 0.3519597240581421
Loss in iteration 152 : 0.35195499062244995
Loss in iteration 153 : 0.35194820637289614
Loss in iteration 154 : 0.3519407644143201
Loss in iteration 155 : 0.3519345926157979
Loss in iteration 156 : 0.35192933732348386
Loss in iteration 157 : 0.3519244292563761
Loss in iteration 158 : 0.3519186451103881
Loss in iteration 159 : 0.35191552535602466
Loss in iteration 160 : 0.3519100524185656
Loss in iteration 161 : 0.3519043471787521
Loss in iteration 162 : 0.3518975475602331
Loss in iteration 163 : 0.35189221976765883
Loss in iteration 164 : 0.35188767702633483
Loss in iteration 165 : 0.3518840677207278
Loss in iteration 166 : 0.35188014172426
Loss in iteration 167 : 0.35187486097932175
Loss in iteration 168 : 0.3518682714443846
Loss in iteration 169 : 0.3518633945285896
Loss in iteration 170 : 0.35185978394492207
Loss in iteration 171 : 0.3518542857768646
Loss in iteration 172 : 0.35184932350874387
Loss in iteration 173 : 0.3518457191454347
Loss in iteration 174 : 0.3518439101738925
Loss in iteration 175 : 0.35184211042952324
Loss in iteration 176 : 0.3518343629601185
Loss in iteration 177 : 0.3518285128106781
Loss in iteration 178 : 0.35182569066703623
Loss in iteration 179 : 0.3518225251462642
Loss in iteration 180 : 0.3518171918754301
Loss in iteration 181 : 0.3518117611244009
Loss in iteration 182 : 0.35180680259469993
Loss in iteration 183 : 0.35180230709704774
Loss in iteration 184 : 0.3517982855552616
Loss in iteration 185 : 0.35179418082926667
Loss in iteration 186 : 0.35179076081862926
Loss in iteration 187 : 0.3517895867574087
Loss in iteration 188 : 0.35178691913234755
Loss in iteration 189 : 0.3517800411202078
Loss in iteration 190 : 0.3517752921186756
Loss in iteration 191 : 0.35177197153446677
Loss in iteration 192 : 0.3517679698241403
Loss in iteration 193 : 0.35176654160617554
Loss in iteration 194 : 0.3517640788865729
Loss in iteration 195 : 0.35175881859740094
Loss in iteration 196 : 0.35175359351768837
Loss in iteration 197 : 0.3517492778288244
Loss in iteration 198 : 0.3517456862277814
Loss in iteration 199 : 0.3517430682614691
Loss in iteration 200 : 0.351738689050934
Testing accuracy  of updater 3 on alg 1 with rate 0.7 = 0.8498249493274369, training accuracy 0.8486179361179361, time elapsed: 2853 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.500382925015275
Loss in iteration 3 : 0.460878059137624
Loss in iteration 4 : 0.44949261704204274
Loss in iteration 5 : 0.4443212680270678
Loss in iteration 6 : 0.4399641293990744
Loss in iteration 7 : 0.43591130039934267
Loss in iteration 8 : 0.43201083598650364
Loss in iteration 9 : 0.42821902059047223
Loss in iteration 10 : 0.4245092855678388
Loss in iteration 11 : 0.4208448457084999
Loss in iteration 12 : 0.417208234440929
Loss in iteration 13 : 0.41359735989838675
Loss in iteration 14 : 0.41001298373513745
Loss in iteration 15 : 0.4065279671078838
Loss in iteration 16 : 0.40316669818830875
Loss in iteration 17 : 0.3999734402261049
Loss in iteration 18 : 0.39698222286078855
Loss in iteration 19 : 0.3941567469680573
Loss in iteration 20 : 0.39151198027752165
Loss in iteration 21 : 0.3890448889338296
Loss in iteration 22 : 0.3867421689768004
Loss in iteration 23 : 0.38460466407712907
Loss in iteration 24 : 0.3826309141876277
Loss in iteration 25 : 0.3808137781246576
Loss in iteration 26 : 0.3791381654566177
Loss in iteration 27 : 0.3776079583075823
Loss in iteration 28 : 0.3761882754090808
Loss in iteration 29 : 0.37488942718131707
Loss in iteration 30 : 0.37370025838676285
Loss in iteration 31 : 0.37261154810281266
Loss in iteration 32 : 0.3716028115677416
Loss in iteration 33 : 0.3706811216986594
Loss in iteration 34 : 0.36983120861271057
Loss in iteration 35 : 0.3690326110449747
Loss in iteration 36 : 0.36828380531431504
Loss in iteration 37 : 0.3675913964011413
Loss in iteration 38 : 0.3669419739080087
Loss in iteration 39 : 0.36632988227248403
Loss in iteration 40 : 0.36575289565907104
Loss in iteration 41 : 0.36521121831806025
Loss in iteration 42 : 0.3647101544281468
Loss in iteration 43 : 0.36423156325154876
Loss in iteration 44 : 0.3637755240037637
Loss in iteration 45 : 0.3633389090955247
Loss in iteration 46 : 0.3629265855480588
Loss in iteration 47 : 0.3625286371307988
Loss in iteration 48 : 0.362148302220793
Loss in iteration 49 : 0.36178729967560513
Loss in iteration 50 : 0.3614476878258596
Loss in iteration 51 : 0.3611252686356649
Loss in iteration 52 : 0.3608155817799551
Loss in iteration 53 : 0.3605198291322944
Loss in iteration 54 : 0.3602406879440679
Loss in iteration 55 : 0.3599739484388467
Loss in iteration 56 : 0.3597198396161776
Loss in iteration 57 : 0.3594754422432381
Loss in iteration 58 : 0.3592419976723854
Loss in iteration 59 : 0.3590165484167389
Loss in iteration 60 : 0.35879541743347315
Loss in iteration 61 : 0.3585830735848772
Loss in iteration 62 : 0.35837568432730477
Loss in iteration 63 : 0.3581758853533455
Loss in iteration 64 : 0.3579875556058986
Loss in iteration 65 : 0.3578064485856952
Loss in iteration 66 : 0.3576364559887194
Loss in iteration 67 : 0.3574711374334855
Loss in iteration 68 : 0.35731096508345517
Loss in iteration 69 : 0.3571573482502363
Loss in iteration 70 : 0.35701025473354925
Loss in iteration 71 : 0.35686771493979685
Loss in iteration 72 : 0.35672854938061177
Loss in iteration 73 : 0.3565968577282894
Loss in iteration 74 : 0.35647202731642585
Loss in iteration 75 : 0.3563540493489587
Loss in iteration 76 : 0.3562409329815374
Loss in iteration 77 : 0.3561317320107536
Loss in iteration 78 : 0.3560260687811828
Loss in iteration 79 : 0.355924197865519
Loss in iteration 80 : 0.35582602687282067
Loss in iteration 81 : 0.35573078557103405
Loss in iteration 82 : 0.3556375056038988
Loss in iteration 83 : 0.35554715155895217
Loss in iteration 84 : 0.35546042298794633
Loss in iteration 85 : 0.35537627484901924
Loss in iteration 86 : 0.35529519519537833
Loss in iteration 87 : 0.35521697735401964
Loss in iteration 88 : 0.35514138115208477
Loss in iteration 89 : 0.3550674337654051
Loss in iteration 90 : 0.354994698035592
Loss in iteration 91 : 0.35492550110684734
Loss in iteration 92 : 0.35486071218652465
Loss in iteration 93 : 0.3547977125473268
Loss in iteration 94 : 0.35473561642289814
Loss in iteration 95 : 0.3546744093270835
Loss in iteration 96 : 0.35461493720453013
Loss in iteration 97 : 0.35455771602518765
Loss in iteration 98 : 0.35450201918051566
Loss in iteration 99 : 0.3544475866426088
Loss in iteration 100 : 0.35439451821300016
Loss in iteration 101 : 0.3543428814374247
Loss in iteration 102 : 0.3542938331837316
Loss in iteration 103 : 0.35424586359629223
Loss in iteration 104 : 0.35419914082828674
Loss in iteration 105 : 0.35415352715655435
Loss in iteration 106 : 0.3541081400511281
Loss in iteration 107 : 0.35406430267215816
Loss in iteration 108 : 0.3540214715588854
Loss in iteration 109 : 0.35397916781184324
Loss in iteration 110 : 0.35393785450429216
Loss in iteration 111 : 0.3538980635399782
Loss in iteration 112 : 0.35385903717610434
Loss in iteration 113 : 0.3538203821714253
Loss in iteration 114 : 0.353782852074128
Loss in iteration 115 : 0.353746480282113
Loss in iteration 116 : 0.3537108402406819
Loss in iteration 117 : 0.35367639420508074
Loss in iteration 118 : 0.3536424280397399
Loss in iteration 119 : 0.3536089764839361
Loss in iteration 120 : 0.353576553140168
Loss in iteration 121 : 0.35354513441368324
Loss in iteration 122 : 0.3535150003975072
Loss in iteration 123 : 0.3534842294084434
Loss in iteration 124 : 0.353455388411045
Loss in iteration 125 : 0.3534256747005806
Loss in iteration 126 : 0.35339729802326436
Loss in iteration 127 : 0.3533696206765489
Loss in iteration 128 : 0.35334243956307954
Loss in iteration 129 : 0.3533148427991216
Loss in iteration 130 : 0.35329014073074233
Loss in iteration 131 : 0.353264799104591
Loss in iteration 132 : 0.35324112924164336
Loss in iteration 133 : 0.3532162487209139
Loss in iteration 134 : 0.3531940908844158
Loss in iteration 135 : 0.3531701939954508
Loss in iteration 136 : 0.3531479008395722
Loss in iteration 137 : 0.35312646775930306
Loss in iteration 138 : 0.3531057563684201
Loss in iteration 139 : 0.35308494712654515
Loss in iteration 140 : 0.3530658370731926
Loss in iteration 141 : 0.3530458069195448
Loss in iteration 142 : 0.35302721308021945
Loss in iteration 143 : 0.35300818754423335
Loss in iteration 144 : 0.3529901373637875
Loss in iteration 145 : 0.352971806337852
Loss in iteration 146 : 0.35295368079631734
Loss in iteration 147 : 0.35293602823695175
Loss in iteration 148 : 0.3529190389693173
Loss in iteration 149 : 0.35290120395113944
Loss in iteration 150 : 0.3528846752741319
Loss in iteration 151 : 0.35286727062915074
Loss in iteration 152 : 0.3528505940009349
Loss in iteration 153 : 0.35283459372846476
Loss in iteration 154 : 0.3528192474165384
Loss in iteration 155 : 0.3528033580443989
Loss in iteration 156 : 0.3527885525874497
Loss in iteration 157 : 0.35277337031301204
Loss in iteration 158 : 0.352759113019187
Loss in iteration 159 : 0.3527451588420183
Loss in iteration 160 : 0.3527315549289502
Loss in iteration 161 : 0.3527180330456868
Loss in iteration 162 : 0.3527048577425338
Loss in iteration 163 : 0.3526924264437351
Loss in iteration 164 : 0.3526792243722974
Loss in iteration 165 : 0.35266646039574867
Loss in iteration 166 : 0.3526534769225511
Loss in iteration 167 : 0.3526408571714606
Loss in iteration 168 : 0.35262862300392256
Loss in iteration 169 : 0.352616709814338
Loss in iteration 170 : 0.35260484988688695
Loss in iteration 171 : 0.35259336306303374
Loss in iteration 172 : 0.35258192842613384
Loss in iteration 173 : 0.352570837526398
Loss in iteration 174 : 0.352560149604089
Loss in iteration 175 : 0.35254972115067956
Loss in iteration 176 : 0.35253918708452686
Loss in iteration 177 : 0.35252904933373236
Loss in iteration 178 : 0.3525191133299761
Loss in iteration 179 : 0.35250950053340696
Loss in iteration 180 : 0.3524994122689006
Loss in iteration 181 : 0.35249005999590477
Loss in iteration 182 : 0.3524805999511921
Loss in iteration 183 : 0.3524714519789878
Loss in iteration 184 : 0.3524621387521806
Loss in iteration 185 : 0.35245303032143055
Loss in iteration 186 : 0.35244415627356207
Loss in iteration 187 : 0.35243532204220995
Loss in iteration 188 : 0.35242656209681306
Loss in iteration 189 : 0.3524178898118222
Loss in iteration 190 : 0.35240934398716744
Loss in iteration 191 : 0.35240118697931094
Loss in iteration 192 : 0.35239267202698393
Loss in iteration 193 : 0.35238460178384856
Loss in iteration 194 : 0.35237673186705626
Loss in iteration 195 : 0.3523687103632928
Loss in iteration 196 : 0.35236105564867437
Loss in iteration 197 : 0.3523534597754324
Loss in iteration 198 : 0.35234554505093757
Loss in iteration 199 : 0.3523379593061028
Loss in iteration 200 : 0.3523305492978879
Testing accuracy  of updater 3 on alg 1 with rate 0.4 = 0.8492107364412506, training accuracy 0.8485565110565111, time elapsed: 3001 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8720016851270128
Loss in iteration 3 : 0.7476583774773887
Loss in iteration 4 : 0.6265444551972207
Loss in iteration 5 : 0.5221182153367173
Loss in iteration 6 : 0.47925858962421775
Loss in iteration 7 : 0.4673638231115323
Loss in iteration 8 : 0.46217405403854267
Loss in iteration 9 : 0.4591556438985386
Loss in iteration 10 : 0.45707837024732195
Loss in iteration 11 : 0.4554411457418072
Loss in iteration 12 : 0.45409986005817715
Loss in iteration 13 : 0.4528866097464573
Loss in iteration 14 : 0.4517449523793229
Loss in iteration 15 : 0.4506720692644968
Loss in iteration 16 : 0.44965276044945457
Loss in iteration 17 : 0.4486660252916138
Loss in iteration 18 : 0.44770289121498014
Loss in iteration 19 : 0.4467562829305696
Loss in iteration 20 : 0.4458225728968804
Loss in iteration 21 : 0.44489805972506286
Loss in iteration 22 : 0.44398155198343936
Loss in iteration 23 : 0.4430734776234119
Loss in iteration 24 : 0.4421751748562105
Loss in iteration 25 : 0.4412846898795088
Loss in iteration 26 : 0.44040012145263574
Loss in iteration 27 : 0.4395220468228444
Loss in iteration 28 : 0.43864935278720574
Loss in iteration 29 : 0.4377836005616883
Loss in iteration 30 : 0.43692410811863036
Loss in iteration 31 : 0.436067644585498
Loss in iteration 32 : 0.43521389043152614
Loss in iteration 33 : 0.4343630988308954
Loss in iteration 34 : 0.43351436417780415
Loss in iteration 35 : 0.4326679186082126
Loss in iteration 36 : 0.43182497006546383
Loss in iteration 37 : 0.4309848108091
Loss in iteration 38 : 0.43014733853726117
Loss in iteration 39 : 0.42931153379594084
Loss in iteration 40 : 0.42847744205388727
Loss in iteration 41 : 0.4276444640613265
Loss in iteration 42 : 0.4268131278360699
Loss in iteration 43 : 0.42598398632885504
Loss in iteration 44 : 0.42515701250969967
Loss in iteration 45 : 0.424331500710964
Loss in iteration 46 : 0.42350733796166085
Loss in iteration 47 : 0.42268489080068933
Loss in iteration 48 : 0.42186384160858376
Loss in iteration 49 : 0.4210441185177316
Loss in iteration 50 : 0.4202258118876437
Loss in iteration 51 : 0.4194084289934514
Loss in iteration 52 : 0.4185925606482324
Loss in iteration 53 : 0.4177784519356182
Loss in iteration 54 : 0.41696531963976474
Loss in iteration 55 : 0.4161531940894183
Loss in iteration 56 : 0.41534237779483196
Loss in iteration 57 : 0.41453303471430303
Loss in iteration 58 : 0.41372436744616403
Loss in iteration 59 : 0.41291657187992464
Loss in iteration 60 : 0.41211147225616196
Loss in iteration 61 : 0.41131030145049624
Loss in iteration 62 : 0.41051696893698586
Loss in iteration 63 : 0.40973175987724036
Loss in iteration 64 : 0.4089585598477734
Loss in iteration 65 : 0.40818973762269745
Loss in iteration 66 : 0.4074241120051555
Loss in iteration 67 : 0.40666079334842997
Loss in iteration 68 : 0.40590225860850465
Loss in iteration 69 : 0.40515219862624857
Loss in iteration 70 : 0.40441104397347155
Loss in iteration 71 : 0.4036797741274541
Loss in iteration 72 : 0.40295269341502554
Loss in iteration 73 : 0.40223165131268296
Loss in iteration 74 : 0.4015298882537659
Loss in iteration 75 : 0.40083511342159234
Loss in iteration 76 : 0.4001482223127232
Loss in iteration 77 : 0.3994724984670936
Loss in iteration 78 : 0.3988039941684814
Loss in iteration 79 : 0.39814053167612645
Loss in iteration 80 : 0.397485650212116
Loss in iteration 81 : 0.39683875718787387
Loss in iteration 82 : 0.3962016809736496
Loss in iteration 83 : 0.39557587338876254
Loss in iteration 84 : 0.394957652905871
Loss in iteration 85 : 0.3943525780205861
Loss in iteration 86 : 0.39375570204967564
Loss in iteration 87 : 0.39316294663296597
Loss in iteration 88 : 0.3925763108404022
Loss in iteration 89 : 0.3920007982116834
Loss in iteration 90 : 0.3914332049606081
Loss in iteration 91 : 0.39087492991923656
Loss in iteration 92 : 0.39033123798676916
Loss in iteration 93 : 0.38979475902011224
Loss in iteration 94 : 0.38926261105946514
Loss in iteration 95 : 0.3887402594734587
Loss in iteration 96 : 0.388229704647859
Loss in iteration 97 : 0.3877245095498108
Loss in iteration 98 : 0.3872254015900925
Loss in iteration 99 : 0.3867369702683697
Loss in iteration 100 : 0.38626056170093687
Loss in iteration 101 : 0.38579052457081703
Loss in iteration 102 : 0.3853274042334859
Loss in iteration 103 : 0.3848683524573727
Loss in iteration 104 : 0.38441742105807863
Loss in iteration 105 : 0.3839810818731748
Loss in iteration 106 : 0.3835556495429211
Loss in iteration 107 : 0.3831339473750036
Loss in iteration 108 : 0.38271753363601596
Loss in iteration 109 : 0.38231038908602394
Loss in iteration 110 : 0.3819106476413094
Loss in iteration 111 : 0.38151768077246
Loss in iteration 112 : 0.38113174362716734
Loss in iteration 113 : 0.3807519022392315
Loss in iteration 114 : 0.3803792465774274
Loss in iteration 115 : 0.38001105961862136
Loss in iteration 116 : 0.37965065685469296
Loss in iteration 117 : 0.37929733844222474
Loss in iteration 118 : 0.3789512019491602
Loss in iteration 119 : 0.37861332150601584
Loss in iteration 120 : 0.3782835035352464
Loss in iteration 121 : 0.3779588776831566
Loss in iteration 122 : 0.3776377998577315
Loss in iteration 123 : 0.3773230129457015
Loss in iteration 124 : 0.3770138625395463
Loss in iteration 125 : 0.3767091977011592
Loss in iteration 126 : 0.3764089933996158
Loss in iteration 127 : 0.37611406810156756
Loss in iteration 128 : 0.37582491987142275
Loss in iteration 129 : 0.3755420888416192
Loss in iteration 130 : 0.375266403292661
Loss in iteration 131 : 0.3749973241684228
Loss in iteration 132 : 0.374731859987009
Loss in iteration 133 : 0.3744705928146261
Loss in iteration 134 : 0.374215562207205
Loss in iteration 135 : 0.37396466723929794
Loss in iteration 136 : 0.3737177256916684
Loss in iteration 137 : 0.3734776847038139
Loss in iteration 138 : 0.3732414517060204
Loss in iteration 139 : 0.373009132075809
Loss in iteration 140 : 0.3727827562636224
Loss in iteration 141 : 0.37255969173955367
Loss in iteration 142 : 0.3723401840797941
Loss in iteration 143 : 0.37212436822417333
Loss in iteration 144 : 0.37191144949967453
Loss in iteration 145 : 0.3717025024150249
Loss in iteration 146 : 0.3714988414383007
Loss in iteration 147 : 0.3712982178342808
Loss in iteration 148 : 0.37110031048903086
Loss in iteration 149 : 0.3709049145385243
Loss in iteration 150 : 0.37071240708966946
Loss in iteration 151 : 0.37052265541536955
Loss in iteration 152 : 0.3703356667452758
Loss in iteration 153 : 0.3701504513089491
Loss in iteration 154 : 0.3699676073199214
Loss in iteration 155 : 0.36978756881072467
Loss in iteration 156 : 0.3696107196846711
Loss in iteration 157 : 0.3694362845342783
Loss in iteration 158 : 0.36926401598533243
Loss in iteration 159 : 0.36909430383715297
Loss in iteration 160 : 0.36892787220759665
Loss in iteration 161 : 0.36876393814084885
Loss in iteration 162 : 0.36860325136783384
Loss in iteration 163 : 0.3684439477479463
Loss in iteration 164 : 0.36828711973229644
Loss in iteration 165 : 0.3681329765706758
Loss in iteration 166 : 0.36798070365339924
Loss in iteration 167 : 0.36783013718583946
Loss in iteration 168 : 0.36768101728225394
Loss in iteration 169 : 0.36753369286489684
Loss in iteration 170 : 0.3673887396783508
Loss in iteration 171 : 0.36724602874164747
Loss in iteration 172 : 0.3671053521683466
Loss in iteration 173 : 0.36696610969968013
Loss in iteration 174 : 0.36682937634469864
Loss in iteration 175 : 0.3666946519720939
Loss in iteration 176 : 0.36656226165494094
Loss in iteration 177 : 0.366431989330176
Loss in iteration 178 : 0.3663038414543773
Loss in iteration 179 : 0.3661776929107179
Loss in iteration 180 : 0.36605303762711244
Loss in iteration 181 : 0.36592963276746565
Loss in iteration 182 : 0.3658077274519041
Loss in iteration 183 : 0.36568808949049914
Loss in iteration 184 : 0.3655698375779127
Loss in iteration 185 : 0.36545335035160453
Loss in iteration 186 : 0.3653383399879364
Loss in iteration 187 : 0.365224413755653
Loss in iteration 188 : 0.36511208555747976
Loss in iteration 189 : 0.3650018043246432
Loss in iteration 190 : 0.36489294355997914
Loss in iteration 191 : 0.36478526702448694
Loss in iteration 192 : 0.36467923634971927
Loss in iteration 193 : 0.36457432228189035
Loss in iteration 194 : 0.3644700889456839
Loss in iteration 195 : 0.3643666412047259
Loss in iteration 196 : 0.3642643473558515
Loss in iteration 197 : 0.3641653827036729
Loss in iteration 198 : 0.3640674794753823
Loss in iteration 199 : 0.363970275867451
Loss in iteration 200 : 0.36387359941862296
Testing accuracy  of updater 3 on alg 1 with rate 0.09999999999999998 = 0.8451569313924207, training accuracy 0.8435810810810811, time elapsed: 2723 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9977087497627372
Loss in iteration 3 : 0.9953579969035659
Loss in iteration 4 : 0.9929664067709228
Loss in iteration 5 : 0.9905431201094781
Loss in iteration 6 : 0.9880935317584683
Loss in iteration 7 : 0.985621185368393
Loss in iteration 8 : 0.9831285763315405
Loss in iteration 9 : 0.9806175498414772
Loss in iteration 10 : 0.9780895205383133
Loss in iteration 11 : 0.975545603559427
Loss in iteration 12 : 0.9729866975429976
Loss in iteration 13 : 0.9704135397413154
Loss in iteration 14 : 0.9678267440433482
Loss in iteration 15 : 0.9652268280511056
Loss in iteration 16 : 0.9626142328817667
Loss in iteration 17 : 0.9599893379828666
Loss in iteration 18 : 0.9573524724357892
Loss in iteration 19 : 0.9547039237281607
Loss in iteration 20 : 0.952043944664537
Loss in iteration 21 : 0.9493727588826766
Loss in iteration 22 : 0.9466905653086299
Loss in iteration 23 : 0.9439975417924332
Loss in iteration 24 : 0.9412938481029236
Loss in iteration 25 : 0.9385796284153707
Loss in iteration 26 : 0.9358550133933228
Loss in iteration 27 : 0.9331201219428202
Loss in iteration 28 : 0.9303750626994401
Loss in iteration 29 : 0.9276199352960015
Loss in iteration 30 : 0.9248548314485451
Loss in iteration 31 : 0.9220798358909987
Loss in iteration 32 : 0.9192950271827545
Loss in iteration 33 : 0.9165004784092066
Loss in iteration 34 : 0.9136962577913823
Loss in iteration 35 : 0.9108824292180975
Loss in iteration 36 : 0.908059052711875
Loss in iteration 37 : 0.9052261848377771
Loss in iteration 38 : 0.9023838790630794
Loss in iteration 39 : 0.8995321860743655
Loss in iteration 40 : 0.8966711540575149
Loss in iteration 41 : 0.8938008289455334
Loss in iteration 42 : 0.8909212546381692
Loss in iteration 43 : 0.8880324731968079
Loss in iteration 44 : 0.8851345250177647
Loss in iteration 45 : 0.8822274489864853
Loss in iteration 46 : 0.8793112826149487
Loss in iteration 47 : 0.8763860621642694
Loss in iteration 48 : 0.8734518227541774
Loss in iteration 49 : 0.8705085984608499
Loss in iteration 50 : 0.8675564224045526
Loss in iteration 51 : 0.8645953268280513
Loss in iteration 52 : 0.8616253431669262
Loss in iteration 53 : 0.8586465021127108
Loss in iteration 54 : 0.8556588336695472
Loss in iteration 55 : 0.8526623672051329
Loss in iteration 56 : 0.8496571314965649
Loss in iteration 57 : 0.8466431547716006
Loss in iteration 58 : 0.8436204647458604
Loss in iteration 59 : 0.8405890886563842
Loss in iteration 60 : 0.8375490532919295
Loss in iteration 61 : 0.8345003850203724
Loss in iteration 62 : 0.8314431098134553
Loss in iteration 63 : 0.828377253269256
Loss in iteration 64 : 0.8253028406325082
Loss in iteration 65 : 0.8222198968131035
Loss in iteration 66 : 0.8191284464028322
Loss in iteration 67 : 0.8160285136906957
Loss in iteration 68 : 0.8129201226767849
Loss in iteration 69 : 0.8098032970850041
Loss in iteration 70 : 0.806678060374646
Loss in iteration 71 : 0.8035444357510212
Loss in iteration 72 : 0.8004024461751766
Loss in iteration 73 : 0.7972521143728094
Loss in iteration 74 : 0.7940934628425066
Loss in iteration 75 : 0.7909265138632673
Loss in iteration 76 : 0.7877512895015191
Loss in iteration 77 : 0.7845678116175413
Loss in iteration 78 : 0.7813761018714369
Loss in iteration 79 : 0.7781761817287067
Loss in iteration 80 : 0.7749680724653699
Loss in iteration 81 : 0.7717517951727897
Loss in iteration 82 : 0.7685273707621609
Loss in iteration 83 : 0.7652948199686974
Loss in iteration 84 : 0.7620541633555835
Loss in iteration 85 : 0.7588054213176681
Loss in iteration 86 : 0.7555486140849521
Loss in iteration 87 : 0.7522837617258629
Loss in iteration 88 : 0.7490108841503947
Loss in iteration 89 : 0.7457300011130314
Loss in iteration 90 : 0.7424411322155554
Loss in iteration 91 : 0.7391442969097187
Loss in iteration 92 : 0.7358395144997578
Loss in iteration 93 : 0.7325268041448685
Loss in iteration 94 : 0.7292061848614665
Loss in iteration 95 : 0.7258776755254549
Loss in iteration 96 : 0.7225412948743655
Loss in iteration 97 : 0.7191970615093684
Loss in iteration 98 : 0.7158449938973126
Loss in iteration 99 : 0.712485110372605
Loss in iteration 100 : 0.7091174291390389
Loss in iteration 101 : 0.7057419682716305
Loss in iteration 102 : 0.7023587457183049
Loss in iteration 103 : 0.6989677793016047
Loss in iteration 104 : 0.6955690867202924
Loss in iteration 105 : 0.6921626855509708
Loss in iteration 106 : 0.6887485932495816
Loss in iteration 107 : 0.6853268271529536
Loss in iteration 108 : 0.6818974044802322
Loss in iteration 109 : 0.678460342334328
Loss in iteration 110 : 0.6750156577032986
Loss in iteration 111 : 0.6715633674617241
Loss in iteration 112 : 0.6681034883720306
Loss in iteration 113 : 0.6646360370858122
Loss in iteration 114 : 0.6611610301450661
Loss in iteration 115 : 0.6576784839835038
Loss in iteration 116 : 0.6541884149277297
Loss in iteration 117 : 0.6506908391984609
Loss in iteration 118 : 0.6471857729117172
Loss in iteration 119 : 0.643673232079948
Loss in iteration 120 : 0.6401532326132013
Loss in iteration 121 : 0.6366257903202119
Loss in iteration 122 : 0.6330909209095311
Loss in iteration 123 : 0.6295486399905743
Loss in iteration 124 : 0.625998963074682
Loss in iteration 125 : 0.6224419055761862
Loss in iteration 126 : 0.6188774828133994
Loss in iteration 127 : 0.6153057100096583
Loss in iteration 128 : 0.6117266022942881
Loss in iteration 129 : 0.6081401747035867
Loss in iteration 130 : 0.6045464421817941
Loss in iteration 131 : 0.6009454195820173
Loss in iteration 132 : 0.5973371216671889
Loss in iteration 133 : 0.5937215631109656
Loss in iteration 134 : 0.5900987584986261
Loss in iteration 135 : 0.5864687223279763
Loss in iteration 136 : 0.5828314690102302
Loss in iteration 137 : 0.5791870128708456
Loss in iteration 138 : 0.575535368150409
Loss in iteration 139 : 0.5718765490054537
Loss in iteration 140 : 0.5682105695092902
Loss in iteration 141 : 0.5645374436528346
Loss in iteration 142 : 0.5608571853453889
Loss in iteration 143 : 0.5571698084154612
Loss in iteration 144 : 0.553475326611528
Loss in iteration 145 : 0.5497737536028113
Loss in iteration 146 : 0.5460651029800441
Loss in iteration 147 : 0.542349388256213
Loss in iteration 148 : 0.5386266228672953
Loss in iteration 149 : 0.5348968201730112
Loss in iteration 150 : 0.531159993457507
Loss in iteration 151 : 0.5274161559300995
Loss in iteration 152 : 0.5236653207259719
Loss in iteration 153 : 0.5199075009068397
Loss in iteration 154 : 0.5161427094616552
Loss in iteration 155 : 0.5123709593072967
Loss in iteration 156 : 0.5085922632891916
Loss in iteration 157 : 0.5048066341820087
Loss in iteration 158 : 0.5010140846902981
Loss in iteration 159 : 0.4972146274491133
Loss in iteration 160 : 0.4934082750246682
Loss in iteration 161 : 0.4895950399149443
Loss in iteration 162 : 0.4857749345503077
Loss in iteration 163 : 0.48252415431212503
Loss in iteration 164 : 0.4824638140561152
Loss in iteration 165 : 0.4820813663726612
Loss in iteration 166 : 0.48182888927255657
Loss in iteration 167 : 0.48166260441360437
Loss in iteration 168 : 0.4814422189549062
Loss in iteration 169 : 0.48126452179695167
Loss in iteration 170 : 0.48108893579965945
Loss in iteration 171 : 0.48091943019781797
Loss in iteration 172 : 0.48075097927077126
Loss in iteration 173 : 0.48058247456008396
Loss in iteration 174 : 0.4804139420331518
Loss in iteration 175 : 0.480245713404757
Loss in iteration 176 : 0.4800784114318449
Loss in iteration 177 : 0.47991149350466383
Loss in iteration 178 : 0.4797445591302583
Loss in iteration 179 : 0.4795771951890519
Loss in iteration 180 : 0.47940960139540334
Loss in iteration 181 : 0.4792415055055576
Loss in iteration 182 : 0.47907266955791084
Loss in iteration 183 : 0.47890294306401093
Loss in iteration 184 : 0.4787323343365096
Loss in iteration 185 : 0.4785607435208494
Loss in iteration 186 : 0.47838811846453155
Loss in iteration 187 : 0.4782144289549595
Loss in iteration 188 : 0.47803967733343533
Loss in iteration 189 : 0.47786368466662843
Loss in iteration 190 : 0.47768635696316364
Loss in iteration 191 : 0.4775076505627495
Loss in iteration 192 : 0.47732761912286575
Loss in iteration 193 : 0.47714606337154863
Loss in iteration 194 : 0.4769630004550087
Loss in iteration 195 : 0.4767783560410737
Loss in iteration 196 : 0.476592104625517
Loss in iteration 197 : 0.47640432310049674
Loss in iteration 198 : 0.4762148648892651
Loss in iteration 199 : 0.47602370000718075
Loss in iteration 200 : 0.47583077403890645
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.7637737239727289, training accuracy 0.7591830466830467, time elapsed: 2789 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9977087497627372
Loss in iteration 3 : 0.9953579969035659
Loss in iteration 4 : 0.9929664067709228
Loss in iteration 5 : 0.9905431201094781
Loss in iteration 6 : 0.9880935317584683
Loss in iteration 7 : 0.985621185368393
Loss in iteration 8 : 0.9831285763315405
Loss in iteration 9 : 0.9806175498414772
Loss in iteration 10 : 0.9780895205383133
Loss in iteration 11 : 0.975545603559427
Loss in iteration 12 : 0.9729866975429976
Loss in iteration 13 : 0.9704135397413154
Loss in iteration 14 : 0.9678267440433482
Loss in iteration 15 : 0.9652268280511056
Loss in iteration 16 : 0.9626142328817667
Loss in iteration 17 : 0.9599893379828666
Loss in iteration 18 : 0.9573524724357892
Loss in iteration 19 : 0.9547039237281607
Loss in iteration 20 : 0.952043944664537
Loss in iteration 21 : 0.9493727588826766
Loss in iteration 22 : 0.9466905653086299
Loss in iteration 23 : 0.9439975417924332
Loss in iteration 24 : 0.9412938481029236
Loss in iteration 25 : 0.9385796284153707
Loss in iteration 26 : 0.9358550133933228
Loss in iteration 27 : 0.9331201219428202
Loss in iteration 28 : 0.9303750626994401
Loss in iteration 29 : 0.9276199352960015
Loss in iteration 30 : 0.9248548314485451
Loss in iteration 31 : 0.9220798358909987
Loss in iteration 32 : 0.9192950271827545
Loss in iteration 33 : 0.9165004784092066
Loss in iteration 34 : 0.9136962577913823
Loss in iteration 35 : 0.9108824292180975
Loss in iteration 36 : 0.908059052711875
Loss in iteration 37 : 0.9052261848377771
Loss in iteration 38 : 0.9023838790630794
Loss in iteration 39 : 0.8995321860743655
Loss in iteration 40 : 0.8966711540575149
Loss in iteration 41 : 0.8938008289455334
Loss in iteration 42 : 0.8909212546381692
Loss in iteration 43 : 0.8880324731968079
Loss in iteration 44 : 0.8851345250177647
Loss in iteration 45 : 0.8822274489864853
Loss in iteration 46 : 0.8793112826149487
Loss in iteration 47 : 0.8763860621642694
Loss in iteration 48 : 0.8734518227541774
Loss in iteration 49 : 0.8705085984608499
Loss in iteration 50 : 0.8675564224045526
Loss in iteration 51 : 0.8645953268280513
Loss in iteration 52 : 0.8616253431669262
Loss in iteration 53 : 0.8586465021127108
Loss in iteration 54 : 0.8556588336695472
Loss in iteration 55 : 0.8526623672051329
Loss in iteration 56 : 0.8496571314965649
Loss in iteration 57 : 0.8466431547716006
Loss in iteration 58 : 0.8436204647458604
Loss in iteration 59 : 0.8405890886563842
Loss in iteration 60 : 0.8375490532919295
Loss in iteration 61 : 0.8345003850203724
Loss in iteration 62 : 0.8314431098134553
Loss in iteration 63 : 0.828377253269256
Loss in iteration 64 : 0.8253028406325082
Loss in iteration 65 : 0.8222198968131035
Loss in iteration 66 : 0.8191284464028322
Loss in iteration 67 : 0.8160285136906957
Loss in iteration 68 : 0.8129201226767849
Loss in iteration 69 : 0.8098032970850041
Loss in iteration 70 : 0.806678060374646
Loss in iteration 71 : 0.8035444357510212
Loss in iteration 72 : 0.8004024461751766
Loss in iteration 73 : 0.7972521143728094
Loss in iteration 74 : 0.7940934628425066
Loss in iteration 75 : 0.7909265138632673
Loss in iteration 76 : 0.7877512895015191
Loss in iteration 77 : 0.7845678116175413
Loss in iteration 78 : 0.7813761018714369
Loss in iteration 79 : 0.7781761817287067
Loss in iteration 80 : 0.7749680724653699
Loss in iteration 81 : 0.7717517951727897
Loss in iteration 82 : 0.7685273707621609
Loss in iteration 83 : 0.7652948199686974
Loss in iteration 84 : 0.7620541633555835
Loss in iteration 85 : 0.7588054213176681
Loss in iteration 86 : 0.7555486140849521
Loss in iteration 87 : 0.7522837617258629
Loss in iteration 88 : 0.7490108841503947
Loss in iteration 89 : 0.7457300011130314
Loss in iteration 90 : 0.7424411322155554
Loss in iteration 91 : 0.7391442969097187
Loss in iteration 92 : 0.7358395144997578
Loss in iteration 93 : 0.7325268041448685
Loss in iteration 94 : 0.7292061848614665
Loss in iteration 95 : 0.7258776755254549
Loss in iteration 96 : 0.7225412948743655
Loss in iteration 97 : 0.7191970615093684
Loss in iteration 98 : 0.7158449938973126
Loss in iteration 99 : 0.712485110372605
Loss in iteration 100 : 0.7091174291390389
Loss in iteration 101 : 0.7057419682716305
Loss in iteration 102 : 0.7023587457183049
Loss in iteration 103 : 0.6989677793016047
Loss in iteration 104 : 0.6955690867202924
Loss in iteration 105 : 0.6921626855509708
Loss in iteration 106 : 0.6887485932495816
Loss in iteration 107 : 0.6853268271529536
Loss in iteration 108 : 0.6818974044802322
Loss in iteration 109 : 0.678460342334328
Loss in iteration 110 : 0.6750156577032986
Loss in iteration 111 : 0.6715633674617241
Loss in iteration 112 : 0.6681034883720306
Loss in iteration 113 : 0.6646360370858122
Loss in iteration 114 : 0.6611610301450661
Loss in iteration 115 : 0.6576784839835038
Loss in iteration 116 : 0.6541884149277297
Loss in iteration 117 : 0.6506908391984609
Loss in iteration 118 : 0.6471857729117172
Loss in iteration 119 : 0.643673232079948
Loss in iteration 120 : 0.6401532326132013
Loss in iteration 121 : 0.6366257903202119
Loss in iteration 122 : 0.6330909209095311
Loss in iteration 123 : 0.6295486399905743
Loss in iteration 124 : 0.625998963074682
Loss in iteration 125 : 0.6224419055761862
Loss in iteration 126 : 0.6188774828133994
Loss in iteration 127 : 0.6153057100096583
Loss in iteration 128 : 0.6117266022942881
Loss in iteration 129 : 0.6081401747035867
Loss in iteration 130 : 0.6045464421817941
Loss in iteration 131 : 0.6009454195820173
Loss in iteration 132 : 0.5973371216671889
Loss in iteration 133 : 0.5937215631109656
Loss in iteration 134 : 0.5900987584986261
Loss in iteration 135 : 0.5864687223279763
Loss in iteration 136 : 0.5828314690102302
Loss in iteration 137 : 0.5791870128708456
Loss in iteration 138 : 0.575535368150409
Loss in iteration 139 : 0.5718765490054537
Loss in iteration 140 : 0.5682105695092902
Loss in iteration 141 : 0.5645374436528346
Loss in iteration 142 : 0.5608571853453889
Loss in iteration 143 : 0.5571698084154612
Loss in iteration 144 : 0.553475326611528
Loss in iteration 145 : 0.5497737536028113
Loss in iteration 146 : 0.5460651029800441
Loss in iteration 147 : 0.542349388256213
Loss in iteration 148 : 0.5386266228672953
Loss in iteration 149 : 0.5348968201730112
Loss in iteration 150 : 0.531159993457507
Loss in iteration 151 : 0.5274161559300995
Loss in iteration 152 : 0.5236653207259719
Loss in iteration 153 : 0.5199075009068397
Loss in iteration 154 : 0.5161427094616552
Loss in iteration 155 : 0.5123709593072967
Loss in iteration 156 : 0.5085922632891916
Loss in iteration 157 : 0.5048066341820087
Loss in iteration 158 : 0.5010140846902981
Loss in iteration 159 : 0.4972146274491133
Loss in iteration 160 : 0.4934082750246682
Loss in iteration 161 : 0.4895950399149443
Loss in iteration 162 : 0.4857749345503077
Loss in iteration 163 : 0.48252415431212503
Loss in iteration 164 : 0.4824638140561152
Loss in iteration 165 : 0.4820813663726612
Loss in iteration 166 : 0.48182888927255657
Loss in iteration 167 : 0.48166260441360437
Loss in iteration 168 : 0.4814422189549062
Loss in iteration 169 : 0.48126452179695167
Loss in iteration 170 : 0.48108893579965945
Loss in iteration 171 : 0.48091943019781797
Loss in iteration 172 : 0.48075097927077126
Loss in iteration 173 : 0.48058247456008396
Loss in iteration 174 : 0.4804139420331518
Loss in iteration 175 : 0.480245713404757
Loss in iteration 176 : 0.4800784114318449
Loss in iteration 177 : 0.47991149350466383
Loss in iteration 178 : 0.4797445591302583
Loss in iteration 179 : 0.4795771951890519
Loss in iteration 180 : 0.47940960139540334
Loss in iteration 181 : 0.4792415055055576
Loss in iteration 182 : 0.47907266955791084
Loss in iteration 183 : 0.47890294306401093
Loss in iteration 184 : 0.4787323343365096
Loss in iteration 185 : 0.4785607435208494
Loss in iteration 186 : 0.47838811846453155
Loss in iteration 187 : 0.4782144289549595
Loss in iteration 188 : 0.47803967733343533
Loss in iteration 189 : 0.47786368466662843
Loss in iteration 190 : 0.47768635696316364
Loss in iteration 191 : 0.4775076505627495
Loss in iteration 192 : 0.47732761912286575
Loss in iteration 193 : 0.47714606337154863
Loss in iteration 194 : 0.4769630004550087
Loss in iteration 195 : 0.4767783560410737
Loss in iteration 196 : 0.476592104625517
Loss in iteration 197 : 0.47640432310049674
Loss in iteration 198 : 0.4762148648892651
Loss in iteration 199 : 0.47602370000718075
Loss in iteration 200 : 0.47583077403890645
Testing accuracy  of updater 4 on alg 1 with rate 70.0 = 0.7637737239727289, training accuracy 0.7591830466830467, time elapsed: 2777 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9977087497627372
Loss in iteration 3 : 0.9953579969035659
Loss in iteration 4 : 0.9929664067709228
Loss in iteration 5 : 0.9905431201094781
Loss in iteration 6 : 0.9880935317584683
Loss in iteration 7 : 0.985621185368393
Loss in iteration 8 : 0.9831285763315405
Loss in iteration 9 : 0.9806175498414772
Loss in iteration 10 : 0.9780895205383133
Loss in iteration 11 : 0.975545603559427
Loss in iteration 12 : 0.9729866975429976
Loss in iteration 13 : 0.9704135397413154
Loss in iteration 14 : 0.9678267440433482
Loss in iteration 15 : 0.9652268280511056
Loss in iteration 16 : 0.9626142328817667
Loss in iteration 17 : 0.9599893379828666
Loss in iteration 18 : 0.9573524724357892
Loss in iteration 19 : 0.9547039237281607
Loss in iteration 20 : 0.952043944664537
Loss in iteration 21 : 0.9493727588826766
Loss in iteration 22 : 0.9466905653086299
Loss in iteration 23 : 0.9439975417924332
Loss in iteration 24 : 0.9412938481029236
Loss in iteration 25 : 0.9385796284153707
Loss in iteration 26 : 0.9358550133933228
Loss in iteration 27 : 0.9331201219428202
Loss in iteration 28 : 0.9303750626994401
Loss in iteration 29 : 0.9276199352960015
Loss in iteration 30 : 0.9248548314485451
Loss in iteration 31 : 0.9220798358909987
Loss in iteration 32 : 0.9192950271827545
Loss in iteration 33 : 0.9165004784092066
Loss in iteration 34 : 0.9136962577913823
Loss in iteration 35 : 0.9108824292180975
Loss in iteration 36 : 0.908059052711875
Loss in iteration 37 : 0.9052261848377771
Loss in iteration 38 : 0.9023838790630794
Loss in iteration 39 : 0.8995321860743655
Loss in iteration 40 : 0.8966711540575149
Loss in iteration 41 : 0.8938008289455334
Loss in iteration 42 : 0.8909212546381692
Loss in iteration 43 : 0.8880324731968079
Loss in iteration 44 : 0.8851345250177647
Loss in iteration 45 : 0.8822274489864853
Loss in iteration 46 : 0.8793112826149487
Loss in iteration 47 : 0.8763860621642694
Loss in iteration 48 : 0.8734518227541774
Loss in iteration 49 : 0.8705085984608499
Loss in iteration 50 : 0.8675564224045526
Loss in iteration 51 : 0.8645953268280513
Loss in iteration 52 : 0.8616253431669262
Loss in iteration 53 : 0.8586465021127108
Loss in iteration 54 : 0.8556588336695472
Loss in iteration 55 : 0.8526623672051329
Loss in iteration 56 : 0.8496571314965649
Loss in iteration 57 : 0.8466431547716006
Loss in iteration 58 : 0.8436204647458604
Loss in iteration 59 : 0.8405890886563842
Loss in iteration 60 : 0.8375490532919295
Loss in iteration 61 : 0.8345003850203724
Loss in iteration 62 : 0.8314431098134553
Loss in iteration 63 : 0.828377253269256
Loss in iteration 64 : 0.8253028406325082
Loss in iteration 65 : 0.8222198968131035
Loss in iteration 66 : 0.8191284464028322
Loss in iteration 67 : 0.8160285136906957
Loss in iteration 68 : 0.8129201226767849
Loss in iteration 69 : 0.8098032970850041
Loss in iteration 70 : 0.806678060374646
Loss in iteration 71 : 0.8035444357510212
Loss in iteration 72 : 0.8004024461751766
Loss in iteration 73 : 0.7972521143728094
Loss in iteration 74 : 0.7940934628425066
Loss in iteration 75 : 0.7909265138632673
Loss in iteration 76 : 0.7877512895015191
Loss in iteration 77 : 0.7845678116175413
Loss in iteration 78 : 0.7813761018714369
Loss in iteration 79 : 0.7781761817287067
Loss in iteration 80 : 0.7749680724653699
Loss in iteration 81 : 0.7717517951727897
Loss in iteration 82 : 0.7685273707621609
Loss in iteration 83 : 0.7652948199686974
Loss in iteration 84 : 0.7620541633555835
Loss in iteration 85 : 0.7588054213176681
Loss in iteration 86 : 0.7555486140849521
Loss in iteration 87 : 0.7522837617258629
Loss in iteration 88 : 0.7490108841503947
Loss in iteration 89 : 0.7457300011130314
Loss in iteration 90 : 0.7424411322155554
Loss in iteration 91 : 0.7391442969097187
Loss in iteration 92 : 0.7358395144997578
Loss in iteration 93 : 0.7325268041448685
Loss in iteration 94 : 0.7292061848614665
Loss in iteration 95 : 0.7258776755254549
Loss in iteration 96 : 0.7225412948743655
Loss in iteration 97 : 0.7191970615093684
Loss in iteration 98 : 0.7158449938973126
Loss in iteration 99 : 0.712485110372605
Loss in iteration 100 : 0.7091174291390389
Loss in iteration 101 : 0.7057419682716305
Loss in iteration 102 : 0.7023587457183049
Loss in iteration 103 : 0.6989677793016047
Loss in iteration 104 : 0.6955690867202924
Loss in iteration 105 : 0.6921626855509708
Loss in iteration 106 : 0.6887485932495816
Loss in iteration 107 : 0.6853268271529536
Loss in iteration 108 : 0.6818974044802322
Loss in iteration 109 : 0.678460342334328
Loss in iteration 110 : 0.6750156577032986
Loss in iteration 111 : 0.6715633674617241
Loss in iteration 112 : 0.6681034883720306
Loss in iteration 113 : 0.6646360370858122
Loss in iteration 114 : 0.6611610301450661
Loss in iteration 115 : 0.6576784839835038
Loss in iteration 116 : 0.6541884149277297
Loss in iteration 117 : 0.6506908391984609
Loss in iteration 118 : 0.6471857729117172
Loss in iteration 119 : 0.643673232079948
Loss in iteration 120 : 0.6401532326132013
Loss in iteration 121 : 0.6366257903202119
Loss in iteration 122 : 0.6330909209095311
Loss in iteration 123 : 0.6295486399905743
Loss in iteration 124 : 0.625998963074682
Loss in iteration 125 : 0.6224419055761862
Loss in iteration 126 : 0.6188774828133994
Loss in iteration 127 : 0.6153057100096583
Loss in iteration 128 : 0.6117266022942881
Loss in iteration 129 : 0.6081401747035867
Loss in iteration 130 : 0.6045464421817941
Loss in iteration 131 : 0.6009454195820173
Loss in iteration 132 : 0.5973371216671889
Loss in iteration 133 : 0.5937215631109656
Loss in iteration 134 : 0.5900987584986261
Loss in iteration 135 : 0.5864687223279763
Loss in iteration 136 : 0.5828314690102302
Loss in iteration 137 : 0.5791870128708456
Loss in iteration 138 : 0.575535368150409
Loss in iteration 139 : 0.5718765490054537
Loss in iteration 140 : 0.5682105695092902
Loss in iteration 141 : 0.5645374436528346
Loss in iteration 142 : 0.5608571853453889
Loss in iteration 143 : 0.5571698084154612
Loss in iteration 144 : 0.553475326611528
Loss in iteration 145 : 0.5497737536028113
Loss in iteration 146 : 0.5460651029800441
Loss in iteration 147 : 0.542349388256213
Loss in iteration 148 : 0.5386266228672953
Loss in iteration 149 : 0.5348968201730112
Loss in iteration 150 : 0.531159993457507
Loss in iteration 151 : 0.5274161559300995
Loss in iteration 152 : 0.5236653207259719
Loss in iteration 153 : 0.5199075009068397
Loss in iteration 154 : 0.5161427094616552
Loss in iteration 155 : 0.5123709593072967
Loss in iteration 156 : 0.5085922632891916
Loss in iteration 157 : 0.5048066341820087
Loss in iteration 158 : 0.5010140846902981
Loss in iteration 159 : 0.4972146274491133
Loss in iteration 160 : 0.4934082750246682
Loss in iteration 161 : 0.4895950399149443
Loss in iteration 162 : 0.4857749345503077
Loss in iteration 163 : 0.48252415431212503
Loss in iteration 164 : 0.4824638140561152
Loss in iteration 165 : 0.4820813663726612
Loss in iteration 166 : 0.48182888927255657
Loss in iteration 167 : 0.48166260441360437
Loss in iteration 168 : 0.4814422189549062
Loss in iteration 169 : 0.48126452179695167
Loss in iteration 170 : 0.48108893579965945
Loss in iteration 171 : 0.48091943019781797
Loss in iteration 172 : 0.48075097927077126
Loss in iteration 173 : 0.48058247456008396
Loss in iteration 174 : 0.4804139420331518
Loss in iteration 175 : 0.480245713404757
Loss in iteration 176 : 0.4800784114318449
Loss in iteration 177 : 0.47991149350466383
Loss in iteration 178 : 0.4797445591302583
Loss in iteration 179 : 0.4795771951890519
Loss in iteration 180 : 0.47940960139540334
Loss in iteration 181 : 0.4792415055055576
Loss in iteration 182 : 0.47907266955791084
Loss in iteration 183 : 0.47890294306401093
Loss in iteration 184 : 0.4787323343365096
Loss in iteration 185 : 0.4785607435208494
Loss in iteration 186 : 0.47838811846453155
Loss in iteration 187 : 0.4782144289549595
Loss in iteration 188 : 0.47803967733343533
Loss in iteration 189 : 0.47786368466662843
Loss in iteration 190 : 0.47768635696316364
Loss in iteration 191 : 0.4775076505627495
Loss in iteration 192 : 0.47732761912286575
Loss in iteration 193 : 0.47714606337154863
Loss in iteration 194 : 0.4769630004550087
Loss in iteration 195 : 0.4767783560410737
Loss in iteration 196 : 0.476592104625517
Loss in iteration 197 : 0.47640432310049674
Loss in iteration 198 : 0.4762148648892651
Loss in iteration 199 : 0.47602370000718075
Loss in iteration 200 : 0.47583077403890645
Testing accuracy  of updater 4 on alg 1 with rate 40.0 = 0.7637737239727289, training accuracy 0.7591830466830467, time elapsed: 3651 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9977087497627372
Loss in iteration 3 : 0.9953579969035659
Loss in iteration 4 : 0.9929664067709228
Loss in iteration 5 : 0.9905431201094781
Loss in iteration 6 : 0.9880935317584683
Loss in iteration 7 : 0.985621185368393
Loss in iteration 8 : 0.9831285763315405
Loss in iteration 9 : 0.9806175498414772
Loss in iteration 10 : 0.9780895205383133
Loss in iteration 11 : 0.975545603559427
Loss in iteration 12 : 0.9729866975429976
Loss in iteration 13 : 0.9704135397413154
Loss in iteration 14 : 0.9678267440433482
Loss in iteration 15 : 0.9652268280511056
Loss in iteration 16 : 0.9626142328817667
Loss in iteration 17 : 0.9599893379828666
Loss in iteration 18 : 0.9573524724357892
Loss in iteration 19 : 0.9547039237281607
Loss in iteration 20 : 0.952043944664537
Loss in iteration 21 : 0.9493727588826766
Loss in iteration 22 : 0.9466905653086299
Loss in iteration 23 : 0.9439975417924332
Loss in iteration 24 : 0.9412938481029236
Loss in iteration 25 : 0.9385796284153707
Loss in iteration 26 : 0.9358550133933228
Loss in iteration 27 : 0.9331201219428202
Loss in iteration 28 : 0.9303750626994401
Loss in iteration 29 : 0.9276199352960015
Loss in iteration 30 : 0.9248548314485451
Loss in iteration 31 : 0.9220798358909987
Loss in iteration 32 : 0.9192950271827545
Loss in iteration 33 : 0.9165004784092066
Loss in iteration 34 : 0.9136962577913823
Loss in iteration 35 : 0.9108824292180975
Loss in iteration 36 : 0.908059052711875
Loss in iteration 37 : 0.9052261848377771
Loss in iteration 38 : 0.9023838790630794
Loss in iteration 39 : 0.8995321860743655
Loss in iteration 40 : 0.8966711540575149
Loss in iteration 41 : 0.8938008289455334
Loss in iteration 42 : 0.8909212546381692
Loss in iteration 43 : 0.8880324731968079
Loss in iteration 44 : 0.8851345250177647
Loss in iteration 45 : 0.8822274489864853
Loss in iteration 46 : 0.8793112826149487
Loss in iteration 47 : 0.8763860621642694
Loss in iteration 48 : 0.8734518227541774
Loss in iteration 49 : 0.8705085984608499
Loss in iteration 50 : 0.8675564224045526
Loss in iteration 51 : 0.8645953268280513
Loss in iteration 52 : 0.8616253431669262
Loss in iteration 53 : 0.8586465021127108
Loss in iteration 54 : 0.8556588336695472
Loss in iteration 55 : 0.8526623672051329
Loss in iteration 56 : 0.8496571314965649
Loss in iteration 57 : 0.8466431547716006
Loss in iteration 58 : 0.8436204647458604
Loss in iteration 59 : 0.8405890886563842
Loss in iteration 60 : 0.8375490532919295
Loss in iteration 61 : 0.8345003850203724
Loss in iteration 62 : 0.8314431098134553
Loss in iteration 63 : 0.828377253269256
Loss in iteration 64 : 0.8253028406325082
Loss in iteration 65 : 0.8222198968131035
Loss in iteration 66 : 0.8191284464028322
Loss in iteration 67 : 0.8160285136906957
Loss in iteration 68 : 0.8129201226767849
Loss in iteration 69 : 0.8098032970850041
Loss in iteration 70 : 0.806678060374646
Loss in iteration 71 : 0.8035444357510212
Loss in iteration 72 : 0.8004024461751766
Loss in iteration 73 : 0.7972521143728094
Loss in iteration 74 : 0.7940934628425066
Loss in iteration 75 : 0.7909265138632673
Loss in iteration 76 : 0.7877512895015191
Loss in iteration 77 : 0.7845678116175413
Loss in iteration 78 : 0.7813761018714369
Loss in iteration 79 : 0.7781761817287067
Loss in iteration 80 : 0.7749680724653699
Loss in iteration 81 : 0.7717517951727897
Loss in iteration 82 : 0.7685273707621609
Loss in iteration 83 : 0.7652948199686974
Loss in iteration 84 : 0.7620541633555835
Loss in iteration 85 : 0.7588054213176681
Loss in iteration 86 : 0.7555486140849521
Loss in iteration 87 : 0.7522837617258629
Loss in iteration 88 : 0.7490108841503947
Loss in iteration 89 : 0.7457300011130314
Loss in iteration 90 : 0.7424411322155554
Loss in iteration 91 : 0.7391442969097187
Loss in iteration 92 : 0.7358395144997578
Loss in iteration 93 : 0.7325268041448685
Loss in iteration 94 : 0.7292061848614665
Loss in iteration 95 : 0.7258776755254549
Loss in iteration 96 : 0.7225412948743655
Loss in iteration 97 : 0.7191970615093684
Loss in iteration 98 : 0.7158449938973126
Loss in iteration 99 : 0.712485110372605
Loss in iteration 100 : 0.7091174291390389
Loss in iteration 101 : 0.7057419682716305
Loss in iteration 102 : 0.7023587457183049
Loss in iteration 103 : 0.6989677793016047
Loss in iteration 104 : 0.6955690867202924
Loss in iteration 105 : 0.6921626855509708
Loss in iteration 106 : 0.6887485932495816
Loss in iteration 107 : 0.6853268271529536
Loss in iteration 108 : 0.6818974044802322
Loss in iteration 109 : 0.678460342334328
Loss in iteration 110 : 0.6750156577032986
Loss in iteration 111 : 0.6715633674617241
Loss in iteration 112 : 0.6681034883720306
Loss in iteration 113 : 0.6646360370858122
Loss in iteration 114 : 0.6611610301450661
Loss in iteration 115 : 0.6576784839835038
Loss in iteration 116 : 0.6541884149277297
Loss in iteration 117 : 0.6506908391984609
Loss in iteration 118 : 0.6471857729117172
Loss in iteration 119 : 0.643673232079948
Loss in iteration 120 : 0.6401532326132013
Loss in iteration 121 : 0.6366257903202119
Loss in iteration 122 : 0.6330909209095311
Loss in iteration 123 : 0.6295486399905743
Loss in iteration 124 : 0.625998963074682
Loss in iteration 125 : 0.6224419055761862
Loss in iteration 126 : 0.6188774828133994
Loss in iteration 127 : 0.6153057100096583
Loss in iteration 128 : 0.6117266022942881
Loss in iteration 129 : 0.6081401747035867
Loss in iteration 130 : 0.6045464421817941
Loss in iteration 131 : 0.6009454195820173
Loss in iteration 132 : 0.5973371216671889
Loss in iteration 133 : 0.5937215631109656
Loss in iteration 134 : 0.5900987584986261
Loss in iteration 135 : 0.5864687223279763
Loss in iteration 136 : 0.5828314690102302
Loss in iteration 137 : 0.5791870128708456
Loss in iteration 138 : 0.575535368150409
Loss in iteration 139 : 0.5718765490054537
Loss in iteration 140 : 0.5682105695092902
Loss in iteration 141 : 0.5645374436528346
Loss in iteration 142 : 0.5608571853453889
Loss in iteration 143 : 0.5571698084154612
Loss in iteration 144 : 0.553475326611528
Loss in iteration 145 : 0.5497737536028113
Loss in iteration 146 : 0.5460651029800441
Loss in iteration 147 : 0.542349388256213
Loss in iteration 148 : 0.5386266228672953
Loss in iteration 149 : 0.5348968201730112
Loss in iteration 150 : 0.531159993457507
Loss in iteration 151 : 0.5274161559300995
Loss in iteration 152 : 0.5236653207259719
Loss in iteration 153 : 0.5199075009068397
Loss in iteration 154 : 0.5161427094616552
Loss in iteration 155 : 0.5123709593072967
Loss in iteration 156 : 0.5085922632891916
Loss in iteration 157 : 0.5048066341820087
Loss in iteration 158 : 0.5010140846902981
Loss in iteration 159 : 0.4972146274491133
Loss in iteration 160 : 0.4934082750246682
Loss in iteration 161 : 0.4895950399149443
Loss in iteration 162 : 0.4857749345503077
Loss in iteration 163 : 0.48252415431212503
Loss in iteration 164 : 0.4824638140561152
Loss in iteration 165 : 0.4820813663726612
Loss in iteration 166 : 0.48182888927255657
Loss in iteration 167 : 0.48166260441360437
Loss in iteration 168 : 0.4814422189549062
Loss in iteration 169 : 0.48126452179695167
Loss in iteration 170 : 0.48108893579965945
Loss in iteration 171 : 0.48091943019781797
Loss in iteration 172 : 0.48075097927077126
Loss in iteration 173 : 0.48058247456008396
Loss in iteration 174 : 0.4804139420331518
Loss in iteration 175 : 0.480245713404757
Loss in iteration 176 : 0.4800784114318449
Loss in iteration 177 : 0.47991149350466383
Loss in iteration 178 : 0.4797445591302583
Loss in iteration 179 : 0.4795771951890519
Loss in iteration 180 : 0.47940960139540334
Loss in iteration 181 : 0.4792415055055576
Loss in iteration 182 : 0.47907266955791084
Loss in iteration 183 : 0.47890294306401093
Loss in iteration 184 : 0.4787323343365096
Loss in iteration 185 : 0.4785607435208494
Loss in iteration 186 : 0.47838811846453155
Loss in iteration 187 : 0.4782144289549595
Loss in iteration 188 : 0.47803967733343533
Loss in iteration 189 : 0.47786368466662843
Loss in iteration 190 : 0.47768635696316364
Loss in iteration 191 : 0.4775076505627495
Loss in iteration 192 : 0.47732761912286575
Loss in iteration 193 : 0.47714606337154863
Loss in iteration 194 : 0.4769630004550087
Loss in iteration 195 : 0.4767783560410737
Loss in iteration 196 : 0.476592104625517
Loss in iteration 197 : 0.47640432310049674
Loss in iteration 198 : 0.4762148648892651
Loss in iteration 199 : 0.47602370000718075
Loss in iteration 200 : 0.47583077403890645
Testing accuracy  of updater 4 on alg 1 with rate 10.0 = 0.7637737239727289, training accuracy 0.7591830466830467, time elapsed: 3667 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9977087497627372
Loss in iteration 3 : 0.9953579969035659
Loss in iteration 4 : 0.9929664067709228
Loss in iteration 5 : 0.9905431201094781
Loss in iteration 6 : 0.9880935317584683
Loss in iteration 7 : 0.985621185368393
Loss in iteration 8 : 0.9831285763315405
Loss in iteration 9 : 0.9806175498414772
Loss in iteration 10 : 0.9780895205383133
Loss in iteration 11 : 0.975545603559427
Loss in iteration 12 : 0.9729866975429976
Loss in iteration 13 : 0.9704135397413154
Loss in iteration 14 : 0.9678267440433482
Loss in iteration 15 : 0.9652268280511056
Loss in iteration 16 : 0.9626142328817667
Loss in iteration 17 : 0.9599893379828666
Loss in iteration 18 : 0.9573524724357892
Loss in iteration 19 : 0.9547039237281607
Loss in iteration 20 : 0.952043944664537
Loss in iteration 21 : 0.9493727588826766
Loss in iteration 22 : 0.9466905653086299
Loss in iteration 23 : 0.9439975417924332
Loss in iteration 24 : 0.9412938481029236
Loss in iteration 25 : 0.9385796284153707
Loss in iteration 26 : 0.9358550133933228
Loss in iteration 27 : 0.9331201219428202
Loss in iteration 28 : 0.9303750626994401
Loss in iteration 29 : 0.9276199352960015
Loss in iteration 30 : 0.9248548314485451
Loss in iteration 31 : 0.9220798358909987
Loss in iteration 32 : 0.9192950271827545
Loss in iteration 33 : 0.9165004784092066
Loss in iteration 34 : 0.9136962577913823
Loss in iteration 35 : 0.9108824292180975
Loss in iteration 36 : 0.908059052711875
Loss in iteration 37 : 0.9052261848377771
Loss in iteration 38 : 0.9023838790630794
Loss in iteration 39 : 0.8995321860743655
Loss in iteration 40 : 0.8966711540575149
Loss in iteration 41 : 0.8938008289455334
Loss in iteration 42 : 0.8909212546381692
Loss in iteration 43 : 0.8880324731968079
Loss in iteration 44 : 0.8851345250177647
Loss in iteration 45 : 0.8822274489864853
Loss in iteration 46 : 0.8793112826149487
Loss in iteration 47 : 0.8763860621642694
Loss in iteration 48 : 0.8734518227541774
Loss in iteration 49 : 0.8705085984608499
Loss in iteration 50 : 0.8675564224045526
Loss in iteration 51 : 0.8645953268280513
Loss in iteration 52 : 0.8616253431669262
Loss in iteration 53 : 0.8586465021127108
Loss in iteration 54 : 0.8556588336695472
Loss in iteration 55 : 0.8526623672051329
Loss in iteration 56 : 0.8496571314965649
Loss in iteration 57 : 0.8466431547716006
Loss in iteration 58 : 0.8436204647458604
Loss in iteration 59 : 0.8405890886563842
Loss in iteration 60 : 0.8375490532919295
Loss in iteration 61 : 0.8345003850203724
Loss in iteration 62 : 0.8314431098134553
Loss in iteration 63 : 0.828377253269256
Loss in iteration 64 : 0.8253028406325082
Loss in iteration 65 : 0.8222198968131035
Loss in iteration 66 : 0.8191284464028322
Loss in iteration 67 : 0.8160285136906957
Loss in iteration 68 : 0.8129201226767849
Loss in iteration 69 : 0.8098032970850041
Loss in iteration 70 : 0.806678060374646
Loss in iteration 71 : 0.8035444357510212
Loss in iteration 72 : 0.8004024461751766
Loss in iteration 73 : 0.7972521143728094
Loss in iteration 74 : 0.7940934628425066
Loss in iteration 75 : 0.7909265138632673
Loss in iteration 76 : 0.7877512895015191
Loss in iteration 77 : 0.7845678116175413
Loss in iteration 78 : 0.7813761018714369
Loss in iteration 79 : 0.7781761817287067
Loss in iteration 80 : 0.7749680724653699
Loss in iteration 81 : 0.7717517951727897
Loss in iteration 82 : 0.7685273707621609
Loss in iteration 83 : 0.7652948199686974
Loss in iteration 84 : 0.7620541633555835
Loss in iteration 85 : 0.7588054213176681
Loss in iteration 86 : 0.7555486140849521
Loss in iteration 87 : 0.7522837617258629
Loss in iteration 88 : 0.7490108841503947
Loss in iteration 89 : 0.7457300011130314
Loss in iteration 90 : 0.7424411322155554
Loss in iteration 91 : 0.7391442969097187
Loss in iteration 92 : 0.7358395144997578
Loss in iteration 93 : 0.7325268041448685
Loss in iteration 94 : 0.7292061848614665
Loss in iteration 95 : 0.7258776755254549
Loss in iteration 96 : 0.7225412948743655
Loss in iteration 97 : 0.7191970615093684
Loss in iteration 98 : 0.7158449938973126
Loss in iteration 99 : 0.712485110372605
Loss in iteration 100 : 0.7091174291390389
Loss in iteration 101 : 0.7057419682716305
Loss in iteration 102 : 0.7023587457183049
Loss in iteration 103 : 0.6989677793016047
Loss in iteration 104 : 0.6955690867202924
Loss in iteration 105 : 0.6921626855509708
Loss in iteration 106 : 0.6887485932495816
Loss in iteration 107 : 0.6853268271529536
Loss in iteration 108 : 0.6818974044802322
Loss in iteration 109 : 0.678460342334328
Loss in iteration 110 : 0.6750156577032986
Loss in iteration 111 : 0.6715633674617241
Loss in iteration 112 : 0.6681034883720306
Loss in iteration 113 : 0.6646360370858122
Loss in iteration 114 : 0.6611610301450661
Loss in iteration 115 : 0.6576784839835038
Loss in iteration 116 : 0.6541884149277297
Loss in iteration 117 : 0.6506908391984609
Loss in iteration 118 : 0.6471857729117172
Loss in iteration 119 : 0.643673232079948
Loss in iteration 120 : 0.6401532326132013
Loss in iteration 121 : 0.6366257903202119
Loss in iteration 122 : 0.6330909209095311
Loss in iteration 123 : 0.6295486399905743
Loss in iteration 124 : 0.625998963074682
Loss in iteration 125 : 0.6224419055761862
Loss in iteration 126 : 0.6188774828133994
Loss in iteration 127 : 0.6153057100096583
Loss in iteration 128 : 0.6117266022942881
Loss in iteration 129 : 0.6081401747035867
Loss in iteration 130 : 0.6045464421817941
Loss in iteration 131 : 0.6009454195820173
Loss in iteration 132 : 0.5973371216671889
Loss in iteration 133 : 0.5937215631109656
Loss in iteration 134 : 0.5900987584986261
Loss in iteration 135 : 0.5864687223279763
Loss in iteration 136 : 0.5828314690102302
Loss in iteration 137 : 0.5791870128708456
Loss in iteration 138 : 0.575535368150409
Loss in iteration 139 : 0.5718765490054537
Loss in iteration 140 : 0.5682105695092902
Loss in iteration 141 : 0.5645374436528346
Loss in iteration 142 : 0.5608571853453889
Loss in iteration 143 : 0.5571698084154612
Loss in iteration 144 : 0.553475326611528
Loss in iteration 145 : 0.5497737536028113
Loss in iteration 146 : 0.5460651029800441
Loss in iteration 147 : 0.542349388256213
Loss in iteration 148 : 0.5386266228672953
Loss in iteration 149 : 0.5348968201730112
Loss in iteration 150 : 0.531159993457507
Loss in iteration 151 : 0.5274161559300995
Loss in iteration 152 : 0.5236653207259719
Loss in iteration 153 : 0.5199075009068397
Loss in iteration 154 : 0.5161427094616552
Loss in iteration 155 : 0.5123709593072967
Loss in iteration 156 : 0.5085922632891916
Loss in iteration 157 : 0.5048066341820087
Loss in iteration 158 : 0.5010140846902981
Loss in iteration 159 : 0.4972146274491133
Loss in iteration 160 : 0.4934082750246682
Loss in iteration 161 : 0.4895950399149443
Loss in iteration 162 : 0.4857749345503077
Loss in iteration 163 : 0.48252415431212503
Loss in iteration 164 : 0.4824638140561152
Loss in iteration 165 : 0.4820813663726612
Loss in iteration 166 : 0.48182888927255657
Loss in iteration 167 : 0.48166260441360437
Loss in iteration 168 : 0.4814422189549062
Loss in iteration 169 : 0.48126452179695167
Loss in iteration 170 : 0.48108893579965945
Loss in iteration 171 : 0.48091943019781797
Loss in iteration 172 : 0.48075097927077126
Loss in iteration 173 : 0.48058247456008396
Loss in iteration 174 : 0.4804139420331518
Loss in iteration 175 : 0.480245713404757
Loss in iteration 176 : 0.4800784114318449
Loss in iteration 177 : 0.47991149350466383
Loss in iteration 178 : 0.4797445591302583
Loss in iteration 179 : 0.4795771951890519
Loss in iteration 180 : 0.47940960139540334
Loss in iteration 181 : 0.4792415055055576
Loss in iteration 182 : 0.47907266955791084
Loss in iteration 183 : 0.47890294306401093
Loss in iteration 184 : 0.4787323343365096
Loss in iteration 185 : 0.4785607435208494
Loss in iteration 186 : 0.47838811846453155
Loss in iteration 187 : 0.4782144289549595
Loss in iteration 188 : 0.47803967733343533
Loss in iteration 189 : 0.47786368466662843
Loss in iteration 190 : 0.47768635696316364
Loss in iteration 191 : 0.4775076505627495
Loss in iteration 192 : 0.47732761912286575
Loss in iteration 193 : 0.47714606337154863
Loss in iteration 194 : 0.4769630004550087
Loss in iteration 195 : 0.4767783560410737
Loss in iteration 196 : 0.476592104625517
Loss in iteration 197 : 0.47640432310049674
Loss in iteration 198 : 0.4762148648892651
Loss in iteration 199 : 0.47602370000718075
Loss in iteration 200 : 0.47583077403890645
Testing accuracy  of updater 4 on alg 1 with rate 7.0 = 0.7637737239727289, training accuracy 0.7591830466830467, time elapsed: 4486 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9977087497627372
Loss in iteration 3 : 0.9953579969035659
Loss in iteration 4 : 0.9929664067709228
Loss in iteration 5 : 0.9905431201094781
Loss in iteration 6 : 0.9880935317584683
Loss in iteration 7 : 0.985621185368393
Loss in iteration 8 : 0.9831285763315405
Loss in iteration 9 : 0.9806175498414772
Loss in iteration 10 : 0.9780895205383133
Loss in iteration 11 : 0.975545603559427
Loss in iteration 12 : 0.9729866975429976
Loss in iteration 13 : 0.9704135397413154
Loss in iteration 14 : 0.9678267440433482
Loss in iteration 15 : 0.9652268280511056
Loss in iteration 16 : 0.9626142328817667
Loss in iteration 17 : 0.9599893379828666
Loss in iteration 18 : 0.9573524724357892
Loss in iteration 19 : 0.9547039237281607
Loss in iteration 20 : 0.952043944664537
Loss in iteration 21 : 0.9493727588826766
Loss in iteration 22 : 0.9466905653086299
Loss in iteration 23 : 0.9439975417924332
Loss in iteration 24 : 0.9412938481029236
Loss in iteration 25 : 0.9385796284153707
Loss in iteration 26 : 0.9358550133933228
Loss in iteration 27 : 0.9331201219428202
Loss in iteration 28 : 0.9303750626994401
Loss in iteration 29 : 0.9276199352960015
Loss in iteration 30 : 0.9248548314485451
Loss in iteration 31 : 0.9220798358909987
Loss in iteration 32 : 0.9192950271827545
Loss in iteration 33 : 0.9165004784092066
Loss in iteration 34 : 0.9136962577913823
Loss in iteration 35 : 0.9108824292180975
Loss in iteration 36 : 0.908059052711875
Loss in iteration 37 : 0.9052261848377771
Loss in iteration 38 : 0.9023838790630794
Loss in iteration 39 : 0.8995321860743655
Loss in iteration 40 : 0.8966711540575149
Loss in iteration 41 : 0.8938008289455334
Loss in iteration 42 : 0.8909212546381692
Loss in iteration 43 : 0.8880324731968079
Loss in iteration 44 : 0.8851345250177647
Loss in iteration 45 : 0.8822274489864853
Loss in iteration 46 : 0.8793112826149487
Loss in iteration 47 : 0.8763860621642694
Loss in iteration 48 : 0.8734518227541774
Loss in iteration 49 : 0.8705085984608499
Loss in iteration 50 : 0.8675564224045526
Loss in iteration 51 : 0.8645953268280513
Loss in iteration 52 : 0.8616253431669262
Loss in iteration 53 : 0.8586465021127108
Loss in iteration 54 : 0.8556588336695472
Loss in iteration 55 : 0.8526623672051329
Loss in iteration 56 : 0.8496571314965649
Loss in iteration 57 : 0.8466431547716006
Loss in iteration 58 : 0.8436204647458604
Loss in iteration 59 : 0.8405890886563842
Loss in iteration 60 : 0.8375490532919295
Loss in iteration 61 : 0.8345003850203724
Loss in iteration 62 : 0.8314431098134553
Loss in iteration 63 : 0.828377253269256
Loss in iteration 64 : 0.8253028406325082
Loss in iteration 65 : 0.8222198968131035
Loss in iteration 66 : 0.8191284464028322
Loss in iteration 67 : 0.8160285136906957
Loss in iteration 68 : 0.8129201226767849
Loss in iteration 69 : 0.8098032970850041
Loss in iteration 70 : 0.806678060374646
Loss in iteration 71 : 0.8035444357510212
Loss in iteration 72 : 0.8004024461751766
Loss in iteration 73 : 0.7972521143728094
Loss in iteration 74 : 0.7940934628425066
Loss in iteration 75 : 0.7909265138632673
Loss in iteration 76 : 0.7877512895015191
Loss in iteration 77 : 0.7845678116175413
Loss in iteration 78 : 0.7813761018714369
Loss in iteration 79 : 0.7781761817287067
Loss in iteration 80 : 0.7749680724653699
Loss in iteration 81 : 0.7717517951727897
Loss in iteration 82 : 0.7685273707621609
Loss in iteration 83 : 0.7652948199686974
Loss in iteration 84 : 0.7620541633555835
Loss in iteration 85 : 0.7588054213176681
Loss in iteration 86 : 0.7555486140849521
Loss in iteration 87 : 0.7522837617258629
Loss in iteration 88 : 0.7490108841503947
Loss in iteration 89 : 0.7457300011130314
Loss in iteration 90 : 0.7424411322155554
Loss in iteration 91 : 0.7391442969097187
Loss in iteration 92 : 0.7358395144997578
Loss in iteration 93 : 0.7325268041448685
Loss in iteration 94 : 0.7292061848614665
Loss in iteration 95 : 0.7258776755254549
Loss in iteration 96 : 0.7225412948743655
Loss in iteration 97 : 0.7191970615093684
Loss in iteration 98 : 0.7158449938973126
Loss in iteration 99 : 0.712485110372605
Loss in iteration 100 : 0.7091174291390389
Loss in iteration 101 : 0.7057419682716305
Loss in iteration 102 : 0.7023587457183049
Loss in iteration 103 : 0.6989677793016047
Loss in iteration 104 : 0.6955690867202924
Loss in iteration 105 : 0.6921626855509708
Loss in iteration 106 : 0.6887485932495816
Loss in iteration 107 : 0.6853268271529536
Loss in iteration 108 : 0.6818974044802322
Loss in iteration 109 : 0.678460342334328
Loss in iteration 110 : 0.6750156577032986
Loss in iteration 111 : 0.6715633674617241
Loss in iteration 112 : 0.6681034883720306
Loss in iteration 113 : 0.6646360370858122
Loss in iteration 114 : 0.6611610301450661
Loss in iteration 115 : 0.6576784839835038
Loss in iteration 116 : 0.6541884149277297
Loss in iteration 117 : 0.6506908391984609
Loss in iteration 118 : 0.6471857729117172
Loss in iteration 119 : 0.643673232079948
Loss in iteration 120 : 0.6401532326132013
Loss in iteration 121 : 0.6366257903202119
Loss in iteration 122 : 0.6330909209095311
Loss in iteration 123 : 0.6295486399905743
Loss in iteration 124 : 0.625998963074682
Loss in iteration 125 : 0.6224419055761862
Loss in iteration 126 : 0.6188774828133994
Loss in iteration 127 : 0.6153057100096583
Loss in iteration 128 : 0.6117266022942881
Loss in iteration 129 : 0.6081401747035867
Loss in iteration 130 : 0.6045464421817941
Loss in iteration 131 : 0.6009454195820173
Loss in iteration 132 : 0.5973371216671889
Loss in iteration 133 : 0.5937215631109656
Loss in iteration 134 : 0.5900987584986261
Loss in iteration 135 : 0.5864687223279763
Loss in iteration 136 : 0.5828314690102302
Loss in iteration 137 : 0.5791870128708456
Loss in iteration 138 : 0.575535368150409
Loss in iteration 139 : 0.5718765490054537
Loss in iteration 140 : 0.5682105695092902
Loss in iteration 141 : 0.5645374436528346
Loss in iteration 142 : 0.5608571853453889
Loss in iteration 143 : 0.5571698084154612
Loss in iteration 144 : 0.553475326611528
Loss in iteration 145 : 0.5497737536028113
Loss in iteration 146 : 0.5460651029800441
Loss in iteration 147 : 0.542349388256213
Loss in iteration 148 : 0.5386266228672953
Loss in iteration 149 : 0.5348968201730112
Loss in iteration 150 : 0.531159993457507
Loss in iteration 151 : 0.5274161559300995
Loss in iteration 152 : 0.5236653207259719
Loss in iteration 153 : 0.5199075009068397
Loss in iteration 154 : 0.5161427094616552
Loss in iteration 155 : 0.5123709593072967
Loss in iteration 156 : 0.5085922632891916
Loss in iteration 157 : 0.5048066341820087
Loss in iteration 158 : 0.5010140846902981
Loss in iteration 159 : 0.4972146274491133
Loss in iteration 160 : 0.4934082750246682
Loss in iteration 161 : 0.4895950399149443
Loss in iteration 162 : 0.4857749345503077
Loss in iteration 163 : 0.48252415431212503
Loss in iteration 164 : 0.4824638140561152
Loss in iteration 165 : 0.4820813663726612
Loss in iteration 166 : 0.48182888927255657
Loss in iteration 167 : 0.48166260441360437
Loss in iteration 168 : 0.4814422189549062
Loss in iteration 169 : 0.48126452179695167
Loss in iteration 170 : 0.48108893579965945
Loss in iteration 171 : 0.48091943019781797
Loss in iteration 172 : 0.48075097927077126
Loss in iteration 173 : 0.48058247456008396
Loss in iteration 174 : 0.4804139420331518
Loss in iteration 175 : 0.480245713404757
Loss in iteration 176 : 0.4800784114318449
Loss in iteration 177 : 0.47991149350466383
Loss in iteration 178 : 0.4797445591302583
Loss in iteration 179 : 0.4795771951890519
Loss in iteration 180 : 0.47940960139540334
Loss in iteration 181 : 0.4792415055055576
Loss in iteration 182 : 0.47907266955791084
Loss in iteration 183 : 0.47890294306401093
Loss in iteration 184 : 0.4787323343365096
Loss in iteration 185 : 0.4785607435208494
Loss in iteration 186 : 0.47838811846453155
Loss in iteration 187 : 0.4782144289549595
Loss in iteration 188 : 0.47803967733343533
Loss in iteration 189 : 0.47786368466662843
Loss in iteration 190 : 0.47768635696316364
Loss in iteration 191 : 0.4775076505627495
Loss in iteration 192 : 0.47732761912286575
Loss in iteration 193 : 0.47714606337154863
Loss in iteration 194 : 0.4769630004550087
Loss in iteration 195 : 0.4767783560410737
Loss in iteration 196 : 0.476592104625517
Loss in iteration 197 : 0.47640432310049674
Loss in iteration 198 : 0.4762148648892651
Loss in iteration 199 : 0.47602370000718075
Loss in iteration 200 : 0.47583077403890645
Testing accuracy  of updater 4 on alg 1 with rate 4.0 = 0.7637737239727289, training accuracy 0.7591830466830467, time elapsed: 5419 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9977087497627372
Loss in iteration 3 : 0.9953579969035659
Loss in iteration 4 : 0.9929664067709228
Loss in iteration 5 : 0.9905431201094781
Loss in iteration 6 : 0.9880935317584683
Loss in iteration 7 : 0.985621185368393
Loss in iteration 8 : 0.9831285763315405
Loss in iteration 9 : 0.9806175498414772
Loss in iteration 10 : 0.9780895205383133
Loss in iteration 11 : 0.975545603559427
Loss in iteration 12 : 0.9729866975429976
Loss in iteration 13 : 0.9704135397413154
Loss in iteration 14 : 0.9678267440433482
Loss in iteration 15 : 0.9652268280511056
Loss in iteration 16 : 0.9626142328817667
Loss in iteration 17 : 0.9599893379828666
Loss in iteration 18 : 0.9573524724357892
Loss in iteration 19 : 0.9547039237281607
Loss in iteration 20 : 0.952043944664537
Loss in iteration 21 : 0.9493727588826766
Loss in iteration 22 : 0.9466905653086299
Loss in iteration 23 : 0.9439975417924332
Loss in iteration 24 : 0.9412938481029236
Loss in iteration 25 : 0.9385796284153707
Loss in iteration 26 : 0.9358550133933228
Loss in iteration 27 : 0.9331201219428202
Loss in iteration 28 : 0.9303750626994401
Loss in iteration 29 : 0.9276199352960015
Loss in iteration 30 : 0.9248548314485451
Loss in iteration 31 : 0.9220798358909987
Loss in iteration 32 : 0.9192950271827545
Loss in iteration 33 : 0.9165004784092066
Loss in iteration 34 : 0.9136962577913823
Loss in iteration 35 : 0.9108824292180975
Loss in iteration 36 : 0.908059052711875
Loss in iteration 37 : 0.9052261848377771
Loss in iteration 38 : 0.9023838790630794
Loss in iteration 39 : 0.8995321860743655
Loss in iteration 40 : 0.8966711540575149
Loss in iteration 41 : 0.8938008289455334
Loss in iteration 42 : 0.8909212546381692
Loss in iteration 43 : 0.8880324731968079
Loss in iteration 44 : 0.8851345250177647
Loss in iteration 45 : 0.8822274489864853
Loss in iteration 46 : 0.8793112826149487
Loss in iteration 47 : 0.8763860621642694
Loss in iteration 48 : 0.8734518227541774
Loss in iteration 49 : 0.8705085984608499
Loss in iteration 50 : 0.8675564224045526
Loss in iteration 51 : 0.8645953268280513
Loss in iteration 52 : 0.8616253431669262
Loss in iteration 53 : 0.8586465021127108
Loss in iteration 54 : 0.8556588336695472
Loss in iteration 55 : 0.8526623672051329
Loss in iteration 56 : 0.8496571314965649
Loss in iteration 57 : 0.8466431547716006
Loss in iteration 58 : 0.8436204647458604
Loss in iteration 59 : 0.8405890886563842
Loss in iteration 60 : 0.8375490532919295
Loss in iteration 61 : 0.8345003850203724
Loss in iteration 62 : 0.8314431098134553
Loss in iteration 63 : 0.828377253269256
Loss in iteration 64 : 0.8253028406325082
Loss in iteration 65 : 0.8222198968131035
Loss in iteration 66 : 0.8191284464028322
Loss in iteration 67 : 0.8160285136906957
Loss in iteration 68 : 0.8129201226767849
Loss in iteration 69 : 0.8098032970850041
Loss in iteration 70 : 0.806678060374646
Loss in iteration 71 : 0.8035444357510212
Loss in iteration 72 : 0.8004024461751766
Loss in iteration 73 : 0.7972521143728094
Loss in iteration 74 : 0.7940934628425066
Loss in iteration 75 : 0.7909265138632673
Loss in iteration 76 : 0.7877512895015191
Loss in iteration 77 : 0.7845678116175413
Loss in iteration 78 : 0.7813761018714369
Loss in iteration 79 : 0.7781761817287067
Loss in iteration 80 : 0.7749680724653699
Loss in iteration 81 : 0.7717517951727897
Loss in iteration 82 : 0.7685273707621609
Loss in iteration 83 : 0.7652948199686974
Loss in iteration 84 : 0.7620541633555835
Loss in iteration 85 : 0.7588054213176681
Loss in iteration 86 : 0.7555486140849521
Loss in iteration 87 : 0.7522837617258629
Loss in iteration 88 : 0.7490108841503947
Loss in iteration 89 : 0.7457300011130314
Loss in iteration 90 : 0.7424411322155554
Loss in iteration 91 : 0.7391442969097187
Loss in iteration 92 : 0.7358395144997578
Loss in iteration 93 : 0.7325268041448685
Loss in iteration 94 : 0.7292061848614665
Loss in iteration 95 : 0.7258776755254549
Loss in iteration 96 : 0.7225412948743655
Loss in iteration 97 : 0.7191970615093684
Loss in iteration 98 : 0.7158449938973126
Loss in iteration 99 : 0.712485110372605
Loss in iteration 100 : 0.7091174291390389
Loss in iteration 101 : 0.7057419682716305
Loss in iteration 102 : 0.7023587457183049
Loss in iteration 103 : 0.6989677793016047
Loss in iteration 104 : 0.6955690867202924
Loss in iteration 105 : 0.6921626855509708
Loss in iteration 106 : 0.6887485932495816
Loss in iteration 107 : 0.6853268271529536
Loss in iteration 108 : 0.6818974044802322
Loss in iteration 109 : 0.678460342334328
Loss in iteration 110 : 0.6750156577032986
Loss in iteration 111 : 0.6715633674617241
Loss in iteration 112 : 0.6681034883720306
Loss in iteration 113 : 0.6646360370858122
Loss in iteration 114 : 0.6611610301450661
Loss in iteration 115 : 0.6576784839835038
Loss in iteration 116 : 0.6541884149277297
Loss in iteration 117 : 0.6506908391984609
Loss in iteration 118 : 0.6471857729117172
Loss in iteration 119 : 0.643673232079948
Loss in iteration 120 : 0.6401532326132013
Loss in iteration 121 : 0.6366257903202119
Loss in iteration 122 : 0.6330909209095311
Loss in iteration 123 : 0.6295486399905743
Loss in iteration 124 : 0.625998963074682
Loss in iteration 125 : 0.6224419055761862
Loss in iteration 126 : 0.6188774828133994
Loss in iteration 127 : 0.6153057100096583
Loss in iteration 128 : 0.6117266022942881
Loss in iteration 129 : 0.6081401747035867
Loss in iteration 130 : 0.6045464421817941
Loss in iteration 131 : 0.6009454195820173
Loss in iteration 132 : 0.5973371216671889
Loss in iteration 133 : 0.5937215631109656
Loss in iteration 134 : 0.5900987584986261
Loss in iteration 135 : 0.5864687223279763
Loss in iteration 136 : 0.5828314690102302
Loss in iteration 137 : 0.5791870128708456
Loss in iteration 138 : 0.575535368150409
Loss in iteration 139 : 0.5718765490054537
Loss in iteration 140 : 0.5682105695092902
Loss in iteration 141 : 0.5645374436528346
Loss in iteration 142 : 0.5608571853453889
Loss in iteration 143 : 0.5571698084154612
Loss in iteration 144 : 0.553475326611528
Loss in iteration 145 : 0.5497737536028113
Loss in iteration 146 : 0.5460651029800441
Loss in iteration 147 : 0.542349388256213
Loss in iteration 148 : 0.5386266228672953
Loss in iteration 149 : 0.5348968201730112
Loss in iteration 150 : 0.531159993457507
Loss in iteration 151 : 0.5274161559300995
Loss in iteration 152 : 0.5236653207259719
Loss in iteration 153 : 0.5199075009068397
Loss in iteration 154 : 0.5161427094616552
Loss in iteration 155 : 0.5123709593072967
Loss in iteration 156 : 0.5085922632891916
Loss in iteration 157 : 0.5048066341820087
Loss in iteration 158 : 0.5010140846902981
Loss in iteration 159 : 0.4972146274491133
Loss in iteration 160 : 0.4934082750246682
Loss in iteration 161 : 0.4895950399149443
Loss in iteration 162 : 0.4857749345503077
Loss in iteration 163 : 0.48252415431212503
Loss in iteration 164 : 0.4824638140561152
Loss in iteration 165 : 0.4820813663726612
Loss in iteration 166 : 0.48182888927255657
Loss in iteration 167 : 0.48166260441360437
Loss in iteration 168 : 0.4814422189549062
Loss in iteration 169 : 0.48126452179695167
Loss in iteration 170 : 0.48108893579965945
Loss in iteration 171 : 0.48091943019781797
Loss in iteration 172 : 0.48075097927077126
Loss in iteration 173 : 0.48058247456008396
Loss in iteration 174 : 0.4804139420331518
Loss in iteration 175 : 0.480245713404757
Loss in iteration 176 : 0.4800784114318449
Loss in iteration 177 : 0.47991149350466383
Loss in iteration 178 : 0.4797445591302583
Loss in iteration 179 : 0.4795771951890519
Loss in iteration 180 : 0.47940960139540334
Loss in iteration 181 : 0.4792415055055576
Loss in iteration 182 : 0.47907266955791084
Loss in iteration 183 : 0.47890294306401093
Loss in iteration 184 : 0.4787323343365096
Loss in iteration 185 : 0.4785607435208494
Loss in iteration 186 : 0.47838811846453155
Loss in iteration 187 : 0.4782144289549595
Loss in iteration 188 : 0.47803967733343533
Loss in iteration 189 : 0.47786368466662843
Loss in iteration 190 : 0.47768635696316364
Loss in iteration 191 : 0.4775076505627495
Loss in iteration 192 : 0.47732761912286575
Loss in iteration 193 : 0.47714606337154863
Loss in iteration 194 : 0.4769630004550087
Loss in iteration 195 : 0.4767783560410737
Loss in iteration 196 : 0.476592104625517
Loss in iteration 197 : 0.47640432310049674
Loss in iteration 198 : 0.4762148648892651
Loss in iteration 199 : 0.47602370000718075
Loss in iteration 200 : 0.47583077403890645
Testing accuracy  of updater 4 on alg 1 with rate 1.0 = 0.7637737239727289, training accuracy 0.7591830466830467, time elapsed: 5707 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.2090697312075958
Loss in iteration 3 : 0.5863991594185615
Loss in iteration 4 : 0.4497615681308293
Loss in iteration 5 : 0.48362575263625435
Loss in iteration 6 : 0.42073430527225475
Loss in iteration 7 : 0.43664762609159613
Loss in iteration 8 : 0.3983409168701646
Loss in iteration 9 : 0.41000536037538376
Loss in iteration 10 : 0.39024074997965164
Loss in iteration 11 : 0.3999820131825791
Loss in iteration 12 : 0.38763509280892483
Loss in iteration 13 : 0.3987719530423214
Loss in iteration 14 : 0.3878550233049902
Loss in iteration 15 : 0.4024474655357427
Loss in iteration 16 : 0.39090287017388686
Loss in iteration 17 : 0.4073008196557579
Loss in iteration 18 : 0.39245433581197053
Loss in iteration 19 : 0.40696665572236473
Loss in iteration 20 : 0.3897055998021229
Loss in iteration 21 : 0.40232957914675505
Loss in iteration 22 : 0.38876363108476664
Loss in iteration 23 : 0.4017421400571512
Loss in iteration 24 : 0.39022145446133527
Loss in iteration 25 : 0.40508905754497865
Loss in iteration 26 : 0.39167003562541863
Loss in iteration 27 : 0.40621738360082715
Loss in iteration 28 : 0.39205146707513017
Loss in iteration 29 : 0.4061158032013916
Loss in iteration 30 : 0.3922524241899845
Loss in iteration 31 : 0.4055985923038719
Loss in iteration 32 : 0.39218461065286747
Loss in iteration 33 : 0.40565019091886184
Loss in iteration 34 : 0.39307857538892643
Loss in iteration 35 : 0.4074229287750092
Loss in iteration 36 : 0.39412670771218183
Loss in iteration 37 : 0.4087295852714695
Loss in iteration 38 : 0.3931281109246481
Loss in iteration 39 : 0.40546495581860237
Loss in iteration 40 : 0.39258330931845253
Loss in iteration 41 : 0.4059070397374296
Loss in iteration 42 : 0.3936272072322483
Loss in iteration 43 : 0.40757516604924815
Loss in iteration 44 : 0.39445047213933965
Loss in iteration 45 : 0.4083479132913909
Loss in iteration 46 : 0.39400783890724794
Loss in iteration 47 : 0.4072041515748403
Loss in iteration 48 : 0.39419583440303263
Loss in iteration 49 : 0.407854286323603
Loss in iteration 50 : 0.39456973464190004
Loss in iteration 51 : 0.40852365641056937
Loss in iteration 52 : 0.3946503489068234
Loss in iteration 53 : 0.40780109775159795
Loss in iteration 54 : 0.39449319039126984
Loss in iteration 55 : 0.4080837068880738
Loss in iteration 56 : 0.3949111638026897
Loss in iteration 57 : 0.4086984112281181
Loss in iteration 58 : 0.3950220731776952
Loss in iteration 59 : 0.4086618824896051
Loss in iteration 60 : 0.39463126200675847
Loss in iteration 61 : 0.4073632664072708
Loss in iteration 62 : 0.39467851872080023
Loss in iteration 63 : 0.40842491647468676
Loss in iteration 64 : 0.39554119720941777
Loss in iteration 65 : 0.40931028608079345
Loss in iteration 66 : 0.39528130937616623
Loss in iteration 67 : 0.4079551375889545
Loss in iteration 68 : 0.3951188659937725
Loss in iteration 69 : 0.4084153747501828
Loss in iteration 70 : 0.39533680576061153
Loss in iteration 71 : 0.4086679068639144
Loss in iteration 72 : 0.3956102590281531
Loss in iteration 73 : 0.40932120130599975
Loss in iteration 74 : 0.395366297505318
Loss in iteration 75 : 0.4079968103428845
Loss in iteration 76 : 0.3951067553149387
Loss in iteration 77 : 0.40782755578336116
Loss in iteration 78 : 0.39530253875689325
Loss in iteration 79 : 0.40893744816804267
Loss in iteration 80 : 0.3960886847839517
Loss in iteration 81 : 0.41008849568963546
Loss in iteration 82 : 0.39620334185840633
Loss in iteration 83 : 0.40909235522879517
Loss in iteration 84 : 0.3950115558316228
Loss in iteration 85 : 0.4066209001538044
Loss in iteration 86 : 0.3948155315868063
Loss in iteration 87 : 0.40798637466551224
Loss in iteration 88 : 0.39581113470056956
Loss in iteration 89 : 0.4098868405932954
Loss in iteration 90 : 0.39642679208138965
Loss in iteration 91 : 0.4092878506879768
Loss in iteration 92 : 0.3953413349940906
Loss in iteration 93 : 0.40745109578791866
Loss in iteration 94 : 0.3949933297100079
Loss in iteration 95 : 0.4078341413659931
Loss in iteration 96 : 0.3957807386201524
Loss in iteration 97 : 0.41001995479765607
Loss in iteration 98 : 0.3960483636014559
Loss in iteration 99 : 0.4088738022335857
Loss in iteration 100 : 0.3952914079100258
Loss in iteration 101 : 0.40735967098949477
Loss in iteration 102 : 0.3950217643720889
Loss in iteration 103 : 0.4079057073794742
Loss in iteration 104 : 0.3957574480631674
Loss in iteration 105 : 0.41014693460196544
Loss in iteration 106 : 0.39580211893068173
Loss in iteration 107 : 0.4086136403458183
Loss in iteration 108 : 0.39513922521048256
Loss in iteration 109 : 0.407218312909788
Loss in iteration 110 : 0.3952227096518496
Loss in iteration 111 : 0.408550133904935
Loss in iteration 112 : 0.3959175007487931
Loss in iteration 113 : 0.41024972158449313
Loss in iteration 114 : 0.3956180304833048
Loss in iteration 115 : 0.40797633704058023
Loss in iteration 116 : 0.39503309260189623
Loss in iteration 117 : 0.40752697201743926
Loss in iteration 118 : 0.39540635493736576
Loss in iteration 119 : 0.40907734012036623
Loss in iteration 120 : 0.3956780623583374
Loss in iteration 121 : 0.4093924903022571
Loss in iteration 122 : 0.39543381128800514
Loss in iteration 123 : 0.4076472890216146
Loss in iteration 124 : 0.3950862000448755
Loss in iteration 125 : 0.4079587724924251
Loss in iteration 126 : 0.39547581205469384
Loss in iteration 127 : 0.409436096893759
Loss in iteration 128 : 0.3956267407422015
Loss in iteration 129 : 0.40898958500959437
Loss in iteration 130 : 0.3950313446125271
Loss in iteration 131 : 0.4066627468582547
Loss in iteration 132 : 0.3949854130473369
Loss in iteration 133 : 0.40889678388610096
Loss in iteration 134 : 0.3958706973500898
Loss in iteration 135 : 0.4103090793389932
Loss in iteration 136 : 0.39544825711807596
Loss in iteration 137 : 0.40749916729422103
Loss in iteration 138 : 0.39495096928628326
Loss in iteration 139 : 0.40721203936606
Loss in iteration 140 : 0.39537556295808146
Loss in iteration 141 : 0.4098494987192244
Loss in iteration 142 : 0.3958351383206277
Loss in iteration 143 : 0.409324440709686
Loss in iteration 144 : 0.3949074973956014
Loss in iteration 145 : 0.40656337482447935
Loss in iteration 146 : 0.39481115823943697
Loss in iteration 147 : 0.4084296219286746
Loss in iteration 148 : 0.3955694682950029
Loss in iteration 149 : 0.40989871537741007
Loss in iteration 150 : 0.39567213762998066
Loss in iteration 151 : 0.4090617670883501
Loss in iteration 152 : 0.39483351225631913
Loss in iteration 153 : 0.40659624422847346
Loss in iteration 154 : 0.39512251594171616
Loss in iteration 155 : 0.40907613454357034
Loss in iteration 156 : 0.3955519048843735
Loss in iteration 157 : 0.40973669348818476
Loss in iteration 158 : 0.3951388463522449
Loss in iteration 159 : 0.4071864138137207
Loss in iteration 160 : 0.39515969046267807
Loss in iteration 161 : 0.40876881184923397
Loss in iteration 162 : 0.39521520472675864
Loss in iteration 163 : 0.408926360946685
Loss in iteration 164 : 0.39501547963154254
Loss in iteration 165 : 0.40736284349899743
Loss in iteration 166 : 0.39536264627777445
Loss in iteration 167 : 0.40978171432256255
Loss in iteration 168 : 0.39573584078616975
Loss in iteration 169 : 0.4092493851115448
Loss in iteration 170 : 0.39463827061783624
Loss in iteration 171 : 0.4059827676500281
Loss in iteration 172 : 0.3945871349626
Loss in iteration 173 : 0.40825699653998737
Loss in iteration 174 : 0.3956966670544075
Loss in iteration 175 : 0.4106048566875849
Loss in iteration 176 : 0.39585674405592775
Loss in iteration 177 : 0.40879147117250436
Loss in iteration 178 : 0.3947260259086228
Loss in iteration 179 : 0.40646159643262897
Loss in iteration 180 : 0.3945600000380149
Loss in iteration 181 : 0.4081780147860588
Loss in iteration 182 : 0.39567623072635244
Loss in iteration 183 : 0.4105119242782156
Loss in iteration 184 : 0.3955843249066154
Loss in iteration 185 : 0.40838797167712254
Loss in iteration 186 : 0.39456247538272676
Loss in iteration 187 : 0.4062954033839295
Loss in iteration 188 : 0.3945577753731333
Loss in iteration 189 : 0.4082301828773212
Loss in iteration 190 : 0.39588988938280234
Loss in iteration 191 : 0.4109449020975536
Loss in iteration 192 : 0.39585064828655364
Loss in iteration 193 : 0.40900341315334826
Loss in iteration 194 : 0.39475019977220344
Loss in iteration 195 : 0.40634466952340614
Loss in iteration 196 : 0.3943667451038689
Loss in iteration 197 : 0.4071509846047775
Loss in iteration 198 : 0.39544064439712595
Loss in iteration 199 : 0.4106235927308386
Loss in iteration 200 : 0.3958003385906763
Testing accuracy  of updater 5 on alg 1 with rate 0.09999999999999999 = 0.8374178490264725, training accuracy 0.8355958230958231, time elapsed: 4662 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6843845567867433
Loss in iteration 3 : 0.4173327866547825
Loss in iteration 4 : 0.40521477082448126
Loss in iteration 5 : 0.3982147191660143
Loss in iteration 6 : 0.39576174779081785
Loss in iteration 7 : 0.39642100877562486
Loss in iteration 8 : 0.39037119999035247
Loss in iteration 9 : 0.39152133493196456
Loss in iteration 10 : 0.3820424316776287
Loss in iteration 11 : 0.3824360277769833
Loss in iteration 12 : 0.37581138152653537
Loss in iteration 13 : 0.3766880154317585
Loss in iteration 14 : 0.37209262142667415
Loss in iteration 15 : 0.37457067722361675
Loss in iteration 16 : 0.3713761880210757
Loss in iteration 17 : 0.3757374814844218
Loss in iteration 18 : 0.37280100727529625
Loss in iteration 19 : 0.37931933692448305
Loss in iteration 20 : 0.3746763122518674
Loss in iteration 21 : 0.3811059974074008
Loss in iteration 22 : 0.37488755606571506
Loss in iteration 23 : 0.38063011990529577
Loss in iteration 24 : 0.37423227883684346
Loss in iteration 25 : 0.37941227050017173
Loss in iteration 26 : 0.37379519799900723
Loss in iteration 27 : 0.37931510937478063
Loss in iteration 28 : 0.374063849113283
Loss in iteration 29 : 0.3790481109623102
Loss in iteration 30 : 0.37397847734470413
Loss in iteration 31 : 0.3793733308415019
Loss in iteration 32 : 0.3749251642446145
Loss in iteration 33 : 0.380910191884882
Loss in iteration 34 : 0.3751882098253938
Loss in iteration 35 : 0.3810799516981621
Loss in iteration 36 : 0.37498060147185247
Loss in iteration 37 : 0.38006552301756996
Loss in iteration 38 : 0.37463807764646234
Loss in iteration 39 : 0.3802296136896727
Loss in iteration 40 : 0.37529956378608087
Loss in iteration 41 : 0.3811702569636236
Loss in iteration 42 : 0.37565216002005064
Loss in iteration 43 : 0.3813617766638222
Loss in iteration 44 : 0.3751190855676512
Loss in iteration 45 : 0.3800322530284479
Loss in iteration 46 : 0.37430709517121047
Loss in iteration 47 : 0.3787181374242341
Loss in iteration 48 : 0.37496075855610733
Loss in iteration 49 : 0.38121244431254
Loss in iteration 50 : 0.37668002263711725
Loss in iteration 51 : 0.38341022067736696
Loss in iteration 52 : 0.37631671295961744
Loss in iteration 53 : 0.38190799806187065
Loss in iteration 54 : 0.37555646276117355
Loss in iteration 55 : 0.3800719148368607
Loss in iteration 56 : 0.3749053543509301
Loss in iteration 57 : 0.37947802252918084
Loss in iteration 58 : 0.37518904984252716
Loss in iteration 59 : 0.38056230634170335
Loss in iteration 60 : 0.3763087943232527
Loss in iteration 61 : 0.38318914432589307
Loss in iteration 62 : 0.3766102978706157
Loss in iteration 63 : 0.38308112417741386
Loss in iteration 64 : 0.37583213287939254
Loss in iteration 65 : 0.38041297000503044
Loss in iteration 66 : 0.3743572395033967
Loss in iteration 67 : 0.378194787255891
Loss in iteration 68 : 0.37427966389100203
Loss in iteration 69 : 0.3794457185769072
Loss in iteration 70 : 0.3761999863389599
Loss in iteration 71 : 0.38430103888890943
Loss in iteration 72 : 0.3775729060239626
Loss in iteration 73 : 0.384178838469649
Loss in iteration 74 : 0.37637495603752813
Loss in iteration 75 : 0.380889338494181
Loss in iteration 76 : 0.3745223227802646
Loss in iteration 77 : 0.37810285647378405
Loss in iteration 78 : 0.37439537599619555
Loss in iteration 79 : 0.37955836203594584
Loss in iteration 80 : 0.3759605677860838
Loss in iteration 81 : 0.38391567761519185
Loss in iteration 82 : 0.3776562602763759
Loss in iteration 83 : 0.38473046729184857
Loss in iteration 84 : 0.37669912449443427
Loss in iteration 85 : 0.38105448032007766
Loss in iteration 86 : 0.3743053032098582
Loss in iteration 87 : 0.37806951908260916
Loss in iteration 88 : 0.3741860717925435
Loss in iteration 89 : 0.37934556607738046
Loss in iteration 90 : 0.37578827910399765
Loss in iteration 91 : 0.38320834852145313
Loss in iteration 92 : 0.37787587867047806
Loss in iteration 93 : 0.38524810713639485
Loss in iteration 94 : 0.376779635032594
Loss in iteration 95 : 0.38124299537946194
Loss in iteration 96 : 0.3745210914263042
Loss in iteration 97 : 0.37804480204895324
Loss in iteration 98 : 0.37393871853383
Loss in iteration 99 : 0.378658890719914
Loss in iteration 100 : 0.3756026980645278
Loss in iteration 101 : 0.38306821284689524
Loss in iteration 102 : 0.37763524053166664
Loss in iteration 103 : 0.3851353645819667
Loss in iteration 104 : 0.37685090550795824
Loss in iteration 105 : 0.3815727688068164
Loss in iteration 106 : 0.37461884119481526
Loss in iteration 107 : 0.37861018904767835
Loss in iteration 108 : 0.37413911544279516
Loss in iteration 109 : 0.37895513694620314
Loss in iteration 110 : 0.37537534125126665
Loss in iteration 111 : 0.38278325936695623
Loss in iteration 112 : 0.37711227982183015
Loss in iteration 113 : 0.38378408736072744
Loss in iteration 114 : 0.3764025604024532
Loss in iteration 115 : 0.38184352848827613
Loss in iteration 116 : 0.37490739883850177
Loss in iteration 117 : 0.3792213725538348
Loss in iteration 118 : 0.374270692310331
Loss in iteration 119 : 0.37901918974306953
Loss in iteration 120 : 0.375294266026078
Loss in iteration 121 : 0.38226498377579293
Loss in iteration 122 : 0.37672644644677006
Loss in iteration 123 : 0.3838971729391316
Loss in iteration 124 : 0.37666404189857333
Loss in iteration 125 : 0.38226177614354295
Loss in iteration 126 : 0.37504220343410544
Loss in iteration 127 : 0.3791254266430864
Loss in iteration 128 : 0.3741548571221463
Loss in iteration 129 : 0.3786749658064165
Loss in iteration 130 : 0.37521454243086627
Loss in iteration 131 : 0.381738237788256
Loss in iteration 132 : 0.3767112353058422
Loss in iteration 133 : 0.3840075143540729
Loss in iteration 134 : 0.3767961271427939
Loss in iteration 135 : 0.38267042091549625
Loss in iteration 136 : 0.37515531008970693
Loss in iteration 137 : 0.3795905081278476
Loss in iteration 138 : 0.3739884107816294
Loss in iteration 139 : 0.37852267445820376
Loss in iteration 140 : 0.374856537421073
Loss in iteration 141 : 0.38118612036879795
Loss in iteration 142 : 0.37661805943807547
Loss in iteration 143 : 0.3840098628159991
Loss in iteration 144 : 0.37681589802911375
Loss in iteration 145 : 0.3831937193697836
Loss in iteration 146 : 0.37533882568959276
Loss in iteration 147 : 0.3795738346421132
Loss in iteration 148 : 0.3737463484583433
Loss in iteration 149 : 0.37819612648042944
Loss in iteration 150 : 0.3747438038343855
Loss in iteration 151 : 0.3808923542907933
Loss in iteration 152 : 0.37633471613290115
Loss in iteration 153 : 0.38384537450626655
Loss in iteration 154 : 0.3769428389297143
Loss in iteration 155 : 0.3835299030785946
Loss in iteration 156 : 0.3754215630231191
Loss in iteration 157 : 0.3801226423273207
Loss in iteration 158 : 0.3740733298816447
Loss in iteration 159 : 0.3785630942046263
Loss in iteration 160 : 0.37463232406156277
Loss in iteration 161 : 0.3808453024711122
Loss in iteration 162 : 0.3759707284870886
Loss in iteration 163 : 0.38297758277438143
Loss in iteration 164 : 0.3763314863460608
Loss in iteration 165 : 0.3828058459348764
Loss in iteration 166 : 0.3754801371029659
Loss in iteration 167 : 0.3801066912698783
Loss in iteration 168 : 0.37423139680814593
Loss in iteration 169 : 0.3791333583814619
Loss in iteration 170 : 0.37472085038411784
Loss in iteration 171 : 0.38104357422218516
Loss in iteration 172 : 0.376242178710103
Loss in iteration 173 : 0.3835582142520651
Loss in iteration 174 : 0.37660214397396147
Loss in iteration 175 : 0.38272632022477476
Loss in iteration 176 : 0.37501528157957387
Loss in iteration 177 : 0.3794705239542272
Loss in iteration 178 : 0.37401142056807446
Loss in iteration 179 : 0.37873180518483807
Loss in iteration 180 : 0.37477125636251274
Loss in iteration 181 : 0.3814314971512098
Loss in iteration 182 : 0.37637608588170707
Loss in iteration 183 : 0.38358049329437643
Loss in iteration 184 : 0.3765528908467309
Loss in iteration 185 : 0.38236509741851477
Loss in iteration 186 : 0.3749073238635519
Loss in iteration 187 : 0.3796738254582778
Loss in iteration 188 : 0.37411073283573804
Loss in iteration 189 : 0.3790135429532524
Loss in iteration 190 : 0.374713586969581
Loss in iteration 191 : 0.38125966257285526
Loss in iteration 192 : 0.3761599725585983
Loss in iteration 193 : 0.3834136777733545
Loss in iteration 194 : 0.37650072793510003
Loss in iteration 195 : 0.3827664013040445
Loss in iteration 196 : 0.37496917981577715
Loss in iteration 197 : 0.37953123166866853
Loss in iteration 198 : 0.37389621540231965
Loss in iteration 199 : 0.37847796174126813
Loss in iteration 200 : 0.37470192554855036
Testing accuracy  of updater 5 on alg 1 with rate 0.06999999999999999 = 0.8415330753639212, training accuracy 0.8420761670761671, time elapsed: 3789 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5154326783310322
Loss in iteration 3 : 0.4783246235328652
Loss in iteration 4 : 0.45744209483950543
Loss in iteration 5 : 0.43037916465315235
Loss in iteration 6 : 0.41495267134058644
Loss in iteration 7 : 0.40517487387049367
Loss in iteration 8 : 0.39711469653007914
Loss in iteration 9 : 0.3907770591141609
Loss in iteration 10 : 0.3855962076384159
Loss in iteration 11 : 0.3811526198489838
Loss in iteration 12 : 0.37756072980829986
Loss in iteration 13 : 0.37438906043896
Loss in iteration 14 : 0.37189710262799813
Loss in iteration 15 : 0.3695909018506481
Loss in iteration 16 : 0.36769746554138444
Loss in iteration 17 : 0.3659608311033904
Loss in iteration 18 : 0.3645567358248322
Loss in iteration 19 : 0.36322503524808974
Loss in iteration 20 : 0.3620744497470421
Loss in iteration 21 : 0.36107598097034516
Loss in iteration 22 : 0.3604586234901989
Loss in iteration 23 : 0.36031951250730443
Loss in iteration 24 : 0.361536874212232
Loss in iteration 25 : 0.36402824116688853
Loss in iteration 26 : 0.371389315545856
Loss in iteration 27 : 0.37403171531180296
Loss in iteration 28 : 0.37979679629626967
Loss in iteration 29 : 0.3678991668021487
Loss in iteration 30 : 0.3674486800938167
Loss in iteration 31 : 0.3621058084156237
Loss in iteration 32 : 0.36221025996438
Loss in iteration 33 : 0.35976668367319986
Loss in iteration 34 : 0.3603808436596484
Loss in iteration 35 : 0.3593830020746914
Loss in iteration 36 : 0.3607545164507336
Loss in iteration 37 : 0.36080289167337487
Loss in iteration 38 : 0.36372638742682734
Loss in iteration 39 : 0.36268225629661155
Loss in iteration 40 : 0.36486560334378526
Loss in iteration 41 : 0.36269436031013347
Loss in iteration 42 : 0.36376864584611224
Loss in iteration 43 : 0.36143168112459745
Loss in iteration 44 : 0.3624081257436785
Loss in iteration 45 : 0.36068850664929575
Loss in iteration 46 : 0.361751709291612
Loss in iteration 47 : 0.360706632028355
Loss in iteration 48 : 0.3618592194254919
Loss in iteration 49 : 0.3611697400482681
Loss in iteration 50 : 0.3627209696607452
Loss in iteration 51 : 0.36155902975679916
Loss in iteration 52 : 0.36315270108663955
Loss in iteration 53 : 0.36158546774919254
Loss in iteration 54 : 0.3624892589123419
Loss in iteration 55 : 0.36068685132429207
Loss in iteration 56 : 0.36088804492061877
Loss in iteration 57 : 0.3600915136344951
Loss in iteration 58 : 0.3607294665691712
Loss in iteration 59 : 0.3604153651782867
Loss in iteration 60 : 0.3617555295155703
Loss in iteration 61 : 0.36146302871801894
Loss in iteration 62 : 0.36317506895899937
Loss in iteration 63 : 0.36223048864455576
Loss in iteration 64 : 0.3633120817675243
Loss in iteration 65 : 0.36153921443791853
Loss in iteration 66 : 0.36170243455994394
Loss in iteration 67 : 0.360198636318063
Loss in iteration 68 : 0.360424358176693
Loss in iteration 69 : 0.3599487595759762
Loss in iteration 70 : 0.36085462176234046
Loss in iteration 71 : 0.3605833506186641
Loss in iteration 72 : 0.3622278012805087
Loss in iteration 73 : 0.36179110495792904
Loss in iteration 74 : 0.36360660472144285
Loss in iteration 75 : 0.3621400867958645
Loss in iteration 76 : 0.36298074116668044
Loss in iteration 77 : 0.36099641629617196
Loss in iteration 78 : 0.36116369970633355
Loss in iteration 79 : 0.3598654492550539
Loss in iteration 80 : 0.3601311785265141
Loss in iteration 81 : 0.3596551058863095
Loss in iteration 82 : 0.36091230565026683
Loss in iteration 83 : 0.3607228925134931
Loss in iteration 84 : 0.362553026201975
Loss in iteration 85 : 0.3620327184026309
Loss in iteration 86 : 0.36341897330129574
Loss in iteration 87 : 0.3618576401078193
Loss in iteration 88 : 0.36258905438253203
Loss in iteration 89 : 0.36067163108549916
Loss in iteration 90 : 0.3610078732969544
Loss in iteration 91 : 0.3598829181766097
Loss in iteration 92 : 0.3602647722204737
Loss in iteration 93 : 0.35964672671120695
Loss in iteration 94 : 0.3603355465919028
Loss in iteration 95 : 0.3606349652850461
Loss in iteration 96 : 0.3624003889901135
Loss in iteration 97 : 0.3619705662678336
Loss in iteration 98 : 0.36339860004459845
Loss in iteration 99 : 0.3619369323748009
Loss in iteration 100 : 0.362735445427578
Loss in iteration 101 : 0.3607795810276
Loss in iteration 102 : 0.3610568552033765
Loss in iteration 103 : 0.3599343180626741
Loss in iteration 104 : 0.3602089062259584
Loss in iteration 105 : 0.35959140938118683
Loss in iteration 106 : 0.36017127033804924
Loss in iteration 107 : 0.36025909437298215
Loss in iteration 108 : 0.36186006785038416
Loss in iteration 109 : 0.3617035871839521
Loss in iteration 110 : 0.3635319690576382
Loss in iteration 111 : 0.3621611250994049
Loss in iteration 112 : 0.36300585225437365
Loss in iteration 113 : 0.360879388287033
Loss in iteration 114 : 0.3610020700173751
Loss in iteration 115 : 0.3599466757754784
Loss in iteration 116 : 0.35991608542171194
Loss in iteration 117 : 0.35939723046672944
Loss in iteration 118 : 0.3598715503411374
Loss in iteration 119 : 0.3600002869807824
Loss in iteration 120 : 0.3613547817569041
Loss in iteration 121 : 0.36185349023845453
Loss in iteration 122 : 0.36381304428516226
Loss in iteration 123 : 0.36225907910207816
Loss in iteration 124 : 0.3631126415478302
Loss in iteration 125 : 0.3608784889921971
Loss in iteration 126 : 0.36085766409486286
Loss in iteration 127 : 0.3599022254878172
Loss in iteration 128 : 0.35980349438703607
Loss in iteration 129 : 0.35940469316865925
Loss in iteration 130 : 0.3599322778244594
Loss in iteration 131 : 0.3603930272431855
Loss in iteration 132 : 0.36174578503931504
Loss in iteration 133 : 0.3619202951770551
Loss in iteration 134 : 0.3636861111268244
Loss in iteration 135 : 0.3621530670153544
Loss in iteration 136 : 0.3627270191994546
Loss in iteration 137 : 0.3606415677969758
Loss in iteration 138 : 0.36031464004734876
Loss in iteration 139 : 0.3594492792061664
Loss in iteration 140 : 0.35941536146717573
Loss in iteration 141 : 0.3592131479541257
Loss in iteration 142 : 0.3599734272143891
Loss in iteration 143 : 0.3607307674359353
Loss in iteration 144 : 0.3626113094302916
Loss in iteration 145 : 0.36251502228516597
Loss in iteration 146 : 0.3638852874113197
Loss in iteration 147 : 0.3622136283290972
Loss in iteration 148 : 0.3623292926457982
Loss in iteration 149 : 0.3603319146040432
Loss in iteration 150 : 0.35976572685277375
Loss in iteration 151 : 0.3589032266050705
Loss in iteration 152 : 0.35899015708968235
Loss in iteration 153 : 0.35912644798175497
Loss in iteration 154 : 0.3602786223727623
Loss in iteration 155 : 0.3610492096880728
Loss in iteration 156 : 0.3630838807736781
Loss in iteration 157 : 0.3627220509885077
Loss in iteration 158 : 0.3635324311495282
Loss in iteration 159 : 0.36166124707441505
Loss in iteration 160 : 0.36182853153582223
Loss in iteration 161 : 0.3604666075247886
Loss in iteration 162 : 0.360131742406474
Loss in iteration 163 : 0.3593837176472748
Loss in iteration 164 : 0.3595354844540859
Loss in iteration 165 : 0.3594458173196376
Loss in iteration 166 : 0.36037363698340524
Loss in iteration 167 : 0.36085641122153295
Loss in iteration 168 : 0.3624884014141482
Loss in iteration 169 : 0.36225272830761285
Loss in iteration 170 : 0.36327223550528787
Loss in iteration 171 : 0.36165918703169114
Loss in iteration 172 : 0.3618094791691311
Loss in iteration 173 : 0.3604009044177573
Loss in iteration 174 : 0.35998078573415776
Loss in iteration 175 : 0.35931187017681665
Loss in iteration 176 : 0.3594977770394257
Loss in iteration 177 : 0.35948021134476027
Loss in iteration 178 : 0.3606297056514317
Loss in iteration 179 : 0.3610050310001612
Loss in iteration 180 : 0.3628093052832805
Loss in iteration 181 : 0.36243751494146964
Loss in iteration 182 : 0.3633777152224956
Loss in iteration 183 : 0.36141467971528424
Loss in iteration 184 : 0.3614019101762617
Loss in iteration 185 : 0.35996543356495053
Loss in iteration 186 : 0.35968725824062225
Loss in iteration 187 : 0.3590349803004074
Loss in iteration 188 : 0.3592167054583161
Loss in iteration 189 : 0.3594473908850023
Loss in iteration 190 : 0.36091866570424536
Loss in iteration 191 : 0.36142461871054066
Loss in iteration 192 : 0.36339175756529946
Loss in iteration 193 : 0.362528201166851
Loss in iteration 194 : 0.36318562549550854
Loss in iteration 195 : 0.36115589525063463
Loss in iteration 196 : 0.360749715714248
Loss in iteration 197 : 0.35960779662968884
Loss in iteration 198 : 0.35951166948821117
Loss in iteration 199 : 0.3591739681435887
Loss in iteration 200 : 0.35978328591973413
Testing accuracy  of updater 5 on alg 1 with rate 0.04 = 0.8450340888151834, training accuracy 0.8482186732186732, time elapsed: 3726 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7847591553585358
Loss in iteration 3 : 0.6244521860317278
Loss in iteration 4 : 0.4900979948687368
Loss in iteration 5 : 0.47341113369124804
Loss in iteration 6 : 0.46563463420505175
Loss in iteration 7 : 0.46346891724787936
Loss in iteration 8 : 0.45946977197987726
Loss in iteration 9 : 0.4567860826958593
Loss in iteration 10 : 0.4540460482790506
Loss in iteration 11 : 0.4515707813849381
Loss in iteration 12 : 0.4491055627380698
Loss in iteration 13 : 0.446665178325208
Loss in iteration 14 : 0.44420383974468125
Loss in iteration 15 : 0.44170822324213066
Loss in iteration 16 : 0.4391702601962923
Loss in iteration 17 : 0.43658773610742335
Loss in iteration 18 : 0.433960666387508
Loss in iteration 19 : 0.431289399122104
Loss in iteration 20 : 0.4285719754568857
Loss in iteration 21 : 0.4258087044070415
Loss in iteration 22 : 0.4230006637514477
Loss in iteration 23 : 0.42014469405022525
Loss in iteration 24 : 0.4172419149762025
Loss in iteration 25 : 0.4143083477387184
Loss in iteration 26 : 0.4114333737760338
Loss in iteration 27 : 0.4085851051462935
Loss in iteration 28 : 0.40572245386225886
Loss in iteration 29 : 0.4028801546868961
Loss in iteration 30 : 0.4000758118324085
Loss in iteration 31 : 0.3973817922912693
Loss in iteration 32 : 0.39482467911841196
Loss in iteration 33 : 0.3924209463976128
Loss in iteration 34 : 0.3901626123804815
Loss in iteration 35 : 0.3880202059256672
Loss in iteration 36 : 0.3859901615464057
Loss in iteration 37 : 0.3840598887669253
Loss in iteration 38 : 0.38224633671176145
Loss in iteration 39 : 0.3805259132484983
Loss in iteration 40 : 0.37888755093669035
Loss in iteration 41 : 0.37735448290118245
Loss in iteration 42 : 0.37589987289498045
Loss in iteration 43 : 0.3745267337340673
Loss in iteration 44 : 0.3732408339756661
Loss in iteration 45 : 0.37204018946911627
Loss in iteration 46 : 0.37089288216312144
Loss in iteration 47 : 0.3698101973571784
Loss in iteration 48 : 0.3688073723928737
Loss in iteration 49 : 0.3678600180915951
Loss in iteration 50 : 0.36697198946808846
Loss in iteration 51 : 0.3661426857963372
Loss in iteration 52 : 0.36536363150842877
Loss in iteration 53 : 0.3646365184148168
Loss in iteration 54 : 0.36394850833403286
Loss in iteration 55 : 0.3632719070452071
Loss in iteration 56 : 0.3626338886882285
Loss in iteration 57 : 0.36205266151954557
Loss in iteration 58 : 0.361535855927933
Loss in iteration 59 : 0.3611338794071968
Loss in iteration 60 : 0.3610996544169945
Loss in iteration 61 : 0.36113612627132113
Loss in iteration 62 : 0.3611683116142979
Loss in iteration 63 : 0.36037846424985837
Loss in iteration 64 : 0.3598508262705486
Loss in iteration 65 : 0.35898872740987525
Loss in iteration 66 : 0.3586003311903589
Loss in iteration 67 : 0.3579831792513277
Loss in iteration 68 : 0.35777769282672034
Loss in iteration 69 : 0.35733239811184186
Loss in iteration 70 : 0.35706290281929753
Loss in iteration 71 : 0.3567755256042517
Loss in iteration 72 : 0.35672962409533643
Loss in iteration 73 : 0.3565179339920727
Loss in iteration 74 : 0.356579130520604
Loss in iteration 75 : 0.3563241015552207
Loss in iteration 76 : 0.35636260002305126
Loss in iteration 77 : 0.35611818806512047
Loss in iteration 78 : 0.3560448038276384
Loss in iteration 79 : 0.3556635948936795
Loss in iteration 80 : 0.3554272459501627
Loss in iteration 81 : 0.35504020721352986
Loss in iteration 82 : 0.3549172353054769
Loss in iteration 83 : 0.35468687436694724
Loss in iteration 84 : 0.3546338073740451
Loss in iteration 85 : 0.3544759631704471
Loss in iteration 86 : 0.3544449913067245
Loss in iteration 87 : 0.3543527262622994
Loss in iteration 88 : 0.35435830965926385
Loss in iteration 89 : 0.35422231970997836
Loss in iteration 90 : 0.3542488158517674
Loss in iteration 91 : 0.35408551593559623
Loss in iteration 92 : 0.3540179831454466
Loss in iteration 93 : 0.35383429111152037
Loss in iteration 94 : 0.35378288811253883
Loss in iteration 95 : 0.35361757342075495
Loss in iteration 96 : 0.3535307062871097
Loss in iteration 97 : 0.3533891201083098
Loss in iteration 98 : 0.3533186351821317
Loss in iteration 99 : 0.35320718844822657
Loss in iteration 100 : 0.35331411831128023
Loss in iteration 101 : 0.3533314582510408
Loss in iteration 102 : 0.35353735951469345
Loss in iteration 103 : 0.35335133287894993
Loss in iteration 104 : 0.35355159262894725
Loss in iteration 105 : 0.35318703856048933
Loss in iteration 106 : 0.35313789335140167
Loss in iteration 107 : 0.3527738348228052
Loss in iteration 108 : 0.35275406063262016
Loss in iteration 109 : 0.35259560769482334
Loss in iteration 110 : 0.3526768274501514
Loss in iteration 111 : 0.3526021713757226
Loss in iteration 112 : 0.35277725454841713
Loss in iteration 113 : 0.352756240952393
Loss in iteration 114 : 0.3530486263075608
Loss in iteration 115 : 0.3529034514263656
Loss in iteration 116 : 0.35310542087312136
Loss in iteration 117 : 0.3527943038702534
Loss in iteration 118 : 0.3528327657769276
Loss in iteration 119 : 0.3524905357056883
Loss in iteration 120 : 0.35245606699340715
Loss in iteration 121 : 0.3522918716156903
Loss in iteration 122 : 0.35239173760872117
Loss in iteration 123 : 0.352314143622238
Loss in iteration 124 : 0.3524995481650386
Loss in iteration 125 : 0.35254129929866074
Loss in iteration 126 : 0.35285082215246577
Loss in iteration 127 : 0.35278763224475596
Loss in iteration 128 : 0.35293593068484974
Loss in iteration 129 : 0.35263489396309405
Loss in iteration 130 : 0.3526707014533061
Loss in iteration 131 : 0.3523577397955181
Loss in iteration 132 : 0.3523356281775538
Loss in iteration 133 : 0.35211439708067555
Loss in iteration 134 : 0.3521798520937415
Loss in iteration 135 : 0.35209564865434795
Loss in iteration 136 : 0.3521865713548986
Loss in iteration 137 : 0.3521684788902356
Loss in iteration 138 : 0.3524297440435129
Loss in iteration 139 : 0.3525486738083732
Loss in iteration 140 : 0.3529280348294577
Loss in iteration 141 : 0.3527771216644608
Loss in iteration 142 : 0.35282563430584357
Loss in iteration 143 : 0.35239385580701316
Loss in iteration 144 : 0.35239274799733605
Loss in iteration 145 : 0.35212364808447255
Loss in iteration 146 : 0.352076913801793
Loss in iteration 147 : 0.35190233018402917
Loss in iteration 148 : 0.3519813399857831
Loss in iteration 149 : 0.3519560446340593
Loss in iteration 150 : 0.3521533176655148
Loss in iteration 151 : 0.3522446655195246
Loss in iteration 152 : 0.352704240239957
Loss in iteration 153 : 0.3526581043330854
Loss in iteration 154 : 0.352848606095592
Loss in iteration 155 : 0.3524561820482875
Loss in iteration 156 : 0.35245041008589423
Loss in iteration 157 : 0.3520952154184999
Loss in iteration 158 : 0.35207366709240373
Loss in iteration 159 : 0.3518901341278579
Loss in iteration 160 : 0.3519616789959093
Loss in iteration 161 : 0.35185253252653953
Loss in iteration 162 : 0.352007188748108
Loss in iteration 163 : 0.35210259184604914
Loss in iteration 164 : 0.35241259199287245
Loss in iteration 165 : 0.35247511376489715
Loss in iteration 166 : 0.35282739094662346
Loss in iteration 167 : 0.35248643196278856
Loss in iteration 168 : 0.35258127414225293
Loss in iteration 169 : 0.35213931573150614
Loss in iteration 170 : 0.3521377636136852
Loss in iteration 171 : 0.3518920181997302
Loss in iteration 172 : 0.3519049454909938
Loss in iteration 173 : 0.35178001921742036
Loss in iteration 174 : 0.35190363200802527
Loss in iteration 175 : 0.35191135444296934
Loss in iteration 176 : 0.3521297173768757
Loss in iteration 177 : 0.3522353407782102
Loss in iteration 178 : 0.35265754523627907
Loss in iteration 179 : 0.35251820751562474
Loss in iteration 180 : 0.35273770782370645
Loss in iteration 181 : 0.3522361149495772
Loss in iteration 182 : 0.3522036047530338
Loss in iteration 183 : 0.3518950558542965
Loss in iteration 184 : 0.3518783005131076
Loss in iteration 185 : 0.35171169910965727
Loss in iteration 186 : 0.35180265118807486
Loss in iteration 187 : 0.35177033019599674
Loss in iteration 188 : 0.351957180462292
Loss in iteration 189 : 0.352030690815759
Loss in iteration 190 : 0.3524636720331216
Loss in iteration 191 : 0.3525786570262811
Loss in iteration 192 : 0.3528197700115313
Loss in iteration 193 : 0.3522857459281471
Loss in iteration 194 : 0.3523252302855507
Loss in iteration 195 : 0.3519534668210901
Loss in iteration 196 : 0.3519228676617265
Loss in iteration 197 : 0.3517109235924835
Loss in iteration 198 : 0.35176328403672613
Loss in iteration 199 : 0.35166266850838757
Loss in iteration 200 : 0.35177722610239787
Testing accuracy  of updater 5 on alg 1 with rate 0.01 = 0.8487807874209201, training accuracy 0.8492936117936118, time elapsed: 3436 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8410826838273321
Loss in iteration 3 : 0.7252986492989162
Loss in iteration 4 : 0.62818880698964
Loss in iteration 5 : 0.541905621988042
Loss in iteration 6 : 0.4795102287771848
Loss in iteration 7 : 0.4770561565760245
Loss in iteration 8 : 0.47417334485486484
Loss in iteration 9 : 0.46924853186246684
Loss in iteration 10 : 0.46637715457540413
Loss in iteration 11 : 0.46337614890427936
Loss in iteration 12 : 0.4609902346001593
Loss in iteration 13 : 0.4589039051631071
Loss in iteration 14 : 0.4569836123845707
Loss in iteration 15 : 0.4550869269324792
Loss in iteration 16 : 0.4531813076309786
Loss in iteration 17 : 0.4512707066964526
Loss in iteration 18 : 0.44935792074539427
Loss in iteration 19 : 0.44743072123144345
Loss in iteration 20 : 0.4454851086467708
Loss in iteration 21 : 0.4435186101761527
Loss in iteration 22 : 0.44153002706293204
Loss in iteration 23 : 0.43952070907406754
Loss in iteration 24 : 0.4374865973726272
Loss in iteration 25 : 0.4354265180216373
Loss in iteration 26 : 0.4333395429566337
Loss in iteration 27 : 0.4312234776172531
Loss in iteration 28 : 0.4290803556736178
Loss in iteration 29 : 0.4269101153103998
Loss in iteration 30 : 0.4247155102268598
Loss in iteration 31 : 0.42249083695430273
Loss in iteration 32 : 0.42023797770840376
Loss in iteration 33 : 0.41795633135131827
Loss in iteration 34 : 0.4156611068185792
Loss in iteration 35 : 0.41339017485235
Loss in iteration 36 : 0.411157500391194
Loss in iteration 37 : 0.40891177135259227
Loss in iteration 38 : 0.4066503863290319
Loss in iteration 39 : 0.4043827370659575
Loss in iteration 40 : 0.40213116884505556
Loss in iteration 41 : 0.3999444908431702
Loss in iteration 42 : 0.3978311628037119
Loss in iteration 43 : 0.3958191065596992
Loss in iteration 44 : 0.393879884497547
Loss in iteration 45 : 0.39202316155323963
Loss in iteration 46 : 0.39025453031908286
Loss in iteration 47 : 0.38853941086651816
Loss in iteration 48 : 0.3868760795823706
Loss in iteration 49 : 0.3852920712614031
Loss in iteration 50 : 0.3837604417092621
Loss in iteration 51 : 0.38231295030882645
Loss in iteration 52 : 0.3809258358029051
Loss in iteration 53 : 0.37958950315684
Loss in iteration 54 : 0.37831842289285667
Loss in iteration 55 : 0.3771103058590539
Loss in iteration 56 : 0.37594907704225355
Loss in iteration 57 : 0.3748412858591341
Loss in iteration 58 : 0.3737799115615216
Loss in iteration 59 : 0.37277742276074477
Loss in iteration 60 : 0.3718104277946417
Loss in iteration 61 : 0.37088463599540095
Loss in iteration 62 : 0.37001966659578533
Loss in iteration 63 : 0.3691994782493661
Loss in iteration 64 : 0.36841882579012986
Loss in iteration 65 : 0.3676754943468811
Loss in iteration 66 : 0.3669623341188136
Loss in iteration 67 : 0.3662878570939359
Loss in iteration 68 : 0.36565257655824335
Loss in iteration 69 : 0.36505868859091267
Loss in iteration 70 : 0.36448390123027175
Loss in iteration 71 : 0.3639655136867299
Loss in iteration 72 : 0.36343407461047506
Loss in iteration 73 : 0.3631094625580717
Loss in iteration 74 : 0.36273920157031436
Loss in iteration 75 : 0.36243252742741805
Loss in iteration 76 : 0.3619207732065999
Loss in iteration 77 : 0.36145878835833695
Loss in iteration 78 : 0.3608320434450274
Loss in iteration 79 : 0.36050285988799136
Loss in iteration 80 : 0.3600235211662257
Loss in iteration 81 : 0.3596388982521638
Loss in iteration 82 : 0.35928042324809534
Loss in iteration 83 : 0.3589475117771035
Loss in iteration 84 : 0.35867664153166245
Loss in iteration 85 : 0.358579030809106
Loss in iteration 86 : 0.35830933764226125
Loss in iteration 87 : 0.3582569243764187
Loss in iteration 88 : 0.35792701834441665
Loss in iteration 89 : 0.3577911363976701
Loss in iteration 90 : 0.3573235508135003
Loss in iteration 91 : 0.357144281793026
Loss in iteration 92 : 0.35671004517371163
Loss in iteration 93 : 0.35653394653362164
Loss in iteration 94 : 0.3562492394859109
Loss in iteration 95 : 0.35605196114425847
Loss in iteration 96 : 0.3558662191078409
Loss in iteration 97 : 0.35578548091758294
Loss in iteration 98 : 0.35566629640101455
Loss in iteration 99 : 0.35578863345386225
Loss in iteration 100 : 0.35561453699791495
Loss in iteration 101 : 0.35554198587929653
Loss in iteration 102 : 0.35522602375267676
Loss in iteration 103 : 0.35509589674222236
Loss in iteration 104 : 0.35486837367406643
Loss in iteration 105 : 0.3547525961113207
Loss in iteration 106 : 0.35459770275688324
Loss in iteration 107 : 0.3544482645349302
Loss in iteration 108 : 0.3543475877137442
Loss in iteration 109 : 0.35422817277148766
Loss in iteration 110 : 0.3541161401684675
Loss in iteration 111 : 0.35403367669195956
Loss in iteration 112 : 0.35403182771055725
Loss in iteration 113 : 0.35397232077121144
Loss in iteration 114 : 0.3538933114003584
Loss in iteration 115 : 0.3538080179595159
Loss in iteration 116 : 0.3537072133553822
Loss in iteration 117 : 0.3536002999689359
Loss in iteration 118 : 0.35348092506392204
Loss in iteration 119 : 0.35334553022015897
Loss in iteration 120 : 0.35327249399843413
Loss in iteration 121 : 0.35314341257527826
Loss in iteration 122 : 0.35308631267856316
Loss in iteration 123 : 0.3530497912784322
Loss in iteration 124 : 0.35314411749805563
Loss in iteration 125 : 0.353217343778799
Loss in iteration 126 : 0.3532384050932975
Loss in iteration 127 : 0.353150194758101
Loss in iteration 128 : 0.3529284003294623
Loss in iteration 129 : 0.352856843017915
Loss in iteration 130 : 0.3527198709497094
Loss in iteration 131 : 0.35258485073873574
Loss in iteration 132 : 0.3525306118685209
Loss in iteration 133 : 0.35247204202971344
Loss in iteration 134 : 0.35245646230424343
Loss in iteration 135 : 0.35250399377096514
Loss in iteration 136 : 0.35261327981249196
Loss in iteration 137 : 0.3528170401947964
Loss in iteration 138 : 0.3528229249135133
Loss in iteration 139 : 0.3527340577497129
Loss in iteration 140 : 0.35255006139841333
Loss in iteration 141 : 0.3525052847210499
Loss in iteration 142 : 0.3523097283354335
Loss in iteration 143 : 0.3522538413388741
Loss in iteration 144 : 0.35215255616505653
Loss in iteration 145 : 0.3521360652670831
Loss in iteration 146 : 0.352134027289047
Loss in iteration 147 : 0.35226650735831305
Loss in iteration 148 : 0.35238827177861654
Loss in iteration 149 : 0.35257499594103525
Loss in iteration 150 : 0.3524725518164089
Loss in iteration 151 : 0.35244744450278825
Loss in iteration 152 : 0.3522387612832374
Loss in iteration 153 : 0.35221491096037844
Loss in iteration 154 : 0.3520981104695995
Loss in iteration 155 : 0.35212259660777456
Loss in iteration 156 : 0.3520368708963492
Loss in iteration 157 : 0.35205421807700915
Loss in iteration 158 : 0.3519983666200811
Loss in iteration 159 : 0.3520594493400306
Loss in iteration 160 : 0.35209774834944746
Loss in iteration 161 : 0.35221147632172306
Loss in iteration 162 : 0.35218752197776254
Loss in iteration 163 : 0.3522519607228334
Loss in iteration 164 : 0.35210026650087756
Loss in iteration 165 : 0.3520878529785657
Loss in iteration 166 : 0.35196445222554784
Loss in iteration 167 : 0.3519707244645423
Loss in iteration 168 : 0.3518467959545843
Loss in iteration 169 : 0.3518446794395715
Loss in iteration 170 : 0.35177653479784204
Loss in iteration 171 : 0.3517883875021874
Loss in iteration 172 : 0.3517882541250234
Loss in iteration 173 : 0.3519925240553225
Loss in iteration 174 : 0.35222389838390933
Loss in iteration 175 : 0.352493728556628
Loss in iteration 176 : 0.35229907033165886
Loss in iteration 177 : 0.3521726189249824
Loss in iteration 178 : 0.3518977324531326
Loss in iteration 179 : 0.35182895073538
Loss in iteration 180 : 0.3517058382013479
Loss in iteration 181 : 0.35164780144896196
Loss in iteration 182 : 0.351604374169645
Loss in iteration 183 : 0.3515840971986628
Loss in iteration 184 : 0.3515821173770786
Loss in iteration 185 : 0.3516063113093634
Loss in iteration 186 : 0.3517221376394594
Loss in iteration 187 : 0.35210733493887186
Loss in iteration 188 : 0.35254742660778815
Loss in iteration 189 : 0.35257570704535646
Loss in iteration 190 : 0.3521068144674732
Loss in iteration 191 : 0.3519828516077048
Loss in iteration 192 : 0.3517528113843953
Loss in iteration 193 : 0.3517250502228928
Loss in iteration 194 : 0.35162048585544775
Loss in iteration 195 : 0.35160173678072265
Loss in iteration 196 : 0.3515553879847675
Loss in iteration 197 : 0.3515592475743872
Loss in iteration 198 : 0.3515636956230544
Loss in iteration 199 : 0.3517144724538616
Loss in iteration 200 : 0.3520289581667229
Testing accuracy  of updater 5 on alg 1 with rate 0.007 = 0.8503777409250046, training accuracy 0.8487407862407862, time elapsed: 3731 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9088954365323879
Loss in iteration 3 : 0.842609356649544
Loss in iteration 4 : 0.7870449047206448
Loss in iteration 5 : 0.7376905157925595
Loss in iteration 6 : 0.6924452267794013
Loss in iteration 7 : 0.6501358975031625
Loss in iteration 8 : 0.6100318044851523
Loss in iteration 9 : 0.5716431064336417
Loss in iteration 10 : 0.5346236296294904
Loss in iteration 11 : 0.49871880569115146
Loss in iteration 12 : 0.48191181568774205
Loss in iteration 13 : 0.4780535309198353
Loss in iteration 14 : 0.4768818426812209
Loss in iteration 15 : 0.4751777282726906
Loss in iteration 16 : 0.47378734746171935
Loss in iteration 17 : 0.4721385096477203
Loss in iteration 18 : 0.4707023983670903
Loss in iteration 19 : 0.46928314083483974
Loss in iteration 20 : 0.4678580950889593
Loss in iteration 21 : 0.46640019399230753
Loss in iteration 22 : 0.4649170051354973
Loss in iteration 23 : 0.4634692410886805
Loss in iteration 24 : 0.46214090877268454
Loss in iteration 25 : 0.4609281495942488
Loss in iteration 26 : 0.45976148368477987
Loss in iteration 27 : 0.4586376314337519
Loss in iteration 28 : 0.4575171371068085
Loss in iteration 29 : 0.45638768175339994
Loss in iteration 30 : 0.45524435826401005
Loss in iteration 31 : 0.4540880585135167
Loss in iteration 32 : 0.45291842475447863
Loss in iteration 33 : 0.4517399320189274
Loss in iteration 34 : 0.45054931727331343
Loss in iteration 35 : 0.44934608291942024
Loss in iteration 36 : 0.44812826417400936
Loss in iteration 37 : 0.4468945198848872
Loss in iteration 38 : 0.4456474958925925
Loss in iteration 39 : 0.44438696904949365
Loss in iteration 40 : 0.4431118425447879
Loss in iteration 41 : 0.4418208098553406
Loss in iteration 42 : 0.4405120121976548
Loss in iteration 43 : 0.439185210249404
Loss in iteration 44 : 0.43784068432991996
Loss in iteration 45 : 0.436479984784832
Loss in iteration 46 : 0.43510543967381615
Loss in iteration 47 : 0.43371545986976906
Loss in iteration 48 : 0.4323110467881044
Loss in iteration 49 : 0.4308913490511595
Loss in iteration 50 : 0.42945586813634573
Loss in iteration 51 : 0.4280043893719957
Loss in iteration 52 : 0.42653912117833165
Loss in iteration 53 : 0.42505760659008573
Loss in iteration 54 : 0.42356473445805
Loss in iteration 55 : 0.42205901145519575
Loss in iteration 56 : 0.4205378756001856
Loss in iteration 57 : 0.4190020941083963
Loss in iteration 58 : 0.41745421450532993
Loss in iteration 59 : 0.41589264560337474
Loss in iteration 60 : 0.4143204788309404
Loss in iteration 61 : 0.41274350414162103
Loss in iteration 62 : 0.4111823343423649
Loss in iteration 63 : 0.4096371884298696
Loss in iteration 64 : 0.408107848950936
Loss in iteration 65 : 0.40657265323934894
Loss in iteration 66 : 0.4050321436290009
Loss in iteration 67 : 0.4034846069227215
Loss in iteration 68 : 0.4019430931926947
Loss in iteration 69 : 0.4004071669268613
Loss in iteration 70 : 0.3988971521386834
Loss in iteration 71 : 0.39740970515804885
Loss in iteration 72 : 0.3959432869085992
Loss in iteration 73 : 0.3945330049072272
Loss in iteration 74 : 0.39317326908555744
Loss in iteration 75 : 0.39186287730508207
Loss in iteration 76 : 0.39057977782948156
Loss in iteration 77 : 0.38933864643580385
Loss in iteration 78 : 0.38812133344995425
Loss in iteration 79 : 0.3869414686208768
Loss in iteration 80 : 0.38581548220153655
Loss in iteration 81 : 0.38472443298723485
Loss in iteration 82 : 0.3836721013739125
Loss in iteration 83 : 0.38266014916917035
Loss in iteration 84 : 0.38168485100177824
Loss in iteration 85 : 0.38074677311812294
Loss in iteration 86 : 0.37984380707832605
Loss in iteration 87 : 0.37897637968174186
Loss in iteration 88 : 0.37813031654678975
Loss in iteration 89 : 0.3773151196085959
Loss in iteration 90 : 0.3765391831506158
Loss in iteration 91 : 0.37577304402290673
Loss in iteration 92 : 0.3749935951604948
Loss in iteration 93 : 0.3742608233496641
Loss in iteration 94 : 0.37353986147949503
Loss in iteration 95 : 0.3728479784185777
Loss in iteration 96 : 0.3722070958515475
Loss in iteration 97 : 0.3715675670077023
Loss in iteration 98 : 0.37096679131714694
Loss in iteration 99 : 0.3703855198069062
Loss in iteration 100 : 0.3698308531513861
Loss in iteration 101 : 0.36928840871810525
Loss in iteration 102 : 0.3687704443258959
Loss in iteration 103 : 0.3682702183294177
Loss in iteration 104 : 0.36777112445231697
Loss in iteration 105 : 0.3672899844029157
Loss in iteration 106 : 0.3668247766856212
Loss in iteration 107 : 0.36637987275330747
Loss in iteration 108 : 0.36595862323580985
Loss in iteration 109 : 0.36558816287465495
Loss in iteration 110 : 0.3652055915222806
Loss in iteration 111 : 0.3648513834373505
Loss in iteration 112 : 0.36445377072198665
Loss in iteration 113 : 0.3640700479291957
Loss in iteration 114 : 0.3636739137375316
Loss in iteration 115 : 0.3633157670193925
Loss in iteration 116 : 0.36295751539423626
Loss in iteration 117 : 0.36261221047595965
Loss in iteration 118 : 0.36229040440528665
Loss in iteration 119 : 0.3619777739756296
Loss in iteration 120 : 0.3616810379583684
Loss in iteration 121 : 0.36140679812945287
Loss in iteration 122 : 0.3611464346212609
Loss in iteration 123 : 0.36095273110470444
Loss in iteration 124 : 0.3606494390519391
Loss in iteration 125 : 0.3604585428407248
Loss in iteration 126 : 0.3601114077902421
Loss in iteration 127 : 0.3598910315690346
Loss in iteration 128 : 0.35958560960766217
Loss in iteration 129 : 0.3593281291215689
Loss in iteration 130 : 0.3590959352049332
Loss in iteration 131 : 0.35888572050771406
Loss in iteration 132 : 0.3586688863593938
Loss in iteration 133 : 0.35849361124244594
Loss in iteration 134 : 0.3582920807335502
Loss in iteration 135 : 0.3581756836425767
Loss in iteration 136 : 0.3580307141682882
Loss in iteration 137 : 0.3578869712658886
Loss in iteration 138 : 0.35760001971011246
Loss in iteration 139 : 0.3574452653351953
Loss in iteration 140 : 0.3572115961456123
Loss in iteration 141 : 0.3570413690698694
Loss in iteration 142 : 0.35687101897862367
Loss in iteration 143 : 0.35668916981912596
Loss in iteration 144 : 0.35653661401341813
Loss in iteration 145 : 0.3563853204430763
Loss in iteration 146 : 0.3562503567643261
Loss in iteration 147 : 0.35613304818320723
Loss in iteration 148 : 0.35603066958356683
Loss in iteration 149 : 0.3559933809378025
Loss in iteration 150 : 0.35588713005489003
Loss in iteration 151 : 0.3557778521656816
Loss in iteration 152 : 0.3555619745381512
Loss in iteration 153 : 0.3554322335189416
Loss in iteration 154 : 0.35526326081573356
Loss in iteration 155 : 0.3551305244457895
Loss in iteration 156 : 0.35503959441499566
Loss in iteration 157 : 0.354941905907572
Loss in iteration 158 : 0.35486167702121046
Loss in iteration 159 : 0.3547984969127815
Loss in iteration 160 : 0.3546839662402338
Loss in iteration 161 : 0.3545692991356499
Loss in iteration 162 : 0.3544581600436254
Loss in iteration 163 : 0.35434969456620363
Loss in iteration 164 : 0.354268692593525
Loss in iteration 165 : 0.3541991757699441
Loss in iteration 166 : 0.3541438890208748
Loss in iteration 167 : 0.35410298184850825
Loss in iteration 168 : 0.3540172312097025
Loss in iteration 169 : 0.3539786720098624
Loss in iteration 170 : 0.35388473314755553
Loss in iteration 171 : 0.35381343632083045
Loss in iteration 172 : 0.35368205763158916
Loss in iteration 173 : 0.3535725983280783
Loss in iteration 174 : 0.35351314656425276
Loss in iteration 175 : 0.35343690101149555
Loss in iteration 176 : 0.3534067573134236
Loss in iteration 177 : 0.3533677968494642
Loss in iteration 178 : 0.3533215366680569
Loss in iteration 179 : 0.3533375139628637
Loss in iteration 180 : 0.353278018006385
Loss in iteration 181 : 0.3532615512506299
Loss in iteration 182 : 0.3531264800211139
Loss in iteration 183 : 0.3530244575173976
Loss in iteration 184 : 0.35295031743872124
Loss in iteration 185 : 0.35288529732348795
Loss in iteration 186 : 0.35284096332249326
Loss in iteration 187 : 0.35278868396077084
Loss in iteration 188 : 0.3527673552284579
Loss in iteration 189 : 0.3527952504597427
Loss in iteration 190 : 0.35283677589203244
Loss in iteration 191 : 0.3528761385937608
Loss in iteration 192 : 0.3528254283393645
Loss in iteration 193 : 0.3526948780396965
Loss in iteration 194 : 0.35260810217624555
Loss in iteration 195 : 0.3525345738987051
Loss in iteration 196 : 0.3524768155529829
Loss in iteration 197 : 0.3524311022866815
Loss in iteration 198 : 0.35241495427179775
Loss in iteration 199 : 0.3524033516416871
Loss in iteration 200 : 0.35241465431019214
Testing accuracy  of updater 5 on alg 1 with rate 0.004 = 0.849640685461581, training accuracy 0.8481879606879607, time elapsed: 3697 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9771601290401741
Loss in iteration 3 : 0.9605635817122714
Loss in iteration 4 : 0.9466584033313407
Loss in iteration 5 : 0.9343106304357993
Loss in iteration 6 : 0.9229928084786524
Loss in iteration 7 : 0.9124106244259133
Loss in iteration 8 : 0.9023808458747308
Loss in iteration 9 : 0.8927806865325896
Loss in iteration 10 : 0.8835233964186381
Loss in iteration 11 : 0.8745451954778818
Loss in iteration 12 : 0.8657977147108145
Loss in iteration 13 : 0.8572433527590892
Loss in iteration 14 : 0.8488522833604322
Loss in iteration 15 : 0.8406004495768551
Loss in iteration 16 : 0.8324681746796136
Loss in iteration 17 : 0.8244391730305932
Loss in iteration 18 : 0.8164998287821772
Loss in iteration 19 : 0.8086386588644604
Loss in iteration 20 : 0.8008459058388517
Loss in iteration 21 : 0.7931132242069817
Loss in iteration 22 : 0.7854334352359308
Loss in iteration 23 : 0.777800332858994
Loss in iteration 24 : 0.7702085282259679
Loss in iteration 25 : 0.7626533238994286
Loss in iteration 26 : 0.7551306110744236
Loss in iteration 27 : 0.7476367848813276
Loss in iteration 28 : 0.7401686740404813
Loss in iteration 29 : 0.7327234820167641
Loss in iteration 30 : 0.725298737471
Loss in iteration 31 : 0.7178922522891908
Loss in iteration 32 : 0.710502085835728
Loss in iteration 33 : 0.7031265143550329
Loss in iteration 34 : 0.6957640046600942
Loss in iteration 35 : 0.6884131914126537
Loss in iteration 36 : 0.6810728574297108
Loss in iteration 37 : 0.6737419165537174
Loss in iteration 38 : 0.666419398705294
Loss in iteration 39 : 0.6591044368024824
Loss in iteration 40 : 0.6517962552833798
Loss in iteration 41 : 0.6444941600112049
Loss in iteration 42 : 0.6371975293759754
Loss in iteration 43 : 0.6299058064353907
Loss in iteration 44 : 0.6226184919609343
Loss in iteration 45 : 0.6153351382749428
Loss in iteration 46 : 0.6080553437804935
Loss in iteration 47 : 0.6007787480997541
Loss in iteration 48 : 0.5935050277477841
Loss in iteration 49 : 0.5862338922785921
Loss in iteration 50 : 0.5789650808485042
Loss in iteration 51 : 0.5716983591487794
Loss in iteration 52 : 0.5644335166657085
Loss in iteration 53 : 0.5571703642312895
Loss in iteration 54 : 0.5499087318323018
Loss in iteration 55 : 0.5426484666493148
Loss in iteration 56 : 0.5353894313004569
Loss in iteration 57 : 0.5281315022679607
Loss in iteration 58 : 0.5208745684876998
Loss in iteration 59 : 0.5136185300843669
Loss in iteration 60 : 0.5063632972368758
Loss in iteration 61 : 0.4991087891602312
Loss in iteration 62 : 0.491854933191602
Loss in iteration 63 : 0.48460166396982257
Loss in iteration 64 : 0.48249537739668646
Loss in iteration 65 : 0.48129755221566883
Loss in iteration 66 : 0.4809966064884309
Loss in iteration 67 : 0.48071599764861817
Loss in iteration 68 : 0.4803260795005379
Loss in iteration 69 : 0.4800022438263351
Loss in iteration 70 : 0.47967496798164966
Loss in iteration 71 : 0.479345373335231
Loss in iteration 72 : 0.4790086029692249
Loss in iteration 73 : 0.4786619490537553
Loss in iteration 74 : 0.47830479146506355
Loss in iteration 75 : 0.47793689436375364
Loss in iteration 76 : 0.4775580162515893
Loss in iteration 77 : 0.4771678728864325
Loss in iteration 78 : 0.47676634511505844
Loss in iteration 79 : 0.4763531584379249
Loss in iteration 80 : 0.475928466579198
Loss in iteration 81 : 0.47549213567789517
Loss in iteration 82 : 0.47504428614292504
Loss in iteration 83 : 0.4745848917176474
Loss in iteration 84 : 0.47411437490193586
Loss in iteration 85 : 0.47363345136819496
Loss in iteration 86 : 0.4731448652278813
Loss in iteration 87 : 0.47264866123982585
Loss in iteration 88 : 0.47214457138327126
Loss in iteration 89 : 0.4716343113780017
Loss in iteration 90 : 0.47111833075613296
Loss in iteration 91 : 0.4705938662686417
Loss in iteration 92 : 0.4700614357120195
Loss in iteration 93 : 0.46952087872541454
Loss in iteration 94 : 0.4689708600436743
Loss in iteration 95 : 0.46841161232187206
Loss in iteration 96 : 0.46784313607368233
Loss in iteration 97 : 0.46726861806175457
Loss in iteration 98 : 0.46669666914469743
Loss in iteration 99 : 0.46613500715909556
Loss in iteration 100 : 0.46559392974471314
Loss in iteration 101 : 0.4650763563523088
Loss in iteration 102 : 0.4645750042871389
Loss in iteration 103 : 0.46409466410422495
Loss in iteration 104 : 0.4636310500478676
Loss in iteration 105 : 0.4631753858180498
Loss in iteration 106 : 0.4627239899703683
Loss in iteration 107 : 0.46227774745590344
Loss in iteration 108 : 0.4618353265921174
Loss in iteration 109 : 0.46140027737778927
Loss in iteration 110 : 0.46096778888145856
Loss in iteration 111 : 0.46053621928185706
Loss in iteration 112 : 0.4601039386496745
Loss in iteration 113 : 0.4596724862046577
Loss in iteration 114 : 0.4592394814029366
Loss in iteration 115 : 0.4588049343455567
Loss in iteration 116 : 0.4583699861795229
Loss in iteration 117 : 0.4579341471246307
Loss in iteration 118 : 0.4574964883506927
Loss in iteration 119 : 0.4570565850115298
Loss in iteration 120 : 0.4566151560682836
Loss in iteration 121 : 0.4561731724983008
Loss in iteration 122 : 0.4557304281498368
Loss in iteration 123 : 0.4552869787564309
Loss in iteration 124 : 0.4548415388686215
Loss in iteration 125 : 0.4543946113640239
Loss in iteration 126 : 0.4539463640393379
Loss in iteration 127 : 0.4534978199266636
Loss in iteration 128 : 0.4530484753175748
Loss in iteration 129 : 0.45259956106238486
Loss in iteration 130 : 0.45214936427040747
Loss in iteration 131 : 0.4516969936806574
Loss in iteration 132 : 0.45124246306005905
Loss in iteration 133 : 0.4507871041680549
Loss in iteration 134 : 0.45033005504797796
Loss in iteration 135 : 0.4498715510294838
Loss in iteration 136 : 0.44941202633825483
Loss in iteration 137 : 0.4489515630792047
Loss in iteration 138 : 0.4484897671010721
Loss in iteration 139 : 0.44802812258447733
Loss in iteration 140 : 0.44756675284328884
Loss in iteration 141 : 0.4471040235107145
Loss in iteration 142 : 0.44664048584106925
Loss in iteration 143 : 0.44617719449757853
Loss in iteration 144 : 0.4457148605003724
Loss in iteration 145 : 0.4452506342684903
Loss in iteration 146 : 0.4447859890861532
Loss in iteration 147 : 0.4443216906581267
Loss in iteration 148 : 0.4438591719340017
Loss in iteration 149 : 0.4433946780597312
Loss in iteration 150 : 0.44292987041274473
Loss in iteration 151 : 0.4424696437880546
Loss in iteration 152 : 0.44200234760260876
Loss in iteration 153 : 0.4415424693918678
Loss in iteration 154 : 0.44107511370110664
Loss in iteration 155 : 0.44061348239720716
Loss in iteration 156 : 0.4401482181597325
Loss in iteration 157 : 0.43968672169763956
Loss in iteration 158 : 0.43922350101442226
Loss in iteration 159 : 0.43876043118695646
Loss in iteration 160 : 0.4382977616205313
Loss in iteration 161 : 0.43783427312024203
Loss in iteration 162 : 0.4373709204081291
Loss in iteration 163 : 0.43690956651376656
Loss in iteration 164 : 0.43644924806712454
Loss in iteration 165 : 0.4359887335123932
Loss in iteration 166 : 0.4355283598078458
Loss in iteration 167 : 0.43507010932594614
Loss in iteration 168 : 0.43461057733954916
Loss in iteration 169 : 0.4341518520710989
Loss in iteration 170 : 0.43369201409951647
Loss in iteration 171 : 0.43323090969388833
Loss in iteration 172 : 0.4327712305094337
Loss in iteration 173 : 0.4323134497283568
Loss in iteration 174 : 0.4318550796188927
Loss in iteration 175 : 0.4313978926934732
Loss in iteration 176 : 0.43094148322881237
Loss in iteration 177 : 0.43048555443773445
Loss in iteration 178 : 0.430028981701223
Loss in iteration 179 : 0.4295728280334005
Loss in iteration 180 : 0.4291163496075931
Loss in iteration 181 : 0.4286605572952559
Loss in iteration 182 : 0.4282042553504437
Loss in iteration 183 : 0.4277492251685606
Loss in iteration 184 : 0.4272928272814745
Loss in iteration 185 : 0.42683846827066724
Loss in iteration 186 : 0.42638220196333304
Loss in iteration 187 : 0.4259279615649736
Loss in iteration 188 : 0.4254721999907827
Loss in iteration 189 : 0.42501768977850224
Loss in iteration 190 : 0.4245618343874765
Loss in iteration 191 : 0.4241080067065011
Loss in iteration 192 : 0.4236522057218675
Loss in iteration 193 : 0.42319949545011687
Loss in iteration 194 : 0.42274478356368506
Loss in iteration 195 : 0.4222925477843953
Loss in iteration 196 : 0.42183765066242956
Loss in iteration 197 : 0.42138421122494346
Loss in iteration 198 : 0.42092798089741223
Loss in iteration 199 : 0.42047501228345535
Loss in iteration 200 : 0.4200198288353255
Testing accuracy  of updater 5 on alg 1 with rate 9.999999999999992E-4 = 0.8000737055463424, training accuracy 0.7964987714987715, time elapsed: 3876 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.7991563988378765
Loss in iteration 3 : 4.911638239607042
Loss in iteration 4 : 3.4013901839673886
Loss in iteration 5 : 1.734611529625915
Loss in iteration 6 : 1.5537453716454859
Loss in iteration 7 : 2.6110811422002738
Loss in iteration 8 : 3.0652336935215776
Loss in iteration 9 : 2.6077221552666625
Loss in iteration 10 : 2.0046222317381988
Loss in iteration 11 : 1.8365235842977923
Loss in iteration 12 : 1.9993459007581105
Loss in iteration 13 : 2.244900258073459
Loss in iteration 14 : 2.3696130132177413
Loss in iteration 15 : 2.282645214869475
Loss in iteration 16 : 2.023192032084982
Loss in iteration 17 : 1.7410926146024723
Loss in iteration 18 : 1.6091483430189963
Loss in iteration 19 : 1.704889097717567
Loss in iteration 20 : 1.8546197259446093
Loss in iteration 21 : 1.8232311934420284
Loss in iteration 22 : 1.5994132485891817
Loss in iteration 23 : 1.3883950851586824
Loss in iteration 24 : 1.3979213616424437
Loss in iteration 25 : 1.5102243550872398
Loss in iteration 26 : 1.4744621146171433
Loss in iteration 27 : 1.2553693665831038
Loss in iteration 28 : 1.0985997558954914
Loss in iteration 29 : 1.1787719320895176
Loss in iteration 30 : 1.1938130897809005
Loss in iteration 31 : 0.988853798613895
Loss in iteration 32 : 0.9031014813472031
Loss in iteration 33 : 1.0019363040986342
Loss in iteration 34 : 0.933551732566694
Loss in iteration 35 : 0.7709772289839169
Loss in iteration 36 : 0.8702109236863327
Loss in iteration 37 : 0.7646895878454228
Loss in iteration 38 : 0.681018802831187
Loss in iteration 39 : 0.744748560169892
Loss in iteration 40 : 0.5997873061966291
Loss in iteration 41 : 0.6536518760582595
Loss in iteration 42 : 0.5859690463882884
Loss in iteration 43 : 0.541980778218604
Loss in iteration 44 : 0.5855919569820032
Loss in iteration 45 : 0.4751986344513698
Loss in iteration 46 : 0.5212596712569406
Loss in iteration 47 : 0.4653071908801597
Loss in iteration 48 : 0.44408352268402285
Loss in iteration 49 : 0.4974776839175836
Loss in iteration 50 : 0.4988899966853378
Loss in iteration 51 : 0.4669710143575869
Loss in iteration 52 : 0.44775104719522424
Loss in iteration 53 : 0.4558983055659572
Loss in iteration 54 : 0.4422255183807363
Loss in iteration 55 : 0.41376756178253504
Loss in iteration 56 : 0.41740854971297553
Loss in iteration 57 : 0.44518334813474775
Loss in iteration 58 : 0.48847749699431076
Loss in iteration 59 : 0.5295242460418109
Loss in iteration 60 : 0.4269444704919101
Loss in iteration 61 : 0.42861641919357235
Loss in iteration 62 : 0.4612892933260936
Loss in iteration 63 : 0.40615035167455105
Loss in iteration 64 : 0.4218061601316877
Loss in iteration 65 : 0.4589772167420396
Loss in iteration 66 : 0.40524263914781355
Loss in iteration 67 : 0.38320066836260985
Loss in iteration 68 : 0.4102461359528358
Loss in iteration 69 : 0.46606504703781937
Loss in iteration 70 : 0.5829685516581374
Loss in iteration 71 : 0.3966155496840654
Loss in iteration 72 : 0.440322916388745
Loss in iteration 73 : 0.5752092677303555
Loss in iteration 74 : 0.3850404905465878
Loss in iteration 75 : 0.5588334887478132
Loss in iteration 76 : 0.6418361180670854
Loss in iteration 77 : 0.4874089657402113
Loss in iteration 78 : 0.7360827908185609
Loss in iteration 79 : 0.42721626734906815
Loss in iteration 80 : 0.6124356274271531
Loss in iteration 81 : 0.44968578331047465
Loss in iteration 82 : 0.5974533920640889
Loss in iteration 83 : 0.43660509058624164
Loss in iteration 84 : 0.5609882808520683
Loss in iteration 85 : 0.41667243976941615
Loss in iteration 86 : 0.6059036474246019
Loss in iteration 87 : 0.46535786612240926
Loss in iteration 88 : 0.4478204796308891
Loss in iteration 89 : 0.5989133846680497
Loss in iteration 90 : 0.4472940549366001
Loss in iteration 91 : 0.4337535526261124
Loss in iteration 92 : 0.5337647329135811
Loss in iteration 93 : 0.3960690584052172
Loss in iteration 94 : 0.457775019688179
Loss in iteration 95 : 0.46245795775007653
Loss in iteration 96 : 0.378116519287342
Loss in iteration 97 : 0.3897394120223482
Loss in iteration 98 : 0.4724512569147778
Loss in iteration 99 : 0.5872755158651973
Loss in iteration 100 : 0.3686581415852928
Loss in iteration 101 : 0.5922673903029604
Loss in iteration 102 : 0.6224302445400705
Loss in iteration 103 : 0.5190026406169953
Loss in iteration 104 : 0.6137333310025677
Loss in iteration 105 : 0.47567925153031393
Loss in iteration 106 : 0.5652516931250181
Loss in iteration 107 : 0.5266967552617007
Loss in iteration 108 : 0.4989876011935472
Loss in iteration 109 : 0.49836998010843325
Loss in iteration 110 : 0.492600182381491
Loss in iteration 111 : 0.4598941754551438
Loss in iteration 112 : 0.5351668519730512
Loss in iteration 113 : 0.3941299700102796
Loss in iteration 114 : 0.4371354785687774
Loss in iteration 115 : 0.49851555672331754
Loss in iteration 116 : 0.44308604273994956
Loss in iteration 117 : 0.3704871469629917
Loss in iteration 118 : 0.4425423787481652
Loss in iteration 119 : 0.5053896842917173
Loss in iteration 120 : 0.3712431928504368
Loss in iteration 121 : 0.4229932699810669
Loss in iteration 122 : 0.5523558336375485
Loss in iteration 123 : 0.3698935541106167
Loss in iteration 124 : 0.5709126796728948
Loss in iteration 125 : 0.6382937896942249
Loss in iteration 126 : 0.5333525392984649
Loss in iteration 127 : 0.6198472884879404
Loss in iteration 128 : 0.46738934151867106
Loss in iteration 129 : 0.5682485860462677
Loss in iteration 130 : 0.5272965307132957
Loss in iteration 131 : 0.4957563567175424
Loss in iteration 132 : 0.511613024457298
Loss in iteration 133 : 0.4833446074399941
Loss in iteration 134 : 0.47666098053997136
Loss in iteration 135 : 0.49806743157895805
Loss in iteration 136 : 0.3824540319002788
Loss in iteration 137 : 0.47406185602917583
Loss in iteration 138 : 0.4242237191894993
Loss in iteration 139 : 0.36804322615707324
Loss in iteration 140 : 0.3703505387157933
Loss in iteration 141 : 0.45111969358040166
Loss in iteration 142 : 0.6415074243788085
Loss in iteration 143 : 0.36817606935910424
Loss in iteration 144 : 0.6172427254005302
Loss in iteration 145 : 0.6369552055228896
Loss in iteration 146 : 0.5840798889562928
Loss in iteration 147 : 0.5397130044574809
Loss in iteration 148 : 0.5852986407808757
Loss in iteration 149 : 0.5049899225586264
Loss in iteration 150 : 0.6049684144744057
Loss in iteration 151 : 0.45446761806183306
Loss in iteration 152 : 0.5860238609235501
Loss in iteration 153 : 0.4295842958870207
Loss in iteration 154 : 0.545412863525395
Loss in iteration 155 : 0.40952060915924693
Loss in iteration 156 : 0.6036014898302835
Loss in iteration 157 : 0.5212619300744645
Loss in iteration 158 : 0.44762103878968434
Loss in iteration 159 : 0.6230506888999481
Loss in iteration 160 : 0.4112888040355848
Loss in iteration 161 : 0.5266931846630027
Loss in iteration 162 : 0.41246917485730333
Loss in iteration 163 : 0.4627541244132257
Loss in iteration 164 : 0.4698385686752101
Loss in iteration 165 : 0.38765703863171647
Loss in iteration 166 : 0.5821744121240991
Loss in iteration 167 : 0.6659336667674894
Loss in iteration 168 : 0.5322693565567771
Loss in iteration 169 : 0.6745184138915931
Loss in iteration 170 : 0.46527020544480646
Loss in iteration 171 : 0.5875442370056728
Loss in iteration 172 : 0.5504646081540284
Loss in iteration 173 : 0.48669303348508536
Loss in iteration 174 : 0.5641635556562892
Loss in iteration 175 : 0.4494532570824885
Loss in iteration 176 : 0.5340089502548088
Loss in iteration 177 : 0.4152605714862903
Loss in iteration 178 : 0.4957424022964716
Loss in iteration 179 : 0.5235888917168822
Loss in iteration 180 : 0.3969355485722355
Loss in iteration 181 : 0.6606754480368833
Loss in iteration 182 : 0.5546503076856143
Loss in iteration 183 : 0.5137471007683094
Loss in iteration 184 : 0.5657434431374775
Loss in iteration 185 : 0.4620453423003902
Loss in iteration 186 : 0.5370337214078519
Loss in iteration 187 : 0.46876879059193377
Loss in iteration 188 : 0.5211269723651669
Loss in iteration 189 : 0.4129924557834581
Loss in iteration 190 : 0.5206558778861075
Loss in iteration 191 : 0.38853679939600844
Loss in iteration 192 : 0.5745544475421879
Loss in iteration 193 : 0.5631345367039658
Loss in iteration 194 : 0.4709166907692846
Loss in iteration 195 : 0.6493355005164145
Loss in iteration 196 : 0.4096593709512425
Loss in iteration 197 : 0.5600074992674985
Loss in iteration 198 : 0.41604053595988566
Loss in iteration 199 : 0.556273113456793
Loss in iteration 200 : 0.40322585325108135
Testing accuracy  of updater 6 on alg 1 with rate 2.0 = 0.8286346047540077, training accuracy 0.8257985257985258, time elapsed: 4457 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.6381922774476496
Loss in iteration 3 : 2.103112786838095
Loss in iteration 4 : 1.7469039065469139
Loss in iteration 5 : 1.0377822379890171
Loss in iteration 6 : 0.9005668640823964
Loss in iteration 7 : 1.3703354179496543
Loss in iteration 8 : 1.6434092409103354
Loss in iteration 9 : 1.4904782734018593
Loss in iteration 10 : 1.2109392996885213
Loss in iteration 11 : 1.1593552736985981
Loss in iteration 12 : 1.2787976179030425
Loss in iteration 13 : 1.3998379224236934
Loss in iteration 14 : 1.4174365714523673
Loss in iteration 15 : 1.3162047093758797
Loss in iteration 16 : 1.1615629024548424
Loss in iteration 17 : 1.063109086256862
Loss in iteration 18 : 1.0915266243293276
Loss in iteration 19 : 1.1657541129083435
Loss in iteration 20 : 1.1389903745874157
Loss in iteration 21 : 1.0072111304592226
Loss in iteration 22 : 0.9174140373216542
Loss in iteration 23 : 0.9534535441402041
Loss in iteration 24 : 0.9863895578639463
Loss in iteration 25 : 0.9069218566789558
Loss in iteration 26 : 0.785136144692104
Loss in iteration 27 : 0.8016113811006854
Loss in iteration 28 : 0.8193481211275713
Loss in iteration 29 : 0.6846430117604927
Loss in iteration 30 : 0.6598505057825538
Loss in iteration 31 : 0.6893304229679166
Loss in iteration 32 : 0.5963984584179334
Loss in iteration 33 : 0.5687721953568821
Loss in iteration 34 : 0.599554106666197
Loss in iteration 35 : 0.5014630602067625
Loss in iteration 36 : 0.557773124600128
Loss in iteration 37 : 0.4660862326934633
Loss in iteration 38 : 0.5322679648806881
Loss in iteration 39 : 0.44618030418491006
Loss in iteration 40 : 0.5162485273113151
Loss in iteration 41 : 0.46683924032736196
Loss in iteration 42 : 0.43955516250239035
Loss in iteration 43 : 0.46397232832914304
Loss in iteration 44 : 0.42718292773484084
Loss in iteration 45 : 0.45905512556100153
Loss in iteration 46 : 0.42981543789305043
Loss in iteration 47 : 0.44052827981423637
Loss in iteration 48 : 0.42059743684597545
Loss in iteration 49 : 0.4231461023087971
Loss in iteration 50 : 0.41008112177130873
Loss in iteration 51 : 0.40599188172363415
Loss in iteration 52 : 0.4126854411568603
Loss in iteration 53 : 0.39416096913865545
Loss in iteration 54 : 0.4015420618069122
Loss in iteration 55 : 0.40278915933278975
Loss in iteration 56 : 0.3883751621210798
Loss in iteration 57 : 0.3943240915048189
Loss in iteration 58 : 0.39191314614633616
Loss in iteration 59 : 0.3758521407630879
Loss in iteration 60 : 0.37473193327401555
Loss in iteration 61 : 0.38006655187630134
Loss in iteration 62 : 0.37512167695854826
Loss in iteration 63 : 0.36837570727216
Loss in iteration 64 : 0.3760094127306549
Loss in iteration 65 : 0.37749774164833894
Loss in iteration 66 : 0.3649323782527771
Loss in iteration 67 : 0.3692594990149827
Loss in iteration 68 : 0.3776889700783217
Loss in iteration 69 : 0.3672904425229882
Loss in iteration 70 : 0.36092827670529565
Loss in iteration 71 : 0.3657765552356322
Loss in iteration 72 : 0.37223716150060554
Loss in iteration 73 : 0.36994468529303276
Loss in iteration 74 : 0.35787542770364006
Loss in iteration 75 : 0.3564108422196782
Loss in iteration 76 : 0.36219654905118165
Loss in iteration 77 : 0.36180150189223786
Loss in iteration 78 : 0.3563469165088698
Loss in iteration 79 : 0.35358375100640027
Loss in iteration 80 : 0.35548604636761205
Loss in iteration 81 : 0.3589956968350004
Loss in iteration 82 : 0.3592266569466428
Loss in iteration 83 : 0.3569512785672939
Loss in iteration 84 : 0.35445616905704175
Loss in iteration 85 : 0.3534430885653848
Loss in iteration 86 : 0.3537778853147373
Loss in iteration 87 : 0.35557979121079497
Loss in iteration 88 : 0.3583111581116931
Loss in iteration 89 : 0.3597441229436096
Loss in iteration 90 : 0.3593940110942797
Loss in iteration 91 : 0.3553038498053474
Loss in iteration 92 : 0.3524378638573153
Loss in iteration 93 : 0.35230370366738445
Loss in iteration 94 : 0.35442157765930976
Loss in iteration 95 : 0.3568126316917261
Loss in iteration 96 : 0.3562085495607755
Loss in iteration 97 : 0.35670217271561866
Loss in iteration 98 : 0.35554513213714045
Loss in iteration 99 : 0.35437321778500086
Loss in iteration 100 : 0.35306486215453703
Loss in iteration 101 : 0.3523699659209495
Loss in iteration 102 : 0.35227377125539683
Loss in iteration 103 : 0.3528883474052922
Loss in iteration 104 : 0.35370892927555114
Loss in iteration 105 : 0.35645768544393036
Loss in iteration 106 : 0.36010939308717593
Loss in iteration 107 : 0.3658266764855075
Loss in iteration 108 : 0.36171566021426466
Loss in iteration 109 : 0.35567469224986903
Loss in iteration 110 : 0.3524280920921713
Loss in iteration 111 : 0.3564897440304026
Loss in iteration 112 : 0.3613141680090508
Loss in iteration 113 : 0.3600349758542993
Loss in iteration 114 : 0.3571551330990458
Loss in iteration 115 : 0.3536182789708475
Loss in iteration 116 : 0.3516783479487648
Loss in iteration 117 : 0.35122768675985777
Loss in iteration 118 : 0.3519737554927024
Loss in iteration 119 : 0.3547701784554191
Loss in iteration 120 : 0.3616294953824824
Loss in iteration 121 : 0.3782328636388749
Loss in iteration 122 : 0.37186880016747714
Loss in iteration 123 : 0.36488905123317006
Loss in iteration 124 : 0.35426717302182553
Loss in iteration 125 : 0.3577574828959179
Loss in iteration 126 : 0.3656542143722008
Loss in iteration 127 : 0.3611682856471941
Loss in iteration 128 : 0.3539989951338812
Loss in iteration 129 : 0.3520560569563039
Loss in iteration 130 : 0.3552802996999587
Loss in iteration 131 : 0.3651035987054501
Loss in iteration 132 : 0.3772491173233683
Loss in iteration 133 : 0.3957716187162236
Loss in iteration 134 : 0.36756926248422045
Loss in iteration 135 : 0.3544279468327314
Loss in iteration 136 : 0.36371885290142697
Loss in iteration 137 : 0.3695941305805575
Loss in iteration 138 : 0.3610922611470512
Loss in iteration 139 : 0.3530051860880698
Loss in iteration 140 : 0.3572598103647915
Loss in iteration 141 : 0.368239249160486
Loss in iteration 142 : 0.3742085237727716
Loss in iteration 143 : 0.3849834270557143
Loss in iteration 144 : 0.36758844544179703
Loss in iteration 145 : 0.35532655482238895
Loss in iteration 146 : 0.35536621568807547
Loss in iteration 147 : 0.36407222097738057
Loss in iteration 148 : 0.3674628362117987
Loss in iteration 149 : 0.3614619115686492
Loss in iteration 150 : 0.3553209785478568
Loss in iteration 151 : 0.35209847852689014
Loss in iteration 152 : 0.35225437422637024
Loss in iteration 153 : 0.35696176063914503
Loss in iteration 154 : 0.3678228974717564
Loss in iteration 155 : 0.39072436752482637
Loss in iteration 156 : 0.3782048912886505
Loss in iteration 157 : 0.36673134838734495
Loss in iteration 158 : 0.3551200349129977
Loss in iteration 159 : 0.35761285400771475
Loss in iteration 160 : 0.36695692038370537
Loss in iteration 161 : 0.3646350361406877
Loss in iteration 162 : 0.3574979354595467
Loss in iteration 163 : 0.3521035222906303
Loss in iteration 164 : 0.35233312084488855
Loss in iteration 165 : 0.3578761446231948
Loss in iteration 166 : 0.3753305579597712
Loss in iteration 167 : 0.41358645009232975
Loss in iteration 168 : 0.38370227186804784
Loss in iteration 169 : 0.361985373999221
Loss in iteration 170 : 0.3568605189573841
Loss in iteration 171 : 0.3670904322498574
Loss in iteration 172 : 0.36807379186658123
Loss in iteration 173 : 0.3574108439419755
Loss in iteration 174 : 0.35418192543204174
Loss in iteration 175 : 0.36313263459513284
Loss in iteration 176 : 0.38209693317304577
Loss in iteration 177 : 0.42463682186156376
Loss in iteration 178 : 0.3781826176300856
Loss in iteration 179 : 0.35654637602710754
Loss in iteration 180 : 0.36515194240369636
Loss in iteration 181 : 0.37580188847843726
Loss in iteration 182 : 0.3694476111547
Loss in iteration 183 : 0.35705949648674745
Loss in iteration 184 : 0.3544118095188362
Loss in iteration 185 : 0.3624994586206932
Loss in iteration 186 : 0.3752431183536608
Loss in iteration 187 : 0.40230044302596796
Loss in iteration 188 : 0.38079563633669017
Loss in iteration 189 : 0.36454485411259513
Loss in iteration 190 : 0.3551655828180643
Loss in iteration 191 : 0.36449127287027067
Loss in iteration 192 : 0.3786436344132863
Loss in iteration 193 : 0.37034972223447066
Loss in iteration 194 : 0.36067817248116385
Loss in iteration 195 : 0.35328729107101975
Loss in iteration 196 : 0.3530374127340964
Loss in iteration 197 : 0.3599762561313331
Loss in iteration 198 : 0.37498763549012265
Loss in iteration 199 : 0.4073305730517402
Loss in iteration 200 : 0.3865970184439793
Testing accuracy  of updater 6 on alg 1 with rate 1.4000000000000001 = 0.845095510103802, training accuracy 0.8449324324324324, time elapsed: 3479 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.217298697592717
Loss in iteration 3 : 1.5269497203971838
Loss in iteration 4 : 1.255117120378111
Loss in iteration 5 : 0.7812736192537464
Loss in iteration 6 : 0.7384430135173633
Loss in iteration 7 : 1.0579223242213032
Loss in iteration 8 : 1.2239876741797275
Loss in iteration 9 : 1.1170046113826004
Loss in iteration 10 : 0.9435802589691301
Loss in iteration 11 : 0.9132016709820383
Loss in iteration 12 : 0.9907961999720016
Loss in iteration 13 : 1.0693972430542145
Loss in iteration 14 : 1.0807986715205333
Loss in iteration 15 : 1.0145461633685464
Loss in iteration 16 : 0.9126566118485108
Loss in iteration 17 : 0.8429363027403951
Loss in iteration 18 : 0.8503506050662272
Loss in iteration 19 : 0.8960708852888716
Loss in iteration 20 : 0.8862996973390921
Loss in iteration 21 : 0.8063516213483439
Loss in iteration 22 : 0.734586641829173
Loss in iteration 23 : 0.7452787446713278
Loss in iteration 24 : 0.7769874036222579
Loss in iteration 25 : 0.7434130484406615
Loss in iteration 26 : 0.6580174151610262
Loss in iteration 27 : 0.6351910619281044
Loss in iteration 28 : 0.6683528358269233
Loss in iteration 29 : 0.6133215930462286
Loss in iteration 30 : 0.5423939510382249
Loss in iteration 31 : 0.5665601181887165
Loss in iteration 32 : 0.5497989477744095
Loss in iteration 33 : 0.4825960779161063
Loss in iteration 34 : 0.5053949604567191
Loss in iteration 35 : 0.4918154776666455
Loss in iteration 36 : 0.4435211262978097
Loss in iteration 37 : 0.48273992417796957
Loss in iteration 38 : 0.42159275812481023
Loss in iteration 39 : 0.45587650423562476
Loss in iteration 40 : 0.41845453302555224
Loss in iteration 41 : 0.4455282299286127
Loss in iteration 42 : 0.408277528804582
Loss in iteration 43 : 0.452476170714194
Loss in iteration 44 : 0.3974497321163859
Loss in iteration 45 : 0.426632959833215
Loss in iteration 46 : 0.39615293733208884
Loss in iteration 47 : 0.42123188700944897
Loss in iteration 48 : 0.39883847685652185
Loss in iteration 49 : 0.4095464236340627
Loss in iteration 50 : 0.39824690915708866
Loss in iteration 51 : 0.3930028744090695
Loss in iteration 52 : 0.392140811100168
Loss in iteration 53 : 0.38287724238068793
Loss in iteration 54 : 0.3862610603642266
Loss in iteration 55 : 0.3768729279590829
Loss in iteration 56 : 0.38065255169620543
Loss in iteration 57 : 0.37420736100782537
Loss in iteration 58 : 0.37483910168261275
Loss in iteration 59 : 0.3739132441341099
Loss in iteration 60 : 0.3694217542517491
Loss in iteration 61 : 0.3714711398162944
Loss in iteration 62 : 0.36335257525468945
Loss in iteration 63 : 0.36787346539752186
Loss in iteration 64 : 0.3611330420925837
Loss in iteration 65 : 0.3616107845679398
Loss in iteration 66 : 0.36154017291086366
Loss in iteration 67 : 0.3585201142879841
Loss in iteration 68 : 0.361734323438258
Loss in iteration 69 : 0.3574682871375361
Loss in iteration 70 : 0.3602006271030259
Loss in iteration 71 : 0.35790016124622864
Loss in iteration 72 : 0.3570714917844244
Loss in iteration 73 : 0.3577762905182018
Loss in iteration 74 : 0.354921400146049
Loss in iteration 75 : 0.35584832202939604
Loss in iteration 76 : 0.3545472242807037
Loss in iteration 77 : 0.35346972711131447
Loss in iteration 78 : 0.35391941301724283
Loss in iteration 79 : 0.35301552552514404
Loss in iteration 80 : 0.3521601970173478
Loss in iteration 81 : 0.35224350502277857
Loss in iteration 82 : 0.3521321663426215
Loss in iteration 83 : 0.3516466884246364
Loss in iteration 84 : 0.35166551103882915
Loss in iteration 85 : 0.3517378346722418
Loss in iteration 86 : 0.35160776334421795
Loss in iteration 87 : 0.3516059826765309
Loss in iteration 88 : 0.3518532122480593
Loss in iteration 89 : 0.35186912372691354
Loss in iteration 90 : 0.35177775016814755
Loss in iteration 91 : 0.3517476500007249
Loss in iteration 92 : 0.3516942255622328
Loss in iteration 93 : 0.3516152930853111
Loss in iteration 94 : 0.351492812777796
Loss in iteration 95 : 0.3513605554805697
Loss in iteration 96 : 0.35126020714174067
Loss in iteration 97 : 0.35120788896127325
Loss in iteration 98 : 0.35114195635388923
Loss in iteration 99 : 0.3510691555805649
Loss in iteration 100 : 0.3510576210145098
Loss in iteration 101 : 0.35102798947707176
Loss in iteration 102 : 0.351032592719161
Loss in iteration 103 : 0.351050187757598
Loss in iteration 104 : 0.35105178392396263
Loss in iteration 105 : 0.35110229354328154
Loss in iteration 106 : 0.351417639598695
Loss in iteration 107 : 0.35148478479265327
Loss in iteration 108 : 0.351177836957281
Loss in iteration 109 : 0.351090653622663
Loss in iteration 110 : 0.351275675624448
Loss in iteration 111 : 0.35127964440415865
Loss in iteration 112 : 0.3510239172148771
Loss in iteration 113 : 0.3510803622860129
Loss in iteration 114 : 0.35158861018296544
Loss in iteration 115 : 0.35116421336641956
Loss in iteration 116 : 0.3509823412024323
Loss in iteration 117 : 0.35105626993200145
Loss in iteration 118 : 0.35120965755965555
Loss in iteration 119 : 0.35099969565018196
Loss in iteration 120 : 0.35090989812524653
Loss in iteration 121 : 0.35101364041646915
Loss in iteration 122 : 0.35103006168883144
Loss in iteration 123 : 0.3509827519784442
Loss in iteration 124 : 0.35103155476957193
Loss in iteration 125 : 0.351212913485956
Loss in iteration 126 : 0.3511864940643812
Loss in iteration 127 : 0.35096395876709263
Loss in iteration 128 : 0.35115972298481507
Loss in iteration 129 : 0.3517271968479289
Loss in iteration 130 : 0.35127034278843355
Loss in iteration 131 : 0.35094118904613714
Loss in iteration 132 : 0.35116556301023527
Loss in iteration 133 : 0.3510870110915326
Loss in iteration 134 : 0.35094891192155425
Loss in iteration 135 : 0.3509021278699405
Loss in iteration 136 : 0.35112828198118046
Loss in iteration 137 : 0.3512203895480147
Loss in iteration 138 : 0.3513701736716763
Loss in iteration 139 : 0.3511788249593289
Loss in iteration 140 : 0.35091532156132466
Loss in iteration 141 : 0.35111073416640676
Loss in iteration 142 : 0.3515002672328099
Loss in iteration 143 : 0.3511833935445878
Loss in iteration 144 : 0.3509839097611187
Loss in iteration 145 : 0.3509776826830775
Loss in iteration 146 : 0.3510966931152633
Loss in iteration 147 : 0.3512257392643452
Loss in iteration 148 : 0.3511037015333927
Loss in iteration 149 : 0.3509552810154519
Loss in iteration 150 : 0.3508984857724163
Loss in iteration 151 : 0.3510468278582958
Loss in iteration 152 : 0.3513838284767417
Loss in iteration 153 : 0.3514987069887397
Loss in iteration 154 : 0.3510912043397153
Loss in iteration 155 : 0.3509314928223167
Loss in iteration 156 : 0.35134816930338025
Loss in iteration 157 : 0.3513244946591862
Loss in iteration 158 : 0.35093620992446445
Loss in iteration 159 : 0.35093161643630993
Loss in iteration 160 : 0.3510373581086637
Loss in iteration 161 : 0.3509361454193293
Loss in iteration 162 : 0.3508866805415029
Loss in iteration 163 : 0.3509219434060714
Loss in iteration 164 : 0.35105224358473863
Loss in iteration 165 : 0.3513258236919027
Loss in iteration 166 : 0.35143417506388697
Loss in iteration 167 : 0.3511142964689656
Loss in iteration 168 : 0.3509258568046788
Loss in iteration 169 : 0.3509932599045232
Loss in iteration 170 : 0.3511492732466916
Loss in iteration 171 : 0.3510629358273971
Loss in iteration 172 : 0.350953546393934
Loss in iteration 173 : 0.3509166183938566
Loss in iteration 174 : 0.3509453189820917
Loss in iteration 175 : 0.35109255584378923
Loss in iteration 176 : 0.3511728271201911
Loss in iteration 177 : 0.3512807896406197
Loss in iteration 178 : 0.35107992048229647
Loss in iteration 179 : 0.3508889551329467
Loss in iteration 180 : 0.3509863062831154
Loss in iteration 181 : 0.35141703812214314
Loss in iteration 182 : 0.3516076812368214
Loss in iteration 183 : 0.3511146551702719
Loss in iteration 184 : 0.35100276858951057
Loss in iteration 185 : 0.3513781900953824
Loss in iteration 186 : 0.3513819341020887
Loss in iteration 187 : 0.3511350262842338
Loss in iteration 188 : 0.3509503371648292
Loss in iteration 189 : 0.35096855751212835
Loss in iteration 190 : 0.35103650026043715
Loss in iteration 191 : 0.35101458623056225
Loss in iteration 192 : 0.35112885999857907
Loss in iteration 193 : 0.3511882651563437
Loss in iteration 194 : 0.3511617251802741
Loss in iteration 195 : 0.3511497283755985
Loss in iteration 196 : 0.35091443579736314
Loss in iteration 197 : 0.35109603705897213
Loss in iteration 198 : 0.35177140395919143
Loss in iteration 199 : 0.351680774195534
Loss in iteration 200 : 0.35118228888590924
Testing accuracy  of updater 6 on alg 1 with rate 0.8 = 0.8503777409250046, training accuracy 0.8494471744471744, time elapsed: 3220 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5014152885641225
Loss in iteration 3 : 0.5837341047116551
Loss in iteration 4 : 0.5144028231610072
Loss in iteration 5 : 0.39243485335623435
Loss in iteration 6 : 0.4082898629536303
Loss in iteration 7 : 0.475982795382146
Loss in iteration 8 : 0.48302469354748717
Loss in iteration 9 : 0.44141385217459744
Loss in iteration 10 : 0.41318554792586754
Loss in iteration 11 : 0.425190715818893
Loss in iteration 12 : 0.45033878674721173
Loss in iteration 13 : 0.46056441716389357
Loss in iteration 14 : 0.44912321098036784
Loss in iteration 15 : 0.4271982380715979
Loss in iteration 16 : 0.4145573304480765
Loss in iteration 17 : 0.4197020818911818
Loss in iteration 18 : 0.4310807109223323
Loss in iteration 19 : 0.4313735096224746
Loss in iteration 20 : 0.4184530642192516
Loss in iteration 21 : 0.40525745724288154
Loss in iteration 22 : 0.4042241954228238
Loss in iteration 23 : 0.4101160793603218
Loss in iteration 24 : 0.4109874929730606
Loss in iteration 25 : 0.4009429264478992
Loss in iteration 26 : 0.38965032047204656
Loss in iteration 27 : 0.38691487963501037
Loss in iteration 28 : 0.38972369875336577
Loss in iteration 29 : 0.3872031446219893
Loss in iteration 30 : 0.3773983649776726
Loss in iteration 31 : 0.37085437146194394
Loss in iteration 32 : 0.3721238462888102
Loss in iteration 33 : 0.37341678994742405
Loss in iteration 34 : 0.3680039780372386
Loss in iteration 35 : 0.3631472423802842
Loss in iteration 36 : 0.364604219541854
Loss in iteration 37 : 0.3661207962514649
Loss in iteration 38 : 0.361897023959128
Loss in iteration 39 : 0.35940065046587827
Loss in iteration 40 : 0.36156831359085584
Loss in iteration 41 : 0.36108189909981075
Loss in iteration 42 : 0.35759716287948445
Loss in iteration 43 : 0.35803686953726876
Loss in iteration 44 : 0.35932185172980147
Loss in iteration 45 : 0.3569456696910905
Loss in iteration 46 : 0.3562806646857402
Loss in iteration 47 : 0.35765180639077054
Loss in iteration 48 : 0.3564644435280079
Loss in iteration 49 : 0.3550314004485241
Loss in iteration 50 : 0.3557949672732462
Loss in iteration 51 : 0.35544404643437144
Loss in iteration 52 : 0.354030783611882
Loss in iteration 53 : 0.3543237297098493
Loss in iteration 54 : 0.3545662400470146
Loss in iteration 55 : 0.35372485414695276
Loss in iteration 56 : 0.35350340504642364
Loss in iteration 57 : 0.35386692436808487
Loss in iteration 58 : 0.3534323686670078
Loss in iteration 59 : 0.3528950542989562
Loss in iteration 60 : 0.3530076963678909
Loss in iteration 61 : 0.3528807156013831
Loss in iteration 62 : 0.3524201668811141
Loss in iteration 63 : 0.35222459896427416
Loss in iteration 64 : 0.3523160480118451
Loss in iteration 65 : 0.35203909329629945
Loss in iteration 66 : 0.35180347993330785
Loss in iteration 67 : 0.35188419762102924
Loss in iteration 68 : 0.3517581353863208
Loss in iteration 69 : 0.35156395226667275
Loss in iteration 70 : 0.35159576880327004
Loss in iteration 71 : 0.35151699191846975
Loss in iteration 72 : 0.3513964866415451
Loss in iteration 73 : 0.35143512118890124
Loss in iteration 74 : 0.3513912176401155
Loss in iteration 75 : 0.3513013934693398
Loss in iteration 76 : 0.35132110641907166
Loss in iteration 77 : 0.3512736003458107
Loss in iteration 78 : 0.3512258551602553
Loss in iteration 79 : 0.35123300337593494
Loss in iteration 80 : 0.35118147033878916
Loss in iteration 81 : 0.35117089576960114
Loss in iteration 82 : 0.35115950874571894
Loss in iteration 83 : 0.35113556088936526
Loss in iteration 84 : 0.3511527852720306
Loss in iteration 85 : 0.3511381708815581
Loss in iteration 86 : 0.35112004393949997
Loss in iteration 87 : 0.351121630302143
Loss in iteration 88 : 0.35109093158585253
Loss in iteration 89 : 0.3510979925800848
Loss in iteration 90 : 0.35108413388749665
Loss in iteration 91 : 0.3510769456050721
Loss in iteration 92 : 0.35107503605699897
Loss in iteration 93 : 0.351061981250359
Loss in iteration 94 : 0.35106985200317997
Loss in iteration 95 : 0.3510553307122284
Loss in iteration 96 : 0.3510637582111363
Loss in iteration 97 : 0.3510433630304977
Loss in iteration 98 : 0.3510609987493985
Loss in iteration 99 : 0.3510344295672433
Loss in iteration 100 : 0.35105683766222856
Loss in iteration 101 : 0.3510240464533418
Loss in iteration 102 : 0.3510348491457368
Loss in iteration 103 : 0.35101716095206503
Loss in iteration 104 : 0.3510214116701554
Loss in iteration 105 : 0.3510104075735187
Loss in iteration 106 : 0.3510091971746989
Loss in iteration 107 : 0.35100025081189895
Loss in iteration 108 : 0.3509985799733566
Loss in iteration 109 : 0.35099102155978185
Loss in iteration 110 : 0.3509886453230627
Loss in iteration 111 : 0.350985064620317
Loss in iteration 112 : 0.350980025964159
Loss in iteration 113 : 0.3509790800325301
Loss in iteration 114 : 0.35097828784494484
Loss in iteration 115 : 0.35098315981127354
Loss in iteration 116 : 0.35097070062178737
Loss in iteration 117 : 0.3509806806287749
Loss in iteration 118 : 0.3509679248883303
Loss in iteration 119 : 0.3509702651392502
Loss in iteration 120 : 0.3509638723769485
Loss in iteration 121 : 0.35096546234175463
Loss in iteration 122 : 0.3509648258058468
Loss in iteration 123 : 0.3509604179843809
Loss in iteration 124 : 0.3509617716598904
Loss in iteration 125 : 0.35095664143135286
Loss in iteration 126 : 0.3509579537791302
Loss in iteration 127 : 0.35095184466771073
Loss in iteration 128 : 0.35095993315454566
Loss in iteration 129 : 0.3509573932490236
Loss in iteration 130 : 0.35094929467621705
Loss in iteration 131 : 0.3509518110691122
Loss in iteration 132 : 0.35094536265132376
Loss in iteration 133 : 0.35094847918861466
Loss in iteration 134 : 0.35094279210020074
Loss in iteration 135 : 0.35094383435621934
Loss in iteration 136 : 0.35094514383515274
Loss in iteration 137 : 0.3509400976892242
Loss in iteration 138 : 0.35094387963135243
Loss in iteration 139 : 0.35093973331468337
Loss in iteration 140 : 0.350944015153871
Loss in iteration 141 : 0.3509449016054132
Loss in iteration 142 : 0.35093938866614743
Loss in iteration 143 : 0.35093505881631465
Loss in iteration 144 : 0.350937183354418
Loss in iteration 145 : 0.35093402433491055
Loss in iteration 146 : 0.3509399198371385
Loss in iteration 147 : 0.3509318502078738
Loss in iteration 148 : 0.35094039651404313
Loss in iteration 149 : 0.350937409129429
Loss in iteration 150 : 0.35093049172155444
Loss in iteration 151 : 0.3509306421054895
Loss in iteration 152 : 0.350927857257888
Loss in iteration 153 : 0.3509296778039013
Loss in iteration 154 : 0.35092624579971815
Loss in iteration 155 : 0.3509357333525958
Loss in iteration 156 : 0.3509333136299278
Loss in iteration 157 : 0.35092620821449294
Loss in iteration 158 : 0.3509324984398013
Loss in iteration 159 : 0.350927176982839
Loss in iteration 160 : 0.350926318748746
Loss in iteration 161 : 0.35092427741539817
Loss in iteration 162 : 0.3509228354812083
Loss in iteration 163 : 0.3509201585219462
Loss in iteration 164 : 0.35092144533273034
Loss in iteration 165 : 0.35091887927044935
Loss in iteration 166 : 0.3509163662249831
Loss in iteration 167 : 0.35091749662371785
Loss in iteration 168 : 0.3509158010984712
Loss in iteration 169 : 0.3509157805788038
Loss in iteration 170 : 0.35091464987913923
Loss in iteration 171 : 0.350913286072037
Loss in iteration 172 : 0.35091769547570345
Loss in iteration 173 : 0.350911237872772
Loss in iteration 174 : 0.3509135286830416
Loss in iteration 175 : 0.3509135272536104
Loss in iteration 176 : 0.35091053177172604
Loss in iteration 177 : 0.35091182274714855
Loss in iteration 178 : 0.35090838546107705
Loss in iteration 179 : 0.3509077809479592
Loss in iteration 180 : 0.35091280111024287
Loss in iteration 181 : 0.3509246664713035
Loss in iteration 182 : 0.3509040087003285
Loss in iteration 183 : 0.3509312024313989
Loss in iteration 184 : 0.35091980452977517
Loss in iteration 185 : 0.3509189821452674
Loss in iteration 186 : 0.3509124583973286
Loss in iteration 187 : 0.3509122011114361
Loss in iteration 188 : 0.3509101040284438
Loss in iteration 189 : 0.35091548025434693
Loss in iteration 190 : 0.3509058210768789
Loss in iteration 191 : 0.3509223765789352
Loss in iteration 192 : 0.3509038090115502
Loss in iteration 193 : 0.35091214642445256
Loss in iteration 194 : 0.3509045604093272
Loss in iteration 195 : 0.35091245100513874
Loss in iteration 196 : 0.35090199291037677
Loss in iteration 197 : 0.35091246370179147
Loss in iteration 198 : 0.35089895058674103
Loss in iteration 199 : 0.35091171438776286
Loss in iteration 200 : 0.3508964897723746
Testing accuracy  of updater 6 on alg 1 with rate 0.2 = 0.8497021067501996, training accuracy 0.8501228501228502, time elapsed: 4662 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.46357935514241366
Loss in iteration 3 : 0.5721248547379018
Loss in iteration 4 : 0.5583047052012342
Loss in iteration 5 : 0.44913749666641434
Loss in iteration 6 : 0.3722985907099157
Loss in iteration 7 : 0.42256710216994336
Loss in iteration 8 : 0.46984316634913176
Loss in iteration 9 : 0.4528987784822965
Loss in iteration 10 : 0.4096731841235527
Loss in iteration 11 : 0.39273460686997014
Loss in iteration 12 : 0.40949386049346037
Loss in iteration 13 : 0.4317809477850746
Loss in iteration 14 : 0.4381102399092186
Loss in iteration 15 : 0.42561514635250425
Loss in iteration 16 : 0.4060784958867113
Loss in iteration 17 : 0.39616049517857554
Loss in iteration 18 : 0.40216284039632116
Loss in iteration 19 : 0.4137578078121304
Loss in iteration 20 : 0.41589227128523604
Loss in iteration 21 : 0.4057315185934305
Loss in iteration 22 : 0.39299192556458873
Loss in iteration 23 : 0.38907233064106467
Loss in iteration 24 : 0.3932449941333021
Loss in iteration 25 : 0.39731328991078685
Loss in iteration 26 : 0.3930584966959381
Loss in iteration 27 : 0.38318032720517775
Loss in iteration 28 : 0.37643673752328854
Loss in iteration 29 : 0.3768060838594995
Loss in iteration 30 : 0.3792506351548124
Loss in iteration 31 : 0.37683215909358014
Loss in iteration 32 : 0.3696565255166667
Loss in iteration 33 : 0.3647701180715918
Loss in iteration 34 : 0.36551262538326984
Loss in iteration 35 : 0.3672849651884953
Loss in iteration 36 : 0.36475972417704744
Loss in iteration 37 : 0.36019279431901063
Loss in iteration 38 : 0.358916857191605
Loss in iteration 39 : 0.36107887988751663
Loss in iteration 40 : 0.36085473603035784
Loss in iteration 41 : 0.35752469968114947
Loss in iteration 42 : 0.3563753850124966
Loss in iteration 43 : 0.35813257576519125
Loss in iteration 44 : 0.35810617126481564
Loss in iteration 45 : 0.3558217011384956
Loss in iteration 46 : 0.35496655001758254
Loss in iteration 47 : 0.35612303646272697
Loss in iteration 48 : 0.355798829453139
Loss in iteration 49 : 0.35419467260090237
Loss in iteration 50 : 0.3541958510807302
Loss in iteration 51 : 0.35490397742045404
Loss in iteration 52 : 0.3542866591682995
Loss in iteration 53 : 0.3533240443642472
Loss in iteration 54 : 0.35349517096022753
Loss in iteration 55 : 0.35381872504391126
Loss in iteration 56 : 0.35315605967274183
Loss in iteration 57 : 0.3526891080391773
Loss in iteration 58 : 0.3530315262356616
Loss in iteration 59 : 0.35302066468098087
Loss in iteration 60 : 0.3525120561489527
Loss in iteration 61 : 0.35243546428704436
Loss in iteration 62 : 0.3526700066538331
Loss in iteration 63 : 0.35248079154829115
Loss in iteration 64 : 0.3521338053783617
Loss in iteration 65 : 0.3521624364443943
Loss in iteration 66 : 0.35221210447166124
Loss in iteration 67 : 0.35203511229459594
Loss in iteration 68 : 0.35182001245414457
Loss in iteration 69 : 0.35178215068150004
Loss in iteration 70 : 0.35180586959423465
Loss in iteration 71 : 0.35162842180133236
Loss in iteration 72 : 0.35152781054161736
Loss in iteration 73 : 0.35157632547365425
Loss in iteration 74 : 0.3515400784566028
Loss in iteration 75 : 0.35140124556081415
Loss in iteration 76 : 0.3513991862805845
Loss in iteration 77 : 0.3513952026129488
Loss in iteration 78 : 0.351300569290222
Loss in iteration 79 : 0.35129483356004554
Loss in iteration 80 : 0.3513209055499948
Loss in iteration 81 : 0.3512736931686678
Loss in iteration 82 : 0.3512260739726598
Loss in iteration 83 : 0.35124022858333187
Loss in iteration 84 : 0.3512239135841811
Loss in iteration 85 : 0.35118140237538237
Loss in iteration 86 : 0.35119751789978526
Loss in iteration 87 : 0.35118542071219777
Loss in iteration 88 : 0.3511480641483162
Loss in iteration 89 : 0.3511534815717962
Loss in iteration 90 : 0.3511528778119164
Loss in iteration 91 : 0.3511329479839816
Loss in iteration 92 : 0.3511431653634361
Loss in iteration 93 : 0.3511367038243431
Loss in iteration 94 : 0.3511142913980053
Loss in iteration 95 : 0.35111625096971305
Loss in iteration 96 : 0.35110925799488596
Loss in iteration 97 : 0.35109384217192735
Loss in iteration 98 : 0.3511000311415767
Loss in iteration 99 : 0.3510913148880016
Loss in iteration 100 : 0.35108081909897704
Loss in iteration 101 : 0.35108474355320546
Loss in iteration 102 : 0.35107421292689034
Loss in iteration 103 : 0.3510687145217007
Loss in iteration 104 : 0.35107080728589507
Loss in iteration 105 : 0.35105792676372155
Loss in iteration 106 : 0.3510575214222544
Loss in iteration 107 : 0.3510550541440332
Loss in iteration 108 : 0.3510467023345224
Loss in iteration 109 : 0.35104501688646667
Loss in iteration 110 : 0.351039588269685
Loss in iteration 111 : 0.3510358836555368
Loss in iteration 112 : 0.3510358480360449
Loss in iteration 113 : 0.35102970192369787
Loss in iteration 114 : 0.35102830225943876
Loss in iteration 115 : 0.35102528563043933
Loss in iteration 116 : 0.3510198240582555
Loss in iteration 117 : 0.3510184611455557
Loss in iteration 118 : 0.3510143854990141
Loss in iteration 119 : 0.35101295305462615
Loss in iteration 120 : 0.3510119378371146
Loss in iteration 121 : 0.3510103805167317
Loss in iteration 122 : 0.35100787562773134
Loss in iteration 123 : 0.35100559886928434
Loss in iteration 124 : 0.3510042905125834
Loss in iteration 125 : 0.35100280559566716
Loss in iteration 126 : 0.35100089194389605
Loss in iteration 127 : 0.3509960858085712
Loss in iteration 128 : 0.3509948431559261
Loss in iteration 129 : 0.35099443502430766
Loss in iteration 130 : 0.3509952755536197
Loss in iteration 131 : 0.3509896233991488
Loss in iteration 132 : 0.3509910254258234
Loss in iteration 133 : 0.3509879326370017
Loss in iteration 134 : 0.35098934972650114
Loss in iteration 135 : 0.35098472642021694
Loss in iteration 136 : 0.35098282304535966
Loss in iteration 137 : 0.3509813257358301
Loss in iteration 138 : 0.35097958919051425
Loss in iteration 139 : 0.35097920462482685
Loss in iteration 140 : 0.3509752395592077
Loss in iteration 141 : 0.35097508636559116
Loss in iteration 142 : 0.35097201419613244
Loss in iteration 143 : 0.3509733954088289
Loss in iteration 144 : 0.35098227173812846
Loss in iteration 145 : 0.3509696900796033
Loss in iteration 146 : 0.35097733138191495
Loss in iteration 147 : 0.3509698291182384
Loss in iteration 148 : 0.35097259470671405
Loss in iteration 149 : 0.3509670659902437
Loss in iteration 150 : 0.35097282612698033
Loss in iteration 151 : 0.3509658865936142
Loss in iteration 152 : 0.3509694476801219
Loss in iteration 153 : 0.35096186438875426
Loss in iteration 154 : 0.3509678677674371
Loss in iteration 155 : 0.35095832411773853
Loss in iteration 156 : 0.3509625543702583
Loss in iteration 157 : 0.35095522653202454
Loss in iteration 158 : 0.350963478434507
Loss in iteration 159 : 0.3509580070894779
Loss in iteration 160 : 0.3509561179425895
Loss in iteration 161 : 0.3509550221964366
Loss in iteration 162 : 0.3509535526005367
Loss in iteration 163 : 0.35095110021500786
Loss in iteration 164 : 0.35095098490667803
Loss in iteration 165 : 0.35094976975066483
Loss in iteration 166 : 0.35094842283111266
Loss in iteration 167 : 0.35095176259294336
Loss in iteration 168 : 0.3509466474743424
Loss in iteration 169 : 0.35094980563552314
Loss in iteration 170 : 0.3509452322250018
Loss in iteration 171 : 0.35094580974596795
Loss in iteration 172 : 0.3509437341587166
Loss in iteration 173 : 0.35094145918523645
Loss in iteration 174 : 0.3509415103398591
Loss in iteration 175 : 0.3509400222089657
Loss in iteration 176 : 0.350938893305659
Loss in iteration 177 : 0.3509380154332101
Loss in iteration 178 : 0.35094069067012823
Loss in iteration 179 : 0.35094316914308626
Loss in iteration 180 : 0.3509372661163887
Loss in iteration 181 : 0.3509413956613904
Loss in iteration 182 : 0.3509535823955301
Loss in iteration 183 : 0.3509396624671771
Loss in iteration 184 : 0.35094837740366125
Loss in iteration 185 : 0.35093777075081156
Loss in iteration 186 : 0.3509483424241062
Loss in iteration 187 : 0.35093896192045204
Loss in iteration 188 : 0.35094067612792496
Loss in iteration 189 : 0.35094027888408164
Loss in iteration 190 : 0.3509345709713054
Loss in iteration 191 : 0.3509381610018865
Loss in iteration 192 : 0.3509325371360998
Loss in iteration 193 : 0.35093679107731157
Loss in iteration 194 : 0.3509305839111065
Loss in iteration 195 : 0.35093265690697084
Loss in iteration 196 : 0.3509283678949547
Loss in iteration 197 : 0.35092926436534816
Loss in iteration 198 : 0.3509298443165963
Loss in iteration 199 : 0.3509275557604732
Loss in iteration 200 : 0.35092790791898415
Testing accuracy  of updater 6 on alg 1 with rate 0.14 = 0.8495792641729624, training accuracy 0.8499385749385749, time elapsed: 4849 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5421318756714366
Loss in iteration 3 : 0.5974691391479547
Loss in iteration 4 : 0.7176681970683915
Loss in iteration 5 : 0.7423354565050854
Loss in iteration 6 : 0.6853948021836833
Loss in iteration 7 : 0.5704823736368204
Loss in iteration 8 : 0.4503947592671505
Loss in iteration 9 : 0.4030184008593086
Loss in iteration 10 : 0.43838676659640197
Loss in iteration 11 : 0.5029310825889576
Loss in iteration 12 : 0.544432124333845
Loss in iteration 13 : 0.5432010148538471
Loss in iteration 14 : 0.5091315561163839
Loss in iteration 15 : 0.4673203661158098
Loss in iteration 16 : 0.4390159803622389
Loss in iteration 17 : 0.43424469936916216
Loss in iteration 18 : 0.4458901763984791
Loss in iteration 19 : 0.4615236496279276
Loss in iteration 20 : 0.4717097357590626
Loss in iteration 21 : 0.4712072293317979
Loss in iteration 22 : 0.4601988514210437
Loss in iteration 23 : 0.4425043061136215
Loss in iteration 24 : 0.42438247108766897
Loss in iteration 25 : 0.4130648804341969
Loss in iteration 26 : 0.4113507158614151
Loss in iteration 27 : 0.4165118832870816
Loss in iteration 28 : 0.42233235751115744
Loss in iteration 29 : 0.42290115462089634
Loss in iteration 30 : 0.41678099670314167
Loss in iteration 31 : 0.4065864776812031
Loss in iteration 32 : 0.3971955824492058
Loss in iteration 33 : 0.39283291276702237
Loss in iteration 34 : 0.39339985270342354
Loss in iteration 35 : 0.3957669796628968
Loss in iteration 36 : 0.3965375339939443
Loss in iteration 37 : 0.3931912868460431
Loss in iteration 38 : 0.38673635329274364
Loss in iteration 39 : 0.3803353093926326
Loss in iteration 40 : 0.3766376338238853
Loss in iteration 41 : 0.3761809811606939
Loss in iteration 42 : 0.3770765784271767
Loss in iteration 43 : 0.3764751227611804
Loss in iteration 44 : 0.37325360292543763
Loss in iteration 45 : 0.3687217324352484
Loss in iteration 46 : 0.36557386842846046
Loss in iteration 47 : 0.3647196736106791
Loss in iteration 48 : 0.3651887176270367
Loss in iteration 49 : 0.3652012322188977
Loss in iteration 50 : 0.363639746918208
Loss in iteration 51 : 0.36118924597677454
Loss in iteration 52 : 0.3593002469937437
Loss in iteration 53 : 0.35891838031595075
Loss in iteration 54 : 0.3594484330962501
Loss in iteration 55 : 0.359401984896864
Loss in iteration 56 : 0.35818026712486456
Loss in iteration 57 : 0.35664109746338557
Loss in iteration 58 : 0.3561166557706326
Loss in iteration 59 : 0.3564742546087084
Loss in iteration 60 : 0.35669844977921084
Loss in iteration 61 : 0.35610061002149207
Loss in iteration 62 : 0.3551214058411003
Loss in iteration 63 : 0.35461389376049596
Loss in iteration 64 : 0.35491859867371195
Loss in iteration 65 : 0.3550700207343201
Loss in iteration 66 : 0.35464962139420725
Loss in iteration 67 : 0.35398472718785867
Loss in iteration 68 : 0.3538229575260964
Loss in iteration 69 : 0.35402975443291784
Loss in iteration 70 : 0.3540509414658375
Loss in iteration 71 : 0.35370643256695106
Loss in iteration 72 : 0.3533112933644861
Loss in iteration 73 : 0.3532208171228059
Loss in iteration 74 : 0.3533077059810419
Loss in iteration 75 : 0.3532146120962608
Loss in iteration 76 : 0.352927033170686
Loss in iteration 77 : 0.3527423308282035
Loss in iteration 78 : 0.3527951914667589
Loss in iteration 79 : 0.35282919893530723
Loss in iteration 80 : 0.35270950908912996
Loss in iteration 81 : 0.35255400803635495
Loss in iteration 82 : 0.35247728291684993
Loss in iteration 83 : 0.352519681033734
Loss in iteration 84 : 0.35251304458184324
Loss in iteration 85 : 0.3523977159188505
Loss in iteration 86 : 0.3523042671490076
Loss in iteration 87 : 0.3522892061452197
Loss in iteration 88 : 0.3522865604264926
Loss in iteration 89 : 0.3522379907722679
Loss in iteration 90 : 0.3521471287483585
Loss in iteration 91 : 0.35206976774188775
Loss in iteration 92 : 0.3520349894646815
Loss in iteration 93 : 0.35201863129162253
Loss in iteration 94 : 0.3519647889339344
Loss in iteration 95 : 0.35189413312018747
Loss in iteration 96 : 0.351854605049195
Loss in iteration 97 : 0.35184315340692446
Loss in iteration 98 : 0.35181295717758465
Loss in iteration 99 : 0.3517595222399749
Loss in iteration 100 : 0.3517218304893727
Loss in iteration 101 : 0.3517091912093079
Loss in iteration 102 : 0.3516754223974273
Loss in iteration 103 : 0.3516315411182446
Loss in iteration 104 : 0.35160866899877463
Loss in iteration 105 : 0.35159626159480944
Loss in iteration 106 : 0.3515661720244912
Loss in iteration 107 : 0.35153304627129933
Loss in iteration 108 : 0.3515133594081668
Loss in iteration 109 : 0.3514990438362015
Loss in iteration 110 : 0.3514718527218845
Loss in iteration 111 : 0.3514468391637113
Loss in iteration 112 : 0.351433727387527
Loss in iteration 113 : 0.3514165626184669
Loss in iteration 114 : 0.3513911327233382
Loss in iteration 115 : 0.3513722361472444
Loss in iteration 116 : 0.35135856614742395
Loss in iteration 117 : 0.35134220165878166
Loss in iteration 118 : 0.35132641666154246
Loss in iteration 119 : 0.351313989132476
Loss in iteration 120 : 0.3513020595115576
Loss in iteration 121 : 0.3512852848409481
Loss in iteration 122 : 0.3512712287956761
Loss in iteration 123 : 0.3512672599310024
Loss in iteration 124 : 0.35125089484333155
Loss in iteration 125 : 0.35124332497332217
Loss in iteration 126 : 0.35123413810186094
Loss in iteration 127 : 0.35122436346632696
Loss in iteration 128 : 0.3512169596752948
Loss in iteration 129 : 0.351211087044743
Loss in iteration 130 : 0.3512041890646717
Loss in iteration 131 : 0.3511982392378594
Loss in iteration 132 : 0.35119202215742396
Loss in iteration 133 : 0.3511865938371349
Loss in iteration 134 : 0.3511814027017256
Loss in iteration 135 : 0.3511756783834628
Loss in iteration 136 : 0.35117020078384964
Loss in iteration 137 : 0.35116615831691544
Loss in iteration 138 : 0.35116108131931256
Loss in iteration 139 : 0.3511573441585845
Loss in iteration 140 : 0.3511533934880912
Loss in iteration 141 : 0.35114942283961487
Loss in iteration 142 : 0.3511455413127316
Loss in iteration 143 : 0.35114178804775
Loss in iteration 144 : 0.3511376433426962
Loss in iteration 145 : 0.3511345781481895
Loss in iteration 146 : 0.3511306079267074
Loss in iteration 147 : 0.3511278899800447
Loss in iteration 148 : 0.35112310257950735
Loss in iteration 149 : 0.3511201203128828
Loss in iteration 150 : 0.3511164946305651
Loss in iteration 151 : 0.35111325846396674
Loss in iteration 152 : 0.351110138607917
Loss in iteration 153 : 0.3511074938140555
Loss in iteration 154 : 0.3511038224099261
Loss in iteration 155 : 0.35110116879499126
Loss in iteration 156 : 0.3510975734578158
Loss in iteration 157 : 0.35109520453371157
Loss in iteration 158 : 0.35109190543316976
Loss in iteration 159 : 0.351089731041298
Loss in iteration 160 : 0.35108642718694144
Loss in iteration 161 : 0.3510834583829193
Loss in iteration 162 : 0.35108037530232017
Loss in iteration 163 : 0.351079039664932
Loss in iteration 164 : 0.35107526227142327
Loss in iteration 165 : 0.3510727710285135
Loss in iteration 166 : 0.3510700052604501
Loss in iteration 167 : 0.3510676757633548
Loss in iteration 168 : 0.3510652961926531
Loss in iteration 169 : 0.35106333900369613
Loss in iteration 170 : 0.3510616667630646
Loss in iteration 171 : 0.35105964600835615
Loss in iteration 172 : 0.3510573105668952
Loss in iteration 173 : 0.35105513529140125
Testing accuracy  of updater 6 on alg 1 with rate 0.08000000000000002 = 0.8494564215957251, training accuracy 0.8498464373464374, time elapsed: 2943 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8867549916892055
Loss in iteration 3 : 0.6825707183319702
Loss in iteration 4 : 0.46781886243434784
Loss in iteration 5 : 0.4983021170587815
Loss in iteration 6 : 0.5489144324397034
Loss in iteration 7 : 0.5761278451418141
Loss in iteration 8 : 0.5807670201507372
Loss in iteration 9 : 0.5660963177032354
Loss in iteration 10 : 0.5362313398495838
Loss in iteration 11 : 0.49583151807920833
Loss in iteration 12 : 0.4504084461483919
Loss in iteration 13 : 0.40865460269621456
Loss in iteration 14 : 0.38156267616208583
Loss in iteration 15 : 0.37454982802857006
Loss in iteration 16 : 0.3832874010776481
Loss in iteration 17 : 0.39839335290776684
Loss in iteration 18 : 0.4113029089343183
Loss in iteration 19 : 0.41718110983731993
Loss in iteration 20 : 0.41486619767610106
Loss in iteration 21 : 0.4058265397122301
Loss in iteration 22 : 0.39294817729365744
Loss in iteration 23 : 0.3799458040509975
Loss in iteration 24 : 0.36960567481516193
Loss in iteration 25 : 0.36359625195360723
Loss in iteration 26 : 0.3623169922905449
Loss in iteration 27 : 0.3642257462442859
Loss in iteration 28 : 0.3676964727492887
Loss in iteration 29 : 0.370992521644507
Loss in iteration 30 : 0.37298256511624983
Loss in iteration 31 : 0.3730868204696766
Loss in iteration 32 : 0.37134357283140895
Loss in iteration 33 : 0.3682904346711175
Loss in iteration 34 : 0.3647132299078079
Loss in iteration 35 : 0.36132840217176143
Loss in iteration 36 : 0.3587371974938432
Loss in iteration 37 : 0.3572474445799211
Loss in iteration 38 : 0.3569280513732291
Loss in iteration 39 : 0.35741828775394396
Loss in iteration 40 : 0.3584163257885496
Loss in iteration 41 : 0.3593042526998342
Loss in iteration 42 : 0.35975992393559353
Loss in iteration 43 : 0.3596400929431281
Loss in iteration 44 : 0.35896108911065683
Loss in iteration 45 : 0.3578975033891862
Loss in iteration 46 : 0.35671235942320073
Loss in iteration 47 : 0.3556989341671833
Loss in iteration 48 : 0.3549726463344305
Loss in iteration 49 : 0.3546894064394922
Loss in iteration 50 : 0.35472916391614406
Loss in iteration 51 : 0.3549558361225629
Loss in iteration 52 : 0.35520532392662085
Loss in iteration 53 : 0.3553575729145236
Loss in iteration 54 : 0.3553374624279882
Loss in iteration 55 : 0.3551178651594805
Loss in iteration 56 : 0.3547478678453878
Loss in iteration 57 : 0.3543176222135387
Loss in iteration 58 : 0.3539159288885635
Loss in iteration 59 : 0.35361923178197585
Loss in iteration 60 : 0.3534458555592817
Loss in iteration 61 : 0.3534248948899352
Loss in iteration 62 : 0.35348589483377985
Loss in iteration 63 : 0.35354536959907107
Loss in iteration 64 : 0.353557507052541
Loss in iteration 65 : 0.35350109796917883
Loss in iteration 66 : 0.35337490268373845
Loss in iteration 67 : 0.35320468471469413
Loss in iteration 68 : 0.35302485428615965
Loss in iteration 69 : 0.3528703614304882
Loss in iteration 70 : 0.35277925574501845
Loss in iteration 71 : 0.35274224696493567
Loss in iteration 72 : 0.3527364669189887
Loss in iteration 73 : 0.3527408650326657
Loss in iteration 74 : 0.3527355628497757
Loss in iteration 75 : 0.3527092384293414
Loss in iteration 76 : 0.3526586554509303
Loss in iteration 77 : 0.35259027229224527
Loss in iteration 78 : 0.35251785095284993
Loss in iteration 79 : 0.3524593298394091
Loss in iteration 80 : 0.35242036751591044
Loss in iteration 81 : 0.3523946125060119
Loss in iteration 82 : 0.3523807936015573
Loss in iteration 83 : 0.3523797881233021
Loss in iteration 84 : 0.35237474527441187
Loss in iteration 85 : 0.3523569160899545
Loss in iteration 86 : 0.3523261944724146
Loss in iteration 87 : 0.35228708230097316
Loss in iteration 88 : 0.3522579892415189
Loss in iteration 89 : 0.3522358142019408
Loss in iteration 90 : 0.3522180296224845
Loss in iteration 91 : 0.35220489661370485
Loss in iteration 92 : 0.35219744365503375
Loss in iteration 93 : 0.3521902518088337
Loss in iteration 94 : 0.3521804340725231
Loss in iteration 95 : 0.3521658447534077
Loss in iteration 96 : 0.3521481756148243
Loss in iteration 97 : 0.3521292100916671
Loss in iteration 98 : 0.3521125654312344
Loss in iteration 99 : 0.35210014422693636
Loss in iteration 100 : 0.3520920617863938
Loss in iteration 101 : 0.3520858135301546
Loss in iteration 102 : 0.35207921573994255
Loss in iteration 103 : 0.35207117884155287
Loss in iteration 104 : 0.35206182051550616
Loss in iteration 105 : 0.3520508236385741
Loss in iteration 106 : 0.35203991674118246
Loss in iteration 107 : 0.3520300591883698
Loss in iteration 108 : 0.35202246155475253
Loss in iteration 109 : 0.35201593991054375
Loss in iteration 110 : 0.3520100527277745
Loss in iteration 111 : 0.3520032287208043
Loss in iteration 112 : 0.3519953587455919
Loss in iteration 113 : 0.3519868250336926
Loss in iteration 114 : 0.351979049990355
Loss in iteration 115 : 0.3519717582727196
Loss in iteration 116 : 0.3519657989180401
Loss in iteration 117 : 0.3519603958525341
Loss in iteration 118 : 0.3519544087849586
Loss in iteration 119 : 0.3519478466465435
Loss in iteration 120 : 0.35194097014046627
Loss in iteration 121 : 0.35193510555272084
Loss in iteration 122 : 0.3519296257692142
Loss in iteration 123 : 0.35192409727952323
Loss in iteration 124 : 0.35191848926394254
Loss in iteration 125 : 0.3519128924497663
Loss in iteration 126 : 0.35190734721262834
Loss in iteration 127 : 0.3519018631929376
Loss in iteration 128 : 0.3518964008883335
Loss in iteration 129 : 0.3518910726976266
Loss in iteration 130 : 0.35188590907673645
Loss in iteration 131 : 0.3518809896721348
Loss in iteration 132 : 0.3518759673503857
Loss in iteration 133 : 0.35187079316298053
Loss in iteration 134 : 0.3518655723286622
Loss in iteration 135 : 0.35186053929543987
Loss in iteration 136 : 0.351855872824992
Loss in iteration 137 : 0.35185111253350143
Loss in iteration 138 : 0.3518462629104993
Loss in iteration 139 : 0.35184169878102783
Loss in iteration 140 : 0.35183714519036463
Loss in iteration 141 : 0.3518322994579461
Loss in iteration 142 : 0.3518277812477991
Loss in iteration 143 : 0.3518233662283961
Loss in iteration 144 : 0.3518187885890794
Loss in iteration 145 : 0.3518143101189044
Loss in iteration 146 : 0.35180983576815134
Loss in iteration 147 : 0.3518055792893056
Loss in iteration 148 : 0.3518013338154325
Loss in iteration 149 : 0.3517970077693541
Loss in iteration 150 : 0.3517928723411455
Loss in iteration 151 : 0.3517888288245471
Loss in iteration 152 : 0.3517847445049842
Loss in iteration 153 : 0.3517806115463141
Loss in iteration 154 : 0.3517765169719286
Loss in iteration 155 : 0.35177247912068665
Loss in iteration 156 : 0.3517683944271207
Loss in iteration 157 : 0.3517644209087513
Loss in iteration 158 : 0.3517604564133067
Loss in iteration 159 : 0.35175658181062475
Loss in iteration 160 : 0.35175272483772685
Loss in iteration 161 : 0.3517488516008833
Loss in iteration 162 : 0.3517450153213183
Loss in iteration 163 : 0.3517411415763907
Loss in iteration 164 : 0.35173746716141024
Loss in iteration 165 : 0.3517336760534972
Loss in iteration 166 : 0.351729935480696
Loss in iteration 167 : 0.3517262549564042
Loss in iteration 168 : 0.3517225868836105
Loss in iteration 169 : 0.3517188936518233
Loss in iteration 170 : 0.35171537892912
Loss in iteration 171 : 0.35171176738954074
Loss in iteration 172 : 0.3517081274782364
Loss in iteration 173 : 0.3517046092928815
Loss in iteration 174 : 0.3517009990600345
Loss in iteration 175 : 0.3516974134244479
Loss in iteration 176 : 0.3516938792993389
Loss in iteration 177 : 0.35169031766748393
Loss in iteration 178 : 0.35168678679465
Loss in iteration 179 : 0.3516832608094682
Loss in iteration 180 : 0.3516797697107481
Loss in iteration 181 : 0.3516763137411936
Loss in iteration 182 : 0.35167280484439095
Loss in iteration 183 : 0.35166931696731707
Loss in iteration 184 : 0.35166591043740647
Loss in iteration 185 : 0.3516624061691674
Loss in iteration 186 : 0.35165901704432173
Loss in iteration 187 : 0.3516556323172995
Loss in iteration 188 : 0.35165221704178473
Loss in iteration 189 : 0.35164882497298466
Loss in iteration 190 : 0.35164553085600675
Loss in iteration 191 : 0.3516422063524849
Loss in iteration 192 : 0.3516388933999805
Loss in iteration 193 : 0.35163568400715556
Loss in iteration 194 : 0.3516325618643489
Loss in iteration 195 : 0.35162940617591804
Loss in iteration 196 : 0.351626331939304
Loss in iteration 197 : 0.35162328169072055
Loss in iteration 198 : 0.3516202896424876
Loss in iteration 199 : 0.3516173147768644
Loss in iteration 200 : 0.35161434728289187
Testing accuracy  of updater 6 on alg 1 with rate 0.01999999999999999 = 0.8500706344819114, training accuracy 0.8491400491400491, time elapsed: 3031 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.4648567826197407
Loss in iteration 3 : 3.5542240630564126
Loss in iteration 4 : 2.3980462583320117
Loss in iteration 5 : 1.1762849320354456
Loss in iteration 6 : 1.1407267867022735
Loss in iteration 7 : 1.7842709222120972
Loss in iteration 8 : 2.0799060724988165
Loss in iteration 9 : 1.8231349381105788
Loss in iteration 10 : 1.4127725335197259
Loss in iteration 11 : 1.2098025974331363
Loss in iteration 12 : 1.2271206391456277
Loss in iteration 13 : 1.3437869922436334
Loss in iteration 14 : 1.4544076134269397
Loss in iteration 15 : 1.478365343042573
Loss in iteration 16 : 1.397691692979077
Loss in iteration 17 : 1.2499774833780881
Loss in iteration 18 : 1.1082006068338441
Loss in iteration 19 : 1.0385950117365428
Loss in iteration 20 : 1.0626458708283255
Loss in iteration 21 : 1.1318146930130446
Loss in iteration 22 : 1.1659770722066063
Loss in iteration 23 : 1.121231003539609
Loss in iteration 24 : 1.0209043960902182
Loss in iteration 25 : 0.9282974591528553
Loss in iteration 26 : 0.9011088985506238
Loss in iteration 27 : 0.9309306620732448
Loss in iteration 28 : 0.955760813110948
Loss in iteration 29 : 0.9272030023823625
Loss in iteration 30 : 0.849672330728243
Loss in iteration 31 : 0.7775727308439524
Loss in iteration 32 : 0.7630174809845885
Loss in iteration 33 : 0.7804323791538682
Loss in iteration 34 : 0.7740651302074304
Loss in iteration 35 : 0.7256555882328853
Loss in iteration 36 : 0.6667194322183198
Loss in iteration 37 : 0.6446167688016646
Loss in iteration 38 : 0.6555339501202072
Loss in iteration 39 : 0.6489156015218595
Loss in iteration 40 : 0.6078679704653687
Loss in iteration 41 : 0.5675339578645568
Loss in iteration 42 : 0.5637375740745456
Loss in iteration 43 : 0.5696415861113788
Loss in iteration 44 : 0.5432611075009179
Loss in iteration 45 : 0.5062062226308872
Loss in iteration 46 : 0.5078036175725864
Loss in iteration 47 : 0.51468071761148
Loss in iteration 48 : 0.4817516690636529
Loss in iteration 49 : 0.47608378187328837
Loss in iteration 50 : 0.48751619674333874
Loss in iteration 51 : 0.460000983344162
Loss in iteration 52 : 0.4550780987754163
Loss in iteration 53 : 0.46006405325949223
Loss in iteration 54 : 0.4348465119191715
Loss in iteration 55 : 0.4437559379035038
Loss in iteration 56 : 0.4366693382618967
Loss in iteration 57 : 0.4194133222074759
Loss in iteration 58 : 0.425809609781814
Loss in iteration 59 : 0.4082307163371629
Loss in iteration 60 : 0.40820793054692833
Loss in iteration 61 : 0.4042904745194983
Loss in iteration 62 : 0.3932536660361445
Loss in iteration 63 : 0.3978255958992364
Loss in iteration 64 : 0.38603645746351545
Loss in iteration 65 : 0.388262444729763
Loss in iteration 66 : 0.3841778536587729
Loss in iteration 67 : 0.37925793202866515
Loss in iteration 68 : 0.3813881039509101
Loss in iteration 69 : 0.37427602908863666
Loss in iteration 70 : 0.3767604975617601
Loss in iteration 71 : 0.37245334045480555
Loss in iteration 72 : 0.373199242711017
Loss in iteration 73 : 0.37233514422315706
Loss in iteration 74 : 0.3714218330750379
Loss in iteration 75 : 0.3721845481460432
Loss in iteration 76 : 0.3706496803609688
Loss in iteration 77 : 0.37174205994594306
Loss in iteration 78 : 0.3696674439129157
Loss in iteration 79 : 0.36968097321804816
Loss in iteration 80 : 0.3680343627006137
Loss in iteration 81 : 0.36798653992268276
Loss in iteration 82 : 0.3665949733911075
Loss in iteration 83 : 0.36642697964571047
Loss in iteration 84 : 0.3655721800875427
Loss in iteration 85 : 0.36536834598576684
Loss in iteration 86 : 0.36466372947635084
Loss in iteration 87 : 0.3644802023429476
Loss in iteration 88 : 0.36395947256196737
Loss in iteration 89 : 0.36337332452371857
Loss in iteration 90 : 0.3628708162253657
Loss in iteration 91 : 0.36228169770460655
Loss in iteration 92 : 0.3618977545927912
Loss in iteration 93 : 0.3612193334188398
Loss in iteration 94 : 0.36111890220309645
Loss in iteration 95 : 0.3604522596697552
Loss in iteration 96 : 0.36039962007814136
Loss in iteration 97 : 0.3597380988932297
Loss in iteration 98 : 0.35985752160381596
Loss in iteration 99 : 0.3591964542813305
Loss in iteration 100 : 0.3592993817332584
Loss in iteration 101 : 0.35854962680248986
Loss in iteration 102 : 0.35859570444626276
Loss in iteration 103 : 0.35797530243625386
Loss in iteration 104 : 0.3577889016707914
Loss in iteration 105 : 0.357358700607924
Loss in iteration 106 : 0.35701284173709635
Loss in iteration 107 : 0.35681070816490473
Loss in iteration 108 : 0.35638755679150635
Loss in iteration 109 : 0.35633073606873955
Loss in iteration 110 : 0.3558089089091044
Loss in iteration 111 : 0.3557156094618951
Loss in iteration 112 : 0.35526888313747657
Loss in iteration 113 : 0.3551347942259104
Loss in iteration 114 : 0.3548779814144717
Loss in iteration 115 : 0.35453441807837915
Loss in iteration 116 : 0.35449139484761644
Loss in iteration 117 : 0.3541347021610866
Loss in iteration 118 : 0.3540157975566586
Loss in iteration 119 : 0.35386369874067025
Loss in iteration 120 : 0.3535818884950771
Loss in iteration 121 : 0.3534890956862476
Loss in iteration 122 : 0.35330342046563223
Loss in iteration 123 : 0.35312610018131513
Loss in iteration 124 : 0.3529947404206005
Loss in iteration 125 : 0.3528838090671266
Loss in iteration 126 : 0.352799492039832
Loss in iteration 127 : 0.35264742098441193
Loss in iteration 128 : 0.3525280160820973
Loss in iteration 129 : 0.35242314616838666
Loss in iteration 130 : 0.35234301210435226
Loss in iteration 131 : 0.3522888097462386
Loss in iteration 132 : 0.35218751202535714
Loss in iteration 133 : 0.3521487246173827
Loss in iteration 134 : 0.35204106538854907
Loss in iteration 135 : 0.35196104996457683
Loss in iteration 136 : 0.35189734630706565
Loss in iteration 137 : 0.351840725932817
Loss in iteration 138 : 0.3517824190829208
Loss in iteration 139 : 0.3517514067388167
Loss in iteration 140 : 0.35180597500533894
Loss in iteration 141 : 0.3516587744990321
Loss in iteration 142 : 0.35164079410773175
Loss in iteration 143 : 0.35170229114765306
Loss in iteration 144 : 0.3514992580088285
Loss in iteration 145 : 0.3517192818516913
Loss in iteration 146 : 0.3514934494551404
Loss in iteration 147 : 0.3514209514747728
Loss in iteration 148 : 0.3515012426333876
Loss in iteration 149 : 0.3512990904121593
Loss in iteration 150 : 0.35137966370933565
Loss in iteration 151 : 0.3513215627780101
Loss in iteration 152 : 0.35117114253898635
Loss in iteration 153 : 0.35122566156640456
Loss in iteration 154 : 0.35115516073500874
Loss in iteration 155 : 0.3510687887801318
Loss in iteration 156 : 0.35106971188183395
Loss in iteration 157 : 0.3510552475246062
Loss in iteration 158 : 0.3510349259683609
Loss in iteration 159 : 0.3509756527899359
Loss in iteration 160 : 0.35094986588397303
Loss in iteration 161 : 0.35093225985448306
Loss in iteration 162 : 0.3509171765739861
Loss in iteration 163 : 0.3509073501906302
Loss in iteration 164 : 0.35093205502023583
Loss in iteration 165 : 0.3510389350305936
Loss in iteration 166 : 0.3510005453841087
Loss in iteration 167 : 0.3509222016314238
Loss in iteration 168 : 0.3509293450438419
Loss in iteration 169 : 0.35091388790790046
Loss in iteration 170 : 0.3509418092492435
Loss in iteration 171 : 0.35094271420483
Loss in iteration 172 : 0.3509123067237196
Loss in iteration 173 : 0.3509223748213535
Loss in iteration 174 : 0.3509887326055403
Loss in iteration 175 : 0.35092328459293204
Loss in iteration 176 : 0.350939464833321
Loss in iteration 177 : 0.35099015002352485
Loss in iteration 178 : 0.35091374116680624
Loss in iteration 179 : 0.3508995970986602
Loss in iteration 180 : 0.35095685786742986
Loss in iteration 181 : 0.35094277087908166
Loss in iteration 182 : 0.3509243217971387
Loss in iteration 183 : 0.35089644993945185
Loss in iteration 184 : 0.3509060694778662
Loss in iteration 185 : 0.3509708134046938
Loss in iteration 186 : 0.351002535957323
Loss in iteration 187 : 0.35094545133713956
Loss in iteration 188 : 0.35091924138491637
Loss in iteration 189 : 0.3509702492276986
Loss in iteration 190 : 0.3509553992860067
Loss in iteration 191 : 0.35090027615726044
Loss in iteration 192 : 0.3509416491688381
Loss in iteration 193 : 0.3510381945524927
Loss in iteration 194 : 0.3509143408030446
Loss in iteration 195 : 0.3510264736330087
Loss in iteration 196 : 0.35120440033862194
Loss in iteration 197 : 0.35089381400718106
Loss in iteration 198 : 0.35110792738647717
Loss in iteration 199 : 0.351193632903036
Loss in iteration 200 : 0.35094058143552653
Testing accuracy  of updater 7 on alg 1 with rate 2.0 = 0.8491493151526319, training accuracy 0.8494471744471744, time elapsed: 3049 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5903075422619614
Loss in iteration 3 : 0.6032697238991296
Loss in iteration 4 : 0.40674141749031245
Loss in iteration 5 : 0.4605270912854545
Loss in iteration 6 : 0.5334481123535817
Loss in iteration 7 : 0.4563472656086368
Loss in iteration 8 : 0.42512833017764945
Loss in iteration 9 : 0.46907799867977334
Loss in iteration 10 : 0.496734448302602
Loss in iteration 11 : 0.4743633532637136
Loss in iteration 12 : 0.4416916535553376
Loss in iteration 13 : 0.44923528479497876
Loss in iteration 14 : 0.47679211752130074
Loss in iteration 15 : 0.47622508292830373
Loss in iteration 16 : 0.44962419851645113
Loss in iteration 17 : 0.4381592966358569
Loss in iteration 18 : 0.45163001211710413
Loss in iteration 19 : 0.454122815408657
Loss in iteration 20 : 0.43133550862006875
Loss in iteration 21 : 0.4156475659095404
Loss in iteration 22 : 0.4204426281909233
Loss in iteration 23 : 0.42098390283455545
Loss in iteration 24 : 0.4051327997583081
Loss in iteration 25 : 0.39224933417754554
Loss in iteration 26 : 0.39686466135038867
Loss in iteration 27 : 0.39648070680651276
Loss in iteration 28 : 0.3815226412842129
Loss in iteration 29 : 0.37971798632219456
Loss in iteration 30 : 0.38463894073379157
Loss in iteration 31 : 0.3746503408588013
Loss in iteration 32 : 0.37083083621528584
Loss in iteration 33 : 0.3774523300335546
Loss in iteration 34 : 0.369826741547579
Loss in iteration 35 : 0.3684787494329598
Loss in iteration 36 : 0.37180449742638455
Loss in iteration 37 : 0.36331551211780333
Loss in iteration 38 : 0.36617819013041797
Loss in iteration 39 : 0.36449313891092716
Loss in iteration 40 : 0.3604207908849369
Loss in iteration 41 : 0.3628900556151676
Loss in iteration 42 : 0.3587349082012376
Loss in iteration 43 : 0.35872411253135894
Loss in iteration 44 : 0.3584620374334336
Loss in iteration 45 : 0.35582056383726646
Loss in iteration 46 : 0.3579520420778863
Loss in iteration 47 : 0.35559964859601
Loss in iteration 48 : 0.3559394950301405
Loss in iteration 49 : 0.3566510053277529
Loss in iteration 50 : 0.3546833894781932
Loss in iteration 51 : 0.35604224396453704
Loss in iteration 52 : 0.35496867317312303
Loss in iteration 53 : 0.35420869244799236
Loss in iteration 54 : 0.35499412733863184
Loss in iteration 55 : 0.35358831434655646
Loss in iteration 56 : 0.3540685168447458
Loss in iteration 57 : 0.3539256915371599
Loss in iteration 58 : 0.35304415384861043
Loss in iteration 59 : 0.35370078747002914
Loss in iteration 60 : 0.35293095516154727
Loss in iteration 61 : 0.35326011699781223
Loss in iteration 62 : 0.3530680010045355
Loss in iteration 63 : 0.3527143139494364
Loss in iteration 64 : 0.3529637713247576
Loss in iteration 65 : 0.35223605361730687
Loss in iteration 66 : 0.3524358937627557
Loss in iteration 67 : 0.3520758905596121
Loss in iteration 68 : 0.3519304067283508
Loss in iteration 69 : 0.3518618806239336
Loss in iteration 70 : 0.35166971912168193
Loss in iteration 71 : 0.35179233586115194
Loss in iteration 72 : 0.3515157222445947
Loss in iteration 73 : 0.35165751072389057
Loss in iteration 74 : 0.35146158774679676
Loss in iteration 75 : 0.3515840158322747
Loss in iteration 76 : 0.3513990121506679
Loss in iteration 77 : 0.35140471976843785
Loss in iteration 78 : 0.351294767939687
Loss in iteration 79 : 0.3512588628795662
Loss in iteration 80 : 0.35124860926195006
Loss in iteration 81 : 0.3511915473898547
Loss in iteration 82 : 0.35120190813362384
Loss in iteration 83 : 0.35115084268433133
Loss in iteration 84 : 0.35118541464444086
Loss in iteration 85 : 0.35113941899528855
Loss in iteration 86 : 0.35113296062922467
Loss in iteration 87 : 0.35115356828176825
Loss in iteration 88 : 0.35109652780738765
Loss in iteration 89 : 0.3511288515195686
Loss in iteration 90 : 0.3510623028857987
Loss in iteration 91 : 0.351126029577915
Loss in iteration 92 : 0.3510399932166129
Loss in iteration 93 : 0.351090177774904
Loss in iteration 94 : 0.35102565448542783
Loss in iteration 95 : 0.351062062303605
Loss in iteration 96 : 0.35102842233726467
Loss in iteration 97 : 0.35103494745676844
Loss in iteration 98 : 0.35102185793281543
Loss in iteration 99 : 0.35100188642861563
Loss in iteration 100 : 0.3510103526511279
Loss in iteration 101 : 0.35102234398461274
Loss in iteration 102 : 0.3509942430564363
Loss in iteration 103 : 0.3509873949825819
Loss in iteration 104 : 0.3509907486289453
Loss in iteration 105 : 0.3509761847128424
Loss in iteration 106 : 0.3509728169664261
Loss in iteration 107 : 0.350967685303452
Loss in iteration 108 : 0.3509697425459778
Loss in iteration 109 : 0.35097020510376215
Loss in iteration 110 : 0.35095404251277
Loss in iteration 111 : 0.35096453860337934
Loss in iteration 112 : 0.3510207328298712
Loss in iteration 113 : 0.3509513137380858
Loss in iteration 114 : 0.35100845553764864
Loss in iteration 115 : 0.3510191404222985
Loss in iteration 116 : 0.35096903938625734
Loss in iteration 117 : 0.35102260903919463
Loss in iteration 118 : 0.35095247180468925
Loss in iteration 119 : 0.3510066367293991
Loss in iteration 120 : 0.3509592691923571
Loss in iteration 121 : 0.35098165444644425
Loss in iteration 122 : 0.3509631457503682
Loss in iteration 123 : 0.3509492024859864
Loss in iteration 124 : 0.3509897699827562
Loss in iteration 125 : 0.35093623445101524
Loss in iteration 126 : 0.35096831857219346
Loss in iteration 127 : 0.35094119854001565
Loss in iteration 128 : 0.35094122499003494
Loss in iteration 129 : 0.35095889395830915
Loss in iteration 130 : 0.35092841794135343
Loss in iteration 131 : 0.35098070934371556
Loss in iteration 132 : 0.35094639613439815
Loss in iteration 133 : 0.3509393238259951
Loss in iteration 134 : 0.35097073174236154
Loss in iteration 135 : 0.35092846949512935
Loss in iteration 136 : 0.35093973258321626
Loss in iteration 137 : 0.3509344570569676
Loss in iteration 138 : 0.3509250723654266
Loss in iteration 139 : 0.3509455342040144
Loss in iteration 140 : 0.35091668164229806
Loss in iteration 141 : 0.3509495742126763
Loss in iteration 142 : 0.3509613454629138
Loss in iteration 143 : 0.3509374866911338
Loss in iteration 144 : 0.3509844433189695
Loss in iteration 145 : 0.3509204008528266
Loss in iteration 146 : 0.3509508536322636
Loss in iteration 147 : 0.35092314333909214
Loss in iteration 148 : 0.35092891962380857
Loss in iteration 149 : 0.3509543569097742
Loss in iteration 150 : 0.3509160105180521
Loss in iteration 151 : 0.3509369233762577
Loss in iteration 152 : 0.35090455584148617
Loss in iteration 153 : 0.3509233400369884
Loss in iteration 154 : 0.3509108200052051
Loss in iteration 155 : 0.3508997979271276
Loss in iteration 156 : 0.35092178902374843
Loss in iteration 157 : 0.35090442620872
Loss in iteration 158 : 0.3509143734042532
Loss in iteration 159 : 0.3509525235093814
Loss in iteration 160 : 0.35089484032700224
Loss in iteration 161 : 0.3509995049911901
Loss in iteration 162 : 0.3509260384817094
Loss in iteration 163 : 0.3509367170267079
Loss in iteration 164 : 0.35094508609853087
Loss in iteration 165 : 0.35090201757818723
Loss in iteration 166 : 0.35093340857920036
Loss in iteration 167 : 0.3508983602325957
Loss in iteration 168 : 0.3509156650296263
Loss in iteration 169 : 0.3509134201115552
Loss in iteration 170 : 0.35089951036794625
Loss in iteration 171 : 0.35090387353060515
Loss in iteration 172 : 0.35089362728789103
Loss in iteration 173 : 0.35091476446551373
Loss in iteration 174 : 0.3508831751595018
Loss in iteration 175 : 0.35094138400256053
Loss in iteration 176 : 0.35090979597693334
Loss in iteration 177 : 0.35087829376841984
Loss in iteration 178 : 0.35091780210348106
Loss in iteration 179 : 0.3509279749163785
Loss in iteration 180 : 0.35089003672657704
Loss in iteration 181 : 0.35094008336528515
Loss in iteration 182 : 0.35092934368783096
Loss in iteration 183 : 0.35090296688257205
Loss in iteration 184 : 0.3509500602741253
Loss in iteration 185 : 0.35088990252672
Loss in iteration 186 : 0.3509488186247165
Loss in iteration 187 : 0.3509014517539494
Loss in iteration 188 : 0.3509335814883503
Loss in iteration 189 : 0.35088554304151937
Loss in iteration 190 : 0.3509398399922486
Loss in iteration 191 : 0.35092469405111437
Loss in iteration 192 : 0.35087717207554764
Loss in iteration 193 : 0.3509233361402438
Loss in iteration 194 : 0.3508878525922677
Loss in iteration 195 : 0.3509794526954889
Loss in iteration 196 : 0.3509100559535808
Loss in iteration 197 : 0.350905719499927
Loss in iteration 198 : 0.3509561475933307
Loss in iteration 199 : 0.35087930348325086
Loss in iteration 200 : 0.3509275423166804
Testing accuracy  of updater 7 on alg 1 with rate 1.4000000000000001 = 0.8495792641729624, training accuracy 0.8496928746928747, time elapsed: 3265 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.47085452690397983
Loss in iteration 3 : 0.6822463097188015
Loss in iteration 4 : 0.7240921168521832
Loss in iteration 5 : 0.5969155008776692
Loss in iteration 6 : 0.40866771922841666
Loss in iteration 7 : 0.42371321365030684
Loss in iteration 8 : 0.5428400004348448
Loss in iteration 9 : 0.5527982752382457
Loss in iteration 10 : 0.47106939896548855
Loss in iteration 11 : 0.41503495204815316
Loss in iteration 12 : 0.43197177259228364
Loss in iteration 13 : 0.47532030452174595
Loss in iteration 14 : 0.4948833285288385
Loss in iteration 15 : 0.4752927055195679
Loss in iteration 16 : 0.4373232250453046
Loss in iteration 17 : 0.41729461024066705
Loss in iteration 18 : 0.4317122994616516
Loss in iteration 19 : 0.45588575945091586
Loss in iteration 20 : 0.45885761898511834
Loss in iteration 21 : 0.43769338046838246
Loss in iteration 22 : 0.41506607927251743
Loss in iteration 23 : 0.4121717923169121
Loss in iteration 24 : 0.4240050873353828
Loss in iteration 25 : 0.4290004950189197
Loss in iteration 26 : 0.4161899235608933
Loss in iteration 27 : 0.3978205922443801
Loss in iteration 28 : 0.391920410988484
Loss in iteration 29 : 0.39782322827670974
Loss in iteration 30 : 0.40049255099457903
Loss in iteration 31 : 0.392220490976416
Loss in iteration 32 : 0.37929589964453336
Loss in iteration 33 : 0.3745772180773581
Loss in iteration 34 : 0.3790167976802106
Loss in iteration 35 : 0.37992990565847073
Loss in iteration 36 : 0.37149611690485373
Loss in iteration 37 : 0.364801631676684
Loss in iteration 38 : 0.3668651077724776
Loss in iteration 39 : 0.3693725152307861
Loss in iteration 40 : 0.364963900747009
Loss in iteration 41 : 0.35997533774668544
Loss in iteration 42 : 0.3622036503851988
Loss in iteration 43 : 0.36435710877552663
Loss in iteration 44 : 0.3608526597122458
Loss in iteration 45 : 0.3582105554436997
Loss in iteration 46 : 0.36064252334548047
Loss in iteration 47 : 0.3602957711589727
Loss in iteration 48 : 0.3567090202768077
Loss in iteration 49 : 0.3569401226188803
Loss in iteration 50 : 0.3583328270829892
Loss in iteration 51 : 0.3562333857271458
Loss in iteration 52 : 0.35486757952174003
Loss in iteration 53 : 0.35621890409569007
Loss in iteration 54 : 0.3554706266333889
Loss in iteration 55 : 0.35375027766528105
Loss in iteration 56 : 0.354482636572791
Loss in iteration 57 : 0.3545315685092771
Loss in iteration 58 : 0.3532098322698272
Loss in iteration 59 : 0.35344232982004486
Loss in iteration 60 : 0.35380815098107377
Loss in iteration 61 : 0.3528385827459337
Loss in iteration 62 : 0.35275650483215293
Loss in iteration 63 : 0.3531667806671499
Loss in iteration 64 : 0.35281836207299905
Loss in iteration 65 : 0.35242075887028723
Loss in iteration 66 : 0.35267234491351324
Loss in iteration 67 : 0.35261327591095865
Loss in iteration 68 : 0.35219035008711536
Loss in iteration 69 : 0.3522219048654062
Loss in iteration 70 : 0.3523285474992376
Loss in iteration 71 : 0.35202607134533925
Loss in iteration 72 : 0.35192559927598355
Loss in iteration 73 : 0.35206158612039573
Loss in iteration 74 : 0.3518839387649625
Loss in iteration 75 : 0.35174463462293093
Loss in iteration 76 : 0.35185126484918294
Loss in iteration 77 : 0.3517966606810618
Loss in iteration 78 : 0.351651398023612
Loss in iteration 79 : 0.351719000229011
Loss in iteration 80 : 0.35169458210294635
Loss in iteration 81 : 0.35156235442738953
Loss in iteration 82 : 0.35159810016676607
Loss in iteration 83 : 0.35155918589258595
Loss in iteration 84 : 0.35147086367847613
Loss in iteration 85 : 0.3515089548852158
Loss in iteration 86 : 0.35144819336320376
Loss in iteration 87 : 0.35138604825742903
Loss in iteration 88 : 0.3514095857761345
Loss in iteration 89 : 0.35135543757325954
Loss in iteration 90 : 0.3513253755980891
Loss in iteration 91 : 0.35133297340845193
Loss in iteration 92 : 0.3512859379269308
Loss in iteration 93 : 0.3512757893771976
Loss in iteration 94 : 0.35127239093757445
Loss in iteration 95 : 0.35123895139504274
Loss in iteration 96 : 0.35123453151009665
Loss in iteration 97 : 0.3512188620674358
Loss in iteration 98 : 0.3511990598787215
Loss in iteration 99 : 0.35119585290384275
Loss in iteration 100 : 0.35117645784314566
Loss in iteration 101 : 0.3511691937872607
Loss in iteration 102 : 0.35115954714049946
Loss in iteration 103 : 0.35114875594587497
Loss in iteration 104 : 0.35114647253569475
Loss in iteration 105 : 0.3511298078086585
Loss in iteration 106 : 0.3511245106904111
Loss in iteration 107 : 0.3511166436086465
Loss in iteration 108 : 0.3511152607052618
Loss in iteration 109 : 0.35110662588601543
Loss in iteration 110 : 0.3511044072849795
Loss in iteration 111 : 0.35109801212049846
Loss in iteration 112 : 0.35109561508649106
Loss in iteration 113 : 0.35108942073625676
Loss in iteration 114 : 0.35108599361367265
Loss in iteration 115 : 0.3510799947794608
Loss in iteration 116 : 0.35107762254718444
Loss in iteration 117 : 0.3510742921424578
Loss in iteration 118 : 0.35106890851661754
Loss in iteration 119 : 0.35106589498245616
Loss in iteration 120 : 0.35106260962636815
Loss in iteration 121 : 0.3510599429780752
Loss in iteration 122 : 0.35105745452654535
Loss in iteration 123 : 0.35105440448889774
Loss in iteration 124 : 0.35105311554008883
Loss in iteration 125 : 0.3510534068770118
Loss in iteration 126 : 0.3510479395377168
Loss in iteration 127 : 0.35104454008081776
Loss in iteration 128 : 0.35104204560851365
Loss in iteration 129 : 0.3510393427038381
Loss in iteration 130 : 0.3510379887523432
Loss in iteration 131 : 0.35103446410222494
Loss in iteration 132 : 0.35103459820111965
Loss in iteration 133 : 0.3510400935696089
Loss in iteration 134 : 0.3510288928580606
Loss in iteration 135 : 0.3510354871592322
Loss in iteration 136 : 0.35102996547620346
Loss in iteration 137 : 0.3510237533378603
Loss in iteration 138 : 0.3510240206458565
Loss in iteration 139 : 0.3510192728064833
Loss in iteration 140 : 0.3510199203875448
Loss in iteration 141 : 0.35101716660990445
Loss in iteration 142 : 0.3510127758458149
Loss in iteration 143 : 0.3510115009270589
Loss in iteration 144 : 0.35100750688562804
Loss in iteration 145 : 0.35100607030972064
Loss in iteration 146 : 0.3510054532685279
Loss in iteration 147 : 0.3510014834342937
Loss in iteration 148 : 0.35099965177390924
Loss in iteration 149 : 0.35099974916746085
Loss in iteration 150 : 0.35099564242392817
Loss in iteration 151 : 0.3509951779558819
Loss in iteration 152 : 0.35099211857034096
Loss in iteration 153 : 0.3509896372131537
Loss in iteration 154 : 0.3509889365328047
Loss in iteration 155 : 0.3509929721699733
Loss in iteration 156 : 0.3509871396015198
Loss in iteration 157 : 0.3509848152068803
Loss in iteration 158 : 0.35098657905750247
Loss in iteration 159 : 0.35098489788428516
Loss in iteration 160 : 0.3509803979839257
Loss in iteration 161 : 0.35097983242398595
Loss in iteration 162 : 0.3509767156661805
Loss in iteration 163 : 0.35097445723693094
Loss in iteration 164 : 0.3509730298222505
Loss in iteration 165 : 0.3509731024290239
Loss in iteration 166 : 0.350969700272869
Loss in iteration 167 : 0.3509722042416387
Loss in iteration 168 : 0.35096735525651535
Loss in iteration 169 : 0.3509673966027422
Loss in iteration 170 : 0.3509648741778987
Loss in iteration 171 : 0.3509649440143182
Loss in iteration 172 : 0.3509634000297506
Loss in iteration 173 : 0.3509617496251348
Loss in iteration 174 : 0.3509630488081528
Loss in iteration 175 : 0.35096090622280085
Loss in iteration 176 : 0.35096271924109
Loss in iteration 177 : 0.3509662785672856
Loss in iteration 178 : 0.3509564930180071
Loss in iteration 179 : 0.3509575494940778
Loss in iteration 180 : 0.3509576891903239
Loss in iteration 181 : 0.350952994574977
Loss in iteration 182 : 0.35095612006596927
Loss in iteration 183 : 0.35095187383505044
Loss in iteration 184 : 0.3509502007245111
Loss in iteration 185 : 0.3509528335291773
Loss in iteration 186 : 0.3509474496594825
Loss in iteration 187 : 0.3509518031120853
Loss in iteration 188 : 0.35094571668253915
Loss in iteration 189 : 0.3509532355560627
Loss in iteration 190 : 0.350960390086994
Loss in iteration 191 : 0.35094453309688456
Loss in iteration 192 : 0.3509562210409411
Loss in iteration 193 : 0.3509432617819341
Loss in iteration 194 : 0.3509564253836449
Loss in iteration 195 : 0.3509444351676205
Loss in iteration 196 : 0.35095551247037476
Loss in iteration 197 : 0.35095548977135504
Loss in iteration 198 : 0.35095223425187516
Loss in iteration 199 : 0.35094896221489896
Loss in iteration 200 : 0.3509476578597346
Testing accuracy  of updater 7 on alg 1 with rate 0.8 = 0.849640685461581, training accuracy 0.8498464373464374, time elapsed: 3134 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8574187864095452
Loss in iteration 3 : 0.5871896085231012
Loss in iteration 4 : 0.5285925524996038
Loss in iteration 5 : 0.6086826717357308
Loss in iteration 6 : 0.6389947512156516
Loss in iteration 7 : 0.6242549672887874
Loss in iteration 8 : 0.5726348017248273
Loss in iteration 9 : 0.4974040820102174
Loss in iteration 10 : 0.42023994204207943
Loss in iteration 11 : 0.37901340462697364
Loss in iteration 12 : 0.3914485605496096
Loss in iteration 13 : 0.42778215763823063
Loss in iteration 14 : 0.4566233209380958
Loss in iteration 15 : 0.46409449443034806
Loss in iteration 16 : 0.4497784220256733
Loss in iteration 17 : 0.4227688349239946
Loss in iteration 18 : 0.3964550897949544
Loss in iteration 19 : 0.3810735000284438
Loss in iteration 20 : 0.3792099390482234
Loss in iteration 21 : 0.38633994228027346
Loss in iteration 22 : 0.3961068161317057
Loss in iteration 23 : 0.40297555220653547
Loss in iteration 24 : 0.4040935430844688
Loss in iteration 25 : 0.39936659504460004
Loss in iteration 26 : 0.3906433968545393
Loss in iteration 27 : 0.38131175895191544
Loss in iteration 28 : 0.3743335751611354
Loss in iteration 29 : 0.3715062721944413
Loss in iteration 30 : 0.3727251339600988
Loss in iteration 31 : 0.37620893325097793
Loss in iteration 32 : 0.379504259909378
Loss in iteration 33 : 0.3805446113657012
Loss in iteration 34 : 0.37882634562618905
Loss in iteration 35 : 0.3750735889673438
Loss in iteration 36 : 0.3707298744168176
Loss in iteration 37 : 0.36743612027305267
Loss in iteration 38 : 0.36606983925713105
Loss in iteration 39 : 0.3662881136701149
Loss in iteration 40 : 0.3672442019337963
Loss in iteration 41 : 0.3678927489752127
Loss in iteration 42 : 0.36750103313902704
Loss in iteration 43 : 0.3659192325186367
Loss in iteration 44 : 0.3636267849390425
Loss in iteration 45 : 0.3614211424541038
Loss in iteration 46 : 0.3598660317908723
Loss in iteration 47 : 0.35930961505595493
Loss in iteration 48 : 0.3595147865999149
Loss in iteration 49 : 0.35977954118536354
Loss in iteration 50 : 0.3596529592668055
Loss in iteration 51 : 0.35895376687378866
Loss in iteration 52 : 0.35784558635873553
Loss in iteration 53 : 0.35672046051001566
Loss in iteration 54 : 0.35595255562404693
Loss in iteration 55 : 0.3556911061935167
Loss in iteration 56 : 0.3557740833300013
Loss in iteration 57 : 0.355861336527425
Loss in iteration 58 : 0.3557016334310067
Loss in iteration 59 : 0.35524341613574534
Loss in iteration 60 : 0.3546343687338117
Loss in iteration 61 : 0.35413155245655586
Loss in iteration 62 : 0.3538573630676879
Loss in iteration 63 : 0.35380989840561206
Loss in iteration 64 : 0.3538518433092608
Loss in iteration 65 : 0.3538450291541279
Loss in iteration 66 : 0.3536882843432151
Loss in iteration 67 : 0.35341208006172226
Loss in iteration 68 : 0.3531555724758482
Loss in iteration 69 : 0.3530055828472905
Loss in iteration 70 : 0.35296162559531147
Loss in iteration 71 : 0.35298935041343515
Loss in iteration 72 : 0.352991570677201
Loss in iteration 73 : 0.3529220278166059
Loss in iteration 74 : 0.3527842080213459
Loss in iteration 75 : 0.35264623098317704
Loss in iteration 76 : 0.3525654089690389
Loss in iteration 77 : 0.3525487501066664
Loss in iteration 78 : 0.3525655557006673
Loss in iteration 79 : 0.3525610226760436
Loss in iteration 80 : 0.35251253626781864
Loss in iteration 81 : 0.35244158741673653
Loss in iteration 82 : 0.3523746480056114
Loss in iteration 83 : 0.35233196878866047
Loss in iteration 84 : 0.3523243848899944
Loss in iteration 85 : 0.3523237227688822
Loss in iteration 86 : 0.35230909758815454
Loss in iteration 87 : 0.3522672992811112
Loss in iteration 88 : 0.3522198512296897
Loss in iteration 89 : 0.3521861052512438
Loss in iteration 90 : 0.35216704486904615
Loss in iteration 91 : 0.3521664496861496
Loss in iteration 92 : 0.3521556370420278
Loss in iteration 93 : 0.3521306905380784
Loss in iteration 94 : 0.35209987613736393
Loss in iteration 95 : 0.3520760151398
Loss in iteration 96 : 0.35206442497146406
Loss in iteration 97 : 0.3520596312932774
Loss in iteration 98 : 0.3520492584969483
Loss in iteration 99 : 0.35202966168020494
Loss in iteration 100 : 0.35201126558108564
Loss in iteration 101 : 0.35199688000530915
Loss in iteration 102 : 0.3519865129559556
Loss in iteration 103 : 0.3519779457575359
Loss in iteration 104 : 0.3519676592069101
Loss in iteration 105 : 0.35195477230911276
Loss in iteration 106 : 0.3519402270451307
Loss in iteration 107 : 0.3519266245603376
Loss in iteration 108 : 0.35191797823846327
Loss in iteration 109 : 0.3519116694270926
Loss in iteration 110 : 0.35190200635353
Loss in iteration 111 : 0.3518884465742852
Loss in iteration 112 : 0.351877351543773
Loss in iteration 113 : 0.3518698444396264
Loss in iteration 114 : 0.3518626482940732
Loss in iteration 115 : 0.3518541121149948
Loss in iteration 116 : 0.3518448261851053
Loss in iteration 117 : 0.3518372847658839
Loss in iteration 118 : 0.35183068808442736
Loss in iteration 119 : 0.3518236106427827
Loss in iteration 120 : 0.3518155750920965
Loss in iteration 121 : 0.35180764682790683
Loss in iteration 122 : 0.3517997457751168
Loss in iteration 123 : 0.3517925055834679
Loss in iteration 124 : 0.3517862748647011
Loss in iteration 125 : 0.35177984854788236
Loss in iteration 126 : 0.3517723168962966
Loss in iteration 127 : 0.3517651539101396
Loss in iteration 128 : 0.3517588674066186
Loss in iteration 129 : 0.35175303363630434
Loss in iteration 130 : 0.35174614969198753
Loss in iteration 131 : 0.35173945362218223
Loss in iteration 132 : 0.3517334749938621
Loss in iteration 133 : 0.3517276207500086
Loss in iteration 134 : 0.3517214494051718
Loss in iteration 135 : 0.3517153931268221
Loss in iteration 136 : 0.35170948311357697
Loss in iteration 137 : 0.3517036583906154
Loss in iteration 138 : 0.35169776885046544
Loss in iteration 139 : 0.351692688286448
Loss in iteration 140 : 0.3516869571181812
Loss in iteration 141 : 0.3516812915205759
Loss in iteration 142 : 0.35167588378528636
Loss in iteration 143 : 0.3516708038605286
Loss in iteration 144 : 0.35166565158384466
Loss in iteration 145 : 0.351660252273768
Loss in iteration 146 : 0.3516549529618901
Loss in iteration 147 : 0.35164989227458043
Loss in iteration 148 : 0.35164466057129373
Loss in iteration 149 : 0.3516400167432431
Loss in iteration 150 : 0.35163533066955444
Loss in iteration 151 : 0.35163053277721623
Loss in iteration 152 : 0.35162591841023655
Loss in iteration 153 : 0.351621293096113
Loss in iteration 154 : 0.35161641393007603
Loss in iteration 155 : 0.3516118841462224
Loss in iteration 156 : 0.3516074314336288
Loss in iteration 157 : 0.35160295952674875
Loss in iteration 158 : 0.35159865402027285
Loss in iteration 159 : 0.3515942678652885
Loss in iteration 160 : 0.35158990950111574
Loss in iteration 161 : 0.3515855131395726
Loss in iteration 162 : 0.3515812949556461
Loss in iteration 163 : 0.3515771121278879
Loss in iteration 164 : 0.35157289687941257
Loss in iteration 165 : 0.3515686932342662
Loss in iteration 166 : 0.3515646879632837
Loss in iteration 167 : 0.3515604316259434
Loss in iteration 168 : 0.3515563762106773
Loss in iteration 169 : 0.3515523635014622
Loss in iteration 170 : 0.35154809957912075
Loss in iteration 171 : 0.35154424738912676
Loss in iteration 172 : 0.35153983662219246
Loss in iteration 173 : 0.351535718983285
Loss in iteration 174 : 0.35153181153636076
Loss in iteration 175 : 0.35152747293333236
Loss in iteration 176 : 0.35152355355263076
Loss in iteration 177 : 0.35151959433933905
Loss in iteration 178 : 0.3515154847393224
Loss in iteration 179 : 0.35151148217975814
Loss in iteration 180 : 0.35150727740448295
Loss in iteration 181 : 0.35150361464003127
Loss in iteration 182 : 0.35149953555400487
Loss in iteration 183 : 0.35149572689611963
Loss in iteration 184 : 0.35149147970263095
Loss in iteration 185 : 0.35148786751753786
Loss in iteration 186 : 0.35148406655941844
Loss in iteration 187 : 0.35148025469349864
Loss in iteration 188 : 0.35147684670564927
Loss in iteration 189 : 0.3514732673379676
Loss in iteration 190 : 0.3514696634602642
Loss in iteration 191 : 0.35146603525152786
Loss in iteration 192 : 0.35146258396699326
Loss in iteration 193 : 0.3514590818780703
Loss in iteration 194 : 0.35145562342357417
Loss in iteration 195 : 0.3514522510073095
Loss in iteration 196 : 0.3514487922363557
Loss in iteration 197 : 0.35144534993142185
Loss in iteration 198 : 0.35144195532253275
Loss in iteration 199 : 0.3514384693071007
Loss in iteration 200 : 0.35143509271873796
Testing accuracy  of updater 7 on alg 1 with rate 0.2 = 0.8498249493274369, training accuracy 0.8495700245700246, time elapsed: 3173 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9006273988923482
Loss in iteration 3 : 0.7118572219095936
Loss in iteration 4 : 0.4728652803813341
Loss in iteration 5 : 0.5389980136413084
Loss in iteration 6 : 0.5955261967730691
Loss in iteration 7 : 0.6170188260910578
Loss in iteration 8 : 0.6068847478192694
Loss in iteration 9 : 0.5701503936734269
Loss in iteration 10 : 0.5147001920485529
Loss in iteration 11 : 0.45145213690772557
Loss in iteration 12 : 0.3976603508100744
Loss in iteration 13 : 0.37596507414016184
Loss in iteration 14 : 0.38915287737829707
Loss in iteration 15 : 0.4160183265358957
Loss in iteration 16 : 0.4363678364308844
Loss in iteration 17 : 0.44202563460115757
Loss in iteration 18 : 0.4322320702187859
Loss in iteration 19 : 0.4122804890080918
Loss in iteration 20 : 0.3901984001853698
Loss in iteration 21 : 0.3740021969198797
Loss in iteration 22 : 0.36743390893610056
Loss in iteration 23 : 0.369392217085529
Loss in iteration 24 : 0.3759370253392161
Loss in iteration 25 : 0.3826970903661612
Loss in iteration 26 : 0.38671831720603367
Loss in iteration 27 : 0.3868049459682327
Loss in iteration 28 : 0.3831438301681796
Loss in iteration 29 : 0.37703417781484494
Loss in iteration 30 : 0.3703465904281429
Loss in iteration 31 : 0.3650286922115662
Loss in iteration 32 : 0.36224776355575505
Loss in iteration 33 : 0.3621931613841204
Loss in iteration 34 : 0.36410268609522745
Loss in iteration 35 : 0.3666589823607748
Loss in iteration 36 : 0.3684411736259668
Loss in iteration 37 : 0.3686632807548034
Loss in iteration 38 : 0.3673168848178726
Loss in iteration 39 : 0.36493455565203664
Loss in iteration 40 : 0.36224886584368027
Loss in iteration 41 : 0.3601409191320168
Loss in iteration 42 : 0.35912430198827866
Loss in iteration 43 : 0.3590228675175955
Loss in iteration 44 : 0.359528113768308
Loss in iteration 45 : 0.36016633702090906
Loss in iteration 46 : 0.36047840045767743
Loss in iteration 47 : 0.3601805179540384
Loss in iteration 48 : 0.3593023340376814
Loss in iteration 49 : 0.3581026061114777
Loss in iteration 50 : 0.35700072045430087
Loss in iteration 51 : 0.35622219165442304
Loss in iteration 52 : 0.35584109262011693
Loss in iteration 53 : 0.3558660403334513
Loss in iteration 54 : 0.356068815963566
Loss in iteration 55 : 0.3561903552381409
Loss in iteration 56 : 0.356088126872996
Loss in iteration 57 : 0.3557352077591625
Loss in iteration 58 : 0.3552331459281485
Loss in iteration 59 : 0.3547135202025304
Loss in iteration 60 : 0.35432906881592796
Loss in iteration 61 : 0.3541362215989703
Loss in iteration 62 : 0.3541229086752972
Loss in iteration 63 : 0.3541736006510349
Loss in iteration 64 : 0.3541844240635317
Loss in iteration 65 : 0.35408571601330363
Loss in iteration 66 : 0.353884352279887
Loss in iteration 67 : 0.35363493647307725
Loss in iteration 68 : 0.35341125527208805
Loss in iteration 69 : 0.353263211936147
Loss in iteration 70 : 0.35321219960651584
Loss in iteration 71 : 0.35321606490128665
Loss in iteration 72 : 0.3532196011187894
Loss in iteration 73 : 0.35318871428953347
Loss in iteration 74 : 0.35311414681761516
Loss in iteration 75 : 0.35300391678529197
Loss in iteration 76 : 0.35288459726536825
Loss in iteration 77 : 0.3527924397242226
Loss in iteration 78 : 0.35274154853149775
Loss in iteration 79 : 0.35271861575163027
Loss in iteration 80 : 0.3527093292896028
Loss in iteration 81 : 0.3526971840502536
Loss in iteration 82 : 0.35266675567810957
Loss in iteration 83 : 0.3526119145672235
Loss in iteration 84 : 0.35254947059436675
Loss in iteration 85 : 0.35250250889756995
Loss in iteration 86 : 0.35247304277228103
Loss in iteration 87 : 0.35245624150413346
Loss in iteration 88 : 0.35244674290843875
Loss in iteration 89 : 0.35243465859180245
Loss in iteration 90 : 0.35241495917624044
Loss in iteration 91 : 0.35238638161631225
Loss in iteration 92 : 0.3523559378450243
Loss in iteration 93 : 0.35233078365456444
Loss in iteration 94 : 0.3523124505346839
Loss in iteration 95 : 0.352302586380577
Loss in iteration 96 : 0.3522959495265256
Loss in iteration 97 : 0.35228589780876374
Loss in iteration 98 : 0.35227079795725297
Loss in iteration 99 : 0.3522524983160628
Loss in iteration 100 : 0.35223476283911337
Loss in iteration 101 : 0.35222180305399897
Loss in iteration 102 : 0.352211923899929
Loss in iteration 103 : 0.3522034886555534
Loss in iteration 104 : 0.3521948046999101
Loss in iteration 105 : 0.35218478749135623
Loss in iteration 106 : 0.3521732619604744
Loss in iteration 107 : 0.35216106846261164
Loss in iteration 108 : 0.3521497259020606
Loss in iteration 109 : 0.35214107799005223
Loss in iteration 110 : 0.3521336152428979
Loss in iteration 111 : 0.35212548006943606
Loss in iteration 112 : 0.3521161426438756
Loss in iteration 113 : 0.3521058284816778
Loss in iteration 114 : 0.3520969479745419
Loss in iteration 115 : 0.3520894697744429
Loss in iteration 116 : 0.3520829342545045
Loss in iteration 117 : 0.35207525628560343
Loss in iteration 118 : 0.35206649625538416
Loss in iteration 119 : 0.3520586246735944
Loss in iteration 120 : 0.35205173118634336
Loss in iteration 121 : 0.35204482235596624
Loss in iteration 122 : 0.35203756390060054
Loss in iteration 123 : 0.352029782325124
Loss in iteration 124 : 0.35202241052387095
Loss in iteration 125 : 0.3520152917073778
Loss in iteration 126 : 0.3520085578351018
Loss in iteration 127 : 0.35200213888872073
Loss in iteration 128 : 0.3519957092524368
Loss in iteration 129 : 0.35198890597270255
Loss in iteration 130 : 0.3519824202042526
Loss in iteration 131 : 0.3519763322163823
Loss in iteration 132 : 0.3519703166924011
Loss in iteration 133 : 0.35196422035307534
Loss in iteration 134 : 0.3519582114019697
Loss in iteration 135 : 0.35195225086569715
Loss in iteration 136 : 0.3519466855159326
Loss in iteration 137 : 0.3519410629905692
Loss in iteration 138 : 0.3519353052382257
Loss in iteration 139 : 0.3519297851104965
Loss in iteration 140 : 0.35192427840547513
Loss in iteration 141 : 0.3519186889428132
Loss in iteration 142 : 0.35191319078118466
Loss in iteration 143 : 0.351907796917343
Loss in iteration 144 : 0.3519024657697678
Loss in iteration 145 : 0.35189722487598957
Loss in iteration 146 : 0.3518920053824229
Loss in iteration 147 : 0.35188679194937555
Loss in iteration 148 : 0.35188156149912153
Loss in iteration 149 : 0.3518763064047532
Loss in iteration 150 : 0.35187107131023665
Loss in iteration 151 : 0.351866069592037
Loss in iteration 152 : 0.3518610184842984
Loss in iteration 153 : 0.35185606629299204
Loss in iteration 154 : 0.35185108951804095
Loss in iteration 155 : 0.3518461689812511
Loss in iteration 156 : 0.3518411222836738
Loss in iteration 157 : 0.35183626833379245
Loss in iteration 158 : 0.3518314315204456
Loss in iteration 159 : 0.3518266926432254
Loss in iteration 160 : 0.3518218844796443
Loss in iteration 161 : 0.35181711514762193
Loss in iteration 162 : 0.3518123284669367
Loss in iteration 163 : 0.3518076001256139
Loss in iteration 164 : 0.3518029837316676
Loss in iteration 165 : 0.35179842426383956
Loss in iteration 166 : 0.3517938156741548
Loss in iteration 167 : 0.35178928220628475
Loss in iteration 168 : 0.35178469565855863
Loss in iteration 169 : 0.35178025220223175
Loss in iteration 170 : 0.35177586220583595
Loss in iteration 171 : 0.35177138899250715
Loss in iteration 172 : 0.35176702737358295
Loss in iteration 173 : 0.3517627585839579
Loss in iteration 174 : 0.3517585155459146
Loss in iteration 175 : 0.3517544340208259
Loss in iteration 176 : 0.35175019299466653
Loss in iteration 177 : 0.3517462472663826
Loss in iteration 178 : 0.3517421609634834
Loss in iteration 179 : 0.3517380382038729
Loss in iteration 180 : 0.35173428667162193
Loss in iteration 181 : 0.35173017639725696
Loss in iteration 182 : 0.351726295747582
Loss in iteration 183 : 0.35172247190992295
Loss in iteration 184 : 0.35171840046003994
Loss in iteration 185 : 0.35171467834820336
Loss in iteration 186 : 0.35171093223562416
Loss in iteration 187 : 0.35170687249672555
Loss in iteration 188 : 0.35170346027712596
Loss in iteration 189 : 0.3516994067329347
Loss in iteration 190 : 0.3516957983846601
Loss in iteration 191 : 0.3516923103527753
Loss in iteration 192 : 0.3516884964293574
Loss in iteration 193 : 0.35168468109969186
Loss in iteration 194 : 0.3516810214473499
Loss in iteration 195 : 0.3516774000968129
Loss in iteration 196 : 0.3516736019845293
Loss in iteration 197 : 0.3516701001460334
Loss in iteration 198 : 0.3516665495410433
Loss in iteration 199 : 0.3516628544296362
Loss in iteration 200 : 0.3516590921104666
Testing accuracy  of updater 7 on alg 1 with rate 0.14 = 0.8498249493274369, training accuracy 0.8496928746928747, time elapsed: 3183 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9431312775094001
Loss in iteration 3 : 0.8351869555093793
Loss in iteration 4 : 0.6812723853486072
Loss in iteration 5 : 0.48932302865246496
Loss in iteration 6 : 0.5011357285408764
Loss in iteration 7 : 0.5490663232671394
Loss in iteration 8 : 0.5768481915375628
Loss in iteration 9 : 0.5851186343995898
Loss in iteration 10 : 0.5760182816990136
Loss in iteration 11 : 0.5523825766277195
Loss in iteration 12 : 0.5181284207158943
Loss in iteration 13 : 0.4777151264197681
Loss in iteration 14 : 0.43664996310482623
Loss in iteration 15 : 0.40225692094658333
Loss in iteration 16 : 0.3836707281891371
Loss in iteration 17 : 0.38283091854635964
Loss in iteration 18 : 0.39361104073374126
Loss in iteration 19 : 0.40696991871977184
Loss in iteration 20 : 0.41624963539848964
Loss in iteration 21 : 0.4185892492066997
Loss in iteration 22 : 0.41375804911315406
Loss in iteration 23 : 0.40340689280340125
Loss in iteration 24 : 0.39053967976791215
Loss in iteration 25 : 0.3782405638553517
Loss in iteration 26 : 0.36921796080559127
Loss in iteration 27 : 0.36485599090112697
Loss in iteration 28 : 0.36448522902731323
Loss in iteration 29 : 0.3666756518793527
Loss in iteration 30 : 0.3698095422863928
Loss in iteration 31 : 0.3725197449198314
Loss in iteration 32 : 0.3738530205867037
Loss in iteration 33 : 0.37349319103597134
Loss in iteration 34 : 0.3715828532917029
Loss in iteration 35 : 0.3686485736635834
Loss in iteration 36 : 0.3653473844900913
Loss in iteration 37 : 0.3622479256875729
Loss in iteration 38 : 0.35991116257662653
Loss in iteration 39 : 0.35863368270983764
Loss in iteration 40 : 0.35833659991483896
Loss in iteration 41 : 0.3588130776402117
Loss in iteration 42 : 0.3596204768397257
Loss in iteration 43 : 0.36047448783188035
Loss in iteration 44 : 0.36095265431213347
Loss in iteration 45 : 0.36087752971169157
Loss in iteration 46 : 0.36029311038392303
Loss in iteration 47 : 0.35934838805000197
Loss in iteration 48 : 0.35827242355762057
Loss in iteration 49 : 0.3572853857131402
Loss in iteration 50 : 0.3566469827879997
Loss in iteration 51 : 0.35629492038584337
Loss in iteration 52 : 0.35621978891949085
Loss in iteration 53 : 0.356359291207538
Loss in iteration 54 : 0.3565785475503084
Loss in iteration 55 : 0.3567381081172324
Loss in iteration 56 : 0.3567652366858767
Loss in iteration 57 : 0.3566334806997452
Loss in iteration 58 : 0.3563655158710412
Loss in iteration 59 : 0.3560224423529944
Loss in iteration 60 : 0.35566196034667935
Loss in iteration 61 : 0.3553493901211693
Loss in iteration 62 : 0.35513645851178766
Loss in iteration 63 : 0.35501515446818604
Loss in iteration 64 : 0.35497743350320077
Loss in iteration 65 : 0.35498795995665977
Loss in iteration 66 : 0.3550177477679765
Loss in iteration 67 : 0.3550186055110178
Loss in iteration 68 : 0.3549749965495245
Loss in iteration 69 : 0.3548842226795928
Loss in iteration 70 : 0.3547560755929449
Loss in iteration 71 : 0.3546094899911348
Loss in iteration 72 : 0.3544761318087982
Loss in iteration 73 : 0.3543647589897867
Loss in iteration 74 : 0.35428730033857414
Loss in iteration 75 : 0.35424603142813
Loss in iteration 76 : 0.35422017649153803
Loss in iteration 77 : 0.35420043859826156
Loss in iteration 78 : 0.35417921805089886
Loss in iteration 79 : 0.35414506279662317
Loss in iteration 80 : 0.35409490138584393
Loss in iteration 81 : 0.3540307845233038
Loss in iteration 82 : 0.35396130550899735
Loss in iteration 83 : 0.35389532557188225
Loss in iteration 84 : 0.35383705557548023
Loss in iteration 85 : 0.35378981373540264
Loss in iteration 86 : 0.3537631675946646
Loss in iteration 87 : 0.3537425424520861
Loss in iteration 88 : 0.3537215390315364
Loss in iteration 89 : 0.3536955509422093
Loss in iteration 90 : 0.3536627933303498
Loss in iteration 91 : 0.35362471906620846
Loss in iteration 92 : 0.35358438664793984
Loss in iteration 93 : 0.3535441538926326
Loss in iteration 94 : 0.3535088216525209
Loss in iteration 95 : 0.35347930252019877
Loss in iteration 96 : 0.3534541422942054
Loss in iteration 97 : 0.3534322045165513
Loss in iteration 98 : 0.3534101639481259
Loss in iteration 99 : 0.35338685470287545
Loss in iteration 100 : 0.35336135247839423
Loss in iteration 101 : 0.3533341113632415
Loss in iteration 102 : 0.35330627453647856
Loss in iteration 103 : 0.3532799398337761
Loss in iteration 104 : 0.35325526233121873
Loss in iteration 105 : 0.3532325776705236
Loss in iteration 106 : 0.35321135733982695
Loss in iteration 107 : 0.3531908211447451
Loss in iteration 108 : 0.35317028531315886
Loss in iteration 109 : 0.35314954879853516
Loss in iteration 110 : 0.3531284926348268
Loss in iteration 111 : 0.3531071318472496
Loss in iteration 112 : 0.3530858198385307
Loss in iteration 113 : 0.35306478568716376
Loss in iteration 114 : 0.35304507875427815
Loss in iteration 115 : 0.35302579851760685
Loss in iteration 116 : 0.3530068146609726
Loss in iteration 117 : 0.3529886249627781
Loss in iteration 118 : 0.35297080016866134
Loss in iteration 119 : 0.3529529386977106
Loss in iteration 120 : 0.35293485265164715
Loss in iteration 121 : 0.35291687696274315
Loss in iteration 122 : 0.35289985023659937
Loss in iteration 123 : 0.3528833125159822
Loss in iteration 124 : 0.3528670938160607
Loss in iteration 125 : 0.35285123987285383
Loss in iteration 126 : 0.35283513596582017
Loss in iteration 127 : 0.3528190932582928
Loss in iteration 128 : 0.35280352768612366
Loss in iteration 129 : 0.352788485018441
Loss in iteration 130 : 0.35277359972066835
Loss in iteration 131 : 0.35275890953448985
Loss in iteration 132 : 0.35274430245032223
Loss in iteration 133 : 0.3527299189242107
Loss in iteration 134 : 0.3527157476922853
Loss in iteration 135 : 0.35270182412633266
Loss in iteration 136 : 0.3526882511725334
Loss in iteration 137 : 0.3526750596244575
Loss in iteration 138 : 0.352661909399135
Loss in iteration 139 : 0.3526493852706798
Loss in iteration 140 : 0.35263726701411835
Loss in iteration 141 : 0.35262534612389096
Loss in iteration 142 : 0.35261367949747807
Loss in iteration 143 : 0.3526024324280377
Loss in iteration 144 : 0.3525913375817309
Loss in iteration 145 : 0.352580521159066
Loss in iteration 146 : 0.35256987050961214
Loss in iteration 147 : 0.35255933494173597
Loss in iteration 148 : 0.35254911014844276
Loss in iteration 149 : 0.35253911547383676
Loss in iteration 150 : 0.3525292068198483
Loss in iteration 151 : 0.3525193519940218
Loss in iteration 152 : 0.35250973559412246
Loss in iteration 153 : 0.3525002184720998
Loss in iteration 154 : 0.3524909367861871
Loss in iteration 155 : 0.3524817039984616
Loss in iteration 156 : 0.3524727058063519
Loss in iteration 157 : 0.35246384443210904
Loss in iteration 158 : 0.35245493004592415
Loss in iteration 159 : 0.3524461106113603
Loss in iteration 160 : 0.35243745183016023
Loss in iteration 161 : 0.352429008587242
Loss in iteration 162 : 0.3524207100878177
Loss in iteration 163 : 0.35241257574162166
Loss in iteration 164 : 0.35240451366574993
Loss in iteration 165 : 0.3523965960987804
Loss in iteration 166 : 0.3523888020465159
Loss in iteration 167 : 0.3523812144528871
Loss in iteration 168 : 0.35237373757928686
Loss in iteration 169 : 0.3523664635401355
Loss in iteration 170 : 0.3523592717589808
Loss in iteration 171 : 0.3523521836670271
Loss in iteration 172 : 0.3523452355394765
Loss in iteration 173 : 0.35233841048871084
Loss in iteration 174 : 0.3523315488440822
Testing accuracy  of updater 7 on alg 1 with rate 0.08000000000000002 = 0.8495792641729624, training accuracy 0.8483108108108108, time elapsed: 2715 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9857475061134248
Loss in iteration 3 : 0.9587364910259777
Loss in iteration 4 : 0.920242266532749
Loss in iteration 5 : 0.871412636678789
Loss in iteration 6 : 0.8132806461821316
Loss in iteration 7 : 0.746776054247567
Loss in iteration 8 : 0.6727356612089572
Loss in iteration 9 : 0.5919126026965252
Loss in iteration 10 : 0.5049847145577736
Loss in iteration 11 : 0.46956736397443644
Loss in iteration 12 : 0.48458797521838687
Loss in iteration 13 : 0.5043572793546551
Loss in iteration 14 : 0.5197508428483698
Loss in iteration 15 : 0.5300283472156364
Loss in iteration 16 : 0.5353509841944347
Loss in iteration 17 : 0.5360951213333568
Loss in iteration 18 : 0.5327923196274257
Loss in iteration 19 : 0.5260018102491413
Loss in iteration 20 : 0.5163761044416691
Loss in iteration 21 : 0.504554419648254
Loss in iteration 22 : 0.49119494781414663
Loss in iteration 23 : 0.4769880779982192
Loss in iteration 24 : 0.46270975277722637
Loss in iteration 25 : 0.4489836009567942
Loss in iteration 26 : 0.4367458621216932
Loss in iteration 27 : 0.42686724813321003
Loss in iteration 28 : 0.4194664447622396
Loss in iteration 29 : 0.41447491438119394
Loss in iteration 30 : 0.41168367144769247
Loss in iteration 31 : 0.4104267152083031
Loss in iteration 32 : 0.4099579903288962
Loss in iteration 33 : 0.40981606165450085
Loss in iteration 34 : 0.4097149344717151
Loss in iteration 35 : 0.40940989888017676
Loss in iteration 36 : 0.40875486327552585
Loss in iteration 37 : 0.4076489971726339
Loss in iteration 38 : 0.40609718824758867
Loss in iteration 39 : 0.40412919623883026
Loss in iteration 40 : 0.40180459998849194
Loss in iteration 41 : 0.39922751416616886
Loss in iteration 42 : 0.396536872140036
Loss in iteration 43 : 0.3938837892225783
Loss in iteration 44 : 0.3913422050308921
Loss in iteration 45 : 0.3890330747811626
Loss in iteration 46 : 0.38701781395482404
Loss in iteration 47 : 0.3853164787925935
Loss in iteration 48 : 0.3839220601291472
Loss in iteration 49 : 0.3828390513717669
Loss in iteration 50 : 0.3819770660747873
Loss in iteration 51 : 0.381292733942702
Loss in iteration 52 : 0.3807165755852237
Loss in iteration 53 : 0.38020621026447776
Loss in iteration 54 : 0.37972242721267657
Loss in iteration 55 : 0.3792238070711189
Loss in iteration 56 : 0.378693426854213
Loss in iteration 57 : 0.3781280988729286
Loss in iteration 58 : 0.3775298787272107
Loss in iteration 59 : 0.37690516192885
Loss in iteration 60 : 0.37626338144268273
Loss in iteration 61 : 0.3756136217607551
Loss in iteration 62 : 0.3749672648286134
Loss in iteration 63 : 0.37433574596995756
Loss in iteration 64 : 0.3737337675430268
Loss in iteration 65 : 0.37316428684020536
Loss in iteration 66 : 0.37264080050774856
Loss in iteration 67 : 0.3721609924146303
Loss in iteration 68 : 0.371725643178658
Loss in iteration 69 : 0.3713298006701734
Loss in iteration 70 : 0.37096755665153736
Loss in iteration 71 : 0.37063543370015756
Loss in iteration 72 : 0.3703251522807917
Loss in iteration 73 : 0.37003036326338534
Loss in iteration 74 : 0.36974686114785466
Loss in iteration 75 : 0.3694677832865094
Loss in iteration 76 : 0.36919277772405135
Loss in iteration 77 : 0.36892308481255887
Loss in iteration 78 : 0.3686555519887844
Loss in iteration 79 : 0.36839216334329733
Loss in iteration 80 : 0.36813262946809755
Loss in iteration 81 : 0.36787665824589705
Loss in iteration 82 : 0.3676253241842622
Loss in iteration 83 : 0.36737851521327003
Loss in iteration 84 : 0.36713666759993796
Loss in iteration 85 : 0.36690101039729645
Loss in iteration 86 : 0.3666724447031251
Loss in iteration 87 : 0.36645238432973604
Loss in iteration 88 : 0.36624135068633595
Loss in iteration 89 : 0.36603787956241884
Loss in iteration 90 : 0.3658425152146028
Loss in iteration 91 : 0.36565505810966287
Loss in iteration 92 : 0.36547396402568916
Loss in iteration 93 : 0.36529938098727943
Loss in iteration 94 : 0.36512973483414385
Loss in iteration 95 : 0.3649646896719521
Loss in iteration 96 : 0.3648030143168589
Loss in iteration 97 : 0.3646439568790826
Loss in iteration 98 : 0.3644879284317551
Loss in iteration 99 : 0.3643347485101826
Loss in iteration 100 : 0.36418547510111565
Loss in iteration 101 : 0.3640390513903454
Loss in iteration 102 : 0.3638952945763394
Loss in iteration 103 : 0.36375445493121056
Loss in iteration 104 : 0.3636152434390531
Loss in iteration 105 : 0.3634780505727335
Loss in iteration 106 : 0.36334254246367553
Loss in iteration 107 : 0.36320902982814596
Loss in iteration 108 : 0.3630782552567018
Loss in iteration 109 : 0.36295029898652964
Loss in iteration 110 : 0.3628245626964703
Loss in iteration 111 : 0.36270061704388806
Loss in iteration 112 : 0.36257832401607265
Loss in iteration 113 : 0.36245777009059726
Loss in iteration 114 : 0.3623405711085822
Loss in iteration 115 : 0.36222608630163955
Loss in iteration 116 : 0.36211412067431337
Loss in iteration 117 : 0.36200452093987295
Loss in iteration 118 : 0.3618977407379686
Loss in iteration 119 : 0.3617928938490416
Loss in iteration 120 : 0.3616898226497499
Loss in iteration 121 : 0.3615885349621501
Loss in iteration 122 : 0.36148898385955536
Loss in iteration 123 : 0.36139098404476533
Loss in iteration 124 : 0.36129474422721614
Loss in iteration 125 : 0.3612001093017063
Loss in iteration 126 : 0.361107286562363
Loss in iteration 127 : 0.361016371687531
Loss in iteration 128 : 0.3609270170115201
Loss in iteration 129 : 0.36083880254151895
Loss in iteration 130 : 0.3607521750378009
Loss in iteration 131 : 0.3606671359372118
Loss in iteration 132 : 0.36058347518137823
Loss in iteration 133 : 0.36050093088402124
Loss in iteration 134 : 0.36041951175128484
Loss in iteration 135 : 0.36033958216776896
Loss in iteration 136 : 0.36026132390894583
Loss in iteration 137 : 0.3601842927528113
Loss in iteration 138 : 0.36010808041278053
Loss in iteration 139 : 0.3600328157078131
Loss in iteration 140 : 0.3599585585951205
Loss in iteration 141 : 0.35988532916432303
Loss in iteration 142 : 0.3598133840155503
Loss in iteration 143 : 0.3597424573106727
Loss in iteration 144 : 0.3596725243548956
Loss in iteration 145 : 0.35960351400523205
Loss in iteration 146 : 0.35953548809263614
Loss in iteration 147 : 0.35946852989078226
Loss in iteration 148 : 0.3594023761943484
Loss in iteration 149 : 0.359336882917749
Loss in iteration 150 : 0.35927235812963687
Loss in iteration 151 : 0.35920880866409827
Loss in iteration 152 : 0.359145961500957
Loss in iteration 153 : 0.35908384589234443
Loss in iteration 154 : 0.35902234078504186
Loss in iteration 155 : 0.3589614653156642
Loss in iteration 156 : 0.35890112544145875
Loss in iteration 157 : 0.3588412944340766
Loss in iteration 158 : 0.35878202321065567
Loss in iteration 159 : 0.3587236324866205
Loss in iteration 160 : 0.35866643131979487
Loss in iteration 161 : 0.3586097759686788
Loss in iteration 162 : 0.3585539711506881
Loss in iteration 163 : 0.35849903663815136
Loss in iteration 164 : 0.35844483826692664
Loss in iteration 165 : 0.35839163547619013
Loss in iteration 166 : 0.35833891152483205
Loss in iteration 167 : 0.35828683065356143
Loss in iteration 168 : 0.35823547744704937
Loss in iteration 169 : 0.35818481301494326
Loss in iteration 170 : 0.35813478881217314
Loss in iteration 171 : 0.3580853467368409
Loss in iteration 172 : 0.35803642010467035
Loss in iteration 173 : 0.357988023490956
Loss in iteration 174 : 0.35794031423632794
Loss in iteration 175 : 0.35789308486182386
Loss in iteration 176 : 0.3578464301640255
Loss in iteration 177 : 0.3578002283819647
Loss in iteration 178 : 0.3577550800514698
Loss in iteration 179 : 0.3577106841625422
Loss in iteration 180 : 0.357666897710632
Loss in iteration 181 : 0.357623677465956
Loss in iteration 182 : 0.3575813409412779
Loss in iteration 183 : 0.357539640284232
Loss in iteration 184 : 0.35749840993223425
Loss in iteration 185 : 0.3574575931218358
Loss in iteration 186 : 0.35741722654075914
Loss in iteration 187 : 0.3573771405802288
Loss in iteration 188 : 0.3573374736641968
Loss in iteration 189 : 0.3572982339862782
Loss in iteration 190 : 0.35725936217014104
Loss in iteration 191 : 0.35722083309807007
Loss in iteration 192 : 0.3571828507588066
Loss in iteration 193 : 0.357145216038414
Loss in iteration 194 : 0.3571079868927159
Loss in iteration 195 : 0.3570711387885564
Loss in iteration 196 : 0.3570349203532184
Loss in iteration 197 : 0.3569989464035121
Loss in iteration 198 : 0.356963217951722
Loss in iteration 199 : 0.3569279536930734
Loss in iteration 200 : 0.35689303808467365
Testing accuracy  of updater 7 on alg 1 with rate 0.01999999999999999 = 0.8469995700509797, training accuracy 0.8463452088452088, time elapsed: 3183 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 7.675451768520838
Loss in iteration 3 : 5.271777378676827
Loss in iteration 4 : 2.5471211438901515
Loss in iteration 5 : 1.3902852633527785
Loss in iteration 6 : 1.736632362273488
Loss in iteration 7 : 1.8665764586719065
Loss in iteration 8 : 1.700625126788882
Loss in iteration 9 : 1.5748219568388364
Loss in iteration 10 : 1.5211686214067253
Loss in iteration 11 : 1.4895115489222905
Loss in iteration 12 : 1.4460599872675628
Loss in iteration 13 : 1.3868872824975251
Loss in iteration 14 : 1.3259785431748166
Loss in iteration 15 : 1.2759383588970181
Loss in iteration 16 : 1.2321662750832911
Loss in iteration 17 : 1.1867588200670633
Loss in iteration 18 : 1.1418999383791242
Loss in iteration 19 : 1.0941604778049638
Loss in iteration 20 : 1.0393402122883266
Loss in iteration 21 : 0.9777959926665042
Loss in iteration 22 : 0.911485359092043
Loss in iteration 23 : 0.8457061459177034
Loss in iteration 24 : 0.7856328065833375
Loss in iteration 25 : 0.7393156934884935
Loss in iteration 26 : 0.707730031720856
Loss in iteration 27 : 0.6911509581977662
Loss in iteration 28 : 0.7459528307112628
Loss in iteration 29 : 0.9920125027422338
Loss in iteration 30 : 1.3433545584488789
Loss in iteration 31 : 0.7030671518485053
Loss in iteration 32 : 0.6643469223069774
Loss in iteration 33 : 0.6562043201582538
Loss in iteration 34 : 0.8166321285007327
Loss in iteration 35 : 0.8828738232782978
Loss in iteration 36 : 1.0512714099447305
Loss in iteration 37 : 0.6541045285359389
Loss in iteration 38 : 0.5885081430618563
Loss in iteration 39 : 0.5536779527154579
Loss in iteration 40 : 0.5403643989179494
Loss in iteration 41 : 0.5827334932946389
Loss in iteration 42 : 0.8465696004963821
Loss in iteration 43 : 0.9588610717815059
Loss in iteration 44 : 1.3879679934385734
Loss in iteration 45 : 0.5280465454217456
Loss in iteration 46 : 0.5842321907447158
Loss in iteration 47 : 0.5565960328203591
Loss in iteration 48 : 0.5496009913200617
Loss in iteration 49 : 0.609001054284514
Loss in iteration 50 : 0.693744476827289
Loss in iteration 51 : 0.9813843653626296
Loss in iteration 52 : 0.612507936717506
Loss in iteration 53 : 0.6886463709852635
Loss in iteration 54 : 0.6756042837594477
Loss in iteration 55 : 0.7882538295614827
Loss in iteration 56 : 0.6001823585581428
Loss in iteration 57 : 0.6021984188656682
Loss in iteration 58 : 0.5726879865621333
Loss in iteration 59 : 0.6475960770374082
Loss in iteration 60 : 0.6711674841657733
Loss in iteration 61 : 0.9052447724198582
Loss in iteration 62 : 0.593745888318871
Loss in iteration 63 : 0.616354030154819
Loss in iteration 64 : 0.6021009873560901
Loss in iteration 65 : 0.6984064082641446
Loss in iteration 66 : 0.6282193503973772
Loss in iteration 67 : 0.7549917805923334
Loss in iteration 68 : 0.6112064756339136
Loss in iteration 69 : 0.684096502450709
Loss in iteration 70 : 0.6096371593351242
Loss in iteration 71 : 0.6954656847178874
Loss in iteration 72 : 0.6039335912928732
Loss in iteration 73 : 0.685519601948259
Loss in iteration 74 : 0.6125473557388744
Loss in iteration 75 : 0.7240640289451215
Loss in iteration 76 : 0.6029121845057694
Loss in iteration 77 : 0.6893158619911469
Loss in iteration 78 : 0.6108575939027243
Loss in iteration 79 : 0.7201221515472299
Loss in iteration 80 : 0.5980285898875274
Loss in iteration 81 : 0.6705473877100602
Loss in iteration 82 : 0.6044336198064058
Loss in iteration 83 : 0.7081298189033088
Loss in iteration 84 : 0.5956840391243949
Loss in iteration 85 : 0.6770487705353626
Loss in iteration 86 : 0.6043135283429599
Loss in iteration 87 : 0.7005609170103134
Loss in iteration 88 : 0.5970780165168968
Loss in iteration 89 : 0.6749473902463119
Loss in iteration 90 : 0.598048185793954
Loss in iteration 91 : 0.6909186101146674
Loss in iteration 92 : 0.5946979094768455
Loss in iteration 93 : 0.6753853383849066
Loss in iteration 94 : 0.5981907452301957
Loss in iteration 95 : 0.6968333930060732
Loss in iteration 96 : 0.5930852934587024
Loss in iteration 97 : 0.6712117258710086
Loss in iteration 98 : 0.594428541299797
Loss in iteration 99 : 0.6875103144770481
Loss in iteration 100 : 0.5933869469458958
Loss in iteration 101 : 0.6809079104787299
Loss in iteration 102 : 0.5926938927412881
Loss in iteration 103 : 0.6816893091718214
Loss in iteration 104 : 0.5905611105616441
Loss in iteration 105 : 0.6752289127247497
Loss in iteration 106 : 0.5913879684731749
Loss in iteration 107 : 0.6875474056543553
Loss in iteration 108 : 0.5910630787545363
Loss in iteration 109 : 0.6757352783546495
Loss in iteration 110 : 0.58826653976151
Loss in iteration 111 : 0.6735062821295112
Loss in iteration 112 : 0.5893874352984891
Loss in iteration 113 : 0.6834452321762189
Loss in iteration 114 : 0.5906246658411728
Loss in iteration 115 : 0.6848524619261666
Loss in iteration 116 : 0.586887549983588
Loss in iteration 117 : 0.6664734121800776
Loss in iteration 118 : 0.586950833534043
Loss in iteration 119 : 0.6788925252902787
Loss in iteration 120 : 0.5879528085265268
Loss in iteration 121 : 0.6846460055913117
Loss in iteration 122 : 0.5859832213067571
Loss in iteration 123 : 0.6673086407648587
Loss in iteration 124 : 0.5856602800785676
Loss in iteration 125 : 0.6758061543782348
Loss in iteration 126 : 0.5860067166086448
Loss in iteration 127 : 0.6803364383862904
Loss in iteration 128 : 0.5860570711305336
Loss in iteration 129 : 0.673897481938661
Loss in iteration 130 : 0.5834703349219175
Loss in iteration 131 : 0.6642165270171965
Loss in iteration 132 : 0.5863962829974875
Loss in iteration 133 : 0.6889129416190996
Loss in iteration 134 : 0.5818021116286162
Loss in iteration 135 : 0.6587759149057875
Loss in iteration 136 : 0.5849358514322638
Loss in iteration 137 : 0.6831988910304968
Loss in iteration 138 : 0.5822713058802055
Loss in iteration 139 : 0.668349562143208
Loss in iteration 140 : 0.5875325510096432
Loss in iteration 141 : 0.688814235476297
Loss in iteration 142 : 0.5773597085756281
Loss in iteration 143 : 0.6430713785906332
Loss in iteration 144 : 0.5833609407924925
Loss in iteration 145 : 0.6873228418502945
Loss in iteration 146 : 0.5818899530661947
Loss in iteration 147 : 0.6721795541634855
Loss in iteration 148 : 0.5844965360215052
Loss in iteration 149 : 0.6797367571173123
Loss in iteration 150 : 0.5787033742654738
Loss in iteration 151 : 0.6533837809526175
Loss in iteration 152 : 0.5825240450909698
Loss in iteration 153 : 0.6851090109057891
Loss in iteration 154 : 0.5786065984737855
Loss in iteration 155 : 0.6602413874273466
Loss in iteration 156 : 0.584145270877433
Loss in iteration 157 : 0.6869548085567649
Loss in iteration 158 : 0.5752709058827318
Loss in iteration 159 : 0.6466850646836516
Loss in iteration 160 : 0.5834591474789831
Loss in iteration 161 : 0.6914647804049151
Loss in iteration 162 : 0.5771263531796741
Loss in iteration 163 : 0.6582714213766708
Loss in iteration 164 : 0.5840783240741164
Loss in iteration 165 : 0.6831698269105015
Loss in iteration 166 : 0.5729369031364063
Loss in iteration 167 : 0.6443139648225021
Loss in iteration 168 : 0.5841789263273
Loss in iteration 169 : 0.6996251327142243
Loss in iteration 170 : 0.5759511753924373
Loss in iteration 171 : 0.6520791894889801
Loss in iteration 172 : 0.5805724506241633
Loss in iteration 173 : 0.6741785678739591
Loss in iteration 174 : 0.5770322188367498
Loss in iteration 175 : 0.6582544098762296
Loss in iteration 176 : 0.5819970662525963
Loss in iteration 177 : 0.6825842942931201
Loss in iteration 178 : 0.5751507992081328
Loss in iteration 179 : 0.6497118046131956
Loss in iteration 180 : 0.5821753411394269
Loss in iteration 181 : 0.6907415582763214
Loss in iteration 182 : 0.5735795793154721
Loss in iteration 183 : 0.6458331722095249
Loss in iteration 184 : 0.5811392329901408
Loss in iteration 185 : 0.6866823491827423
Loss in iteration 186 : 0.5744643380533647
Loss in iteration 187 : 0.6523813648622411
Loss in iteration 188 : 0.5808561174422783
Loss in iteration 189 : 0.6833065593862727
Loss in iteration 190 : 0.5731769643233535
Loss in iteration 191 : 0.6487786487843386
Loss in iteration 192 : 0.5807353768980609
Loss in iteration 193 : 0.6883661403616603
Loss in iteration 194 : 0.574030675608884
Loss in iteration 195 : 0.6489913290786121
Loss in iteration 196 : 0.5783254007289284
Loss in iteration 197 : 0.679742065936793
Loss in iteration 198 : 0.5721992625468656
Loss in iteration 199 : 0.6539139318765762
Loss in iteration 200 : 0.582546738406937
Testing accuracy  of updater 8 on alg 1 with rate 2.0 = 0.8196056753270684, training accuracy 0.8163697788697789, time elapsed: 3085 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.5282340329077484
Loss in iteration 3 : 2.297035296179491
Loss in iteration 4 : 1.499335242490414
Loss in iteration 5 : 0.9572607557800801
Loss in iteration 6 : 0.9961545292245473
Loss in iteration 7 : 1.1253762451822817
Loss in iteration 8 : 1.1339489356971342
Loss in iteration 9 : 1.0877271286077874
Loss in iteration 10 : 1.0586589050461213
Loss in iteration 11 : 1.0451465929040398
Loss in iteration 12 : 1.029323406071075
Loss in iteration 13 : 1.001393380826459
Loss in iteration 14 : 0.9659265956315901
Loss in iteration 15 : 0.9297351407024758
Loss in iteration 16 : 0.8951334819469371
Loss in iteration 17 : 0.8612982243388975
Loss in iteration 18 : 0.8271242581801336
Loss in iteration 19 : 0.7949203856124933
Loss in iteration 20 : 0.7637479274849969
Loss in iteration 21 : 0.7327109034613629
Loss in iteration 22 : 0.7027308391584093
Loss in iteration 23 : 0.6738804709531501
Loss in iteration 24 : 0.6456774928324225
Loss in iteration 25 : 0.6160739285937074
Loss in iteration 26 : 0.5834144533362796
Loss in iteration 27 : 0.5494515471771794
Loss in iteration 28 : 0.517419033623248
Loss in iteration 29 : 0.4899993700335455
Loss in iteration 30 : 0.4670425361017295
Loss in iteration 31 : 0.45213082579182107
Loss in iteration 32 : 0.4650303262336099
Loss in iteration 33 : 0.643979219573172
Loss in iteration 34 : 1.1119389291198776
Loss in iteration 35 : 0.45794306633758664
Loss in iteration 36 : 0.5271159779824371
Loss in iteration 37 : 0.5371690219069305
Loss in iteration 38 : 0.5593760886316502
Loss in iteration 39 : 0.5856340058179887
Loss in iteration 40 : 0.5316734115601245
Loss in iteration 41 : 0.5399532786746962
Loss in iteration 42 : 0.5043982039872379
Loss in iteration 43 : 0.5159966829706575
Loss in iteration 44 : 0.4916452180503662
Loss in iteration 45 : 0.5239791484964099
Loss in iteration 46 : 0.5001925084248027
Loss in iteration 47 : 0.560470752373739
Loss in iteration 48 : 0.5091750539300434
Loss in iteration 49 : 0.5608348984083528
Loss in iteration 50 : 0.5024636556583376
Loss in iteration 51 : 0.5265328376523357
Loss in iteration 52 : 0.4889413283912525
Loss in iteration 53 : 0.5049734903607346
Loss in iteration 54 : 0.47946678009517296
Loss in iteration 55 : 0.5094049097510657
Loss in iteration 56 : 0.49791364233076596
Loss in iteration 57 : 0.5678500670586178
Loss in iteration 58 : 0.5120130634472451
Loss in iteration 59 : 0.5674067693796039
Loss in iteration 60 : 0.4894047564684162
Loss in iteration 61 : 0.5009726487259283
Loss in iteration 62 : 0.4769691529570101
Loss in iteration 63 : 0.507429919043101
Loss in iteration 64 : 0.490943870671192
Loss in iteration 65 : 0.5755521239436238
Loss in iteration 66 : 0.4979072535310046
Loss in iteration 67 : 0.573723168521894
Loss in iteration 68 : 0.489402428191057
Loss in iteration 69 : 0.5180565557783593
Loss in iteration 70 : 0.47961313730195526
Loss in iteration 71 : 0.5154049550292067
Loss in iteration 72 : 0.48023461965819025
Loss in iteration 73 : 0.5374345493528524
Loss in iteration 74 : 0.4860384811382502
Loss in iteration 75 : 0.5535937719454007
Loss in iteration 76 : 0.4966900665874321
Loss in iteration 77 : 0.5573401234389636
Loss in iteration 78 : 0.48733410941705163
Loss in iteration 79 : 0.5143081086402891
Loss in iteration 80 : 0.4777942901980595
Loss in iteration 81 : 0.5128632165610314
Loss in iteration 82 : 0.4840711952734091
Loss in iteration 83 : 0.5483614857831751
Loss in iteration 84 : 0.49873243681583407
Loss in iteration 85 : 0.5723199189131194
Loss in iteration 86 : 0.4891876623777474
Loss in iteration 87 : 0.5164289812677619
Loss in iteration 88 : 0.4797941255635316
Loss in iteration 89 : 0.506480119911909
Loss in iteration 90 : 0.48071516889661686
Loss in iteration 91 : 0.5404192946973274
Loss in iteration 92 : 0.4955667455905742
Loss in iteration 93 : 0.5752285152329742
Loss in iteration 94 : 0.4884529075964791
Loss in iteration 95 : 0.5240801709539085
Loss in iteration 96 : 0.4791818497231373
Loss in iteration 97 : 0.5099625918859105
Loss in iteration 98 : 0.48241831332063556
Loss in iteration 99 : 0.5422053347454963
Loss in iteration 100 : 0.49123142072697473
Loss in iteration 101 : 0.5614202116056647
Loss in iteration 102 : 0.49162749143208595
Loss in iteration 103 : 0.5350025028112424
Loss in iteration 104 : 0.47851705696213265
Loss in iteration 105 : 0.505620682089334
Loss in iteration 106 : 0.4778691226294326
Loss in iteration 107 : 0.5335237726520476
Loss in iteration 108 : 0.49060761335046676
Loss in iteration 109 : 0.5711849004505567
Loss in iteration 110 : 0.49167505010037454
Loss in iteration 111 : 0.5392995135627539
Loss in iteration 112 : 0.4813167232091777
Loss in iteration 113 : 0.5091015164599463
Loss in iteration 114 : 0.480072881387474
Loss in iteration 115 : 0.5292894244498028
Loss in iteration 116 : 0.4836004701518048
Loss in iteration 117 : 0.5515682334050032
Loss in iteration 118 : 0.4971396986350588
Loss in iteration 119 : 0.5649749817989086
Loss in iteration 120 : 0.4861487344617623
Loss in iteration 121 : 0.5075681138664082
Loss in iteration 122 : 0.4733860527474376
Loss in iteration 123 : 0.5054722422329516
Loss in iteration 124 : 0.4845716543697543
Loss in iteration 125 : 0.5609967923785355
Loss in iteration 126 : 0.49658853093460886
Loss in iteration 127 : 0.567169178964611
Loss in iteration 128 : 0.4872608800261518
Loss in iteration 129 : 0.5163184027334052
Loss in iteration 130 : 0.47781830456980545
Loss in iteration 131 : 0.5106559775057263
Loss in iteration 132 : 0.4822635953391351
Loss in iteration 133 : 0.5414630347807408
Loss in iteration 134 : 0.4919959412000486
Loss in iteration 135 : 0.5664833216490213
Loss in iteration 136 : 0.48981234972982235
Loss in iteration 137 : 0.5288395402296626
Loss in iteration 138 : 0.47729765691498893
Loss in iteration 139 : 0.5073980149399768
Loss in iteration 140 : 0.48087393627122704
Loss in iteration 141 : 0.5385049062221592
Loss in iteration 142 : 0.4923566876575693
Loss in iteration 143 : 0.5704983775013261
Loss in iteration 144 : 0.4881208491803377
Loss in iteration 145 : 0.5270147702246093
Loss in iteration 146 : 0.4783208515552518
Loss in iteration 147 : 0.509080251758828
Loss in iteration 148 : 0.4831084254460002
Loss in iteration 149 : 0.5448742365036995
Loss in iteration 150 : 0.4906997942654545
Loss in iteration 151 : 0.5567799253653566
Loss in iteration 152 : 0.48947065142074964
Loss in iteration 153 : 0.5299590131590134
Loss in iteration 154 : 0.4757447392446905
Loss in iteration 155 : 0.5067332270827312
Loss in iteration 156 : 0.4823268248877445
Loss in iteration 157 : 0.546660929761016
Loss in iteration 158 : 0.4942131754439681
Loss in iteration 159 : 0.5691523237657287
Loss in iteration 160 : 0.48655044508686646
Loss in iteration 161 : 0.5161965577168947
Loss in iteration 162 : 0.47845261772070385
Loss in iteration 163 : 0.5127433141079291
Loss in iteration 164 : 0.4824138127138188
Loss in iteration 165 : 0.5423094682648758
Loss in iteration 166 : 0.4907764600326309
Loss in iteration 167 : 0.56349933834819
Loss in iteration 168 : 0.4893661026249142
Loss in iteration 169 : 0.5279661451662512
Loss in iteration 170 : 0.47631070136483705
Loss in iteration 171 : 0.5070790705968443
Loss in iteration 172 : 0.48176076782368155
Loss in iteration 173 : 0.546007884190689
Loss in iteration 174 : 0.49381001801361635
Loss in iteration 175 : 0.5660594343172259
Loss in iteration 176 : 0.4870807479982663
Loss in iteration 177 : 0.5189117714764044
Loss in iteration 178 : 0.47828913358110214
Loss in iteration 179 : 0.5106012505067333
Loss in iteration 180 : 0.4811899592185361
Loss in iteration 181 : 0.540282005776413
Loss in iteration 182 : 0.49166086283402854
Loss in iteration 183 : 0.569355159087749
Loss in iteration 184 : 0.4881973743334225
Loss in iteration 185 : 0.5277427168606968
Loss in iteration 186 : 0.47703380087015756
Loss in iteration 187 : 0.5045430236603101
Loss in iteration 188 : 0.4785914465232857
Loss in iteration 189 : 0.5393031237153698
Loss in iteration 190 : 0.4932152953514396
Loss in iteration 191 : 0.5735812825247782
Loss in iteration 192 : 0.4871098486609237
Loss in iteration 193 : 0.5209006462257826
Loss in iteration 194 : 0.48040108292096456
Loss in iteration 195 : 0.517117649817743
Loss in iteration 196 : 0.4809201212017964
Loss in iteration 197 : 0.5335640643965378
Loss in iteration 198 : 0.4857827440290428
Loss in iteration 199 : 0.5556946107153269
Loss in iteration 200 : 0.49484016014114346
Testing accuracy  of updater 8 on alg 1 with rate 1.4000000000000001 = 0.8234137952214238, training accuracy 0.821529484029484, time elapsed: 3251 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.6082563683111968
Loss in iteration 3 : 1.4686090641503056
Loss in iteration 4 : 0.9886028748071575
Loss in iteration 5 : 0.6627040264103563
Loss in iteration 6 : 0.6951653437943641
Loss in iteration 7 : 0.7671606403046136
Loss in iteration 8 : 0.7732853512806641
Loss in iteration 9 : 0.747564896429925
Loss in iteration 10 : 0.7311219389075305
Loss in iteration 11 : 0.7231711094221239
Loss in iteration 12 : 0.7150475587432796
Loss in iteration 13 : 0.7003446786668612
Loss in iteration 14 : 0.6809831121534003
Loss in iteration 15 : 0.6606018975063128
Loss in iteration 16 : 0.6410350049110822
Loss in iteration 17 : 0.6226803926052471
Loss in iteration 18 : 0.6044066726107314
Loss in iteration 19 : 0.5867938696133271
Loss in iteration 20 : 0.5698139913932342
Loss in iteration 21 : 0.553968070035369
Loss in iteration 22 : 0.5383727416685296
Loss in iteration 23 : 0.5230983143321902
Loss in iteration 24 : 0.5077209682415337
Loss in iteration 25 : 0.49160621802005117
Loss in iteration 26 : 0.474569955189718
Loss in iteration 27 : 0.45709162013961807
Loss in iteration 28 : 0.440277895836408
Loss in iteration 29 : 0.4254228888537521
Loss in iteration 30 : 0.4135696594201588
Loss in iteration 31 : 0.4042956533384171
Loss in iteration 32 : 0.397490116612329
Loss in iteration 33 : 0.39262235844571847
Loss in iteration 34 : 0.389676493535354
Loss in iteration 35 : 0.38930618645736814
Loss in iteration 36 : 0.39887502408679226
Loss in iteration 37 : 0.42867974779620116
Loss in iteration 38 : 0.4792309910150708
Loss in iteration 39 : 0.5752427381649149
Loss in iteration 40 : 0.4084018171064547
Loss in iteration 41 : 0.39263090034090364
Loss in iteration 42 : 0.3913097149098169
Loss in iteration 43 : 0.38604588836949305
Loss in iteration 44 : 0.38259464122040043
Loss in iteration 45 : 0.37965718263750625
Loss in iteration 46 : 0.3766147505028561
Loss in iteration 47 : 0.3744393740071914
Loss in iteration 48 : 0.37265573030369975
Loss in iteration 49 : 0.3725835994481114
Loss in iteration 50 : 0.3757040167406633
Loss in iteration 51 : 0.38799647901001283
Loss in iteration 52 : 0.40136634370834523
Loss in iteration 53 : 0.43634226295812556
Loss in iteration 54 : 0.4116696739035965
Loss in iteration 55 : 0.42078714393841266
Loss in iteration 56 : 0.39601179145531284
Loss in iteration 57 : 0.38752353473428663
Loss in iteration 58 : 0.3778068249646359
Loss in iteration 59 : 0.3719767262872703
Loss in iteration 60 : 0.3701154700787062
Loss in iteration 61 : 0.3694018417526382
Loss in iteration 62 : 0.3720404572237999
Loss in iteration 63 : 0.38205767337378405
Loss in iteration 64 : 0.3888407148275191
Loss in iteration 65 : 0.42406131984738465
Loss in iteration 66 : 0.40684360339388603
Loss in iteration 67 : 0.4337493968243376
Loss in iteration 68 : 0.3947695664249504
Loss in iteration 69 : 0.3938659446568496
Loss in iteration 70 : 0.3816271341491079
Loss in iteration 71 : 0.37701034560162455
Loss in iteration 72 : 0.37124487647421117
Loss in iteration 73 : 0.3715109838311695
Loss in iteration 74 : 0.37194532668583064
Loss in iteration 75 : 0.3813686265004394
Loss in iteration 76 : 0.38468902384797005
Loss in iteration 77 : 0.4134231865814651
Loss in iteration 78 : 0.40584283969613305
Loss in iteration 79 : 0.43641042953793513
Loss in iteration 80 : 0.3941749826311538
Loss in iteration 81 : 0.3950625665749
Loss in iteration 82 : 0.38573924261466036
Loss in iteration 83 : 0.38249979523853406
Loss in iteration 84 : 0.37445778159417176
Loss in iteration 85 : 0.3736402775974162
Loss in iteration 86 : 0.3720024890219671
Loss in iteration 87 : 0.3790178281097507
Loss in iteration 88 : 0.38515288630830397
Loss in iteration 89 : 0.4148687227644171
Loss in iteration 90 : 0.4044356953759315
Loss in iteration 91 : 0.43416663432804703
Loss in iteration 92 : 0.3955780733294885
Loss in iteration 93 : 0.39715452186083333
Loss in iteration 94 : 0.38620871003118284
Loss in iteration 95 : 0.3834656593205242
Loss in iteration 96 : 0.3762685554861259
Loss in iteration 97 : 0.37594024236295787
Loss in iteration 98 : 0.3747166349334841
Loss in iteration 99 : 0.3843078735025528
Loss in iteration 100 : 0.3871619018985287
Loss in iteration 101 : 0.41475752883941736
Loss in iteration 102 : 0.4033829251784712
Loss in iteration 103 : 0.4314577354836182
Loss in iteration 104 : 0.3960782627432184
Loss in iteration 105 : 0.39948521876163245
Loss in iteration 106 : 0.38724971053187146
Loss in iteration 107 : 0.38581712277475166
Loss in iteration 108 : 0.3770814623560665
Loss in iteration 109 : 0.37722086969410934
Loss in iteration 110 : 0.3759511341512886
Loss in iteration 111 : 0.3853587395011194
Loss in iteration 112 : 0.3885035816868316
Loss in iteration 113 : 0.41663328916618464
Loss in iteration 114 : 0.4045775605301985
Loss in iteration 115 : 0.43238083210280337
Loss in iteration 116 : 0.39586327570085295
Loss in iteration 117 : 0.39898880642938067
Loss in iteration 118 : 0.38770955183070716
Loss in iteration 119 : 0.3865325299135932
Loss in iteration 120 : 0.37847482097529106
Loss in iteration 121 : 0.37888785983782586
Loss in iteration 122 : 0.37684618517657253
Loss in iteration 123 : 0.38624942361061687
Loss in iteration 124 : 0.38907127547110154
Loss in iteration 125 : 0.41832294240358603
Loss in iteration 126 : 0.40530273553860285
Loss in iteration 127 : 0.43358206332125515
Loss in iteration 128 : 0.39695961386065665
Loss in iteration 129 : 0.399750613513282
Loss in iteration 130 : 0.3884570407176129
Loss in iteration 131 : 0.38773413831298736
Loss in iteration 132 : 0.3795005841091076
Loss in iteration 133 : 0.3804867389935918
Loss in iteration 134 : 0.37829980950104825
Loss in iteration 135 : 0.388658935639968
Loss in iteration 136 : 0.39014841085956886
Loss in iteration 137 : 0.41854168482545556
Loss in iteration 138 : 0.4054950079082007
Loss in iteration 139 : 0.4344547825174645
Loss in iteration 140 : 0.39722162100499014
Loss in iteration 141 : 0.39963999638566716
Loss in iteration 142 : 0.38875329047833435
Loss in iteration 143 : 0.38813509319184364
Loss in iteration 144 : 0.3800696484579843
Loss in iteration 145 : 0.3809370261584473
Loss in iteration 146 : 0.3799888666049595
Loss in iteration 147 : 0.3910685655661345
Loss in iteration 148 : 0.3925105217733895
Loss in iteration 149 : 0.4228315337936122
Loss in iteration 150 : 0.4054295474645045
Loss in iteration 151 : 0.43109321911888937
Loss in iteration 152 : 0.39823201005409575
Loss in iteration 153 : 0.4018312476711897
Loss in iteration 154 : 0.3892895317642591
Loss in iteration 155 : 0.3883969998181135
Loss in iteration 156 : 0.3804245484656919
Loss in iteration 157 : 0.38171382881196525
Loss in iteration 158 : 0.3796506935885278
Loss in iteration 159 : 0.39115258929918967
Loss in iteration 160 : 0.39436007721532296
Loss in iteration 161 : 0.4269657441812415
Loss in iteration 162 : 0.4058441586875979
Loss in iteration 163 : 0.4310952119745779
Loss in iteration 164 : 0.3982811100730335
Loss in iteration 165 : 0.4015840021636043
Loss in iteration 166 : 0.38939574384419745
Loss in iteration 167 : 0.3892120295836281
Loss in iteration 168 : 0.3808657829953786
Loss in iteration 169 : 0.38353854383895497
Loss in iteration 170 : 0.3825532029772835
Loss in iteration 171 : 0.3961005969812585
Loss in iteration 172 : 0.3971484806915452
Loss in iteration 173 : 0.43037776497705493
Loss in iteration 174 : 0.4039127957664521
Loss in iteration 175 : 0.42483215733007645
Loss in iteration 176 : 0.39899680794412995
Loss in iteration 177 : 0.4027143871223455
Loss in iteration 178 : 0.39043346724566935
Loss in iteration 179 : 0.3897286021267366
Loss in iteration 180 : 0.38159545906044073
Loss in iteration 181 : 0.3842816361246371
Loss in iteration 182 : 0.3831262272007869
Loss in iteration 183 : 0.3982775287280446
Loss in iteration 184 : 0.3986230609924827
Loss in iteration 185 : 0.4328231032456248
Loss in iteration 186 : 0.4045459636062649
Loss in iteration 187 : 0.42440261387318023
Loss in iteration 188 : 0.39955286655899136
Loss in iteration 189 : 0.4028809844617251
Loss in iteration 190 : 0.39086988130080114
Loss in iteration 191 : 0.3901184193303228
Loss in iteration 192 : 0.3822657808252432
Loss in iteration 193 : 0.3855012169081271
Loss in iteration 194 : 0.38484019944986786
Loss in iteration 195 : 0.4001149016127775
Loss in iteration 196 : 0.40060394803702565
Loss in iteration 197 : 0.436375900088835
Loss in iteration 198 : 0.4044997066731755
Loss in iteration 199 : 0.42217555845241944
Loss in iteration 200 : 0.39977568471309954
Testing accuracy  of updater 8 on alg 1 with rate 0.8 = 0.839199066396413, training accuracy 0.8375614250614251, time elapsed: 3238 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.61114167454827
Loss in iteration 3 : 0.5683465965782621
Loss in iteration 4 : 0.4589596936776047
Loss in iteration 5 : 0.3845476727124039
Loss in iteration 6 : 0.3872473088741523
Loss in iteration 7 : 0.3958114341493372
Loss in iteration 8 : 0.3949801213536384
Loss in iteration 9 : 0.3901191300443118
Loss in iteration 10 : 0.38764752359951987
Loss in iteration 11 : 0.3876520674197855
Loss in iteration 12 : 0.3879657234527211
Loss in iteration 13 : 0.38729964302681
Loss in iteration 14 : 0.38564041880694316
Loss in iteration 15 : 0.3835253107533457
Loss in iteration 16 : 0.38150589935143886
Loss in iteration 17 : 0.3798525951675936
Loss in iteration 18 : 0.3783957533816605
Loss in iteration 19 : 0.3769621291725823
Loss in iteration 20 : 0.37566745039768684
Loss in iteration 21 : 0.3744431384461372
Loss in iteration 22 : 0.3732109453473667
Loss in iteration 23 : 0.3719224927087539
Loss in iteration 24 : 0.37050213162217877
Loss in iteration 25 : 0.36894238881334857
Loss in iteration 26 : 0.3672804215521679
Loss in iteration 27 : 0.3655785716456541
Loss in iteration 28 : 0.3638813253930251
Loss in iteration 29 : 0.3622936198160786
Loss in iteration 30 : 0.3608218862724234
Loss in iteration 31 : 0.35950890064015995
Loss in iteration 32 : 0.35839946105425796
Loss in iteration 33 : 0.3574824813642293
Loss in iteration 34 : 0.35674422434784175
Loss in iteration 35 : 0.35616756500501706
Loss in iteration 36 : 0.35572888352538673
Loss in iteration 37 : 0.3553704648227113
Loss in iteration 38 : 0.3550882650972421
Loss in iteration 39 : 0.354863685729543
Loss in iteration 40 : 0.35465151863309163
Loss in iteration 41 : 0.3544394938187572
Loss in iteration 42 : 0.354234689250909
Loss in iteration 43 : 0.35403148137811247
Loss in iteration 44 : 0.35383748270079485
Loss in iteration 45 : 0.3536641077548579
Loss in iteration 46 : 0.3535127867018284
Loss in iteration 47 : 0.3533653983916909
Loss in iteration 48 : 0.3532205027896263
Loss in iteration 49 : 0.35308547467506135
Loss in iteration 50 : 0.35296159829493406
Loss in iteration 51 : 0.3528016483951037
Loss in iteration 52 : 0.3526695946061796
Loss in iteration 53 : 0.3525369426830892
Loss in iteration 54 : 0.3524094187366264
Loss in iteration 55 : 0.35227827866191025
Loss in iteration 56 : 0.35216145778808866
Loss in iteration 57 : 0.35205279535606493
Loss in iteration 58 : 0.35195393855618573
Loss in iteration 59 : 0.351864658631421
Loss in iteration 60 : 0.3517815119047609
Loss in iteration 61 : 0.3517027487306513
Loss in iteration 62 : 0.3516311513297797
Loss in iteration 63 : 0.35156931597510965
Loss in iteration 64 : 0.351513599717232
Loss in iteration 65 : 0.35146466556103007
Loss in iteration 66 : 0.35142145972016325
Loss in iteration 67 : 0.35138427286350915
Loss in iteration 68 : 0.3513508072098861
Loss in iteration 69 : 0.35132046882443557
Loss in iteration 70 : 0.35129620570919556
Loss in iteration 71 : 0.3512764447940191
Loss in iteration 72 : 0.3512588983683631
Loss in iteration 73 : 0.35124494606369067
Loss in iteration 74 : 0.3512333581236726
Loss in iteration 75 : 0.35122337655552177
Loss in iteration 76 : 0.35121304599638287
Loss in iteration 77 : 0.35120560677848056
Loss in iteration 78 : 0.35119813481863554
Loss in iteration 79 : 0.35119648398419123
Loss in iteration 80 : 0.3512010243759751
Loss in iteration 81 : 0.35120525343322695
Loss in iteration 82 : 0.35117986266281825
Loss in iteration 83 : 0.3511709148605729
Loss in iteration 84 : 0.3511662375894805
Loss in iteration 85 : 0.3511639912772369
Loss in iteration 86 : 0.3511673761242133
Loss in iteration 87 : 0.35115796091372764
Loss in iteration 88 : 0.35114686260181754
Loss in iteration 89 : 0.35113996219391935
Loss in iteration 90 : 0.3511328387086921
Loss in iteration 91 : 0.35112628283083813
Loss in iteration 92 : 0.351120915849422
Loss in iteration 93 : 0.351114605032111
Loss in iteration 94 : 0.35111000418293975
Loss in iteration 95 : 0.35110817366567404
Loss in iteration 96 : 0.35110652587931884
Loss in iteration 97 : 0.3511022024802153
Loss in iteration 98 : 0.35109734845432855
Loss in iteration 99 : 0.35111211974199774
Loss in iteration 100 : 0.351085305443175
Loss in iteration 101 : 0.35107803265314086
Loss in iteration 102 : 0.3510731561343203
Loss in iteration 103 : 0.3510678498228965
Loss in iteration 104 : 0.35106414729280977
Loss in iteration 105 : 0.3510601269965665
Loss in iteration 106 : 0.3510585552113267
Loss in iteration 107 : 0.3510573133066691
Loss in iteration 108 : 0.351052504066525
Loss in iteration 109 : 0.35104836614381163
Loss in iteration 110 : 0.3510424692379711
Loss in iteration 111 : 0.3510382001877529
Loss in iteration 112 : 0.35103471860285257
Loss in iteration 113 : 0.3510313743160839
Loss in iteration 114 : 0.3510299896048497
Loss in iteration 115 : 0.3510301281818262
Loss in iteration 116 : 0.3510295496299055
Loss in iteration 117 : 0.35103794161305957
Loss in iteration 118 : 0.35103656388535287
Loss in iteration 119 : 0.3510582274828224
Loss in iteration 120 : 0.3510309189770473
Loss in iteration 121 : 0.35101956236321313
Loss in iteration 122 : 0.35101475447577857
Loss in iteration 123 : 0.35101092089169
Loss in iteration 124 : 0.3510090928975277
Loss in iteration 125 : 0.35100534212583623
Loss in iteration 126 : 0.351003198027572
Loss in iteration 127 : 0.3510018647065827
Loss in iteration 128 : 0.35100707357683714
Loss in iteration 129 : 0.3510073434937115
Loss in iteration 130 : 0.35101024938545533
Loss in iteration 131 : 0.35101550167647594
Loss in iteration 132 : 0.35100507828984906
Loss in iteration 133 : 0.35099457091264386
Loss in iteration 134 : 0.35099132661662963
Loss in iteration 135 : 0.35098834250177763
Loss in iteration 136 : 0.35098609842316014
Loss in iteration 137 : 0.3509837152676548
Loss in iteration 138 : 0.35098346465832575
Loss in iteration 139 : 0.350985498874064
Loss in iteration 140 : 0.35099089488227936
Loss in iteration 141 : 0.35101859963192356
Loss in iteration 142 : 0.35103617941677523
Loss in iteration 143 : 0.35098584751099404
Loss in iteration 144 : 0.35097672555800186
Loss in iteration 145 : 0.3509741491664432
Loss in iteration 146 : 0.35097227542658793
Loss in iteration 147 : 0.3509717498781817
Loss in iteration 148 : 0.35097266627921453
Loss in iteration 149 : 0.3509776068688744
Loss in iteration 150 : 0.3509844402847191
Loss in iteration 151 : 0.3510014219285284
Loss in iteration 152 : 0.3509777799764169
Loss in iteration 153 : 0.3509715210385808
Loss in iteration 154 : 0.3509663960742554
Loss in iteration 155 : 0.35096276537977616
Loss in iteration 156 : 0.3509601731662106
Loss in iteration 157 : 0.35095835293047783
Loss in iteration 158 : 0.35095846046887874
Loss in iteration 159 : 0.35095936905477876
Loss in iteration 160 : 0.3509798721977146
Loss in iteration 161 : 0.35097846830744456
Loss in iteration 162 : 0.35100190354783894
Loss in iteration 163 : 0.35097485244716065
Loss in iteration 164 : 0.3509639931028336
Loss in iteration 165 : 0.3509599397466305
Loss in iteration 166 : 0.35095426192915635
Loss in iteration 167 : 0.35095234573405204
Loss in iteration 168 : 0.35095018275190504
Loss in iteration 169 : 0.35094830570540786
Loss in iteration 170 : 0.35094760763425814
Loss in iteration 171 : 0.3509451052497362
Testing accuracy  of updater 8 on alg 1 with rate 0.2 = 0.8497021067501996, training accuracy 0.8498771498771499, time elapsed: 2636 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5240051234496211
Loss in iteration 3 : 0.49215164705355224
Loss in iteration 4 : 0.42718589899172643
Loss in iteration 5 : 0.3831314403835193
Loss in iteration 6 : 0.3741674320900499
Loss in iteration 7 : 0.37442159449906603
Loss in iteration 8 : 0.37206761699573476
Loss in iteration 9 : 0.36907361297114183
Loss in iteration 10 : 0.3681221398159502
Loss in iteration 11 : 0.3685122420168261
Loss in iteration 12 : 0.36903887879604064
Loss in iteration 13 : 0.36910163149302594
Loss in iteration 14 : 0.3687132746381856
Loss in iteration 15 : 0.36815083301092816
Loss in iteration 16 : 0.3675711133812646
Loss in iteration 17 : 0.36709541414757774
Loss in iteration 18 : 0.3666091890156568
Loss in iteration 19 : 0.36611893194817147
Loss in iteration 20 : 0.36560776315369775
Loss in iteration 21 : 0.3650532514605146
Loss in iteration 22 : 0.36447355736378295
Loss in iteration 23 : 0.363799721585269
Loss in iteration 24 : 0.3630468172176183
Loss in iteration 25 : 0.36221973854803774
Loss in iteration 26 : 0.3613076783348063
Loss in iteration 27 : 0.36035684437836063
Loss in iteration 28 : 0.35939161397435554
Loss in iteration 29 : 0.35845849176279704
Loss in iteration 30 : 0.3576053101932944
Loss in iteration 31 : 0.3568441494963558
Loss in iteration 32 : 0.3561570907262632
Loss in iteration 33 : 0.3555576662772228
Loss in iteration 34 : 0.3550573111378543
Loss in iteration 35 : 0.35462914693318975
Loss in iteration 36 : 0.3542560758028534
Loss in iteration 37 : 0.3539552280709089
Loss in iteration 38 : 0.3537126587883931
Loss in iteration 39 : 0.35351053091824264
Loss in iteration 40 : 0.3533456700248319
Loss in iteration 41 : 0.3532137001536823
Loss in iteration 42 : 0.3531044430853913
Loss in iteration 43 : 0.35299628076182193
Loss in iteration 44 : 0.35288800409914123
Loss in iteration 45 : 0.35278039688294754
Loss in iteration 46 : 0.3526779131052915
Loss in iteration 47 : 0.3525871269674191
Loss in iteration 48 : 0.3525008746170366
Loss in iteration 49 : 0.35241954458205216
Loss in iteration 50 : 0.3523473682668811
Loss in iteration 51 : 0.3522728873256048
Loss in iteration 52 : 0.35219974111385105
Loss in iteration 53 : 0.3521280833067073
Loss in iteration 54 : 0.35205903396312666
Loss in iteration 55 : 0.351993631741021
Loss in iteration 56 : 0.3519328463759413
Loss in iteration 57 : 0.3518765597634145
Loss in iteration 58 : 0.3518170260456604
Loss in iteration 59 : 0.3517611230763333
Loss in iteration 60 : 0.35170799545790266
Loss in iteration 61 : 0.3516573487563973
Loss in iteration 62 : 0.35161372306673855
Loss in iteration 63 : 0.35157447806477043
Loss in iteration 64 : 0.3515387014705199
Loss in iteration 65 : 0.35150390314352326
Loss in iteration 66 : 0.3514726413275955
Loss in iteration 67 : 0.3514451228196767
Loss in iteration 68 : 0.35141962230500035
Loss in iteration 69 : 0.35139632724519243
Loss in iteration 70 : 0.35137619112654156
Loss in iteration 71 : 0.3513587369841596
Loss in iteration 72 : 0.3513427177624164
Loss in iteration 73 : 0.35132662898785133
Loss in iteration 74 : 0.35131284458507667
Loss in iteration 75 : 0.35130159451921367
Loss in iteration 76 : 0.3512930645808214
Loss in iteration 77 : 0.35128141589335143
Loss in iteration 78 : 0.3512708184848729
Loss in iteration 79 : 0.35126182986269555
Loss in iteration 80 : 0.3512532191130082
Loss in iteration 81 : 0.35124608460038453
Loss in iteration 82 : 0.35123989332456584
Loss in iteration 83 : 0.3512343668613484
Loss in iteration 84 : 0.3512277415575525
Loss in iteration 85 : 0.3512213404693378
Loss in iteration 86 : 0.3512158880914929
Loss in iteration 87 : 0.3512117031585227
Loss in iteration 88 : 0.3512069582198381
Loss in iteration 89 : 0.35120294895794824
Loss in iteration 90 : 0.35120007594570357
Loss in iteration 91 : 0.3511935214671367
Loss in iteration 92 : 0.351188466092885
Loss in iteration 93 : 0.35118475136734645
Loss in iteration 94 : 0.3511806949113025
Loss in iteration 95 : 0.3511793261541286
Loss in iteration 96 : 0.3511753858313403
Loss in iteration 97 : 0.3511711427363341
Loss in iteration 98 : 0.35116567070351357
Loss in iteration 99 : 0.3511605019444992
Loss in iteration 100 : 0.3511560592500102
Loss in iteration 101 : 0.3511523822482712
Loss in iteration 102 : 0.35114807031711
Loss in iteration 103 : 0.35114408406569264
Loss in iteration 104 : 0.351140531574334
Loss in iteration 105 : 0.3511359133439177
Loss in iteration 106 : 0.3511349385489046
Loss in iteration 107 : 0.3511332758137107
Loss in iteration 108 : 0.351130390146663
Loss in iteration 109 : 0.35113114040267807
Loss in iteration 110 : 0.3511408642085106
Loss in iteration 111 : 0.3511184637861334
Loss in iteration 112 : 0.351114292390754
Loss in iteration 113 : 0.35111050591628623
Loss in iteration 114 : 0.35110728343757747
Loss in iteration 115 : 0.3511039364317455
Loss in iteration 116 : 0.3511025211807609
Loss in iteration 117 : 0.3510995155081732
Loss in iteration 118 : 0.35109401489419423
Loss in iteration 119 : 0.35109045205471556
Loss in iteration 120 : 0.3510879106186954
Loss in iteration 121 : 0.35108649650832047
Loss in iteration 122 : 0.35108305822965263
Loss in iteration 123 : 0.35107877692603395
Loss in iteration 124 : 0.351074960550215
Loss in iteration 125 : 0.3510725110300427
Loss in iteration 126 : 0.3510695276141362
Loss in iteration 127 : 0.3510693461817888
Loss in iteration 128 : 0.3510690728277718
Loss in iteration 129 : 0.35106362657496326
Loss in iteration 130 : 0.35106147095396983
Loss in iteration 131 : 0.351056455197065
Loss in iteration 132 : 0.351053167352518
Loss in iteration 133 : 0.3510503865298171
Loss in iteration 134 : 0.35104780017820747
Loss in iteration 135 : 0.35104581669476026
Loss in iteration 136 : 0.35104735850641827
Loss in iteration 137 : 0.35104636007527235
Loss in iteration 138 : 0.3510481979320939
Loss in iteration 139 : 0.35104156082949106
Loss in iteration 140 : 0.3510353531955755
Loss in iteration 141 : 0.35103288863080784
Loss in iteration 142 : 0.3510303546175865
Loss in iteration 143 : 0.35102769487095253
Loss in iteration 144 : 0.3510268647947279
Loss in iteration 145 : 0.35102698082895584
Loss in iteration 146 : 0.3510235938033796
Loss in iteration 147 : 0.3510202526690408
Loss in iteration 148 : 0.35101889328507374
Loss in iteration 149 : 0.35101685915324143
Loss in iteration 150 : 0.35101858860334867
Loss in iteration 151 : 0.35102363636399675
Loss in iteration 152 : 0.3510180537686958
Loss in iteration 153 : 0.35103096923973626
Loss in iteration 154 : 0.3510167348332087
Loss in iteration 155 : 0.35102961913963854
Loss in iteration 156 : 0.35101102391828
Loss in iteration 157 : 0.35100744318241334
Loss in iteration 158 : 0.3510052132103539
Loss in iteration 159 : 0.35100293283936584
Loss in iteration 160 : 0.35100190235287904
Loss in iteration 161 : 0.3510010122922923
Loss in iteration 162 : 0.35100079491326625
Loss in iteration 163 : 0.35100511088400005
Loss in iteration 164 : 0.35100224803880414
Loss in iteration 165 : 0.35099772211302455
Loss in iteration 166 : 0.3509954777371786
Loss in iteration 167 : 0.3509960784772709
Loss in iteration 168 : 0.3510028509482644
Loss in iteration 169 : 0.3510116784444002
Loss in iteration 170 : 0.35099567154964195
Loss in iteration 171 : 0.35099353957180757
Loss in iteration 172 : 0.3509891790794263
Loss in iteration 173 : 0.35098644463852396
Loss in iteration 174 : 0.35098462724369367
Loss in iteration 175 : 0.3509827525883414
Loss in iteration 176 : 0.3509809175974271
Loss in iteration 177 : 0.3509797104230429
Loss in iteration 178 : 0.3509817418846248
Loss in iteration 179 : 0.3509847560625155
Loss in iteration 180 : 0.35099786807812916
Loss in iteration 181 : 0.35100157159351647
Loss in iteration 182 : 0.35097895531569556
Loss in iteration 183 : 0.350973587379468
Loss in iteration 184 : 0.35097357956416086
Loss in iteration 185 : 0.3509729318137697
Loss in iteration 186 : 0.3509759258441029
Loss in iteration 187 : 0.3509781298426862
Loss in iteration 188 : 0.35099647061201444
Loss in iteration 189 : 0.3509793705218558
Loss in iteration 190 : 0.35097201086923174
Loss in iteration 191 : 0.35096867148535404
Loss in iteration 192 : 0.35096677781120206
Loss in iteration 193 : 0.3509649659501735
Loss in iteration 194 : 0.3509631223472213
Loss in iteration 195 : 0.3509619652707866
Loss in iteration 196 : 0.35096285952127476
Loss in iteration 197 : 0.35096526158108804
Loss in iteration 198 : 0.35096378489166113
Loss in iteration 199 : 0.3509766458828087
Loss in iteration 200 : 0.35097406855576674
Testing accuracy  of updater 8 on alg 1 with rate 0.14 = 0.849640685461581, training accuracy 0.8499692874692875, time elapsed: 3137 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4588568586566021
Loss in iteration 3 : 0.491138353092808
Loss in iteration 4 : 0.4661046532489099
Loss in iteration 5 : 0.42363930281161594
Loss in iteration 6 : 0.3914678411517257
Loss in iteration 7 : 0.37619850128732363
Loss in iteration 8 : 0.3713849400357138
Loss in iteration 9 : 0.36834323544975145
Loss in iteration 10 : 0.3642855686182942
Loss in iteration 11 : 0.3608847549130157
Loss in iteration 12 : 0.359385603726672
Loss in iteration 13 : 0.3590074231525389
Loss in iteration 14 : 0.3590488372790548
Loss in iteration 15 : 0.35907850036515976
Loss in iteration 16 : 0.35898089302131064
Loss in iteration 17 : 0.3588156881123511
Loss in iteration 18 : 0.35864214694855356
Loss in iteration 19 : 0.35852175632110295
Loss in iteration 20 : 0.3584459827983619
Loss in iteration 21 : 0.35835429367710375
Loss in iteration 22 : 0.3582537211447465
Loss in iteration 23 : 0.35813293944504654
Loss in iteration 24 : 0.35798973269029155
Loss in iteration 25 : 0.35779710722255553
Loss in iteration 26 : 0.3575361886635999
Loss in iteration 27 : 0.3572114631194369
Loss in iteration 28 : 0.3568514128362797
Loss in iteration 29 : 0.3564612898503067
Loss in iteration 30 : 0.35606191510249163
Loss in iteration 31 : 0.35566036825905
Loss in iteration 32 : 0.35527111615723045
Loss in iteration 33 : 0.35489788179440934
Loss in iteration 34 : 0.3545351940906864
Loss in iteration 35 : 0.3542069616235507
Loss in iteration 36 : 0.3539101624398115
Loss in iteration 37 : 0.35364594475645017
Loss in iteration 38 : 0.3534071342003601
Loss in iteration 39 : 0.3531817662592586
Loss in iteration 40 : 0.35297756589251866
Loss in iteration 41 : 0.3527950220638502
Loss in iteration 42 : 0.35263687701499863
Loss in iteration 43 : 0.35250104314818187
Loss in iteration 44 : 0.3523934978624723
Loss in iteration 45 : 0.3523094594037227
Loss in iteration 46 : 0.35224554153516047
Loss in iteration 47 : 0.35219278763340345
Loss in iteration 48 : 0.35214562649912373
Loss in iteration 49 : 0.35210240919788593
Loss in iteration 50 : 0.3520634508025527
Loss in iteration 51 : 0.35202655995022897
Loss in iteration 52 : 0.351991009075599
Loss in iteration 53 : 0.3519559063500714
Loss in iteration 54 : 0.35192124868034314
Loss in iteration 55 : 0.351886897472146
Loss in iteration 56 : 0.35185456176141144
Loss in iteration 57 : 0.35182456896214004
Loss in iteration 58 : 0.35179750519878383
Loss in iteration 59 : 0.3517718210645732
Loss in iteration 60 : 0.3517478781054729
Loss in iteration 61 : 0.35172373135017315
Loss in iteration 62 : 0.3516999790890544
Loss in iteration 63 : 0.3516767769402407
Loss in iteration 64 : 0.3516555757791159
Loss in iteration 65 : 0.3516364785136985
Loss in iteration 66 : 0.3516191023660787
Loss in iteration 67 : 0.35160396186243353
Loss in iteration 68 : 0.3515888850135612
Loss in iteration 69 : 0.3515728440707841
Loss in iteration 70 : 0.35155895261552683
Loss in iteration 71 : 0.3515459596292526
Loss in iteration 72 : 0.35153370894960234
Loss in iteration 73 : 0.35152285076433526
Loss in iteration 74 : 0.3515131436682712
Loss in iteration 75 : 0.3515040810490095
Loss in iteration 76 : 0.351493627028979
Loss in iteration 77 : 0.35148486822859626
Loss in iteration 78 : 0.3514762195738849
Loss in iteration 79 : 0.3514677291362725
Loss in iteration 80 : 0.3514590031039016
Loss in iteration 81 : 0.3514508919690201
Loss in iteration 82 : 0.3514430778821236
Loss in iteration 83 : 0.35143531465442895
Loss in iteration 84 : 0.35142816605419
Loss in iteration 85 : 0.35142058993335473
Loss in iteration 86 : 0.35141333510138795
Loss in iteration 87 : 0.35140692773230114
Loss in iteration 88 : 0.3513990207217518
Loss in iteration 89 : 0.3513925979740966
Loss in iteration 90 : 0.35138538106470635
Loss in iteration 91 : 0.35137934127027814
Loss in iteration 92 : 0.3513728563309991
Loss in iteration 93 : 0.35136721412827726
Loss in iteration 94 : 0.3513616541281482
Loss in iteration 95 : 0.3513567720441429
Loss in iteration 96 : 0.35135020231133873
Loss in iteration 97 : 0.351344173366127
Loss in iteration 98 : 0.351338638350076
Loss in iteration 99 : 0.3513343819133637
Loss in iteration 100 : 0.35132977839244967
Loss in iteration 101 : 0.3513243532541951
Loss in iteration 102 : 0.35131863194975754
Loss in iteration 103 : 0.351313160546529
Loss in iteration 104 : 0.3513079664589865
Loss in iteration 105 : 0.3513029283897161
Loss in iteration 106 : 0.3512980815866244
Loss in iteration 107 : 0.3512934134136259
Loss in iteration 108 : 0.35128879535294344
Loss in iteration 109 : 0.3512846738496558
Loss in iteration 110 : 0.3512807274229872
Loss in iteration 111 : 0.3512764764166837
Loss in iteration 112 : 0.35127187565409623
Loss in iteration 113 : 0.3512684450324896
Loss in iteration 114 : 0.3512637702857887
Loss in iteration 115 : 0.3512600525436608
Loss in iteration 116 : 0.3512563193338991
Loss in iteration 117 : 0.3512522690069078
Loss in iteration 118 : 0.35124866952386274
Loss in iteration 119 : 0.3512451250254646
Loss in iteration 120 : 0.3512417751087096
Loss in iteration 121 : 0.3512389909624261
Loss in iteration 122 : 0.3512378087829499
Loss in iteration 123 : 0.35123505527123844
Loss in iteration 124 : 0.35122996363912906
Loss in iteration 125 : 0.3512271571562226
Loss in iteration 126 : 0.3512242696544756
Loss in iteration 127 : 0.35122097681617204
Loss in iteration 128 : 0.3512183753620715
Loss in iteration 129 : 0.3512157753659302
Loss in iteration 130 : 0.35121195286661944
Loss in iteration 131 : 0.35120976070347976
Loss in iteration 132 : 0.3512061559162005
Loss in iteration 133 : 0.3512036191057889
Loss in iteration 134 : 0.3512016014410597
Loss in iteration 135 : 0.3512019631772087
Loss in iteration 136 : 0.3511963997408073
Loss in iteration 137 : 0.3511927186058436
Loss in iteration 138 : 0.3511900310609959
Loss in iteration 139 : 0.3511874792612276
Loss in iteration 140 : 0.35118481624068854
Loss in iteration 141 : 0.3511834663873113
Loss in iteration 142 : 0.35118069703461186
Loss in iteration 143 : 0.3511779100008903
Loss in iteration 144 : 0.3511751085000648
Loss in iteration 145 : 0.3511724590073443
Loss in iteration 146 : 0.3511694367851518
Loss in iteration 147 : 0.35116652844714485
Loss in iteration 148 : 0.3511642569327501
Loss in iteration 149 : 0.3511620922085462
Loss in iteration 150 : 0.3511646952617537
Loss in iteration 151 : 0.3511580636868996
Loss in iteration 152 : 0.3511551684660475
Loss in iteration 153 : 0.3511533890555399
Loss in iteration 154 : 0.3511501471182368
Loss in iteration 155 : 0.3511472960944779
Loss in iteration 156 : 0.3511447374757751
Loss in iteration 157 : 0.35114245483993206
Loss in iteration 158 : 0.35114021852330596
Loss in iteration 159 : 0.35113782058579446
Loss in iteration 160 : 0.35113793295097556
Loss in iteration 161 : 0.3511362348205845
Loss in iteration 162 : 0.3511328663731904
Loss in iteration 163 : 0.35113012725259096
Loss in iteration 164 : 0.35112727954716355
Loss in iteration 165 : 0.35112448161315696
Loss in iteration 166 : 0.35112209570109526
Loss in iteration 167 : 0.35111999586197773
Loss in iteration 168 : 0.35111778321040965
Loss in iteration 169 : 0.35111636785060624
Loss in iteration 170 : 0.3511144386768324
Loss in iteration 171 : 0.3511113244371595
Loss in iteration 172 : 0.35110914243583324
Loss in iteration 173 : 0.3511079265988577
Loss in iteration 174 : 0.3511086186612682
Loss in iteration 175 : 0.3511035854166376
Loss in iteration 176 : 0.3511005813003203
Loss in iteration 177 : 0.3510983751276161
Loss in iteration 178 : 0.3510962196971417
Loss in iteration 179 : 0.3510957594988032
Loss in iteration 180 : 0.3510957661035558
Loss in iteration 181 : 0.35109131785514625
Loss in iteration 182 : 0.3510884710765277
Loss in iteration 183 : 0.35108624822836976
Loss in iteration 184 : 0.35108414480796457
Loss in iteration 185 : 0.3510832287789811
Loss in iteration 186 : 0.35108353462192726
Loss in iteration 187 : 0.3510796218045106
Loss in iteration 188 : 0.351078783644166
Loss in iteration 189 : 0.35107548650700904
Loss in iteration 190 : 0.35107336380502385
Loss in iteration 191 : 0.35107127303599184
Loss in iteration 192 : 0.35106981417556304
Loss in iteration 193 : 0.35106762068022906
Loss in iteration 194 : 0.3510655426977045
Loss in iteration 195 : 0.3510637466948682
Loss in iteration 196 : 0.3510636252913802
Loss in iteration 197 : 0.3510642970153612
Loss in iteration 198 : 0.35106089101728283
Loss in iteration 199 : 0.3510579592391319
Loss in iteration 200 : 0.35105590936992825
Testing accuracy  of updater 8 on alg 1 with rate 0.08000000000000002 = 0.8497635280388183, training accuracy 0.8498464373464374, time elapsed: 3133 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8039061034509898
Loss in iteration 3 : 0.5414228758907544
Loss in iteration 4 : 0.4641862893242307
Loss in iteration 5 : 0.4920947837358424
Loss in iteration 6 : 0.5064849313384241
Loss in iteration 7 : 0.5060987049517219
Loss in iteration 8 : 0.4935450680806945
Loss in iteration 9 : 0.47236007439508665
Loss in iteration 10 : 0.44693835608640653
Loss in iteration 11 : 0.422431514120344
Loss in iteration 12 : 0.4025870370745559
Loss in iteration 13 : 0.38928372834124814
Loss in iteration 14 : 0.38203187140463213
Loss in iteration 15 : 0.37972096321104176
Loss in iteration 16 : 0.3795028913951314
Loss in iteration 17 : 0.3793891044950798
Loss in iteration 18 : 0.37827563245550755
Loss in iteration 19 : 0.37578376678406705
Loss in iteration 20 : 0.37240346778133254
Loss in iteration 21 : 0.3688377262862927
Loss in iteration 22 : 0.3653806135811979
Loss in iteration 23 : 0.3625328120061875
Loss in iteration 24 : 0.36068130567982365
Loss in iteration 25 : 0.3597429768158902
Loss in iteration 26 : 0.35928604922499396
Loss in iteration 27 : 0.3590660213457974
Loss in iteration 28 : 0.35884859655981605
Loss in iteration 29 : 0.35855174258909694
Loss in iteration 30 : 0.3581375618728063
Loss in iteration 31 : 0.35761875361043566
Loss in iteration 32 : 0.3570345463109322
Loss in iteration 33 : 0.3564368966442424
Loss in iteration 34 : 0.35587714972385154
Loss in iteration 35 : 0.35539853815041794
Loss in iteration 36 : 0.3550305392210433
Loss in iteration 37 : 0.35473123987016414
Loss in iteration 38 : 0.35451679510496337
Loss in iteration 39 : 0.3543705159212156
Loss in iteration 40 : 0.3542463020708615
Loss in iteration 41 : 0.35412238643369026
Loss in iteration 42 : 0.3540044438290604
Loss in iteration 43 : 0.3538845323057367
Loss in iteration 44 : 0.3537629458775091
Loss in iteration 45 : 0.35364038160099714
Loss in iteration 46 : 0.3535348319812663
Loss in iteration 47 : 0.3534486924538397
Loss in iteration 48 : 0.353380716743515
Loss in iteration 49 : 0.35332417092320517
Loss in iteration 50 : 0.3532762813100245
Loss in iteration 51 : 0.35323307377405455
Loss in iteration 52 : 0.35318952896173467
Loss in iteration 53 : 0.35314511782931696
Loss in iteration 54 : 0.353100472892347
Loss in iteration 55 : 0.35305450110073666
Loss in iteration 56 : 0.3530078420111387
Loss in iteration 57 : 0.3529623608071575
Loss in iteration 58 : 0.352917815003824
Loss in iteration 59 : 0.35287526612646636
Loss in iteration 60 : 0.35283442744297305
Loss in iteration 61 : 0.3527956694854289
Loss in iteration 62 : 0.3527585504058054
Loss in iteration 63 : 0.35272312454506366
Loss in iteration 64 : 0.3526892701040657
Loss in iteration 65 : 0.3526573123143039
Loss in iteration 66 : 0.3526279480638283
Loss in iteration 67 : 0.35259949080675557
Loss in iteration 68 : 0.3525723621210748
Loss in iteration 69 : 0.35254720538243745
Loss in iteration 70 : 0.35252447690372185
Loss in iteration 71 : 0.3525022806512959
Loss in iteration 72 : 0.35248048743346044
Loss in iteration 73 : 0.35245930356423133
Loss in iteration 74 : 0.35243930765257264
Loss in iteration 75 : 0.3524201201724211
Loss in iteration 76 : 0.35240172090152677
Loss in iteration 77 : 0.3523842326036849
Loss in iteration 78 : 0.35236752180054953
Loss in iteration 79 : 0.35235129823939576
Loss in iteration 80 : 0.35233566521905413
Loss in iteration 81 : 0.352320887224796
Loss in iteration 82 : 0.3523068783020496
Loss in iteration 83 : 0.3522933432969525
Loss in iteration 84 : 0.35228052388104847
Loss in iteration 85 : 0.3522682459193811
Loss in iteration 86 : 0.35225681415207905
Loss in iteration 87 : 0.35224602209884626
Loss in iteration 88 : 0.3522357381709548
Loss in iteration 89 : 0.35222557613838323
Loss in iteration 90 : 0.35221551177620697
Loss in iteration 91 : 0.35220600747888575
Loss in iteration 92 : 0.3521964406057302
Loss in iteration 93 : 0.3521869556403063
Loss in iteration 94 : 0.35217755293338654
Loss in iteration 95 : 0.35216859354648744
Loss in iteration 96 : 0.35215992817030484
Loss in iteration 97 : 0.35215165595471026
Loss in iteration 98 : 0.3521434331128907
Loss in iteration 99 : 0.3521354573319346
Loss in iteration 100 : 0.35212783394047026
Loss in iteration 101 : 0.3521200448450005
Loss in iteration 102 : 0.35211247301736864
Loss in iteration 103 : 0.3521048789704715
Loss in iteration 104 : 0.3520973691425151
Loss in iteration 105 : 0.3520899924249419
Loss in iteration 106 : 0.3520826744091919
Loss in iteration 107 : 0.35207561740716997
Loss in iteration 108 : 0.35206868150278164
Loss in iteration 109 : 0.3520618412223576
Loss in iteration 110 : 0.3520554425845002
Loss in iteration 111 : 0.3520487082343809
Loss in iteration 112 : 0.35204220977421796
Loss in iteration 113 : 0.35203574497842294
Loss in iteration 114 : 0.3520294183967529
Loss in iteration 115 : 0.35202323433373733
Loss in iteration 116 : 0.3520169904044703
Loss in iteration 117 : 0.35201066410877635
Loss in iteration 118 : 0.3520044244629673
Loss in iteration 119 : 0.35199840387305265
Loss in iteration 120 : 0.3519922953555152
Loss in iteration 121 : 0.3519863169514114
Loss in iteration 122 : 0.3519804151929521
Loss in iteration 123 : 0.35197450735658903
Loss in iteration 124 : 0.3519687870467147
Loss in iteration 125 : 0.35196303452977556
Loss in iteration 126 : 0.3519574615006045
Loss in iteration 127 : 0.3519518587159094
Loss in iteration 128 : 0.3519467901625234
Loss in iteration 129 : 0.35194078334419326
Loss in iteration 130 : 0.3519353678217639
Loss in iteration 131 : 0.35193034368900017
Loss in iteration 132 : 0.3519246275159706
Loss in iteration 133 : 0.3519194850661889
Loss in iteration 134 : 0.3519142970511792
Loss in iteration 135 : 0.35190917307853403
Loss in iteration 136 : 0.35190413356195344
Loss in iteration 137 : 0.3518990431796065
Loss in iteration 138 : 0.35189398996703586
Loss in iteration 139 : 0.3518892833707927
Loss in iteration 140 : 0.3518845775350794
Loss in iteration 141 : 0.3518798033011886
Loss in iteration 142 : 0.351875053586256
Loss in iteration 143 : 0.3518704974918845
Loss in iteration 144 : 0.3518657917543356
Loss in iteration 145 : 0.3518611275682856
Loss in iteration 146 : 0.3518564962823707
Loss in iteration 147 : 0.3518520204382537
Loss in iteration 148 : 0.35184747367422037
Loss in iteration 149 : 0.35184305902397334
Loss in iteration 150 : 0.351838830348936
Loss in iteration 151 : 0.3518345348848844
Loss in iteration 152 : 0.3518303989660298
Loss in iteration 153 : 0.3518259657661617
Loss in iteration 154 : 0.3518217897135366
Loss in iteration 155 : 0.3518175870876911
Loss in iteration 156 : 0.35181337620709385
Loss in iteration 157 : 0.3518092281005864
Loss in iteration 158 : 0.3518050889217936
Loss in iteration 159 : 0.35180107377608627
Loss in iteration 160 : 0.35179683773263354
Loss in iteration 161 : 0.3517928804489596
Loss in iteration 162 : 0.35178896786399566
Loss in iteration 163 : 0.3517848891405335
Loss in iteration 164 : 0.3517809859334681
Loss in iteration 165 : 0.35177707433106514
Loss in iteration 166 : 0.3517732147917256
Loss in iteration 167 : 0.35176940435447224
Loss in iteration 168 : 0.35176556033193745
Loss in iteration 169 : 0.35176184550276446
Loss in iteration 170 : 0.3517580235417998
Loss in iteration 171 : 0.3517543312605408
Loss in iteration 172 : 0.3517505557507185
Loss in iteration 173 : 0.35174697924700943
Loss in iteration 174 : 0.3517432622347774
Loss in iteration 175 : 0.35173949090929196
Loss in iteration 176 : 0.35173578769984476
Loss in iteration 177 : 0.3517321837427411
Loss in iteration 178 : 0.35172858123674255
Loss in iteration 179 : 0.3517250137980331
Loss in iteration 180 : 0.3517214198005229
Loss in iteration 181 : 0.3517178650503951
Loss in iteration 182 : 0.351714359213686
Loss in iteration 183 : 0.3517108339438952
Loss in iteration 184 : 0.3517077983472372
Loss in iteration 185 : 0.3517038125320485
Loss in iteration 186 : 0.35170036289354345
Loss in iteration 187 : 0.3516970485580651
Loss in iteration 188 : 0.3516936504637218
Loss in iteration 189 : 0.35169040223315506
Loss in iteration 190 : 0.3516872486362857
Loss in iteration 191 : 0.35168375728146745
Loss in iteration 192 : 0.3516803771832245
Loss in iteration 193 : 0.3516770996922834
Loss in iteration 194 : 0.3516739136399136
Loss in iteration 195 : 0.3516705821426176
Loss in iteration 196 : 0.35166731575440463
Loss in iteration 197 : 0.35166411995412966
Loss in iteration 198 : 0.3516609289551189
Loss in iteration 199 : 0.35165778348247617
Loss in iteration 200 : 0.35165473180905893
Testing accuracy  of updater 8 on alg 1 with rate 0.01999999999999999 = 0.8505005835022419, training accuracy 0.8488636363636364, time elapsed: 3096 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.1779124438025863
Loss in iteration 3 : 2.2335951378894845
Loss in iteration 4 : 1.4355032765184277
Loss in iteration 5 : 0.7839162333191887
Loss in iteration 6 : 1.1710360772097042
Loss in iteration 7 : 1.6073593118818916
Loss in iteration 8 : 1.407862842415345
Loss in iteration 9 : 1.0922233667874433
Loss in iteration 10 : 1.064081027058235
Loss in iteration 11 : 1.2055144654695393
Loss in iteration 12 : 1.3301294119623
Loss in iteration 13 : 1.3261100906351717
Loss in iteration 14 : 1.1938491302912777
Loss in iteration 15 : 1.0381112711225966
Loss in iteration 16 : 0.9832143045199639
Loss in iteration 17 : 1.0588778737584221
Loss in iteration 18 : 1.1272535597658087
Loss in iteration 19 : 1.0673698028225407
Loss in iteration 20 : 0.9319468067148796
Loss in iteration 21 : 0.8692350389748923
Loss in iteration 22 : 0.920230625889885
Loss in iteration 23 : 0.9468309497936218
Loss in iteration 24 : 0.86344064612256
Loss in iteration 25 : 0.7508966058609186
Loss in iteration 26 : 0.7503119253136845
Loss in iteration 27 : 0.7852906676626802
Loss in iteration 28 : 0.7211089733365791
Loss in iteration 29 : 0.626649625518927
Loss in iteration 30 : 0.6456480014736548
Loss in iteration 31 : 0.6580888531175518
Loss in iteration 32 : 0.5882134580967325
Loss in iteration 33 : 0.5568584422150199
Loss in iteration 34 : 0.5896091032267273
Loss in iteration 35 : 0.5159360934657037
Loss in iteration 36 : 0.5097058967339688
Loss in iteration 37 : 0.5153485341949413
Loss in iteration 38 : 0.45914047326374285
Loss in iteration 39 : 0.49739586858094403
Loss in iteration 40 : 0.43982835472527654
Loss in iteration 41 : 0.4782758867198677
Loss in iteration 42 : 0.42124958286216674
Loss in iteration 43 : 0.45499587946459735
Loss in iteration 44 : 0.40910213553859687
Loss in iteration 45 : 0.42633661961091024
Loss in iteration 46 : 0.39875940071858573
Loss in iteration 47 : 0.41555855812080256
Loss in iteration 48 : 0.392318302101365
Loss in iteration 49 : 0.4174133441214191
Loss in iteration 50 : 0.4015462136023519
Loss in iteration 51 : 0.41537358224919935
Loss in iteration 52 : 0.39590404951566627
Loss in iteration 53 : 0.40441225862902147
Loss in iteration 54 : 0.3888434619668396
Loss in iteration 55 : 0.3940695448739244
Loss in iteration 56 : 0.377747228377044
Loss in iteration 57 : 0.3874863255550057
Loss in iteration 58 : 0.37387265581519513
Loss in iteration 59 : 0.38396532607714845
Loss in iteration 60 : 0.3702354910645152
Loss in iteration 61 : 0.37811725457539125
Loss in iteration 62 : 0.36830561917333876
Loss in iteration 63 : 0.37365642517834136
Loss in iteration 64 : 0.3692132343980692
Loss in iteration 65 : 0.3682401565654762
Loss in iteration 66 : 0.3689590216816767
Loss in iteration 67 : 0.36353645914622423
Loss in iteration 68 : 0.3670201719131872
Loss in iteration 69 : 0.36047825319663634
Loss in iteration 70 : 0.36412702973345595
Loss in iteration 71 : 0.3596807205285416
Loss in iteration 72 : 0.36066864711635505
Loss in iteration 73 : 0.35860869912848176
Loss in iteration 74 : 0.357217637280454
Loss in iteration 75 : 0.35871071976437574
Loss in iteration 76 : 0.3554986717595809
Loss in iteration 77 : 0.358472808752128
Loss in iteration 78 : 0.3559645435774815
Loss in iteration 79 : 0.3562829988036563
Loss in iteration 80 : 0.35641822151087177
Loss in iteration 81 : 0.35407696772488784
Loss in iteration 82 : 0.35581918552666386
Loss in iteration 83 : 0.35444982030405203
Loss in iteration 84 : 0.3535927358865965
Loss in iteration 85 : 0.35499711969793335
Loss in iteration 86 : 0.35318602724125536
Loss in iteration 87 : 0.3540861784809644
Loss in iteration 88 : 0.35385415583235696
Loss in iteration 89 : 0.3523097341626093
Loss in iteration 90 : 0.35386121860778474
Loss in iteration 91 : 0.35245706594034437
Loss in iteration 92 : 0.3518735938509215
Loss in iteration 93 : 0.35281374532043897
Loss in iteration 94 : 0.35147464151047547
Loss in iteration 95 : 0.3516191465997327
Loss in iteration 96 : 0.3524251375170209
Loss in iteration 97 : 0.3512214626148277
Loss in iteration 98 : 0.35195030431755037
Loss in iteration 99 : 0.35243539903210797
Loss in iteration 100 : 0.3512368176554122
Loss in iteration 101 : 0.351982936308103
Loss in iteration 102 : 0.3521014799526854
Loss in iteration 103 : 0.3512571188159603
Loss in iteration 104 : 0.35179990585777704
Loss in iteration 105 : 0.351852808067797
Loss in iteration 106 : 0.3512277887797821
Loss in iteration 107 : 0.3517378821735434
Loss in iteration 108 : 0.3517573289755834
Loss in iteration 109 : 0.3511943137050198
Loss in iteration 110 : 0.3514582527164603
Loss in iteration 111 : 0.3517194953894871
Loss in iteration 112 : 0.35107175036306154
Loss in iteration 113 : 0.35145427757248604
Loss in iteration 114 : 0.35174920887347133
Loss in iteration 115 : 0.35104617894594076
Loss in iteration 116 : 0.3514598352805142
Loss in iteration 117 : 0.3516086927315035
Loss in iteration 118 : 0.3509621434963035
Loss in iteration 119 : 0.3514906227712954
Loss in iteration 120 : 0.35171337144133985
Loss in iteration 121 : 0.35099496568633615
Loss in iteration 122 : 0.3512403054643207
Loss in iteration 123 : 0.35134908481896965
Loss in iteration 124 : 0.3509843311168033
Loss in iteration 125 : 0.3510067400593691
Loss in iteration 126 : 0.3513135294576594
Loss in iteration 127 : 0.35110249935108845
Loss in iteration 128 : 0.3509556240948752
Loss in iteration 129 : 0.3509304604795664
Loss in iteration 130 : 0.3509696342748692
Loss in iteration 131 : 0.35104933252291637
Loss in iteration 132 : 0.35106223273972775
Loss in iteration 133 : 0.3511035680828157
Loss in iteration 134 : 0.3510765002541151
Loss in iteration 135 : 0.35094333387093707
Loss in iteration 136 : 0.3509367668695815
Loss in iteration 137 : 0.3510310141592567
Loss in iteration 138 : 0.35099566618321054
Loss in iteration 139 : 0.3509297346054192
Loss in iteration 140 : 0.3508912352315083
Loss in iteration 141 : 0.35087351788283494
Loss in iteration 142 : 0.35089333027382635
Loss in iteration 143 : 0.35098809668328057
Loss in iteration 144 : 0.35114282860988005
Loss in iteration 145 : 0.35131800659998924
Loss in iteration 146 : 0.351214264521608
Loss in iteration 147 : 0.35095500352046805
Loss in iteration 148 : 0.3512362397992583
Loss in iteration 149 : 0.3514020508139625
Loss in iteration 150 : 0.3509242624528268
Loss in iteration 151 : 0.35118859942842773
Loss in iteration 152 : 0.3514802544924579
Loss in iteration 153 : 0.35092910101563446
Loss in iteration 154 : 0.351435957921863
Loss in iteration 155 : 0.3519427118038089
Loss in iteration 156 : 0.35094268035127507
Loss in iteration 157 : 0.3512619494315157
Loss in iteration 158 : 0.35190037474714625
Loss in iteration 159 : 0.35101934963640324
Loss in iteration 160 : 0.35189891246796423
Loss in iteration 161 : 0.3525406934048935
Loss in iteration 162 : 0.350908170019096
Loss in iteration 163 : 0.35229064252609066
Loss in iteration 164 : 0.3521419668509218
Loss in iteration 165 : 0.35092318055623284
Loss in iteration 166 : 0.3522033994234727
Loss in iteration 167 : 0.35166687501523225
Loss in iteration 168 : 0.3509112204117773
Loss in iteration 169 : 0.35156932020802956
Loss in iteration 170 : 0.3511694533758138
Loss in iteration 171 : 0.35090083647905507
Loss in iteration 172 : 0.35126640275972587
Loss in iteration 173 : 0.3513486788231208
Loss in iteration 174 : 0.3509654814596994
Loss in iteration 175 : 0.35094652419836114
Loss in iteration 176 : 0.3513120106772862
Loss in iteration 177 : 0.3511319083566943
Loss in iteration 178 : 0.35095493145696083
Loss in iteration 179 : 0.35090954486027104
Loss in iteration 180 : 0.351148786931118
Loss in iteration 181 : 0.35119415228017664
Loss in iteration 182 : 0.35101021918586295
Loss in iteration 183 : 0.35091681716384265
Loss in iteration 184 : 0.3509025073724592
Loss in iteration 185 : 0.3509763863237773
Loss in iteration 186 : 0.3512063456729396
Loss in iteration 187 : 0.35126396205559546
Loss in iteration 188 : 0.3509737964828569
Loss in iteration 189 : 0.3509218209097301
Loss in iteration 190 : 0.35111912821919516
Loss in iteration 191 : 0.3511663029077736
Loss in iteration 192 : 0.3509704425468094
Loss in iteration 193 : 0.35089163989999583
Loss in iteration 194 : 0.3508820620964808
Loss in iteration 195 : 0.350939314019802
Loss in iteration 196 : 0.3511718343705788
Loss in iteration 197 : 0.3513588299412589
Loss in iteration 198 : 0.35110440539086446
Loss in iteration 199 : 0.3509212260653257
Loss in iteration 200 : 0.3512802613153173
Testing accuracy  of updater 9 on alg 1 with rate 0.19999999999999998 = 0.8504391622136233, training accuracy 0.849539312039312, time elapsed: 3143 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8039250356743509
Loss in iteration 3 : 0.9149984427383523
Loss in iteration 4 : 0.6727561360665223
Loss in iteration 5 : 0.4548738834364575
Loss in iteration 6 : 0.6047708836225043
Loss in iteration 7 : 0.7398484568903454
Loss in iteration 8 : 0.6687336883873732
Loss in iteration 9 : 0.5663177192058378
Loss in iteration 10 : 0.569996143031233
Loss in iteration 11 : 0.6281485658372331
Loss in iteration 12 : 0.6639721411222863
Loss in iteration 13 : 0.6443377275558334
Loss in iteration 14 : 0.5875806348919013
Loss in iteration 15 : 0.5457221650944699
Loss in iteration 16 : 0.5578785185037993
Loss in iteration 17 : 0.5914137877131082
Loss in iteration 18 : 0.5868154128632328
Loss in iteration 19 : 0.5425308030973873
Loss in iteration 20 : 0.5110392091452965
Loss in iteration 21 : 0.5228342269918617
Loss in iteration 22 : 0.5382213445192621
Loss in iteration 23 : 0.5153109111020623
Loss in iteration 24 : 0.47471228471014837
Loss in iteration 25 : 0.46666663565551736
Loss in iteration 26 : 0.4804629360733098
Loss in iteration 27 : 0.4663758844387901
Loss in iteration 28 : 0.43195941267726223
Loss in iteration 29 : 0.4282381650700917
Loss in iteration 30 : 0.44009648038681487
Loss in iteration 31 : 0.42180839063286363
Loss in iteration 32 : 0.4009176017354159
Loss in iteration 33 : 0.4120146287146407
Loss in iteration 34 : 0.4048553639806795
Loss in iteration 35 : 0.3840574500151315
Loss in iteration 36 : 0.39619637632367477
Loss in iteration 37 : 0.3885080417206721
Loss in iteration 38 : 0.37851039692919963
Loss in iteration 39 : 0.38834612848415745
Loss in iteration 40 : 0.37225560232706656
Loss in iteration 41 : 0.37716242768091396
Loss in iteration 42 : 0.37483438789461515
Loss in iteration 43 : 0.3674772138566171
Loss in iteration 44 : 0.3736327726406628
Loss in iteration 45 : 0.36462794262517395
Loss in iteration 46 : 0.3656913142330338
Loss in iteration 47 : 0.36602993107572535
Loss in iteration 48 : 0.3616152632596462
Loss in iteration 49 : 0.3661513774160904
Loss in iteration 50 : 0.3633987796467856
Loss in iteration 51 : 0.36297094669967334
Loss in iteration 52 : 0.3644495508864664
Loss in iteration 53 : 0.36056106431156215
Loss in iteration 54 : 0.3617264962460511
Loss in iteration 55 : 0.3600636651316201
Loss in iteration 56 : 0.35771769600483394
Loss in iteration 57 : 0.3584390065654697
Loss in iteration 58 : 0.35573537229012947
Loss in iteration 59 : 0.35685500704594764
Loss in iteration 60 : 0.35584161890117505
Loss in iteration 61 : 0.3551088002024709
Loss in iteration 62 : 0.35525616721285314
Loss in iteration 63 : 0.35382942065168993
Loss in iteration 64 : 0.3546053463882081
Loss in iteration 65 : 0.35368716958359403
Loss in iteration 66 : 0.3537481700077435
Loss in iteration 67 : 0.35347017663130736
Loss in iteration 68 : 0.3530186509521458
Loss in iteration 69 : 0.3532010596800652
Loss in iteration 70 : 0.35260601954965837
Loss in iteration 71 : 0.35280589939393975
Loss in iteration 72 : 0.3522561024472809
Loss in iteration 73 : 0.35241536341347196
Loss in iteration 74 : 0.3521278246168109
Loss in iteration 75 : 0.3521167910727705
Loss in iteration 76 : 0.3519544436479779
Loss in iteration 77 : 0.3518161015398554
Loss in iteration 78 : 0.3517362001125883
Loss in iteration 79 : 0.3516024827919188
Loss in iteration 80 : 0.35153036255329967
Loss in iteration 81 : 0.3513534244752334
Loss in iteration 82 : 0.3513415548119573
Loss in iteration 83 : 0.35130027706545447
Loss in iteration 84 : 0.35131352996767584
Loss in iteration 85 : 0.3511591011789844
Loss in iteration 86 : 0.35129556502106307
Loss in iteration 87 : 0.3511721321614614
Loss in iteration 88 : 0.351328400037348
Loss in iteration 89 : 0.35111061089240414
Loss in iteration 90 : 0.3512264008766006
Loss in iteration 91 : 0.35115815562037617
Loss in iteration 92 : 0.35123316506044
Loss in iteration 93 : 0.35110677982913285
Loss in iteration 94 : 0.3512128020422089
Loss in iteration 95 : 0.3510728967874288
Loss in iteration 96 : 0.351177583049526
Loss in iteration 97 : 0.3510573278538244
Loss in iteration 98 : 0.35111090166042547
Loss in iteration 99 : 0.35106406130204987
Loss in iteration 100 : 0.3511015275034728
Loss in iteration 101 : 0.3510227688813606
Loss in iteration 102 : 0.3510472359014819
Loss in iteration 103 : 0.3510333133319025
Loss in iteration 104 : 0.3509955472510228
Loss in iteration 105 : 0.3510655787608217
Loss in iteration 106 : 0.35097162618673583
Loss in iteration 107 : 0.35103279041582264
Loss in iteration 108 : 0.3509675975269083
Loss in iteration 109 : 0.3510074023592243
Loss in iteration 110 : 0.3509596248132626
Loss in iteration 111 : 0.350959156997633
Loss in iteration 112 : 0.350971848053513
Loss in iteration 113 : 0.3509367464378687
Loss in iteration 114 : 0.3509527003413446
Loss in iteration 115 : 0.35092757281306486
Loss in iteration 116 : 0.3509268819316404
Loss in iteration 117 : 0.3509233971807298
Loss in iteration 118 : 0.35091632075621854
Loss in iteration 119 : 0.3509077393310694
Loss in iteration 120 : 0.3509027109021587
Loss in iteration 121 : 0.3509001240468025
Loss in iteration 122 : 0.3508977709248162
Loss in iteration 123 : 0.35089740776832934
Loss in iteration 124 : 0.3509190530136607
Loss in iteration 125 : 0.3509172304995281
Loss in iteration 126 : 0.3509170651484111
Loss in iteration 127 : 0.35089537582086977
Loss in iteration 128 : 0.35091280189583823
Loss in iteration 129 : 0.35089309879254577
Loss in iteration 130 : 0.35090603853603514
Loss in iteration 131 : 0.35092846020000384
Loss in iteration 132 : 0.3509026760873649
Loss in iteration 133 : 0.3508997011999412
Loss in iteration 134 : 0.35090038112898775
Loss in iteration 135 : 0.35087965688639106
Loss in iteration 136 : 0.3508962861838434
Loss in iteration 137 : 0.3508801902315203
Loss in iteration 138 : 0.35087205929577253
Loss in iteration 139 : 0.35087781710169785
Loss in iteration 140 : 0.35088655838592375
Loss in iteration 141 : 0.35089030951914146
Loss in iteration 142 : 0.35086974161766693
Loss in iteration 143 : 0.35086620739158536
Loss in iteration 144 : 0.35086400605758056
Loss in iteration 145 : 0.35086598997349944
Loss in iteration 146 : 0.3508620789561934
Loss in iteration 147 : 0.35086013238666214
Loss in iteration 148 : 0.35087230406737946
Loss in iteration 149 : 0.35095723964464404
Loss in iteration 150 : 0.3508772238246734
Loss in iteration 151 : 0.35101157333078087
Loss in iteration 152 : 0.3509195322214003
Loss in iteration 153 : 0.3509241854851025
Loss in iteration 154 : 0.3509538302215486
Loss in iteration 155 : 0.3508849160140097
Loss in iteration 156 : 0.3509165928785029
Loss in iteration 157 : 0.35087855224069886
Loss in iteration 158 : 0.35090400047056813
Loss in iteration 159 : 0.35087226559912066
Loss in iteration 160 : 0.3509001433779193
Loss in iteration 161 : 0.35086348418606655
Loss in iteration 162 : 0.3508570216613935
Loss in iteration 163 : 0.35087031455988915
Loss in iteration 164 : 0.350873301423273
Loss in iteration 165 : 0.3508614669319163
Loss in iteration 166 : 0.3508628014673858
Loss in iteration 167 : 0.35087785559674817
Loss in iteration 168 : 0.35092588372267847
Loss in iteration 169 : 0.3508593015727416
Loss in iteration 170 : 0.35094177812215377
Loss in iteration 171 : 0.35092722704903107
Loss in iteration 172 : 0.3508873045366272
Loss in iteration 173 : 0.3509355078459746
Loss in iteration 174 : 0.3508665680734538
Loss in iteration 175 : 0.3509133763721636
Loss in iteration 176 : 0.3508661362294248
Loss in iteration 177 : 0.35091416488380794
Loss in iteration 178 : 0.3509105125821663
Loss in iteration 179 : 0.35086850947758885
Loss in iteration 180 : 0.3509125378116948
Loss in iteration 181 : 0.3508801440550145
Loss in iteration 182 : 0.35087777286516497
Loss in iteration 183 : 0.35089218403586697
Loss in iteration 184 : 0.3508535610341924
Loss in iteration 185 : 0.3508796755736057
Loss in iteration 186 : 0.3508613753093192
Loss in iteration 187 : 0.350849034515625
Loss in iteration 188 : 0.3508651009041149
Loss in iteration 189 : 0.3508593466578354
Loss in iteration 190 : 0.3508524687000434
Loss in iteration 191 : 0.3508494806686196
Loss in iteration 192 : 0.35086578347179237
Loss in iteration 193 : 0.350911363714152
Loss in iteration 194 : 0.35085714696671144
Loss in iteration 195 : 0.350943837563712
Loss in iteration 196 : 0.3509074800461992
Loss in iteration 197 : 0.35088583160621684
Loss in iteration 198 : 0.35094671810328354
Loss in iteration 199 : 0.35086080918292495
Loss in iteration 200 : 0.35092359327870865
Testing accuracy  of updater 9 on alg 1 with rate 0.13999999999999999 = 0.8493950003071065, training accuracy 0.8501228501228502, time elapsed: 3112 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5053625290511095
Loss in iteration 3 : 0.5726626219279192
Loss in iteration 4 : 0.48209070302905443
Loss in iteration 5 : 0.37553632669616477
Loss in iteration 6 : 0.4284986458596443
Loss in iteration 7 : 0.47080497622151746
Loss in iteration 8 : 0.4379421710093633
Loss in iteration 9 : 0.3993554517332471
Loss in iteration 10 : 0.4055617613364027
Loss in iteration 11 : 0.43146808867806224
Loss in iteration 12 : 0.44229875824216536
Loss in iteration 13 : 0.4300970746037691
Loss in iteration 14 : 0.41002441559221353
Loss in iteration 15 : 0.40365615102184643
Loss in iteration 16 : 0.4145837754034253
Loss in iteration 17 : 0.4247501528199945
Loss in iteration 18 : 0.4196039096919271
Loss in iteration 19 : 0.40513909700416606
Loss in iteration 20 : 0.3977371993655587
Loss in iteration 21 : 0.4016150852510492
Loss in iteration 22 : 0.40667105606032966
Loss in iteration 23 : 0.4015897822197148
Loss in iteration 24 : 0.3900487758987735
Loss in iteration 25 : 0.38337524330367606
Loss in iteration 26 : 0.3851590715186191
Loss in iteration 27 : 0.3865570553225438
Loss in iteration 28 : 0.38064163654991073
Loss in iteration 29 : 0.3719614098011708
Loss in iteration 30 : 0.3697633035707514
Loss in iteration 31 : 0.3721357591611717
Loss in iteration 32 : 0.37055165508309074
Loss in iteration 33 : 0.36456865899937146
Loss in iteration 34 : 0.362186187730245
Loss in iteration 35 : 0.3645281210337513
Loss in iteration 36 : 0.3637189759312551
Loss in iteration 37 : 0.35941443721465804
Loss in iteration 38 : 0.35929314619326536
Loss in iteration 39 : 0.36153312545681837
Loss in iteration 40 : 0.35959665247392625
Loss in iteration 41 : 0.3568917166118211
Loss in iteration 42 : 0.35845409628890934
Loss in iteration 43 : 0.3586499013135082
Loss in iteration 44 : 0.3558109448490564
Loss in iteration 45 : 0.3559154103628439
Loss in iteration 46 : 0.35689968273206957
Loss in iteration 47 : 0.35511921688737225
Loss in iteration 48 : 0.35424885955010976
Loss in iteration 49 : 0.3552816156979891
Loss in iteration 50 : 0.35447811654155315
Loss in iteration 51 : 0.3532842455377995
Loss in iteration 52 : 0.35394011576884166
Loss in iteration 53 : 0.3540058523112736
Loss in iteration 54 : 0.3530308645382208
Loss in iteration 55 : 0.35321242800367425
Loss in iteration 56 : 0.353671652147704
Loss in iteration 57 : 0.3530475867339002
Loss in iteration 58 : 0.35280537895929
Loss in iteration 59 : 0.3530971035094971
Loss in iteration 60 : 0.3529545869214942
Loss in iteration 61 : 0.35249930771516086
Loss in iteration 62 : 0.35242788073104325
Loss in iteration 63 : 0.3525408712766788
Loss in iteration 64 : 0.3521818821215906
Loss in iteration 65 : 0.35197868030449536
Loss in iteration 66 : 0.3520727610066916
Loss in iteration 67 : 0.35194167811976135
Loss in iteration 68 : 0.3517263282877818
Loss in iteration 69 : 0.35179138790611403
Loss in iteration 70 : 0.35175305121866446
Loss in iteration 71 : 0.35155132114773
Loss in iteration 72 : 0.35157350282539085
Loss in iteration 73 : 0.35158609880155184
Loss in iteration 74 : 0.3514381416509219
Loss in iteration 75 : 0.35143043082668624
Loss in iteration 76 : 0.3514480193286345
Loss in iteration 77 : 0.3513056292307072
Loss in iteration 78 : 0.3513186619962329
Loss in iteration 79 : 0.3513126546932118
Loss in iteration 80 : 0.35122221911661156
Loss in iteration 81 : 0.35125332618995525
Loss in iteration 82 : 0.3512121798373576
Loss in iteration 83 : 0.35116135650074143
Loss in iteration 84 : 0.35119542955375327
Loss in iteration 85 : 0.3511593409366084
Loss in iteration 86 : 0.3511334976625493
Loss in iteration 87 : 0.35115927005272335
Loss in iteration 88 : 0.3511211223577245
Loss in iteration 89 : 0.3511168447020352
Loss in iteration 90 : 0.3511169320556391
Loss in iteration 91 : 0.3510893496550925
Loss in iteration 92 : 0.3510954243947848
Loss in iteration 93 : 0.35109010711520205
Loss in iteration 94 : 0.3510742030413973
Loss in iteration 95 : 0.3510894457275378
Loss in iteration 96 : 0.35106503877874895
Loss in iteration 97 : 0.35107647163029104
Loss in iteration 98 : 0.3510636633356968
Loss in iteration 99 : 0.35106107518678215
Loss in iteration 100 : 0.351061384390468
Loss in iteration 101 : 0.3510434905983024
Loss in iteration 102 : 0.3510495118103445
Loss in iteration 103 : 0.35104367499977995
Loss in iteration 104 : 0.3510505989911931
Loss in iteration 105 : 0.35103290408793614
Loss in iteration 106 : 0.3510365449346922
Loss in iteration 107 : 0.35102620691453024
Loss in iteration 108 : 0.3510209173052595
Loss in iteration 109 : 0.35102102052522205
Loss in iteration 110 : 0.3510183260101377
Loss in iteration 111 : 0.3510126923370815
Loss in iteration 112 : 0.3510189781405417
Loss in iteration 113 : 0.3510062735590183
Loss in iteration 114 : 0.35101147384663584
Loss in iteration 115 : 0.3510014432562569
Loss in iteration 116 : 0.35100327829146927
Loss in iteration 117 : 0.35099690355207813
Loss in iteration 118 : 0.35099697256083
Loss in iteration 119 : 0.35099440001658094
Loss in iteration 120 : 0.35099165060665194
Loss in iteration 121 : 0.3509904111589148
Loss in iteration 122 : 0.35099097613982383
Loss in iteration 123 : 0.35098449011572846
Loss in iteration 124 : 0.35098654364525306
Loss in iteration 125 : 0.3509787282035818
Loss in iteration 126 : 0.3509779445396719
Loss in iteration 127 : 0.3509786470839287
Loss in iteration 128 : 0.35097438399255193
Loss in iteration 129 : 0.350978052803512
Loss in iteration 130 : 0.35097177923071676
Loss in iteration 131 : 0.35096887771605684
Loss in iteration 132 : 0.3509669442696997
Loss in iteration 133 : 0.3509661646671872
Loss in iteration 134 : 0.3509669705524254
Loss in iteration 135 : 0.3509625482035539
Loss in iteration 136 : 0.35096623955781286
Loss in iteration 137 : 0.3509725791000494
Loss in iteration 138 : 0.3509601048758021
Loss in iteration 139 : 0.350971733267693
Loss in iteration 140 : 0.3509646720721489
Loss in iteration 141 : 0.35096387664688306
Loss in iteration 142 : 0.3509610465344979
Loss in iteration 143 : 0.3509644950632594
Loss in iteration 144 : 0.3509569730674103
Loss in iteration 145 : 0.35096599102259485
Loss in iteration 146 : 0.3509522068704311
Loss in iteration 147 : 0.35096072304896986
Loss in iteration 148 : 0.3509510533373854
Loss in iteration 149 : 0.35095712557121655
Loss in iteration 150 : 0.35094756561012347
Loss in iteration 151 : 0.3509533339921677
Loss in iteration 152 : 0.3509501587476185
Loss in iteration 153 : 0.35094892519025495
Loss in iteration 154 : 0.3509453252875132
Loss in iteration 155 : 0.3509471270208849
Loss in iteration 156 : 0.35094534327476645
Loss in iteration 157 : 0.35094579113266505
Loss in iteration 158 : 0.3509416628018527
Loss in iteration 159 : 0.3509500285629095
Loss in iteration 160 : 0.35095271295978975
Loss in iteration 161 : 0.35094027658117133
Loss in iteration 162 : 0.3509565896069004
Loss in iteration 163 : 0.3509381850503754
Loss in iteration 164 : 0.3509565857203858
Loss in iteration 165 : 0.3509376218426914
Loss in iteration 166 : 0.35095024886403886
Loss in iteration 167 : 0.35093702898475504
Loss in iteration 168 : 0.3509418155446876
Loss in iteration 169 : 0.35093620107902224
Loss in iteration 170 : 0.3509364489816079
Loss in iteration 171 : 0.35093204200657313
Loss in iteration 172 : 0.35093258440177166
Loss in iteration 173 : 0.3509305084626976
Loss in iteration 174 : 0.35093246545998474
Loss in iteration 175 : 0.35092974378903985
Loss in iteration 176 : 0.35092910477186584
Loss in iteration 177 : 0.3509265008381859
Loss in iteration 178 : 0.35092503576581413
Loss in iteration 179 : 0.35092492400331854
Loss in iteration 180 : 0.3509278221766509
Loss in iteration 181 : 0.35093363301370545
Loss in iteration 182 : 0.3509230767570773
Loss in iteration 183 : 0.35092534152440613
Loss in iteration 184 : 0.35092569039052357
Loss in iteration 185 : 0.35092046453097286
Loss in iteration 186 : 0.3509246854652147
Loss in iteration 187 : 0.35092928256080863
Loss in iteration 188 : 0.3509238875082108
Loss in iteration 189 : 0.3509256713332045
Loss in iteration 190 : 0.35092183112216546
Loss in iteration 191 : 0.3509204870065414
Loss in iteration 192 : 0.3509179342004918
Loss in iteration 193 : 0.3509174822449947
Loss in iteration 194 : 0.350914599853776
Loss in iteration 195 : 0.35091613420533274
Loss in iteration 196 : 0.3509132936190771
Loss in iteration 197 : 0.35091669185921576
Loss in iteration 198 : 0.35093152475754463
Loss in iteration 199 : 0.3509141242296278
Loss in iteration 200 : 0.3509325634283817
Testing accuracy  of updater 9 on alg 1 with rate 0.08 = 0.8498249493274369, training accuracy 0.8500921375921376, time elapsed: 3111 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7992002293466746
Loss in iteration 3 : 0.47975125237457483
Loss in iteration 4 : 0.5314156304556776
Loss in iteration 5 : 0.5978509488572874
Loss in iteration 6 : 0.618051785276246
Loss in iteration 7 : 0.5989270772496299
Loss in iteration 8 : 0.5497109210132739
Loss in iteration 9 : 0.4821325095690287
Loss in iteration 10 : 0.41377750116851425
Loss in iteration 11 : 0.3752213760936361
Loss in iteration 12 : 0.3819705249888836
Loss in iteration 13 : 0.4116282164764979
Loss in iteration 14 : 0.43753009739497273
Loss in iteration 15 : 0.4464186582615055
Loss in iteration 16 : 0.4365657883560205
Loss in iteration 17 : 0.41486160378392817
Loss in iteration 18 : 0.39161461624122523
Loss in iteration 19 : 0.37516041447904946
Loss in iteration 20 : 0.3699431349349807
Loss in iteration 21 : 0.3735840810447172
Loss in iteration 22 : 0.3814986316649132
Loss in iteration 23 : 0.38869528489844835
Loss in iteration 24 : 0.39204970136804834
Loss in iteration 25 : 0.39050983221162916
Loss in iteration 26 : 0.38485463071791515
Loss in iteration 27 : 0.37725587862956195
Loss in iteration 28 : 0.37001096926785904
Loss in iteration 29 : 0.3653767944212101
Loss in iteration 30 : 0.3640626841398484
Loss in iteration 31 : 0.3653941361675259
Loss in iteration 32 : 0.36817342660910557
Loss in iteration 33 : 0.37051408427268506
Loss in iteration 34 : 0.3711253225497909
Loss in iteration 35 : 0.36969605450572224
Loss in iteration 36 : 0.3667760763929028
Loss in iteration 37 : 0.36349551592285234
Loss in iteration 38 : 0.36102122532282244
Loss in iteration 39 : 0.3598467862832296
Loss in iteration 40 : 0.35998506126710333
Loss in iteration 41 : 0.3607942852149529
Loss in iteration 42 : 0.3615775041398755
Loss in iteration 43 : 0.36172871763985887
Loss in iteration 44 : 0.36102027292957556
Loss in iteration 45 : 0.35963933682455373
Loss in iteration 46 : 0.3580833655952632
Loss in iteration 47 : 0.35683882347741636
Loss in iteration 48 : 0.356199659707004
Loss in iteration 49 : 0.35617632341241945
Loss in iteration 50 : 0.356471675261071
Loss in iteration 51 : 0.3566962919239083
Loss in iteration 52 : 0.35657142611227177
Loss in iteration 53 : 0.35605646049481693
Loss in iteration 54 : 0.3553027719615463
Loss in iteration 55 : 0.3545836045288056
Loss in iteration 56 : 0.3541094581364091
Loss in iteration 57 : 0.3539484013792065
Loss in iteration 58 : 0.3540074191869526
Loss in iteration 59 : 0.3540926201175846
Loss in iteration 60 : 0.3540531000699721
Loss in iteration 61 : 0.35382906585695223
Loss in iteration 62 : 0.35347925383838524
Loss in iteration 63 : 0.3531430883718987
Loss in iteration 64 : 0.3529227428850654
Loss in iteration 65 : 0.35283874105938823
Loss in iteration 66 : 0.35286526736517493
Loss in iteration 67 : 0.3529008398130299
Loss in iteration 68 : 0.3528784067688789
Loss in iteration 69 : 0.3527725075684538
Loss in iteration 70 : 0.3526143482241834
Loss in iteration 71 : 0.3524626259701416
Loss in iteration 72 : 0.35236812671258244
Loss in iteration 73 : 0.3523443458770968
Loss in iteration 74 : 0.35236069059242625
Loss in iteration 75 : 0.35237228126678205
Loss in iteration 76 : 0.3523500644058808
Loss in iteration 77 : 0.35229391707225405
Loss in iteration 78 : 0.35221593972959
Loss in iteration 79 : 0.3521456022816715
Loss in iteration 80 : 0.35211409191825754
Loss in iteration 81 : 0.35210535537225757
Loss in iteration 82 : 0.3521097894299088
Loss in iteration 83 : 0.35210968992951514
Loss in iteration 84 : 0.3520800438686586
Loss in iteration 85 : 0.3520323079445254
Loss in iteration 86 : 0.3519924183488027
Loss in iteration 87 : 0.35197240045564676
Loss in iteration 88 : 0.3519694801052132
Loss in iteration 89 : 0.35196762336038706
Loss in iteration 90 : 0.35195898198554765
Loss in iteration 91 : 0.3519407977686117
Loss in iteration 92 : 0.35191574334498144
Loss in iteration 93 : 0.35188984040461113
Loss in iteration 94 : 0.35186865825098396
Loss in iteration 95 : 0.35186337757475705
Loss in iteration 96 : 0.3518603570043768
Loss in iteration 97 : 0.35185215797112257
Loss in iteration 98 : 0.3518364458035508
Loss in iteration 99 : 0.35181752929256116
Loss in iteration 100 : 0.3518008963167603
Loss in iteration 101 : 0.3517916484025974
Loss in iteration 102 : 0.3517860624853904
Loss in iteration 103 : 0.35177880572067666
Loss in iteration 104 : 0.35176746970650524
Loss in iteration 105 : 0.3517563647444887
Loss in iteration 106 : 0.3517465221761335
Loss in iteration 107 : 0.3517388731696649
Loss in iteration 108 : 0.35173272176449355
Loss in iteration 109 : 0.351725946434657
Loss in iteration 110 : 0.3517181139052046
Loss in iteration 111 : 0.35170971093835235
Loss in iteration 112 : 0.35170135670168334
Loss in iteration 113 : 0.35169516561156033
Loss in iteration 114 : 0.3516897851178107
Loss in iteration 115 : 0.35168359725439274
Loss in iteration 116 : 0.35167623946725723
Loss in iteration 117 : 0.3516695832786993
Loss in iteration 118 : 0.3516641117030477
Loss in iteration 119 : 0.3516590378949179
Loss in iteration 120 : 0.35165391941561525
Loss in iteration 121 : 0.3516475855217308
Loss in iteration 122 : 0.3516423255178149
Loss in iteration 123 : 0.35163761567316293
Loss in iteration 124 : 0.35163251548180724
Loss in iteration 125 : 0.35162685149402867
Loss in iteration 126 : 0.35162112861434225
Loss in iteration 127 : 0.3516162921908241
Loss in iteration 128 : 0.3516115807556538
Loss in iteration 129 : 0.35160667941552
Loss in iteration 130 : 0.35160141312932763
Loss in iteration 131 : 0.35159667240005776
Loss in iteration 132 : 0.35159212208158075
Loss in iteration 133 : 0.3515875055563685
Loss in iteration 134 : 0.3515827882556088
Loss in iteration 135 : 0.35157797748802
Loss in iteration 136 : 0.3515731981675962
Loss in iteration 137 : 0.35156883901289965
Loss in iteration 138 : 0.3515644743424647
Loss in iteration 139 : 0.3515598518388329
Loss in iteration 140 : 0.351555303864434
Loss in iteration 141 : 0.35155110670317785
Loss in iteration 142 : 0.3515467260069105
Loss in iteration 143 : 0.35154246191395405
Loss in iteration 144 : 0.35153825466323313
Loss in iteration 145 : 0.3515340035436936
Loss in iteration 146 : 0.35152987909258304
Loss in iteration 147 : 0.35152561598779974
Loss in iteration 148 : 0.3515214908479844
Loss in iteration 149 : 0.3515175230711839
Loss in iteration 150 : 0.35151351777861123
Loss in iteration 151 : 0.35150966339448486
Loss in iteration 152 : 0.3515054757144172
Loss in iteration 153 : 0.3515014810389718
Loss in iteration 154 : 0.3514976665606767
Loss in iteration 155 : 0.35149387196005816
Loss in iteration 156 : 0.35149009761091343
Loss in iteration 157 : 0.3514863093310996
Loss in iteration 158 : 0.3514823828667161
Loss in iteration 159 : 0.35147876910399806
Loss in iteration 160 : 0.3514751602366301
Loss in iteration 161 : 0.35147147493893266
Loss in iteration 162 : 0.35146768595082495
Loss in iteration 163 : 0.35146393805968085
Loss in iteration 164 : 0.35146028933997514
Loss in iteration 165 : 0.3514566293490343
Loss in iteration 166 : 0.35145294822189366
Loss in iteration 167 : 0.35144931867223533
Loss in iteration 168 : 0.35144568033337076
Loss in iteration 169 : 0.3514421239673023
Loss in iteration 170 : 0.3514386805530895
Loss in iteration 171 : 0.35143516362223304
Loss in iteration 172 : 0.3514318029402876
Loss in iteration 173 : 0.3514282836186926
Loss in iteration 174 : 0.35142497209890344
Loss in iteration 175 : 0.35142166597586416
Loss in iteration 176 : 0.3514182547139683
Loss in iteration 177 : 0.3514152385369328
Loss in iteration 178 : 0.3514120249695576
Loss in iteration 179 : 0.35140878729287967
Loss in iteration 180 : 0.351405846475583
Loss in iteration 181 : 0.35140264235734076
Loss in iteration 182 : 0.351399704858166
Loss in iteration 183 : 0.35139683617445533
Loss in iteration 184 : 0.3513935122847785
Loss in iteration 185 : 0.351390643599035
Loss in iteration 186 : 0.35138742226021535
Loss in iteration 187 : 0.3513845616103697
Loss in iteration 188 : 0.3513815674795075
Loss in iteration 189 : 0.35137854783056655
Loss in iteration 190 : 0.35137553474595734
Loss in iteration 191 : 0.3513725692026698
Loss in iteration 192 : 0.35136961127178146
Loss in iteration 193 : 0.35136695203368995
Loss in iteration 194 : 0.3513639597762544
Loss in iteration 195 : 0.35136095541689927
Loss in iteration 196 : 0.3513581073330228
Loss in iteration 197 : 0.35135544993366147
Loss in iteration 198 : 0.3513526364581413
Loss in iteration 199 : 0.35134980520342424
Loss in iteration 200 : 0.3513471522741503
Testing accuracy  of updater 9 on alg 1 with rate 0.02 = 0.8500092131932928, training accuracy 0.8495700245700246, time elapsed: 3168 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8820649842902683
Loss in iteration 3 : 0.6733125756478509
Loss in iteration 4 : 0.46484877222585685
Loss in iteration 5 : 0.5153896688102727
Loss in iteration 6 : 0.5651742387076852
Loss in iteration 7 : 0.5872149766869494
Loss in iteration 8 : 0.5844941073227279
Loss in iteration 9 : 0.5614241791708453
Loss in iteration 10 : 0.523460603286699
Loss in iteration 11 : 0.47632060798080184
Loss in iteration 12 : 0.4278572493571933
Loss in iteration 13 : 0.3904307114466087
Loss in iteration 14 : 0.37526992901005096
Loss in iteration 15 : 0.38202560499098415
Loss in iteration 16 : 0.39914849073519754
Loss in iteration 17 : 0.41452668219244804
Loss in iteration 18 : 0.4216360440778698
Loss in iteration 19 : 0.41893625200421086
Loss in iteration 20 : 0.40831269024383704
Loss in iteration 21 : 0.39351098343867436
Loss in iteration 22 : 0.3791345407890965
Loss in iteration 23 : 0.368558663202006
Loss in iteration 24 : 0.36368232973684533
Loss in iteration 25 : 0.36377030128275983
Loss in iteration 26 : 0.36698965594861777
Loss in iteration 27 : 0.37116941051632574
Loss in iteration 28 : 0.37444536591445665
Loss in iteration 29 : 0.37568918607949503
Loss in iteration 30 : 0.37468100419967093
Loss in iteration 31 : 0.3717603603622569
Loss in iteration 32 : 0.36778126495049623
Loss in iteration 33 : 0.3636997658573298
Loss in iteration 34 : 0.36039693372556325
Loss in iteration 35 : 0.35831032323920403
Loss in iteration 36 : 0.3576320741165234
Loss in iteration 37 : 0.35810838192991
Loss in iteration 38 : 0.3592751794234753
Loss in iteration 39 : 0.3604389498285498
Loss in iteration 40 : 0.36110375487221125
Loss in iteration 41 : 0.3610350923801245
Loss in iteration 42 : 0.36024523200558867
Loss in iteration 43 : 0.35896439854293505
Loss in iteration 44 : 0.35755378629914353
Loss in iteration 45 : 0.35635104823585156
Loss in iteration 46 : 0.35555974164807697
Loss in iteration 47 : 0.35528766595605965
Loss in iteration 48 : 0.3554043573891379
Loss in iteration 49 : 0.35570976854370456
Loss in iteration 50 : 0.3560006078066737
Loss in iteration 51 : 0.35614353786182995
Loss in iteration 52 : 0.3560506146371531
Loss in iteration 53 : 0.3557165742306791
Loss in iteration 54 : 0.3552236589638511
Loss in iteration 55 : 0.3546898573431153
Loss in iteration 56 : 0.35423198192935446
Loss in iteration 57 : 0.3539250904673069
Loss in iteration 58 : 0.3538035540907923
Loss in iteration 59 : 0.3538465667979774
Loss in iteration 60 : 0.35393851164915424
Loss in iteration 61 : 0.35399628051674326
Loss in iteration 62 : 0.3539741162800456
Loss in iteration 63 : 0.3538618645010101
Loss in iteration 64 : 0.3536767333931377
Loss in iteration 65 : 0.35345962580944046
Loss in iteration 66 : 0.3532488810493149
Loss in iteration 67 : 0.35309069782460056
Loss in iteration 68 : 0.3530153496027846
Loss in iteration 69 : 0.3529939694334046
Loss in iteration 70 : 0.35299828407592543
Loss in iteration 71 : 0.3530029895451557
Loss in iteration 72 : 0.3529853183958783
Loss in iteration 73 : 0.3529355759377532
Loss in iteration 74 : 0.3528580090929733
Loss in iteration 75 : 0.35276490233161917
Loss in iteration 76 : 0.3526759128183064
Loss in iteration 77 : 0.3526106331195299
Loss in iteration 78 : 0.35256725153130647
Loss in iteration 79 : 0.352542494422617
Loss in iteration 80 : 0.35254366830828104
Loss in iteration 81 : 0.35254249821603845
Loss in iteration 82 : 0.3525271692827393
Loss in iteration 83 : 0.35249619648100017
Loss in iteration 84 : 0.3524524967901316
Loss in iteration 85 : 0.3524025390888425
Loss in iteration 86 : 0.3523679138751495
Loss in iteration 87 : 0.3523433778319854
Loss in iteration 88 : 0.35232632103567224
Loss in iteration 89 : 0.35231810849431305
Loss in iteration 90 : 0.35231263566622184
Loss in iteration 91 : 0.3523024553741684
Loss in iteration 92 : 0.3522846944574434
Loss in iteration 93 : 0.3522615807151031
Loss in iteration 94 : 0.3522371955899166
Loss in iteration 95 : 0.3522151524111617
Loss in iteration 96 : 0.3521992873590587
Loss in iteration 97 : 0.3521901408625206
Loss in iteration 98 : 0.35218313515159816
Loss in iteration 99 : 0.35217580157176737
Loss in iteration 100 : 0.3521666998357657
Loss in iteration 101 : 0.35215578864517083
Loss in iteration 102 : 0.3521433010526342
Loss in iteration 103 : 0.352130528165877
Loss in iteration 104 : 0.3521183519874438
Loss in iteration 105 : 0.3521070853746038
Loss in iteration 106 : 0.35209834048379085
Loss in iteration 107 : 0.3520913827403016
Loss in iteration 108 : 0.35208539280834183
Loss in iteration 109 : 0.3520780433039482
Loss in iteration 110 : 0.35206915427106483
Loss in iteration 111 : 0.3520591965629001
Loss in iteration 112 : 0.3520494853973548
Loss in iteration 113 : 0.3520420000564991
Loss in iteration 114 : 0.35203561391380017
Loss in iteration 115 : 0.35202920124524717
Loss in iteration 116 : 0.35202166701234516
Loss in iteration 117 : 0.3520136493683726
Loss in iteration 118 : 0.3520061162812373
Loss in iteration 119 : 0.3519999963829422
Loss in iteration 120 : 0.35199418543343447
Loss in iteration 121 : 0.35198856316793203
Loss in iteration 122 : 0.35198232650109007
Loss in iteration 123 : 0.3519756878586654
Loss in iteration 124 : 0.3519692424511236
Loss in iteration 125 : 0.3519632108146849
Loss in iteration 126 : 0.35195742340739306
Loss in iteration 127 : 0.35195187316297755
Loss in iteration 128 : 0.3519461369460987
Loss in iteration 129 : 0.35194026374445586
Loss in iteration 130 : 0.35193454872020047
Loss in iteration 131 : 0.3519290238505403
Loss in iteration 132 : 0.35192358138139485
Loss in iteration 133 : 0.3519181032005276
Loss in iteration 134 : 0.35191272184503536
Loss in iteration 135 : 0.3519074494827832
Loss in iteration 136 : 0.3519022491299133
Loss in iteration 137 : 0.3518972149953563
Loss in iteration 138 : 0.35189213571761235
Loss in iteration 139 : 0.3518871481507343
Loss in iteration 140 : 0.35188231668636627
Loss in iteration 141 : 0.35187752187430366
Loss in iteration 142 : 0.3518728518163729
Loss in iteration 143 : 0.3518682350627007
Loss in iteration 144 : 0.35186361487229467
Loss in iteration 145 : 0.35185914065328033
Loss in iteration 146 : 0.3518546194174099
Loss in iteration 147 : 0.3518501576258125
Loss in iteration 148 : 0.3518457855323569
Loss in iteration 149 : 0.35184143085168723
Loss in iteration 150 : 0.3518370231129423
Loss in iteration 151 : 0.35183269098939957
Loss in iteration 152 : 0.35182840772889207
Loss in iteration 153 : 0.35182411913054956
Loss in iteration 154 : 0.3518200284037849
Loss in iteration 155 : 0.3518159042387037
Loss in iteration 156 : 0.3518117440046028
Loss in iteration 157 : 0.35180756890582077
Loss in iteration 158 : 0.3518035588819616
Loss in iteration 159 : 0.35179949127076876
Loss in iteration 160 : 0.3517954339786339
Loss in iteration 161 : 0.3517913057817071
Loss in iteration 162 : 0.35178722476532276
Loss in iteration 163 : 0.3517831852349535
Loss in iteration 164 : 0.35177922138036255
Loss in iteration 165 : 0.3517752237774771
Loss in iteration 166 : 0.35177127433001437
Loss in iteration 167 : 0.35176736933801417
Loss in iteration 168 : 0.35176348219560966
Loss in iteration 169 : 0.3517595946982773
Loss in iteration 170 : 0.35175588010041564
Loss in iteration 171 : 0.3517519025371228
Loss in iteration 172 : 0.3517480458518173
Loss in iteration 173 : 0.351744232093819
Loss in iteration 174 : 0.3517404501374554
Loss in iteration 175 : 0.3517366834558336
Loss in iteration 176 : 0.351732942710902
Loss in iteration 177 : 0.35172918244873314
Loss in iteration 178 : 0.35172540803323266
Loss in iteration 179 : 0.35172169667577025
Loss in iteration 180 : 0.35171801120843943
Loss in iteration 181 : 0.3517144438815473
Loss in iteration 182 : 0.3517108230261245
Loss in iteration 183 : 0.35170712529405834
Loss in iteration 184 : 0.3517035408880587
Loss in iteration 185 : 0.35169988835639926
Loss in iteration 186 : 0.3516963009950736
Loss in iteration 187 : 0.3516927569867583
Loss in iteration 188 : 0.35168924601471874
Loss in iteration 189 : 0.35168575521511786
Loss in iteration 190 : 0.3516823134210771
Loss in iteration 191 : 0.3516790477125945
Loss in iteration 192 : 0.3516757310120343
Loss in iteration 193 : 0.35167259910272186
Loss in iteration 194 : 0.3516692140594163
Loss in iteration 195 : 0.35166604045587846
Loss in iteration 196 : 0.35166285243059997
Loss in iteration 197 : 0.3516595671882726
Loss in iteration 198 : 0.35165643247931705
Loss in iteration 199 : 0.35165334821420885
Loss in iteration 200 : 0.3516501429410374
Testing accuracy  of updater 9 on alg 1 with rate 0.014 = 0.8498249493274369, training accuracy 0.8492014742014742, time elapsed: 3155 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9420117421303319
Loss in iteration 3 : 0.8375819692533307
Loss in iteration 4 : 0.6955205028416948
Loss in iteration 5 : 0.5229395736971499
Loss in iteration 6 : 0.4685577771138711
Loss in iteration 7 : 0.5091826051222221
Loss in iteration 8 : 0.5425832109645001
Loss in iteration 9 : 0.5620417517859922
Loss in iteration 10 : 0.5684660508272702
Loss in iteration 11 : 0.5635701535834378
Loss in iteration 12 : 0.5493402784944313
Loss in iteration 13 : 0.5279247210136061
Loss in iteration 14 : 0.501335739039643
Loss in iteration 15 : 0.4718816797245111
Loss in iteration 16 : 0.4421420513601724
Loss in iteration 17 : 0.41564157733251994
Loss in iteration 18 : 0.3958446492849694
Loss in iteration 19 : 0.3851734762185925
Loss in iteration 20 : 0.3829080573722063
Loss in iteration 21 : 0.3867616058925913
Loss in iteration 22 : 0.3931611100838367
Loss in iteration 23 : 0.3993121096255663
Loss in iteration 24 : 0.4031191812131352
Loss in iteration 25 : 0.4036765042229846
Loss in iteration 26 : 0.40087143518983537
Loss in iteration 27 : 0.39540206381464904
Loss in iteration 28 : 0.3883119808067573
Loss in iteration 29 : 0.38072985677492244
Loss in iteration 30 : 0.37394026107738065
Loss in iteration 31 : 0.3686510657104723
Loss in iteration 32 : 0.3652297757076456
Loss in iteration 33 : 0.36385570417081464
Loss in iteration 34 : 0.3639163100566067
Loss in iteration 35 : 0.364754972336801
Loss in iteration 36 : 0.3658798855786381
Loss in iteration 37 : 0.36687342745244245
Loss in iteration 38 : 0.3674156543739802
Loss in iteration 39 : 0.36735974179453507
Loss in iteration 40 : 0.36669449156648987
Loss in iteration 41 : 0.36550228971183735
Loss in iteration 42 : 0.3639837385754553
Loss in iteration 43 : 0.36239233993332814
Loss in iteration 44 : 0.36083744409395474
Loss in iteration 45 : 0.35950225034889416
Loss in iteration 46 : 0.3585049041895222
Loss in iteration 47 : 0.3577799326232296
Loss in iteration 48 : 0.3574440683419988
Loss in iteration 49 : 0.3573928709931654
Loss in iteration 50 : 0.35747368656052314
Loss in iteration 51 : 0.35765530368615167
Loss in iteration 52 : 0.35778758758908774
Loss in iteration 53 : 0.35780433593958727
Loss in iteration 54 : 0.3576864633129488
Loss in iteration 55 : 0.35743509671268164
Loss in iteration 56 : 0.35708018922104223
Loss in iteration 57 : 0.3566610273387276
Loss in iteration 58 : 0.3562295346496428
Loss in iteration 59 : 0.35585002664346377
Loss in iteration 60 : 0.35556236189856166
Loss in iteration 61 : 0.3553822825844789
Loss in iteration 62 : 0.35528323421270813
Loss in iteration 63 : 0.35523382482680066
Loss in iteration 64 : 0.3552183856082147
Loss in iteration 65 : 0.35521601009813886
Loss in iteration 66 : 0.35520719449773624
Loss in iteration 67 : 0.3551797494987986
Loss in iteration 68 : 0.35512591472274296
Loss in iteration 69 : 0.35504470476900196
Loss in iteration 70 : 0.3549417712186986
Loss in iteration 71 : 0.35482651782423275
Loss in iteration 72 : 0.35470753786279013
Loss in iteration 73 : 0.3545954153857748
Loss in iteration 74 : 0.354495667832548
Loss in iteration 75 : 0.3544110277247182
Loss in iteration 76 : 0.3543466891189611
Loss in iteration 77 : 0.3543026981850419
Loss in iteration 78 : 0.3542710392815514
Loss in iteration 79 : 0.35424547003518453
Loss in iteration 80 : 0.3542204349885287
Loss in iteration 81 : 0.35419329888659323
Loss in iteration 82 : 0.3541556286340516
Loss in iteration 83 : 0.354107555398716
Loss in iteration 84 : 0.3540559501687841
Loss in iteration 85 : 0.35400336666317606
Loss in iteration 86 : 0.35395395799040197
Loss in iteration 87 : 0.3539074731795621
Loss in iteration 88 : 0.35386583222723494
Loss in iteration 89 : 0.35382965093559715
Loss in iteration 90 : 0.35379835167774915
Loss in iteration 91 : 0.3537712766495779
Loss in iteration 92 : 0.35374593938228066
Loss in iteration 93 : 0.35372043462760794
Loss in iteration 94 : 0.3536933776861858
Loss in iteration 95 : 0.3536638877740581
Loss in iteration 96 : 0.3536326023784589
Loss in iteration 97 : 0.3536005011175035
Loss in iteration 98 : 0.3535689115250234
Loss in iteration 99 : 0.35353930030605035
Loss in iteration 100 : 0.3535137632094325
Loss in iteration 101 : 0.35349107562515614
Loss in iteration 102 : 0.3534691150713464
Loss in iteration 103 : 0.353447673293452
Loss in iteration 104 : 0.35342654032060783
Loss in iteration 105 : 0.3534053605119725
Loss in iteration 106 : 0.35338357033868095
Loss in iteration 107 : 0.3533614916342325
Loss in iteration 108 : 0.3533392816394955
Loss in iteration 109 : 0.3533173855181174
Loss in iteration 110 : 0.35329637092373767
Loss in iteration 111 : 0.35327615647600397
Loss in iteration 112 : 0.35325662999571267
Loss in iteration 113 : 0.35323795042786893
Loss in iteration 114 : 0.35321987331780264
Loss in iteration 115 : 0.35320127002485624
Loss in iteration 116 : 0.35318224982376967
Loss in iteration 117 : 0.3531635060005804
Loss in iteration 118 : 0.35314593622326973
Loss in iteration 119 : 0.35312888226301253
Loss in iteration 120 : 0.3531120701532192
Loss in iteration 121 : 0.35309547637519056
Loss in iteration 122 : 0.35307915151275565
Loss in iteration 123 : 0.35306297812592863
Loss in iteration 124 : 0.35304686890755954
Loss in iteration 125 : 0.35303119080245265
Loss in iteration 126 : 0.35301567824205665
Loss in iteration 127 : 0.35300034825394094
Loss in iteration 128 : 0.3529852418603391
Loss in iteration 129 : 0.3529702845877037
Loss in iteration 130 : 0.35295573219172854
Loss in iteration 131 : 0.352941392118582
Loss in iteration 132 : 0.35292708718486693
Loss in iteration 133 : 0.3529129279475143
Loss in iteration 134 : 0.3528988128115168
Loss in iteration 135 : 0.35288482357739825
Loss in iteration 136 : 0.35287138599909934
Loss in iteration 137 : 0.3528580337080032
Loss in iteration 138 : 0.35284484147971706
Loss in iteration 139 : 0.3528320095049153
Loss in iteration 140 : 0.35281932896279794
Loss in iteration 141 : 0.35280691291921096
Loss in iteration 142 : 0.3527947360071624
Loss in iteration 143 : 0.3527826736457106
Loss in iteration 144 : 0.3527707395344888
Loss in iteration 145 : 0.35275902851972546
Loss in iteration 146 : 0.35274733584128326
Loss in iteration 147 : 0.35273575597956447
Loss in iteration 148 : 0.3527243518574823
Loss in iteration 149 : 0.35271317101023353
Loss in iteration 150 : 0.3527021184887169
Loss in iteration 151 : 0.352691142551877
Loss in iteration 152 : 0.3526803752921217
Loss in iteration 153 : 0.35266964560253095
Loss in iteration 154 : 0.3526589761881341
Testing accuracy  of updater 9 on alg 1 with rate 0.008 = 0.8491493151526319, training accuracy 0.8482186732186732, time elapsed: 2431 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9879629200123471
Loss in iteration 3 : 0.9659895694444313
Loss in iteration 4 : 0.935712441303748
Loss in iteration 5 : 0.8984842318423434
Loss in iteration 6 : 0.8554340330955871
Loss in iteration 7 : 0.8075098672364296
Loss in iteration 8 : 0.7555115408708292
Loss in iteration 9 : 0.700116450971249
Loss in iteration 10 : 0.6419001406609682
Loss in iteration 11 : 0.5813528679747931
Loss in iteration 12 : 0.5189532738803793
Loss in iteration 13 : 0.47851279994174334
Loss in iteration 14 : 0.4650215887467593
Loss in iteration 15 : 0.4695217999361576
Loss in iteration 16 : 0.4791627435675805
Loss in iteration 17 : 0.4886136481751801
Loss in iteration 18 : 0.4960790762429652
Loss in iteration 19 : 0.501144973979291
Loss in iteration 20 : 0.5038802819283461
Loss in iteration 21 : 0.5044658970644901
Loss in iteration 22 : 0.5031496325776292
Loss in iteration 23 : 0.5002031083537921
Loss in iteration 24 : 0.49588306720754244
Loss in iteration 25 : 0.4904579800756334
Loss in iteration 26 : 0.4842044521696329
Loss in iteration 27 : 0.4773826052376187
Loss in iteration 28 : 0.470269601045301
Loss in iteration 29 : 0.4631092134981611
Loss in iteration 30 : 0.45617193772277403
Loss in iteration 31 : 0.44976109187380664
Loss in iteration 32 : 0.44412795293274754
Loss in iteration 33 : 0.43936591349883386
Loss in iteration 34 : 0.43551771139307627
Loss in iteration 35 : 0.43253930415231934
Loss in iteration 36 : 0.4303834959296963
Loss in iteration 37 : 0.42890079504077916
Loss in iteration 38 : 0.42788164873987966
Loss in iteration 39 : 0.4271716400371587
Loss in iteration 40 : 0.42660917568673745
Loss in iteration 41 : 0.42608761280204976
Loss in iteration 42 : 0.42552247455153563
Loss in iteration 43 : 0.42486612052358347
Loss in iteration 44 : 0.4240907028566038
Loss in iteration 45 : 0.42318542150401456
Loss in iteration 46 : 0.4221527929109877
Loss in iteration 47 : 0.421005017528034
Loss in iteration 48 : 0.41976323517570235
Loss in iteration 49 : 0.41845260618438046
Loss in iteration 50 : 0.4171010321678272
Loss in iteration 51 : 0.4157288815863877
Loss in iteration 52 : 0.41436469756446265
Loss in iteration 53 : 0.4130342759072754
Loss in iteration 54 : 0.41174552962275873
Loss in iteration 55 : 0.4105118900739792
Loss in iteration 56 : 0.4093421368992014
Loss in iteration 57 : 0.40823437165447946
Loss in iteration 58 : 0.40718935352558255
Loss in iteration 59 : 0.40620780945578056
Loss in iteration 60 : 0.4052894528724835
Loss in iteration 61 : 0.4044245100388379
Loss in iteration 62 : 0.40359499066115617
Loss in iteration 63 : 0.402801613303838
Loss in iteration 64 : 0.40203263441001036
Loss in iteration 65 : 0.4012762544400106
Loss in iteration 66 : 0.4005266196970829
Loss in iteration 67 : 0.3997829775017967
Loss in iteration 68 : 0.3990454243100354
Loss in iteration 69 : 0.39831719176141034
Loss in iteration 70 : 0.39759133572735306
Loss in iteration 71 : 0.39687028303030825
Loss in iteration 72 : 0.3961553495326325
Loss in iteration 73 : 0.3954515679990069
Loss in iteration 74 : 0.39475548669541244
Loss in iteration 75 : 0.3940737732142749
Loss in iteration 76 : 0.3934041363685197
Loss in iteration 77 : 0.392747601790038
Loss in iteration 78 : 0.3921084039802104
Loss in iteration 79 : 0.391486638526688
Loss in iteration 80 : 0.39088322388592694
Loss in iteration 81 : 0.3902935802001982
Loss in iteration 82 : 0.3897200682530594
Loss in iteration 83 : 0.389162099122775
Loss in iteration 84 : 0.38862446769684694
Loss in iteration 85 : 0.38810640482242575
Loss in iteration 86 : 0.3876011339289275
Loss in iteration 87 : 0.3871078672719859
Loss in iteration 88 : 0.3866281705335267
Loss in iteration 89 : 0.38615873326515604
Loss in iteration 90 : 0.3857001925936215
Loss in iteration 91 : 0.38525318286774646
Loss in iteration 92 : 0.38481301315762445
Loss in iteration 93 : 0.38437970318693754
Loss in iteration 94 : 0.3839565151319043
Loss in iteration 95 : 0.38354338148815326
Loss in iteration 96 : 0.3831374172092901
Loss in iteration 97 : 0.38273840630873457
Loss in iteration 98 : 0.38234731573271946
Loss in iteration 99 : 0.38196433826782267
Loss in iteration 100 : 0.3815897482483203
Loss in iteration 101 : 0.3812235231018203
Loss in iteration 102 : 0.38086446064699325
Loss in iteration 103 : 0.3805129750146317
Loss in iteration 104 : 0.3801694153533271
Loss in iteration 105 : 0.3798327273266768
Loss in iteration 106 : 0.3795026706831252
Loss in iteration 107 : 0.379178411995169
Loss in iteration 108 : 0.378860517326057
Loss in iteration 109 : 0.3785483392758683
Loss in iteration 110 : 0.37824162943143363
Loss in iteration 111 : 0.3779406644618535
Loss in iteration 112 : 0.3776460642244173
Loss in iteration 113 : 0.3773577486299527
Loss in iteration 114 : 0.3770750750090773
Loss in iteration 115 : 0.3767973131645007
Loss in iteration 116 : 0.3765244402158816
Loss in iteration 117 : 0.3762564820560497
Loss in iteration 118 : 0.375993588900411
Loss in iteration 119 : 0.37573668386406417
Loss in iteration 120 : 0.37548509424463977
Loss in iteration 121 : 0.37523838415270244
Loss in iteration 122 : 0.37499593224089783
Loss in iteration 123 : 0.37475743659978394
Loss in iteration 124 : 0.3745233144119089
Loss in iteration 125 : 0.37429277701883085
Loss in iteration 126 : 0.37406687792610005
Loss in iteration 127 : 0.3738440569929603
Loss in iteration 128 : 0.37362480977326357
Loss in iteration 129 : 0.3734092409876598
Loss in iteration 130 : 0.37319790939239783
Loss in iteration 131 : 0.372989784435488
Loss in iteration 132 : 0.37278512472460523
Loss in iteration 133 : 0.372583277177948
Loss in iteration 134 : 0.3723851322717147
Loss in iteration 135 : 0.3721905461361153
Loss in iteration 136 : 0.371998994545244
Loss in iteration 137 : 0.3718101203967672
Loss in iteration 138 : 0.3716237926140168
Loss in iteration 139 : 0.371440139920735
Loss in iteration 140 : 0.3712587295661965
Loss in iteration 141 : 0.37107966214920496
Loss in iteration 142 : 0.37090267879351807
Loss in iteration 143 : 0.37072769058962723
Loss in iteration 144 : 0.3705557597373226
Loss in iteration 145 : 0.37038637435000304
Loss in iteration 146 : 0.37021871664769224
Loss in iteration 147 : 0.3700542715582313
Loss in iteration 148 : 0.36989215973031425
Loss in iteration 149 : 0.36973332270744486
Loss in iteration 150 : 0.3695770824359379
Loss in iteration 151 : 0.3694232545680799
Loss in iteration 152 : 0.36927163028316423
Loss in iteration 153 : 0.3691223340269108
Loss in iteration 154 : 0.3689751120283591
Loss in iteration 155 : 0.36883024053285207
Loss in iteration 156 : 0.3686873673795787
Loss in iteration 157 : 0.3685467655973266
Loss in iteration 158 : 0.3684079189486229
Loss in iteration 159 : 0.3682709346926601
Loss in iteration 160 : 0.368135306030486
Loss in iteration 161 : 0.3680013703602982
Loss in iteration 162 : 0.367869281047733
Loss in iteration 163 : 0.36773949604451006
Loss in iteration 164 : 0.36761165556611264
Loss in iteration 165 : 0.3674855099697585
Loss in iteration 166 : 0.3673611285538798
Loss in iteration 167 : 0.367238428291479
Loss in iteration 168 : 0.3671171141541937
Loss in iteration 169 : 0.3669970579950594
Loss in iteration 170 : 0.3668786910728008
Loss in iteration 171 : 0.3667616077081606
Loss in iteration 172 : 0.3666456363637854
Loss in iteration 173 : 0.36653169774005956
Loss in iteration 174 : 0.36641975620440387
Loss in iteration 175 : 0.3663093459022861
Loss in iteration 176 : 0.3662004842610853
Loss in iteration 177 : 0.3660933979869009
Loss in iteration 178 : 0.36598827503600057
Loss in iteration 179 : 0.36588500285599335
Loss in iteration 180 : 0.36578308557424855
Loss in iteration 181 : 0.3656822337764903
Loss in iteration 182 : 0.3655827465269723
Loss in iteration 183 : 0.3654847615123395
Loss in iteration 184 : 0.36538800551533845
Loss in iteration 185 : 0.36529241186560213
Loss in iteration 186 : 0.36519808609922133
Loss in iteration 187 : 0.3651047773103616
Loss in iteration 188 : 0.3650123915913203
Loss in iteration 189 : 0.3649208609575888
Loss in iteration 190 : 0.3648303619957431
Loss in iteration 191 : 0.36474095002300105
Loss in iteration 192 : 0.36465238023681495
Loss in iteration 193 : 0.36456468685665455
Loss in iteration 194 : 0.36447790434840266
Loss in iteration 195 : 0.36439182217511995
Loss in iteration 196 : 0.36430659847209945
Loss in iteration 197 : 0.3642222352989138
Loss in iteration 198 : 0.3641385491584172
Loss in iteration 199 : 0.3640557118252217
Loss in iteration 200 : 0.3639735368121204
Testing accuracy  of updater 9 on alg 1 with rate 0.0019999999999999983 = 0.8455868804127511, training accuracy 0.8445945945945946, time elapsed: 3554 millisecond.
