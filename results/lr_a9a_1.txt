objc[1851]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x10e0354c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10e0b94e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 19:58:06 INFO SparkContext: Running Spark version 2.0.0
18/02/26 19:58:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 19:58:07 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 19:58:07 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 19:58:07 INFO SecurityManager: Changing view acls groups to: 
18/02/26 19:58:07 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 19:58:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 19:58:08 INFO Utils: Successfully started service 'sparkDriver' on port 50849.
18/02/26 19:58:08 INFO SparkEnv: Registering MapOutputTracker
18/02/26 19:58:08 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 19:58:08 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-12eb4878-b908-4cee-b2f7-400bc5f54af1
18/02/26 19:58:08 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 19:58:08 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 19:58:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 19:58:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 19:58:09 INFO Executor: Starting executor ID driver on host localhost
18/02/26 19:58:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50850.
18/02/26 19:58:09 INFO NettyBlockTransferService: Server created on 192.168.2.140:50850
18/02/26 19:58:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50850)
18/02/26 19:58:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50850 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50850)
18/02/26 19:58:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50850)
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5309044865682255
Loss in iteration 3 : 0.48006841651644
Loss in iteration 4 : 0.45713603284681786
Loss in iteration 5 : 0.44001759513598765
Loss in iteration 6 : 0.42690872191348705
Loss in iteration 7 : 0.4166211311753286
Loss in iteration 8 : 0.40836455681194495
Loss in iteration 9 : 0.40160354258794156
Loss in iteration 10 : 0.3959676332341301
Loss in iteration 11 : 0.3911950457719669
Loss in iteration 12 : 0.3870969932970904
Loss in iteration 13 : 0.3835347162171088
Loss in iteration 14 : 0.3804044160573927
Loss in iteration 15 : 0.37762718146127133
Loss in iteration 16 : 0.37514212458771645
Loss in iteration 17 : 0.3729016203087438
Loss in iteration 18 : 0.37086794783533406
Loss in iteration 19 : 0.3690108840421445
Loss in iteration 20 : 0.36730595333526656
Loss in iteration 21 : 0.36573313751186676
Loss in iteration 22 : 0.364275912611671
Loss in iteration 23 : 0.3629205213895037
Loss in iteration 24 : 0.36165541773439897
Loss in iteration 25 : 0.36047083806197394
Loss in iteration 26 : 0.3593584675122685
Loss in iteration 27 : 0.35831117767056986
Loss in iteration 28 : 0.35732281877117633
Loss in iteration 29 : 0.3563880537820448
Loss in iteration 30 : 0.3555022249581998
Loss in iteration 31 : 0.35466124576910435
Loss in iteration 32 : 0.35386151280516115
Loss in iteration 33 : 0.35309983352737745
Loss in iteration 34 : 0.3523733666646589
Loss in iteration 35 : 0.3516795727717562
Loss in iteration 36 : 0.3510161729987353
Loss in iteration 37 : 0.3503811145345544
Loss in iteration 38 : 0.34977254150436254
Loss in iteration 39 : 0.3491887703461915
Loss in iteration 40 : 0.34862826888473336
Loss in iteration 41 : 0.34808963847078156
Loss in iteration 42 : 0.34757159867414106
Loss in iteration 43 : 0.3470729741123783
Loss in iteration 44 : 0.34659268307363805
Loss in iteration 45 : 0.34612972765214217
Loss in iteration 46 : 0.3456831851642783
Loss in iteration 47 : 0.34525220065255496
Loss in iteration 48 : 0.34483598031714835
Loss in iteration 49 : 0.3444337857411108
Loss in iteration 50 : 0.3440449287968642
Loss in iteration 51 : 0.3436687671395277
Loss in iteration 52 : 0.34330470020726744
Loss in iteration 53 : 0.34295216566111725
Loss in iteration 54 : 0.34261063620689663
Loss in iteration 55 : 0.34227961675026664
Loss in iteration 56 : 0.3419586418431681
Loss in iteration 57 : 0.3416472733857935
Loss in iteration 58 : 0.3413450985533085
Loss in iteration 59 : 0.34105172792076965
Loss in iteration 60 : 0.34076679376332225
Loss in iteration 61 : 0.3404899485117659
Loss in iteration 62 : 0.34022086334625706
Loss in iteration 63 : 0.3399592269130999
Loss in iteration 64 : 0.3397047441514636
Loss in iteration 65 : 0.3394571352185613
Loss in iteration 66 : 0.3392161345031907
Loss in iteration 67 : 0.3389814897187571
Loss in iteration 68 : 0.33875296106795577
Loss in iteration 69 : 0.3385303204722137
Loss in iteration 70 : 0.33831335085971603
Loss in iteration 71 : 0.33810184550664896
Loss in iteration 72 : 0.3378956074267543
Loss in iteration 73 : 0.33769444880492505
Loss in iteration 74 : 0.33749819047095064
Loss in iteration 75 : 0.3373066614099796
Loss in iteration 76 : 0.33711969830659105
Loss in iteration 77 : 0.3369371451196871
Loss in iteration 78 : 0.3367588526856969
Loss in iteration 79 : 0.33658467834781597
Loss in iteration 80 : 0.33641448560924847
Loss in iteration 81 : 0.33624814380856377
Loss in iteration 82 : 0.3360855278155239
Loss in iteration 83 : 0.3359265177457852
Loss in iteration 84 : 0.33577099869316024
Loss in iteration 85 : 0.33561886047806855
Loss in iteration 86 : 0.3354699974111051
Loss in iteration 87 : 0.3353243080705922
Loss in iteration 88 : 0.3351816950931625
Loss in iteration 89 : 0.33504206497649003
Loss in iteration 90 : 0.3349053278933235
Loss in iteration 91 : 0.33477139751606716
Loss in iteration 92 : 0.3346401908512218
Loss in iteration 93 : 0.33451162808303303
Loss in iteration 94 : 0.33438563242575736
Loss in iteration 95 : 0.3342621299839687
Loss in iteration 96 : 0.33414104962044977
Loss in iteration 97 : 0.3340223228311344
Loss in iteration 98 : 0.3339058836267066
Loss in iteration 99 : 0.33379166842043706
Loss in iteration 100 : 0.33367961592186274
Loss in iteration 101 : 0.3335696670359757
Loss in iteration 102 : 0.3334617647676024
Loss in iteration 103 : 0.3333558541306328
Loss in iteration 104 : 0.3332518820618591
Loss in iteration 105 : 0.3331497973391056
Loss in iteration 106 : 0.3330495505034616
Loss in iteration 107 : 0.33295109378534926
Loss in iteration 108 : 0.33285438103419723
Loss in iteration 109 : 0.3327593676515509
Loss in iteration 110 : 0.33266601052742295
Loss in iteration 111 : 0.3325742679796633
Loss in iteration 112 : 0.3324840996962599
Loss in iteration 113 : 0.33239546668032327
Loss in iteration 114 : 0.3323083311976863
Loss in iteration 115 : 0.3322226567269368
Loss in iteration 116 : 0.33213840791176097
Loss in iteration 117 : 0.33205555051546776
Loss in iteration 118 : 0.3319740513776274
Loss in iteration 119 : 0.33189387837263046
Loss in iteration 120 : 0.33181500037015105
Loss in iteration 121 : 0.331737387197368
Loss in iteration 122 : 0.33166100960284955
Loss in iteration 123 : 0.33158583922207124
Loss in iteration 124 : 0.33151184854441756
Loss in iteration 125 : 0.33143901088162603
Loss in iteration 126 : 0.33136730033762857
Loss in iteration 127 : 0.3312966917796465
Loss in iteration 128 : 0.3312271608105542
Loss in iteration 129 : 0.3311586837424284
Loss in iteration 130 : 0.33109123757116404
Loss in iteration 131 : 0.33102479995222495
Loss in iteration 132 : 0.3309593491773531
Loss in iteration 133 : 0.3308948641522736
Loss in iteration 134 : 0.33083132437530055
Loss in iteration 135 : 0.3307687099168416
Loss in iteration 136 : 0.33070700139971865
Loss in iteration 137 : 0.3306461799802963
Loss in iteration 138 : 0.3305862273303626
Loss in iteration 139 : 0.3305271256197453
Loss in iteration 140 : 0.33046885749960225
Loss in iteration 141 : 0.3304114060863979
Loss in iteration 142 : 0.3303547549464861
Loss in iteration 143 : 0.33029888808130803
Loss in iteration 144 : 0.3302437899131634
Loss in iteration 145 : 0.3301894452715282
Loss in iteration 146 : 0.33013583937990054
Loss in iteration 147 : 0.3300829578431487
Loss in iteration 148 : 0.33003078663533825
Loss in iteration 149 : 0.3299793120880199
Loss in iteration 150 : 0.32992852087895647
Loss in iteration 151 : 0.3298784000212693
Loss in iteration 152 : 0.32982893685299147
Loss in iteration 153 : 0.32978011902700033
Loss in iteration 154 : 0.32973193450132526
Loss in iteration 155 : 0.32968437152980135
Loss in iteration 156 : 0.32963741865307433
Loss in iteration 157 : 0.3295910646899158
Loss in iteration 158 : 0.3295452987288623
Loss in iteration 159 : 0.32950011012014774
Loss in iteration 160 : 0.32945548846791706
Loss in iteration 161 : 0.32941142362271747
Loss in iteration 162 : 0.3293679056742707
Loss in iteration 163 : 0.32932492494445903
Loss in iteration 164 : 0.3292824719805937
Loss in iteration 165 : 0.3292405375488948
Loss in iteration 166 : 0.32919911262820434
Loss in iteration 167 : 0.32915818840389877
Loss in iteration 168 : 0.3291177562620303
Loss in iteration 169 : 0.32907780778364637
Loss in iteration 170 : 0.3290383347393201
Loss in iteration 171 : 0.3289993290838364
Loss in iteration 172 : 0.32896078295108344
Loss in iteration 173 : 0.32892268864909896
Loss in iteration 174 : 0.32888503865527546
Loss in iteration 175 : 0.3288478256117286
Loss in iteration 176 : 0.328811042320818
Loss in iteration 177 : 0.32877468174081503
Loss in iteration 178 : 0.3287387369816933
Loss in iteration 179 : 0.3287032013010813
Loss in iteration 180 : 0.32866806810032473
Loss in iteration 181 : 0.32863333092066854
Loss in iteration 182 : 0.3285989834395838
Loss in iteration 183 : 0.3285650194671956
Loss in iteration 184 : 0.3285314329428073
Loss in iteration 185 : 0.3284982179315706
Loss in iteration 186 : 0.32846536862121345
Loss in iteration 187 : 0.32843287931891346
Loss in iteration 188 : 0.3284007444482339
Loss in iteration 189 : 0.328368958546158
Loss in iteration 190 : 0.3283375162602398
Loss in iteration 191 : 0.32830641234580127
Loss in iteration 192 : 0.32827564166324974
Loss in iteration 193 : 0.32824519917544065
Loss in iteration 194 : 0.3282150799451595
Loss in iteration 195 : 0.3281852791326341
Loss in iteration 196 : 0.3281557919931617
Loss in iteration 197 : 0.32812661387477315
Loss in iteration 198 : 0.3280977402159827
Loss in iteration 199 : 0.32806916654360047
Loss in iteration 200 : 0.32804088847059953
Testing accuracy  of updater 0 on alg 0 with rate 1.0 = 0.8519746944290891, training accuracy 0.8463759213759213, time elapsed: 8182 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6509822051128222
Loss in iteration 3 : 0.619779051756544
Loss in iteration 4 : 0.5963224403342656
Loss in iteration 5 : 0.5783439049270137
Loss in iteration 6 : 0.5642637446520444
Loss in iteration 7 : 0.552984463900704
Loss in iteration 8 : 0.543740671594539
Loss in iteration 9 : 0.535994765094726
Loss in iteration 10 : 0.5293658102006732
Loss in iteration 11 : 0.5235813245848356
Loss in iteration 12 : 0.5184445275875502
Loss in iteration 13 : 0.513811961853295
Loss in iteration 14 : 0.509578077305628
Loss in iteration 15 : 0.5056645141372714
Loss in iteration 16 : 0.5020125821730204
Loss in iteration 17 : 0.4985779342142503
Loss in iteration 18 : 0.4953267597265628
Loss in iteration 19 : 0.49223304216735253
Loss in iteration 20 : 0.48927656738392106
Loss in iteration 21 : 0.48644146708526764
Loss in iteration 22 : 0.48371514669508237
Loss in iteration 23 : 0.48108749147805946
Loss in iteration 24 : 0.47855027556311913
Loss in iteration 25 : 0.4760967198711903
Loss in iteration 26 : 0.473721159970511
Loss in iteration 27 : 0.47141879551896687
Loss in iteration 28 : 0.4691855005495949
Loss in iteration 29 : 0.46701767932390614
Loss in iteration 30 : 0.46491215644244005
Loss in iteration 31 : 0.4628660927962559
Loss in iteration 32 : 0.460876921068927
Loss in iteration 33 : 0.4589422960688341
Loss in iteration 34 : 0.4570600563376163
Loss in iteration 35 : 0.4552281943503443
Loss in iteration 36 : 0.4534448332745742
Loss in iteration 37 : 0.45170820874524453
Loss in iteration 38 : 0.4500166544819758
Loss in iteration 39 : 0.44836859085485004
Loss in iteration 40 : 0.44676251571672926
Loss in iteration 41 : 0.44519699698141435
Loss in iteration 42 : 0.44367066654949483
Loss in iteration 43 : 0.4421822152773387
Loss in iteration 44 : 0.44073038875616505
Loss in iteration 45 : 0.43931398372258534
Loss in iteration 46 : 0.43793184496383747
Loss in iteration 47 : 0.43658286261281365
Loss in iteration 48 : 0.43526596975243304
Loss in iteration 49 : 0.43398014026764575
Loss in iteration 50 : 0.43272438689756965
Loss in iteration 51 : 0.43149775945133956
Loss in iteration 52 : 0.43029934315950724
Loss in iteration 53 : 0.42912825713930147
Loss in iteration 54 : 0.42798365295692237
Loss in iteration 55 : 0.42686471327376097
Loss in iteration 56 : 0.4257706505663162
Loss in iteration 57 : 0.4247007059116981
Loss in iteration 58 : 0.4236541478323447
Loss in iteration 59 : 0.4226302711947122
Loss in iteration 60 : 0.42162839615780084
Loss in iteration 61 : 0.4206478671679502
Loss in iteration 62 : 0.41968805199707804
Loss in iteration 63 : 0.4187483408217634
Loss in iteration 64 : 0.4178281453410574
Loss in iteration 65 : 0.41692689793107207
Loss in iteration 66 : 0.41604405083463414
Loss in iteration 67 : 0.4151790753843763
Loss in iteration 68 : 0.41433146125788617
Loss in iteration 69 : 0.41350071576346087
Loss in iteration 70 : 0.4126863631552747
Loss in iteration 71 : 0.41188794397666284
Loss in iteration 72 : 0.41110501443044034
Loss in iteration 73 : 0.4103371457750899
Loss in iteration 74 : 0.4095839237458105
Loss in iteration 75 : 0.4088449479993622
Loss in iteration 76 : 0.40811983158178033
Loss in iteration 77 : 0.40740820041795955
Loss in iteration 78 : 0.40670969282226976
Loss in iteration 79 : 0.4060239590292652
Loss in iteration 80 : 0.405350660743692
Loss in iteration 81 : 0.40468947070898803
Loss in iteration 82 : 0.40404007229344346
Loss in iteration 83 : 0.4034021590933795
Loss in iteration 84 : 0.4027754345524956
Loss in iteration 85 : 0.4021596115968106
Loss in iteration 86 : 0.40155441228449684
Loss in iteration 87 : 0.4009595674699313
Loss in iteration 88 : 0.40037481648142037
Loss in iteration 89 : 0.3997999068119872
Loss in iteration 90 : 0.3992345938226375
Loss in iteration 91 : 0.39867864045763596
Loss in iteration 92 : 0.39813181697118394
Loss in iteration 93 : 0.39759390066510003
Loss in iteration 94 : 0.3970646756369867
Loss in iteration 95 : 0.39654393253843007
Loss in iteration 96 : 0.396031468342834
Loss in iteration 97 : 0.3955270861224234
Loss in iteration 98 : 0.3950305948341068
Loss in iteration 99 : 0.39454180911373393
Loss in iteration 100 : 0.3940605490784329
Loss in iteration 101 : 0.39358664013670336
Loss in iteration 102 : 0.3931199128058791
Loss in iteration 103 : 0.39266020253671885
Loss in iteration 104 : 0.39220734954473885
Loss in iteration 105 : 0.3917611986480938
Loss in iteration 106 : 0.3913215991116536
Loss in iteration 107 : 0.3908884044970715
Loss in iteration 108 : 0.3904614725185601
Loss in iteration 109 : 0.39004066490412603
Loss in iteration 110 : 0.38962584726209337
Loss in iteration 111 : 0.3892168889526263
Loss in iteration 112 : 0.3888136629640995
Loss in iteration 113 : 0.3884160457940592
Loss in iteration 114 : 0.3880239173346572
Loss in iteration 115 : 0.3876371607623131
Loss in iteration 116 : 0.38725566243144877
Loss in iteration 117 : 0.3868793117721562
Loss in iteration 118 : 0.3865080011915877
Loss in iteration 119 : 0.38614162597897317
Loss in iteration 120 : 0.38578008421406684
Loss in iteration 121 : 0.3854232766789429
Loss in iteration 122 : 0.38507110677292566
Loss in iteration 123 : 0.38472348043061805
Loss in iteration 124 : 0.3843803060428215
Loss in iteration 125 : 0.38404149438032603
Loss in iteration 126 : 0.38370695852034586
Loss in iteration 127 : 0.3833766137755962
Loss in iteration 128 : 0.38305037762583766
Loss in iteration 129 : 0.3827281696518407
Loss in iteration 130 : 0.38240991147163833
Loss in iteration 131 : 0.38209552667900787
Loss in iteration 132 : 0.38178494078405306
Loss in iteration 133 : 0.38147808115587994
Loss in iteration 134 : 0.38117487696719665
Loss in iteration 135 : 0.3808752591408225
Loss in iteration 136 : 0.38057916029802724
Loss in iteration 137 : 0.38028651470860475
Loss in iteration 138 : 0.3799972582426407
Loss in iteration 139 : 0.3797113283239042
Loss in iteration 140 : 0.37942866388477664
Loss in iteration 141 : 0.3791492053227238
Loss in iteration 142 : 0.3788728944581797
Loss in iteration 143 : 0.3785996744938271
Loss in iteration 144 : 0.37832948997522464
Loss in iteration 145 : 0.37806228675273124
Loss in iteration 146 : 0.3777980119446655
Loss in iteration 147 : 0.37753661390165366
Loss in iteration 148 : 0.3772780421721516
Loss in iteration 149 : 0.37702224746906293
Loss in iteration 150 : 0.37676918163744477
Loss in iteration 151 : 0.37651879762322
Loss in iteration 152 : 0.3762710494429128
Loss in iteration 153 : 0.3760258921543399
Loss in iteration 154 : 0.3757832818282125
Loss in iteration 155 : 0.37554317552063843
Loss in iteration 156 : 0.37530553124650295
Loss in iteration 157 : 0.37507030795365515
Loss in iteration 158 : 0.37483746549790004
Loss in iteration 159 : 0.3746069646188045
Loss in iteration 160 : 0.3743787669161717
Loss in iteration 161 : 0.3741528348273166
Loss in iteration 162 : 0.3739291316049955
Loss in iteration 163 : 0.3737076212960117
Loss in iteration 164 : 0.3734882687204658
Loss in iteration 165 : 0.3732710394516717
Loss in iteration 166 : 0.37305589979660436
Loss in iteration 167 : 0.3728428167770154
Loss in iteration 168 : 0.37263175811104443
Loss in iteration 169 : 0.3724226921954235
Loss in iteration 170 : 0.3722155880881805
Loss in iteration 171 : 0.3720104154918798
Loss in iteration 172 : 0.37180714473731996
Loss in iteration 173 : 0.3716057467677343
Loss in iteration 174 : 0.371406193123462
Loss in iteration 175 : 0.3712084559270241
Loss in iteration 176 : 0.37101250786867224
Loss in iteration 177 : 0.3708183221923313
Loss in iteration 178 : 0.37062587268195823
Loss in iteration 179 : 0.3704351336482718
Loss in iteration 180 : 0.3702460799158957
Loss in iteration 181 : 0.3700586868108164
Loss in iteration 182 : 0.3698729301482463
Loss in iteration 183 : 0.36968878622078993
Loss in iteration 184 : 0.3695062317869752
Loss in iteration 185 : 0.3693252440600675
Loss in iteration 186 : 0.36914580069723424
Loss in iteration 187 : 0.368967879788978
Loss in iteration 188 : 0.36879145984888345
Loss in iteration 189 : 0.3686165198036367
Loss in iteration 190 : 0.36844303898331987
Loss in iteration 191 : 0.36827099711196
Loss in iteration 192 : 0.3681003742983748
Loss in iteration 193 : 0.3679311510271989
Loss in iteration 194 : 0.3677633081502264
Loss in iteration 195 : 0.3675968268779305
Loss in iteration 196 : 0.3674316887712399
Loss in iteration 197 : 0.3672678757335291
Loss in iteration 198 : 0.36710537000281634
Loss in iteration 199 : 0.3669441541441645
Loss in iteration 200 : 0.3667842110423048
Testing accuracy  of updater 0 on alg 0 with rate 0.1 = 0.832626988514219, training accuracy 0.8330773955773956, time elapsed: 5105 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6886400023527108
Loss in iteration 3 : 0.6842601786193969
Loss in iteration 4 : 0.6800037710722232
Loss in iteration 5 : 0.6758669496697721
Loss in iteration 6 : 0.6718459912928539
Loss in iteration 7 : 0.6679372782715293
Loss in iteration 8 : 0.6641372967812018
Loss in iteration 9 : 0.6604426351248066
Loss in iteration 10 : 0.6568499819168391
Loss in iteration 11 : 0.6533561241837569
Loss in iteration 12 : 0.649957945394386
Loss in iteration 13 : 0.6466524234328171
Loss in iteration 14 : 0.6434366285251903
Loss in iteration 15 : 0.6403077211309959
Loss in iteration 16 : 0.637262949808468
Loss in iteration 17 : 0.634299649062869
Loss in iteration 18 : 0.6314152371854855
Loss in iteration 19 : 0.6286072140907204
Loss in iteration 20 : 0.6258731591575498
Loss in iteration 21 : 0.6232107290812692
Loss in iteration 22 : 0.6206176557406182
Loss in iteration 23 : 0.6180917440849446
Loss in iteration 24 : 0.6156308700453633
Loss in iteration 25 : 0.6132329784735517
Loss in iteration 26 : 0.6108960811112276
Loss in iteration 27 : 0.608618254592957
Loss in iteration 28 : 0.6063976384846084
Loss in iteration 29 : 0.6042324333593815
Loss in iteration 30 : 0.6021208989129523
Loss in iteration 31 : 0.6000613521191008
Loss in iteration 32 : 0.5980521654268287
Loss in iteration 33 : 0.5960917649997384
Loss in iteration 34 : 0.594178628998282
Loss in iteration 35 : 0.5923112859052435
Loss in iteration 36 : 0.5904883128946101
Loss in iteration 37 : 0.5887083342439462
Loss in iteration 38 : 0.5869700197900799
Loss in iteration 39 : 0.5852720834279574
Loss in iteration 40 : 0.5836132816522461
Loss in iteration 41 : 0.5819924121413126
Loss in iteration 42 : 0.5804083123830402
Loss in iteration 43 : 0.5788598583418363
Loss in iteration 44 : 0.5773459631662686
Loss in iteration 45 : 0.57586557593652
Loss in iteration 46 : 0.5744176804509892
Loss in iteration 47 : 0.5730012940511713
Loss in iteration 48 : 0.5716154664840445
Loss in iteration 49 : 0.5702592788010978
Loss in iteration 50 : 0.5689318422930981
Loss in iteration 51 : 0.5676322974597768
Loss in iteration 52 : 0.5663598130134923
Loss in iteration 53 : 0.5651135849159863
Loss in iteration 54 : 0.5638928354473577
Loss in iteration 55 : 0.5626968123063173
Loss in iteration 56 : 0.5615247877408713
Loss in iteration 57 : 0.5603760577085196
Loss in iteration 58 : 0.5592499410651042
Loss in iteration 59 : 0.5581457787814444
Loss in iteration 60 : 0.5570629331869101
Loss in iteration 61 : 0.5560007872390669
Loss in iteration 62 : 0.5549587438186196
Loss in iteration 63 : 0.5539362250487755
Loss in iteration 64 : 0.5529326716383465
Loss in iteration 65 : 0.5519475422476627
Loss in iteration 66 : 0.5509803128766962
Loss in iteration 67 : 0.5500304762745434
Loss in iteration 68 : 0.5490975413696533
Loss in iteration 69 : 0.5481810327199695
Loss in iteration 70 : 0.5472804899824265
Loss in iteration 71 : 0.546395467401081
Loss in iteration 72 : 0.5455255333132607
Loss in iteration 73 : 0.5446702696730552
Loss in iteration 74 : 0.5438292715916107
Loss in iteration 75 : 0.5430021468936179
Loss in iteration 76 : 0.5421885156893975
Loss in iteration 77 : 0.5413880099620468
Loss in iteration 78 : 0.5406002731691794
Loss in iteration 79 : 0.5398249598585998
Loss in iteration 80 : 0.5390617352975656
Loss in iteration 81 : 0.5383102751150755
Loss in iteration 82 : 0.5375702649566785
Loss in iteration 83 : 0.5368414001514903
Loss in iteration 84 : 0.536123385390825
Loss in iteration 85 : 0.5354159344181283
Loss in iteration 86 : 0.5347187697297526
Loss in iteration 87 : 0.5340316222862348
Loss in iteration 88 : 0.5333542312336137
Loss in iteration 89 : 0.5326863436345253
Loss in iteration 90 : 0.5320277142086439
Loss in iteration 91 : 0.531378105082172
Loss in iteration 92 : 0.5307372855460284
Loss in iteration 93 : 0.5301050318224573
Loss in iteration 94 : 0.5294811268396897
Loss in iteration 95 : 0.5288653600144418
Loss in iteration 96 : 0.5282575270419065
Loss in iteration 97 : 0.5276574296930077
Loss in iteration 98 : 0.5270648756186045
Loss in iteration 99 : 0.5264796781604646
Loss in iteration 100 : 0.5259016561687185
Loss in iteration 101 : 0.5253306338255415
Loss in iteration 102 : 0.5247664404748904
Loss in iteration 103 : 0.5242089104580285
Loss in iteration 104 : 0.5236578829546955
Loss in iteration 105 : 0.5231132018296237
Loss in iteration 106 : 0.5225747154842929
Loss in iteration 107 : 0.5220422767137092
Loss in iteration 108 : 0.5215157425679976
Loss in iteration 109 : 0.5209949742186939
Loss in iteration 110 : 0.5204798368295148
Loss in iteration 111 : 0.5199701994314685
Loss in iteration 112 : 0.5194659348021645
Loss in iteration 113 : 0.5189669193491486
Loss in iteration 114 : 0.5184730329971344
Loss in iteration 115 : 0.5179841590789909
Loss in iteration 116 : 0.5175001842303356
Loss in iteration 117 : 0.5170209982876576
Loss in iteration 118 : 0.5165464941897778
Loss in iteration 119 : 0.5160765678825687
Loss in iteration 120 : 0.5156111182268338
Loss in iteration 121 : 0.5151500469091732
Loss in iteration 122 : 0.5146932583558006
Loss in iteration 123 : 0.5142406596491975
Loss in iteration 124 : 0.5137921604474263
Loss in iteration 125 : 0.5133476729061304
Loss in iteration 126 : 0.5129071116030429
Loss in iteration 127 : 0.5124703934649356
Loss in iteration 128 : 0.5120374376969646
Loss in iteration 129 : 0.5116081657142305
Loss in iteration 130 : 0.5111825010756212
Loss in iteration 131 : 0.5107603694197211
Loss in iteration 132 : 0.5103416984028057
Loss in iteration 133 : 0.5099264176387909
Loss in iteration 134 : 0.5095144586411309
Loss in iteration 135 : 0.5091057547665359
Loss in iteration 136 : 0.5087002411605092
Loss in iteration 137 : 0.5082978547045665
Loss in iteration 138 : 0.5078985339651572
Loss in iteration 139 : 0.5075022191441968
Loss in iteration 140 : 0.5071088520311372
Loss in iteration 141 : 0.5067183759565422
Loss in iteration 142 : 0.5063307357471502
Loss in iteration 143 : 0.5059458776822884
Loss in iteration 144 : 0.5055637494516915
Loss in iteration 145 : 0.5051843001146165
Loss in iteration 146 : 0.5048074800602221
Loss in iteration 147 : 0.5044332409691747
Loss in iteration 148 : 0.504061535776449
Loss in iteration 149 : 0.5036923186352796
Loss in iteration 150 : 0.5033255448822015
Loss in iteration 151 : 0.5029611710031833
Loss in iteration 152 : 0.5025991546007955
Loss in iteration 153 : 0.5022394543623672
Loss in iteration 154 : 0.5018820300291261
Loss in iteration 155 : 0.5015268423662864
Loss in iteration 156 : 0.5011738531340256
Loss in iteration 157 : 0.5008230250593446
Loss in iteration 158 : 0.5004743218088069
Loss in iteration 159 : 0.5001277079620371
Loss in iteration 160 : 0.49978314898609705
Loss in iteration 161 : 0.4994406112105659
Loss in iteration 162 : 0.49910006180339994
Loss in iteration 163 : 0.49876146874751465
Loss in iteration 164 : 0.49842480081805074
Loss in iteration 165 : 0.4980900275603246
Loss in iteration 166 : 0.497757119268429
Loss in iteration 167 : 0.49742604696449766
Loss in iteration 168 : 0.49709678237853505
Loss in iteration 169 : 0.49676929792888
Loss in iteration 170 : 0.49644356670322276
Loss in iteration 171 : 0.4961195624402059
Loss in iteration 172 : 0.49579725951154313
Loss in iteration 173 : 0.4954766329046529
Loss in iteration 174 : 0.4951576582058483
Loss in iteration 175 : 0.49484031158396113
Loss in iteration 176 : 0.4945245697744672
Loss in iteration 177 : 0.49421041006409155
Loss in iteration 178 : 0.49389781027581836
Loss in iteration 179 : 0.49358674875438485
Loss in iteration 180 : 0.49327720435214895
Loss in iteration 181 : 0.4929691564153854
Loss in iteration 182 : 0.4926625847709905
Loss in iteration 183 : 0.4923574697135324
Loss in iteration 184 : 0.492053791992712
Loss in iteration 185 : 0.49175153280116723
Loss in iteration 186 : 0.49145067376259327
Loss in iteration 187 : 0.49115119692027576
Loss in iteration 188 : 0.4908530847258784
Loss in iteration 189 : 0.49055632002858424
Loss in iteration 190 : 0.49026088606454316
Loss in iteration 191 : 0.4899667664466173
Loss in iteration 192 : 0.4896739451543962
Loss in iteration 193 : 0.4893824065245295
Loss in iteration 194 : 0.48909213524130485
Loss in iteration 195 : 0.4888031163274992
Loss in iteration 196 : 0.4885153351354934
Loss in iteration 197 : 0.4882287773386135
Loss in iteration 198 : 0.48794342892275133
Loss in iteration 199 : 0.48765927617818927
Loss in iteration 200 : 0.48737630569165225
Testing accuracy  of updater 0 on alg 0 with rate 0.009999999999999995 = 0.7639579878385848, training accuracy 0.7594594594594595, time elapsed: 5512 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5309044865682255
Loss in iteration 3 : 0.5809864492631956
Loss in iteration 4 : 0.47590219663978167
Loss in iteration 5 : 0.3792723652306178
Loss in iteration 6 : 0.46165039817009434
Loss in iteration 7 : 0.4403695827828505
Loss in iteration 8 : 0.37481651053464177
Loss in iteration 9 : 0.404808780678106
Loss in iteration 10 : 0.4370787634843275
Loss in iteration 11 : 0.4116743220583489
Loss in iteration 12 : 0.3783065818289199
Loss in iteration 13 : 0.3897792510724412
Loss in iteration 14 : 0.4078654747971792
Loss in iteration 15 : 0.3855731792911296
Loss in iteration 16 : 0.3606031327201985
Loss in iteration 17 : 0.36643626884891084
Loss in iteration 18 : 0.3747010511836947
Loss in iteration 19 : 0.36049514931362336
Loss in iteration 20 : 0.34381080369745376
Loss in iteration 21 : 0.3502859285610308
Loss in iteration 22 : 0.3586033275413586
Loss in iteration 23 : 0.3471709602031832
Loss in iteration 24 : 0.34147284306118125
Loss in iteration 25 : 0.3509705895481439
Loss in iteration 26 : 0.3519289959767535
Loss in iteration 27 : 0.34208449290700443
Loss in iteration 28 : 0.3414844241824773
Loss in iteration 29 : 0.34631793971565794
Loss in iteration 30 : 0.34023408446691916
Loss in iteration 31 : 0.3338522311110685
Loss in iteration 32 : 0.33620377383647965
Loss in iteration 33 : 0.3360663050076217
Loss in iteration 34 : 0.3306407856773908
Loss in iteration 35 : 0.32971710423085104
Loss in iteration 36 : 0.33253760646314556
Loss in iteration 37 : 0.3310309721818364
Loss in iteration 38 : 0.32856071764509015
Loss in iteration 39 : 0.3301370716024686
Loss in iteration 40 : 0.33127558006601115
Loss in iteration 41 : 0.329268136832325
Loss in iteration 42 : 0.3283670402107795
Loss in iteration 43 : 0.32955785670589405
Loss in iteration 44 : 0.3289173525321304
Loss in iteration 45 : 0.32710205992268726
Loss in iteration 46 : 0.3272085572585722
Loss in iteration 47 : 0.32759767026661285
Loss in iteration 48 : 0.3264420030735576
Loss in iteration 49 : 0.3258224672555753
Loss in iteration 50 : 0.3264609326343168
Loss in iteration 51 : 0.32625508140550613
Loss in iteration 52 : 0.3255179447196167
Loss in iteration 53 : 0.3257838587776452
Loss in iteration 54 : 0.3260334908471016
Loss in iteration 55 : 0.3254689784522358
Loss in iteration 56 : 0.3252571426708098
Loss in iteration 57 : 0.3255215581739473
Loss in iteration 58 : 0.3252612459980973
Loss in iteration 59 : 0.32486961055097924
Loss in iteration 60 : 0.3249833525228502
Loss in iteration 61 : 0.3250145180582727
Loss in iteration 62 : 0.32472360239131
Loss in iteration 63 : 0.3246796450255521
Loss in iteration 64 : 0.32482719416917727
Loss in iteration 65 : 0.3247176326813971
Loss in iteration 66 : 0.32457771650492456
Loss in iteration 67 : 0.3246611551783247
Loss in iteration 68 : 0.3246740473907465
Loss in iteration 69 : 0.3245301563728621
Loss in iteration 70 : 0.3245016155467783
Loss in iteration 71 : 0.32454992901835844
Loss in iteration 72 : 0.32447155095354396
Loss in iteration 73 : 0.3243875606895681
Loss in iteration 74 : 0.3244144864672791
Loss in iteration 75 : 0.3244064317696798
Loss in iteration 76 : 0.3243309839658381
Loss in iteration 77 : 0.3243198163663637
Loss in iteration 78 : 0.32434161820063295
Loss in iteration 79 : 0.3243005709249131
Loss in iteration 80 : 0.3242629705527324
Loss in iteration 81 : 0.32427601458492794
Loss in iteration 82 : 0.3242646047083125
Loss in iteration 83 : 0.3242232551818603
Loss in iteration 84 : 0.32421530291481526
Loss in iteration 85 : 0.32421777101384625
Loss in iteration 86 : 0.32418974604154266
Loss in iteration 87 : 0.324168304071519
Loss in iteration 88 : 0.32417033069250833
Loss in iteration 89 : 0.3241586125784038
Loss in iteration 90 : 0.324135562396593
Loss in iteration 91 : 0.32412996758866375
Loss in iteration 92 : 0.3241271415938183
Loss in iteration 93 : 0.32410980032085107
Loss in iteration 94 : 0.32409705885490037
Loss in iteration 95 : 0.32409442332276217
Loss in iteration 96 : 0.32408398566734503
Loss in iteration 97 : 0.3240689389278521
Loss in iteration 98 : 0.32406246677769673
Loss in iteration 99 : 0.3240564278892316
Loss in iteration 100 : 0.3240437407660581
Loss in iteration 101 : 0.32403417046037025
Loss in iteration 102 : 0.32402932179151417
Loss in iteration 103 : 0.324020640077286
Loss in iteration 104 : 0.3240104876471928
Loss in iteration 105 : 0.3240047288731836
Loss in iteration 106 : 0.3239988375063176
Loss in iteration 107 : 0.32398991076472095
Loss in iteration 108 : 0.32398277724950547
Loss in iteration 109 : 0.32397762104646904
Loss in iteration 110 : 0.3239704017553512
Loss in iteration 111 : 0.3239626823794512
Loss in iteration 112 : 0.3239570576837005
Loss in iteration 113 : 0.3239511531204496
Loss in iteration 114 : 0.32394390536579015
Loss in iteration 115 : 0.3239377083370865
Loss in iteration 116 : 0.3239323937988652
Loss in iteration 117 : 0.323926088981551
Loss in iteration 118 : 0.3239197155550345
Loss in iteration 119 : 0.32391436982187977
Loss in iteration 120 : 0.3239088089266006
Loss in iteration 121 : 0.3239026578062341
Loss in iteration 122 : 0.32389707155319675
Loss in iteration 123 : 0.3238918542485493
Loss in iteration 124 : 0.3238861599455521
Loss in iteration 125 : 0.32388052064948986
Loss in iteration 126 : 0.3238754060795433
Loss in iteration 127 : 0.323870194137309
Loss in iteration 128 : 0.32386477727424456
Loss in iteration 129 : 0.32385970543187087
Loss in iteration 130 : 0.323854822950268
Loss in iteration 131 : 0.32384974011886664
Loss in iteration 132 : 0.3238447365723034
Loss in iteration 133 : 0.3238399939984287
Loss in iteration 134 : 0.3238351957208933
Loss in iteration 135 : 0.323830322111697
Loss in iteration 136 : 0.32382563336556663
Loss in iteration 137 : 0.3238210320831273
Loss in iteration 138 : 0.3238163504705677
Loss in iteration 139 : 0.32381174165470494
Loss in iteration 140 : 0.32380727767612993
Loss in iteration 141 : 0.3238028043324496
Loss in iteration 142 : 0.32379832882164455
Loss in iteration 143 : 0.3237939705876817
Loss in iteration 144 : 0.3237896688737508
Loss in iteration 145 : 0.3237853488612276
Loss in iteration 146 : 0.32378108768774333
Loss in iteration 147 : 0.3237769091418659
Loss in iteration 148 : 0.32377273769968246
Loss in iteration 149 : 0.32376858466712594
Loss in iteration 150 : 0.3237645052965706
Loss in iteration 151 : 0.3237604661779507
Loss in iteration 152 : 0.3237564365951149
Loss in iteration 153 : 0.32375245637429945
Loss in iteration 154 : 0.32374853255560326
Loss in iteration 155 : 0.32374462869512044
Loss in iteration 156 : 0.32374075318452583
Loss in iteration 157 : 0.32373693009125815
Loss in iteration 158 : 0.32373314029685507
Loss in iteration 159 : 0.3237293705464598
Loss in iteration 160 : 0.3237256401906445
Loss in iteration 161 : 0.32372195006097426
Loss in iteration 162 : 0.3237182827826026
Loss in iteration 163 : 0.3237146440331947
Loss in iteration 164 : 0.3237110445155857
Loss in iteration 165 : 0.32370747420627094
Loss in iteration 166 : 0.3237039279380375
Loss in iteration 167 : 0.32370041521597437
Loss in iteration 168 : 0.3236969352500379
Loss in iteration 169 : 0.3236934797103014
Loss in iteration 170 : 0.32369005177918364
Loss in iteration 171 : 0.32368665576598227
Loss in iteration 172 : 0.3236832861627634
Loss in iteration 173 : 0.32367994078006956
Loss in iteration 174 : 0.32367662399691716
Loss in iteration 175 : 0.32367333474204185
Loss in iteration 176 : 0.32367006902330947
Loss in iteration 177 : 0.32366682857482215
Loss in iteration 178 : 0.3236636150885397
Loss in iteration 179 : 0.32366042567150927
Loss in iteration 180 : 0.3236572594518316
Loss in iteration 181 : 0.3236541184117311
Loss in iteration 182 : 0.3236510016709389
Loss in iteration 183 : 0.323647907259939
Loss in iteration 184 : 0.32364483599035787
Loss in iteration 185 : 0.32364178836220703
Loss in iteration 186 : 0.3236387627612154
Loss in iteration 187 : 0.32363575874330713
Loss in iteration 188 : 0.3236327770920114
Loss in iteration 189 : 0.3236298171382132
Loss in iteration 190 : 0.3236268778656638
Loss in iteration 191 : 0.32362395960808965
Loss in iteration 192 : 0.3236210624237415
Loss in iteration 193 : 0.32361818541395554
Loss in iteration 194 : 0.323615328327232
Loss in iteration 195 : 0.3236124914190979
Loss in iteration 196 : 0.32360967420984715
Loss in iteration 197 : 0.32360687612905337
Loss in iteration 198 : 0.32360409724531874
Loss in iteration 199 : 0.3236013374323519
Loss in iteration 200 : 0.32359859613699954
Testing accuracy  of updater 1 on alg 0 with rate 1.0 = 0.8504391622136233, training accuracy 0.8484643734643734, time elapsed: 4547 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6509822051128222
Loss in iteration 3 : 0.59211621713129
Loss in iteration 4 : 0.5436944900346836
Loss in iteration 5 : 0.5179478066288226
Loss in iteration 6 : 0.5108111602911033
Loss in iteration 7 : 0.511396528560556
Loss in iteration 8 : 0.5101683404249144
Loss in iteration 9 : 0.501840308431409
Loss in iteration 10 : 0.4852535234947906
Loss in iteration 11 : 0.4624246930690256
Loss in iteration 12 : 0.43745488186959114
Loss in iteration 13 : 0.41520517543477714
Loss in iteration 14 : 0.399643165621074
Loss in iteration 15 : 0.39230042409218985
Loss in iteration 16 : 0.391728982859289
Loss in iteration 17 : 0.3944207347388191
Loss in iteration 18 : 0.3966025023163361
Loss in iteration 19 : 0.3957976307264793
Loss in iteration 20 : 0.3914541162113571
Loss in iteration 21 : 0.38463689382545235
Loss in iteration 22 : 0.37719291848795455
Loss in iteration 23 : 0.37086254682987746
Loss in iteration 24 : 0.3666789899473001
Loss in iteration 25 : 0.3647899947874746
Loss in iteration 26 : 0.36463791873252543
Loss in iteration 27 : 0.3653200604832927
Loss in iteration 28 : 0.36594833643712193
Loss in iteration 29 : 0.36589721109963314
Loss in iteration 30 : 0.36490794108030145
Loss in iteration 31 : 0.3630685821415001
Loss in iteration 32 : 0.3607104225834242
Loss in iteration 33 : 0.35826524430519857
Loss in iteration 34 : 0.3561245266828879
Loss in iteration 35 : 0.35453458052889786
Loss in iteration 36 : 0.35354963784546123
Loss in iteration 37 : 0.3530479776766015
Loss in iteration 38 : 0.35279775092507165
Loss in iteration 39 : 0.3525454546870564
Loss in iteration 40 : 0.35209619015802385
Loss in iteration 41 : 0.3513618716889291
Loss in iteration 42 : 0.3503677247733893
Loss in iteration 43 : 0.34922259905460673
Loss in iteration 44 : 0.34806930768929073
Loss in iteration 45 : 0.34703449421525046
Loss in iteration 46 : 0.346193706971962
Loss in iteration 47 : 0.3455591902202388
Loss in iteration 48 : 0.34508912868995084
Loss in iteration 49 : 0.34471081030911255
Loss in iteration 50 : 0.3443478101965777
Loss in iteration 51 : 0.3439425024389873
Loss in iteration 52 : 0.3434685752106981
Loss in iteration 53 : 0.34293217918164753
Loss in iteration 54 : 0.34236368769963893
Loss in iteration 55 : 0.3418041449154561
Loss in iteration 56 : 0.34129113358198937
Loss in iteration 57 : 0.3408481326188127
Loss in iteration 58 : 0.34047981012478373
Loss in iteration 59 : 0.34017363643365583
Loss in iteration 60 : 0.33990630833903795
Loss in iteration 61 : 0.3396522787750407
Loss in iteration 62 : 0.3393914728891869
Loss in iteration 63 : 0.33911399075717547
Loss in iteration 64 : 0.33882089448466357
Loss in iteration 65 : 0.338521547599361
Loss in iteration 66 : 0.33822895966341565
Loss in iteration 67 : 0.3379549352141669
Loss in iteration 68 : 0.33770653975985576
Loss in iteration 69 : 0.33748469273627196
Loss in iteration 70 : 0.3372848870803137
Loss in iteration 71 : 0.3370993941589892
Loss in iteration 72 : 0.33692000131878463
Loss in iteration 73 : 0.3367403677562223
Loss in iteration 74 : 0.3365573829861093
Loss in iteration 75 : 0.336371326644851
Loss in iteration 76 : 0.3361850155109021
Loss in iteration 77 : 0.336002380230225
Loss in iteration 78 : 0.3358269947167066
Loss in iteration 79 : 0.3356609967842447
Loss in iteration 80 : 0.33550464334715135
Loss in iteration 81 : 0.3353565128572271
Loss in iteration 82 : 0.3352141744360324
Loss in iteration 83 : 0.33507503772842195
Loss in iteration 84 : 0.33493709615317924
Loss in iteration 85 : 0.3347993612976352
Loss in iteration 86 : 0.3346619166291494
Loss in iteration 87 : 0.3345256465442025
Loss in iteration 88 : 0.33439178349044496
Loss in iteration 89 : 0.33426144282292863
Loss in iteration 90 : 0.3341352856108855
Loss in iteration 91 : 0.3340133839064677
Loss in iteration 92 : 0.3338952881529574
Loss in iteration 93 : 0.33378023673838914
Loss in iteration 94 : 0.33366741782454346
Loss in iteration 95 : 0.3335561961719608
Loss in iteration 96 : 0.33344624528577926
Loss in iteration 97 : 0.3333375646564091
Loss in iteration 98 : 0.33323039932792997
Loss in iteration 99 : 0.33312510408451856
Loss in iteration 100 : 0.3330220024446286
Loss in iteration 101 : 0.3329212824785749
Loss in iteration 102 : 0.3328229526845008
Loss in iteration 103 : 0.3327268592270883
Loss in iteration 104 : 0.3326327477178355
Loss in iteration 105 : 0.3325403429439857
Loss in iteration 106 : 0.3324494198063954
Loss in iteration 107 : 0.33235984642479893
Loss in iteration 108 : 0.3322715921653869
Loss in iteration 109 : 0.3321847049949556
Loss in iteration 110 : 0.33209927067581785
Loss in iteration 111 : 0.33201536919071234
Loss in iteration 112 : 0.3319330416039536
Loss in iteration 113 : 0.33185227495501746
Loss in iteration 114 : 0.33177300605746163
Loss in iteration 115 : 0.33169513941955664
Loss in iteration 116 : 0.33161857134568157
Loss in iteration 117 : 0.3315432120397794
Loss in iteration 118 : 0.33146899970451665
Loss in iteration 119 : 0.3313959041095728
Loss in iteration 120 : 0.3313239206304413
Loss in iteration 121 : 0.3312530583115418
Loss in iteration 122 : 0.3311833265415534
Loss in iteration 123 : 0.3311147244431962
Loss in iteration 124 : 0.33104723550775456
Loss in iteration 125 : 0.3309808280075128
Loss in iteration 126 : 0.33091545995281113
Loss in iteration 127 : 0.3308510862881156
Loss in iteration 128 : 0.33078766581894764
Loss in iteration 129 : 0.3307251659201223
Loss in iteration 130 : 0.3306635640885791
Loss in iteration 131 : 0.33060284649210947
Loss in iteration 132 : 0.33054300450020474
Loss in iteration 133 : 0.33048403057373077
Loss in iteration 134 : 0.3304259148090697
Loss in iteration 135 : 0.3303686429948077
Loss in iteration 136 : 0.33031219643939086
Loss in iteration 137 : 0.33025655327063774
Loss in iteration 138 : 0.3302016905439864
Loss in iteration 139 : 0.3301475863929674
Loss in iteration 140 : 0.33009422159156004
Loss in iteration 141 : 0.33004158018829244
Loss in iteration 142 : 0.3299896492035421
Loss in iteration 143 : 0.3299384176514492
Loss in iteration 144 : 0.32988787529111974
Loss in iteration 145 : 0.3298380115118111
Loss in iteration 146 : 0.32978881464112325
Loss in iteration 147 : 0.3297402717892231
Loss in iteration 148 : 0.32969236916749
Loss in iteration 149 : 0.32964509269654946
Loss in iteration 150 : 0.32959842867152056
Loss in iteration 151 : 0.3295523642806534
Loss in iteration 152 : 0.3295068878544833
Loss in iteration 153 : 0.3294619888231942
Loss in iteration 154 : 0.3294176574474339
Loss in iteration 155 : 0.32937388444008625
Loss in iteration 156 : 0.32933066060505783
Loss in iteration 157 : 0.32928797659025894
Loss in iteration 158 : 0.32924582280084264
Loss in iteration 159 : 0.32920418946442764
Loss in iteration 160 : 0.3291630667984357
Loss in iteration 161 : 0.32912244521032574
Loss in iteration 162 : 0.32908231546551187
Loss in iteration 163 : 0.3290426687796278
Loss in iteration 164 : 0.3290034968216563
Loss in iteration 165 : 0.3289647916425354
Loss in iteration 166 : 0.32892654556229645
Loss in iteration 167 : 0.32888875105426996
Loss in iteration 168 : 0.32885140065851776
Loss in iteration 169 : 0.32881448694210974
Loss in iteration 170 : 0.3287780025073609
Loss in iteration 171 : 0.32874194003523693
Loss in iteration 172 : 0.3287062923437249
Loss in iteration 173 : 0.3286710524405347
Loss in iteration 174 : 0.32863621355511374
Loss in iteration 175 : 0.32860176914369427
Loss in iteration 176 : 0.3285677128699881
Loss in iteration 177 : 0.3285340385705031
Loss in iteration 178 : 0.32850074021619574
Loss in iteration 179 : 0.32846781188093416
Loss in iteration 180 : 0.328435247723366
Loss in iteration 181 : 0.32840304198373016
Loss in iteration 182 : 0.32837118899271456
Loss in iteration 183 : 0.32833968318656626
Loss in iteration 184 : 0.32830851912208053
Loss in iteration 185 : 0.3282776914864037
Loss in iteration 186 : 0.32824719509899586
Loss in iteration 187 : 0.3282170249059271
Loss in iteration 188 : 0.3281871759687632
Loss in iteration 189 : 0.32815764345156123
Loss in iteration 190 : 0.3281284226091922
Loss in iteration 191 : 0.32809950877943395
Loss in iteration 192 : 0.32807089737963996
Loss in iteration 193 : 0.32804258390740915
Loss in iteration 194 : 0.32801456394367556
Loss in iteration 195 : 0.3279868331562872
Loss in iteration 196 : 0.32795938730234225
Loss in iteration 197 : 0.32793222222830476
Loss in iteration 198 : 0.32790533386771265
Loss in iteration 199 : 0.32787871823702425
Loss in iteration 200 : 0.3278523714305965
Testing accuracy  of updater 1 on alg 0 with rate 0.1 = 0.8520361157177078, training accuracy 0.8463144963144963, time elapsed: 4247 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6886400023527108
Loss in iteration 3 : 0.680316308126684
Loss in iteration 4 : 0.668980617509459
Loss in iteration 5 : 0.6554793609744141
Loss in iteration 6 : 0.640639312280403
Loss in iteration 7 : 0.6252181697155329
Loss in iteration 8 : 0.6098685223495371
Loss in iteration 9 : 0.5951156846648783
Loss in iteration 10 : 0.5813488920976078
Loss in iteration 11 : 0.5688242817138195
Loss in iteration 12 : 0.5576771782744571
Loss in iteration 13 : 0.5479406752255669
Loss in iteration 14 : 0.5395674360820915
Loss in iteration 15 : 0.5324520010487551
Loss in iteration 16 : 0.5264515238445113
Loss in iteration 17 : 0.5214036076771125
Loss in iteration 18 : 0.517140605057801
Loss in iteration 19 : 0.5135002984964323
Loss in iteration 20 : 0.5103332533519918
Loss in iteration 21 : 0.5075073410217169
Loss in iteration 22 : 0.5049100057951462
Loss in iteration 23 : 0.5024488346470926
Loss in iteration 24 : 0.5000509243242804
Loss in iteration 25 : 0.49766145338756457
Loss in iteration 26 : 0.4952417775995994
Loss in iteration 27 : 0.49276728607413495
Loss in iteration 28 : 0.4902251878009399
Loss in iteration 29 : 0.48761234457689856
Loss in iteration 30 : 0.48493322592519655
Loss in iteration 31 : 0.482198032215979
Loss in iteration 32 : 0.47942101160207584
Loss in iteration 33 : 0.47661898237282563
Loss in iteration 34 : 0.473810063041088
Loss in iteration 35 : 0.471012606415582
Loss in iteration 36 : 0.46824432993453463
Loss in iteration 37 : 0.46552163182915446
Loss in iteration 38 : 0.4628590806971268
Loss in iteration 39 : 0.4602690644639387
Loss in iteration 40 : 0.4577615833274448
Loss in iteration 41 : 0.45534417007253003
Loss in iteration 42 : 0.45302192014263726
Loss in iteration 43 : 0.45079761313994
Loss in iteration 44 : 0.448671907085761
Loss in iteration 45 : 0.4466435868844673
Loss in iteration 46 : 0.4447098490455644
Loss in iteration 47 : 0.4428666058377692
Loss in iteration 48 : 0.4411087936420836
Loss in iteration 49 : 0.43943067226494725
Loss in iteration 50 : 0.4378261042642806
Loss in iteration 51 : 0.43628880580750656
Loss in iteration 52 : 0.43481256309208954
Loss in iteration 53 : 0.43339141079052346
Loss in iteration 54 : 0.4320197712233827
Loss in iteration 55 : 0.43069255492840064
Loss in iteration 56 : 0.429405224919434
Loss in iteration 57 : 0.4281538281833158
Loss in iteration 58 : 0.4269349988379842
Loss in iteration 59 : 0.4257459378880963
Loss in iteration 60 : 0.4245843746987328
Loss in iteration 61 : 0.4234485152114439
Loss in iteration 62 : 0.42233698160473876
Loss in iteration 63 : 0.42124874761133096
Loss in iteration 64 : 0.42018307310350295
Loss in iteration 65 : 0.41913944089791066
Loss in iteration 66 : 0.4181174980574641
Loss in iteration 67 : 0.41711700331736
Loss in iteration 68 : 0.4161377816631803
Loss in iteration 69 : 0.4151796865613994
Loss in iteration 70 : 0.4142425698978502
Loss in iteration 71 : 0.4133262593231015
Loss in iteration 72 : 0.4124305424345585
Loss in iteration 73 : 0.41155515703783196
Loss in iteration 74 : 0.4106997866168666
Loss in iteration 75 : 0.4098640600927786
Loss in iteration 76 : 0.40904755495422773
Loss in iteration 77 : 0.40824980288618445
Loss in iteration 78 : 0.40747029709790095
Loss in iteration 79 : 0.406708500645226
Loss in iteration 80 : 0.40596385514786637
Loss in iteration 81 : 0.4052357894121592
Loss in iteration 82 : 0.40452372757763855
Loss in iteration 83 : 0.40382709650744014
Loss in iteration 84 : 0.40314533223484417
Loss in iteration 85 : 0.402477885358914
Loss in iteration 86 : 0.4018242253504147
Loss in iteration 87 : 0.40118384378448024
Loss in iteration 88 : 0.40055625655960103
Loss in iteration 89 : 0.3999410051935978
Loss in iteration 90 : 0.3993376573083273
Loss in iteration 91 : 0.3987458064265289
Loss in iteration 92 : 0.3981650712083808
Loss in iteration 93 : 0.3975950942532779
Loss in iteration 94 : 0.3970355405854474
Loss in iteration 95 : 0.3964860959316571
Loss in iteration 96 : 0.3959464648865531
Loss in iteration 97 : 0.39541636904710975
Loss in iteration 98 : 0.39489554518314507
Loss in iteration 99 : 0.3943837434965889
Loss in iteration 100 : 0.3938807260085025
Loss in iteration 101 : 0.393386265100618
Loss in iteration 102 : 0.3929001422269454
Loss in iteration 103 : 0.3924221468017384
Loss in iteration 104 : 0.3919520752621486
Loss in iteration 105 : 0.39148973029780787
Loss in iteration 106 : 0.39103492023480096
Loss in iteration 107 : 0.39058745855837607
Loss in iteration 108 : 0.3901471635565599
Loss in iteration 109 : 0.3897138580661138
Loss in iteration 110 : 0.3892873693021478
Loss in iteration 111 : 0.38886752875345676
Loss in iteration 112 : 0.3884541721269621
Loss in iteration 113 : 0.3880471393262203
Loss in iteration 114 : 0.3876462744509997
Loss in iteration 115 : 0.3872514258068419
Loss in iteration 116 : 0.38686244591565727
Loss in iteration 117 : 0.3864791915204148
Loss in iteration 118 : 0.3861015235787779
Loss in iteration 119 : 0.3857293072423004
Loss in iteration 120 : 0.38536241181923464
Loss in iteration 121 : 0.3850007107202577
Loss in iteration 122 : 0.3846440813874305
Loss in iteration 123 : 0.38429240520760016
Loss in iteration 124 : 0.383945567411866
Loss in iteration 125 : 0.38360345696329895
Loss in iteration 126 : 0.38326596643524363
Loss in iteration 127 : 0.382932991882606
Loss in iteration 128 : 0.38260443270856903
Loss in iteration 129 : 0.382280191528938
Loss in iteration 130 : 0.3819601740362599
Loss in iteration 131 : 0.3816442888654627
Loss in iteration 132 : 0.38133244746258904
Loss in iteration 133 : 0.38102456395791867
Loss in iteration 134 : 0.3807205550444167
Loss in iteration 135 : 0.380420339862183
Loss in iteration 136 : 0.38012383988943815
Loss in iteration 137 : 0.37983097884021283
Loss in iteration 138 : 0.3795416825687803
Loss in iteration 139 : 0.3792558789807369
Loss in iteration 140 : 0.3789734979504412
Loss in iteration 141 : 0.37869447124449396
Loss in iteration 142 : 0.3784187324507949
Loss in iteration 143 : 0.37814621691274375
Loss in iteration 144 : 0.37787686166807977
Loss in iteration 145 : 0.3776106053918583
Loss in iteration 146 : 0.3773473883431349
Loss in iteration 147 : 0.377087152314839
Loss in iteration 148 : 0.37682984058647384
Loss in iteration 149 : 0.3765753978792382
Loss in iteration 150 : 0.37632377031324277
Loss in iteration 151 : 0.37607490536655247
Loss in iteration 152 : 0.37582875183576675
Loss in iteration 153 : 0.37558525979801155
Loss in iteration 154 : 0.37534438057406955
Loss in iteration 155 : 0.375106066692665
Loss in iteration 156 : 0.37487027185565636
Loss in iteration 157 : 0.3746369509042135
Loss in iteration 158 : 0.3744060597857983
Loss in iteration 159 : 0.37417755552202897
Loss in iteration 160 : 0.373951396177325
Loss in iteration 161 : 0.37372754082842363
Loss in iteration 162 : 0.3735059495346312
Loss in iteration 163 : 0.3732865833089579
Loss in iteration 164 : 0.37306940409005135
Loss in iteration 165 : 0.3728543747149509
Loss in iteration 166 : 0.3726414588926804
Loss in iteration 167 : 0.37243062117866066
Loss in iteration 168 : 0.37222182694997363
Loss in iteration 169 : 0.37201504238141664
Loss in iteration 170 : 0.3718102344223822
Loss in iteration 171 : 0.37160737077452394
Loss in iteration 172 : 0.3714064198702206
Loss in iteration 173 : 0.3712073508517679
Loss in iteration 174 : 0.3710101335513262
Loss in iteration 175 : 0.37081473847156426
Loss in iteration 176 : 0.3706211367669995
Loss in iteration 177 : 0.37042930022598
Loss in iteration 178 : 0.3702392012533044
Loss in iteration 179 : 0.3700508128534341
Loss in iteration 180 : 0.36986410861429586
Loss in iteration 181 : 0.3696790626915977
Loss in iteration 182 : 0.36949564979371674
Loss in iteration 183 : 0.36931384516702515
Loss in iteration 184 : 0.3691336245817387
Loss in iteration 185 : 0.3689549643181867
Loss in iteration 186 : 0.36877784115352547
Loss in iteration 187 : 0.36860223234885975
Loss in iteration 188 : 0.36842811563676525
Loss in iteration 189 : 0.3682554692091754
Loss in iteration 190 : 0.36808427170563857
Loss in iteration 191 : 0.3679145022019315
Loss in iteration 192 : 0.367746140198993
Loss in iteration 193 : 0.3675791656121889
Loss in iteration 194 : 0.3674135587608916
Loss in iteration 195 : 0.3672493003583487
Loss in iteration 196 : 0.3670863715018737
Loss in iteration 197 : 0.36692475366327304
Loss in iteration 198 : 0.36676442867959025
Loss in iteration 199 : 0.36660537874409355
Loss in iteration 200 : 0.3664475863975256
Testing accuracy  of updater 1 on alg 0 with rate 0.009999999999999995 = 0.8329955162459308, training accuracy 0.8328316953316953, time elapsed: 3870 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.700828587648649
Loss in iteration 3 : 0.4518223404846068
Loss in iteration 4 : 0.4023148707648162
Loss in iteration 5 : 0.3707650066665527
Loss in iteration 6 : 0.36509815488668884
Loss in iteration 7 : 0.3626482874825865
Loss in iteration 8 : 0.3610023354284626
Loss in iteration 9 : 0.35911776627546715
Loss in iteration 10 : 0.35664881257854564
Loss in iteration 11 : 0.353597044730058
Loss in iteration 12 : 0.35014240704131483
Loss in iteration 13 : 0.34654613460127975
Loss in iteration 14 : 0.343085915965556
Loss in iteration 15 : 0.3400079910629379
Loss in iteration 16 : 0.33749078028537655
Loss in iteration 17 : 0.33562066976135685
Loss in iteration 18 : 0.334383673522483
Loss in iteration 19 : 0.3336760572199383
Loss in iteration 20 : 0.33333285127563933
Loss in iteration 21 : 0.3331675428275468
Loss in iteration 22 : 0.3330122500152161
Loss in iteration 23 : 0.33274773046356587
Loss in iteration 24 : 0.332316748038003
Loss in iteration 25 : 0.3317204375492855
Loss in iteration 26 : 0.33100245349961455
Loss in iteration 27 : 0.33022799729494673
Loss in iteration 28 : 0.3294641916978814
Loss in iteration 29 : 0.32876588330671813
Loss in iteration 30 : 0.3281682408811847
Loss in iteration 31 : 0.3276854559572034
Loss in iteration 32 : 0.3273137710883638
Loss in iteration 33 : 0.3270368252446157
Loss in iteration 34 : 0.32683159495120623
Loss in iteration 35 : 0.3266737182775945
Loss in iteration 36 : 0.3265415115005488
Loss in iteration 37 : 0.3264184233542428
Loss in iteration 38 : 0.32629398890344685
Loss in iteration 39 : 0.3261635499812984
Loss in iteration 40 : 0.32602711908250503
Loss in iteration 41 : 0.325887795746218
Loss in iteration 42 : 0.3257501136408741
Loss in iteration 43 : 0.32561861764078603
Loss in iteration 44 : 0.3254968607332757
Loss in iteration 45 : 0.32538689137282906
Loss in iteration 46 : 0.3252891944296691
Loss in iteration 47 : 0.32520297113236657
Loss in iteration 48 : 0.32512660555091744
Loss in iteration 49 : 0.32505816739374527
Loss in iteration 50 : 0.3249958342551276
Loss in iteration 51 : 0.3249381664096227
Loss in iteration 52 : 0.3248842184188496
Loss in iteration 53 : 0.32483351215534884
Loss in iteration 54 : 0.3247859190340303
Loss in iteration 55 : 0.3247415048444004
Loss in iteration 56 : 0.3247003825368648
Loss in iteration 57 : 0.3246626026526023
Loss in iteration 58 : 0.324628093742193
Loss in iteration 59 : 0.3245966505210049
Loss in iteration 60 : 0.3245679580064588
Loss in iteration 61 : 0.32454163589326407
Loss in iteration 62 : 0.32451728800827745
Loss in iteration 63 : 0.32449454523428956
Loss in iteration 64 : 0.3244730951033689
Loss in iteration 65 : 0.3244526959441194
Loss in iteration 66 : 0.3244331771208355
Loss in iteration 67 : 0.3244144291276257
Loss in iteration 68 : 0.32439638811098004
Loss in iteration 69 : 0.32437901907558625
Loss in iteration 70 : 0.3243623009936798
Loss in iteration 71 : 0.32434621570891353
Loss in iteration 72 : 0.3243307412476972
Loss in iteration 73 : 0.32431584915344774
Loss in iteration 74 : 0.32430150484684594
Loss in iteration 75 : 0.32428766978605983
Loss in iteration 76 : 0.32427430427999304
Loss in iteration 77 : 0.3242613700821393
Loss in iteration 78 : 0.32424883224766365
Loss in iteration 79 : 0.32423666007540525
Loss in iteration 80 : 0.3242248272177656
Loss in iteration 81 : 0.3242133111971382
Loss in iteration 82 : 0.32420209262011884
Loss in iteration 83 : 0.3241911543530379
Loss in iteration 84 : 0.3241804808457618
Loss in iteration 85 : 0.324170057697147
Loss in iteration 86 : 0.32415987147015674
Loss in iteration 87 : 0.32414990970307483
Loss in iteration 88 : 0.3241401610316268
Loss in iteration 89 : 0.32413061533278503
Loss in iteration 90 : 0.3241212638180992
Loss in iteration 91 : 0.32411209903254656
Loss in iteration 92 : 0.3241031147459292
Loss in iteration 93 : 0.3240943057503199
Loss in iteration 94 : 0.32408566759504576
Loss in iteration 95 : 0.32407719629858384
Loss in iteration 96 : 0.32406888807570605
Loss in iteration 97 : 0.32406073911033734
Loss in iteration 98 : 0.32405274539302265
Loss in iteration 99 : 0.3240449026296474
Loss in iteration 100 : 0.3240372062173104
Loss in iteration 101 : 0.32402965127542954
Loss in iteration 102 : 0.32402223271605507
Loss in iteration 103 : 0.3240149453364097
Loss in iteration 104 : 0.3240077839184114
Loss in iteration 105 : 0.32400074332321877
Loss in iteration 106 : 0.3239938185727902
Loss in iteration 107 : 0.32398700491412946
Loss in iteration 108 : 0.3239802978650493
Loss in iteration 109 : 0.32397369324235503
Loss in iteration 110 : 0.32396718717456346
Loss in iteration 111 : 0.3239607761018269
Loss in iteration 112 : 0.3239544567656792
Loss in iteration 113 : 0.32394822619099733
Loss in iteration 114 : 0.32394208166227256
Loss in iteration 115 : 0.3239360206960205
Loss in iteration 116 : 0.3239300410109867
Loss in iteration 117 : 0.3239241404977291
Loss in iteration 118 : 0.3239183171890052
Loss in iteration 119 : 0.3239125692322901
Loss in iteration 120 : 0.32390689486549656
Loss in iteration 121 : 0.323901292396737
Loss in iteration 122 : 0.3238957601884924
Loss in iteration 123 : 0.32389029664627034
Loss in iteration 124 : 0.3238849002114863
Loss in iteration 125 : 0.3238795693578514
Loss in iteration 126 : 0.32387430259061417
Loss in iteration 127 : 0.32386909844771117
Loss in iteration 128 : 0.3238639555020152
Loss in iteration 129 : 0.3238588723639682
Loss in iteration 130 : 0.32385384768400355
Loss in iteration 131 : 0.32384888015444296
Loss in iteration 132 : 0.3238439685105937
Loss in iteration 133 : 0.32383911153109196
Loss in iteration 134 : 0.3238343080374553
Loss in iteration 135 : 0.32382955689308934
Loss in iteration 136 : 0.3238248570018023
Loss in iteration 137 : 0.32382020730606154
Loss in iteration 138 : 0.3238156067850968
Loss in iteration 139 : 0.3238110544529509
Loss in iteration 140 : 0.3238065493565662
Loss in iteration 141 : 0.32380209057394405
Loss in iteration 142 : 0.32379767721238656
Loss in iteration 143 : 0.32379330840687287
Loss in iteration 144 : 0.32378898331851164
Loss in iteration 145 : 0.3237847011331466
Loss in iteration 146 : 0.3237804610600579
Loss in iteration 147 : 0.3237762623307574
Loss in iteration 148 : 0.3237721041979453
Loss in iteration 149 : 0.3237679859345286
Loss in iteration 150 : 0.32376390683275846
Loss in iteration 151 : 0.32375986620345965
Loss in iteration 152 : 0.32375586337533574
Loss in iteration 153 : 0.3237518976943242
Loss in iteration 154 : 0.32374796852302296
Loss in iteration 155 : 0.3237440752401272
Loss in iteration 156 : 0.32374021723992424
Loss in iteration 157 : 0.3237363939317618
Loss in iteration 158 : 0.32373260473957527
Loss in iteration 159 : 0.3237288491014034
Loss in iteration 160 : 0.3237251264688983
Loss in iteration 161 : 0.32372143630689143
Loss in iteration 162 : 0.3237177780929389
Loss in iteration 163 : 0.32371415131689063
Loss in iteration 164 : 0.32371055548048694
Loss in iteration 165 : 0.32370699009696247
Loss in iteration 166 : 0.3237034546906827
Loss in iteration 167 : 0.32369994879677577
Loss in iteration 168 : 0.32369647196081786
Loss in iteration 169 : 0.3236930237384915
Loss in iteration 170 : 0.32368960369529604
Loss in iteration 171 : 0.32368621140624454
Loss in iteration 172 : 0.323682846455591
Loss in iteration 173 : 0.3236795084365636
Loss in iteration 174 : 0.32367619695109523
Loss in iteration 175 : 0.3236729116095867
Loss in iteration 176 : 0.32366965203066284
Loss in iteration 177 : 0.32366641784094324
Loss in iteration 178 : 0.3236632086748168
Loss in iteration 179 : 0.3236600241742465
Loss in iteration 180 : 0.3236568639885357
Loss in iteration 181 : 0.3236537277741519
Loss in iteration 182 : 0.32365061519452787
Loss in iteration 183 : 0.323647525919879
Loss in iteration 184 : 0.32364445962701743
Loss in iteration 185 : 0.32364141599919116
Loss in iteration 186 : 0.3236383947258986
Loss in iteration 187 : 0.3236353955027499
Loss in iteration 188 : 0.32363241803128445
Loss in iteration 189 : 0.3236294620188298
Loss in iteration 190 : 0.32362652717835677
Loss in iteration 191 : 0.32362361322832905
Loss in iteration 192 : 0.323620719892572
Loss in iteration 193 : 0.3236178469001251
Loss in iteration 194 : 0.32361499398512256
Loss in iteration 195 : 0.3236121608866686
Loss in iteration 196 : 0.3236093473487076
Loss in iteration 197 : 0.323606553119914
Loss in iteration 198 : 0.32360377795356887
Loss in iteration 199 : 0.32360102160745796
Loss in iteration 200 : 0.32359828384376255
Testing accuracy  of updater 2 on alg 0 with rate 1.0 = 0.8505005835022419, training accuracy 0.8484643734643734, time elapsed: 3978 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6185220919983576
Loss in iteration 3 : 0.5592156130991557
Loss in iteration 4 : 0.5245466464786019
Loss in iteration 5 : 0.5077593135930132
Loss in iteration 6 : 0.49864672171222846
Loss in iteration 7 : 0.4898789414263737
Loss in iteration 8 : 0.4780242002895382
Loss in iteration 9 : 0.46270891359253863
Loss in iteration 10 : 0.4454639770429764
Loss in iteration 11 : 0.4286012656672294
Loss in iteration 12 : 0.4141975089898948
Loss in iteration 13 : 0.4033643806198135
Loss in iteration 14 : 0.3960310509669396
Loss in iteration 15 : 0.3912643531942372
Loss in iteration 16 : 0.3878597775725799
Loss in iteration 17 : 0.3848486715349714
Loss in iteration 18 : 0.381726360908874
Loss in iteration 19 : 0.3784167422230445
Loss in iteration 20 : 0.37509807448030646
Loss in iteration 21 : 0.3720145109328198
Loss in iteration 22 : 0.3693463804206195
Loss in iteration 23 : 0.36715806831014414
Loss in iteration 24 : 0.36540734191247654
Loss in iteration 25 : 0.36398691019466445
Loss in iteration 26 : 0.3627716255973805
Loss in iteration 27 : 0.3616545684826622
Loss in iteration 28 : 0.36056567636357717
Loss in iteration 29 : 0.35947416281655864
Loss in iteration 30 : 0.35837993362900344
Loss in iteration 31 : 0.3573001816553475
Loss in iteration 32 : 0.3562563801455866
Loss in iteration 33 : 0.35526498309764704
Loss in iteration 34 : 0.35433305513810104
Loss in iteration 35 : 0.3534583360217846
Loss in iteration 36 : 0.3526322015759221
Loss in iteration 37 : 0.35184368047124903
Loss in iteration 38 : 0.3510829865796604
Loss in iteration 39 : 0.35034366691606716
Loss in iteration 40 : 0.3496231604108894
Loss in iteration 41 : 0.3489220952220677
Loss in iteration 42 : 0.34824291777188066
Loss in iteration 43 : 0.3475884526278626
Loss in iteration 44 : 0.3469608211017871
Loss in iteration 45 : 0.3463609063066275
Loss in iteration 46 : 0.34578833630421557
Loss in iteration 47 : 0.34524181967572637
Loss in iteration 48 : 0.34471962271205914
Loss in iteration 49 : 0.3442200067354111
Loss in iteration 50 : 0.34374151489452637
Loss in iteration 51 : 0.3432830765709677
Loss in iteration 52 : 0.34284395968923165
Loss in iteration 53 : 0.34242363482495103
Loss in iteration 54 : 0.3420216198562807
Loss in iteration 55 : 0.3416373577735891
Loss in iteration 56 : 0.3412701542570296
Loss in iteration 57 : 0.34091917613990746
Loss in iteration 58 : 0.34058349394677073
Loss in iteration 59 : 0.34026214409713945
Loss in iteration 60 : 0.3399541880971228
Loss in iteration 61 : 0.3396587537433353
Loss in iteration 62 : 0.3393750529881244
Loss in iteration 63 : 0.33910237929808007
Loss in iteration 64 : 0.33884009216701355
Loss in iteration 65 : 0.3385875975708652
Loss in iteration 66 : 0.3383443314090707
Loss in iteration 67 : 0.3381097497696633
Loss in iteration 68 : 0.3378833265710196
Loss in iteration 69 : 0.33766455674306556
Loss in iteration 70 : 0.3374529620198061
Loss in iteration 71 : 0.3372480965246427
Loss in iteration 72 : 0.3370495502269408
Loss in iteration 73 : 0.33685694952473044
Loss in iteration 74 : 0.3366699552371354
Loss in iteration 75 : 0.33648825892252193
Loss in iteration 76 : 0.3363115786176233
Loss in iteration 77 : 0.33613965491052167
Loss in iteration 78 : 0.3359722478814815
Loss in iteration 79 : 0.33580913503995985
Loss in iteration 80 : 0.3356501100768354
Loss in iteration 81 : 0.3354949820949609
Loss in iteration 82 : 0.33534357497360306
Loss in iteration 83 : 0.33519572661738206
Loss in iteration 84 : 0.3350512879777425
Loss in iteration 85 : 0.3349101218613284
Loss in iteration 86 : 0.3347721016218356
Loss in iteration 87 : 0.3346371098602175
Loss in iteration 88 : 0.33450503724112235
Loss in iteration 89 : 0.3343757814901999
Loss in iteration 90 : 0.3342492465878413
Loss in iteration 91 : 0.3341253421356768
Loss in iteration 92 : 0.3340039828506776
Loss in iteration 93 : 0.33388508813836354
Loss in iteration 94 : 0.33376858170659984
Loss in iteration 95 : 0.333654391197694
Loss in iteration 96 : 0.33354244783241277
Loss in iteration 97 : 0.33343268607084253
Loss in iteration 98 : 0.3333250432999966
Loss in iteration 99 : 0.3332194595574547
Loss in iteration 100 : 0.3331158772961052
Loss in iteration 101 : 0.33301424118967116
Loss in iteration 102 : 0.3329144979741753
Loss in iteration 103 : 0.332816596317867
Loss in iteration 104 : 0.33272048671159604
Loss in iteration 105 : 0.33262612137288095
Loss in iteration 106 : 0.3325334541589531
Loss in iteration 107 : 0.3324424404863144
Loss in iteration 108 : 0.3323530372559507
Loss in iteration 109 : 0.3322652027844284
Loss in iteration 110 : 0.33217889674128154
Loss in iteration 111 : 0.3320940800927755
Loss in iteration 112 : 0.33201071505175705
Loss in iteration 113 : 0.33192876503274565
Loss in iteration 114 : 0.3318481946110933
Loss in iteration 115 : 0.3317689694851596
Loss in iteration 116 : 0.33169105644038693
Loss in iteration 117 : 0.3316144233145997
Loss in iteration 118 : 0.3315390389641179
Loss in iteration 119 : 0.3314648732305019
Loss in iteration 120 : 0.3313918969079144
Loss in iteration 121 : 0.33132008171123895
Loss in iteration 122 : 0.33124940024498467
Loss in iteration 123 : 0.3311798259730144
Loss in iteration 124 : 0.3311113331891302
Loss in iteration 125 : 0.3310438969884132
Loss in iteration 126 : 0.3309774932392574
Loss in iteration 127 : 0.33091209855609255
Loss in iteration 128 : 0.3308476902726983
Loss in iteration 129 : 0.33078424641615556
Loss in iteration 130 : 0.3307217456814258
Loss in iteration 131 : 0.33066016740666715
Loss in iteration 132 : 0.3305994915492351
Loss in iteration 133 : 0.33053969866249294
Loss in iteration 134 : 0.33048076987343306
Loss in iteration 135 : 0.3304226868610944
Loss in iteration 136 : 0.33036543183583333
Loss in iteration 137 : 0.33030898751939136
Loss in iteration 138 : 0.33025333712576505
Loss in iteration 139 : 0.33019846434287636
Loss in iteration 140 : 0.33014435331499237
Loss in iteration 141 : 0.33009098862591113
Loss in iteration 142 : 0.33003835528286696
Loss in iteration 143 : 0.32998643870114913
Loss in iteration 144 : 0.32993522468943076
Loss in iteration 145 : 0.32988469943572113
Loss in iteration 146 : 0.32983484949401504
Loss in iteration 147 : 0.32978566177149804
Loss in iteration 148 : 0.3297371235163737
Loss in iteration 149 : 0.32968922230621944
Loss in iteration 150 : 0.32964194603690544
Loss in iteration 151 : 0.32959528291195545
Loss in iteration 152 : 0.32954922143243004
Loss in iteration 153 : 0.329503750387221
Loss in iteration 154 : 0.3294588588437944
Loss in iteration 155 : 0.32941453613929167
Loss in iteration 156 : 0.3293707718720572
Loss in iteration 157 : 0.32932755589346735
Loss in iteration 158 : 0.32928487830013864
Loss in iteration 159 : 0.32924272942641536
Loss in iteration 160 : 0.3292010998371787
Loss in iteration 161 : 0.3291599803209277
Loss in iteration 162 : 0.3291193618831074
Loss in iteration 163 : 0.3290792357397327
Loss in iteration 164 : 0.32903959331118826
Loss in iteration 165 : 0.32900042621631426
Loss in iteration 166 : 0.32896172626665526
Loss in iteration 167 : 0.328923485460944
Loss in iteration 168 : 0.32888569597976575
Loss in iteration 169 : 0.32884835018040404
Loss in iteration 170 : 0.3288114405918678
Loss in iteration 171 : 0.32877495991009315
Loss in iteration 172 : 0.32873890099327624
Loss in iteration 173 : 0.32870325685740837
Loss in iteration 174 : 0.32866802067190404
Loss in iteration 175 : 0.3286331857554147
Loss in iteration 176 : 0.3285987455717578
Loss in iteration 177 : 0.32856469372597913
Loss in iteration 178 : 0.3285310239605425
Loss in iteration 179 : 0.32849773015164413
Loss in iteration 180 : 0.32846480630563263
Loss in iteration 181 : 0.32843224655556724
Loss in iteration 182 : 0.32840004515784427
Loss in iteration 183 : 0.3283681964889734
Loss in iteration 184 : 0.32833669504242097
Loss in iteration 185 : 0.328305535425573
Loss in iteration 186 : 0.32827471235677513
Loss in iteration 187 : 0.3282442206624801
Loss in iteration 188 : 0.3282140552744695
Loss in iteration 189 : 0.3281842112271662
Loss in iteration 190 : 0.3281546836550425
Loss in iteration 191 : 0.32812546779007007
Loss in iteration 192 : 0.3280965589592917
Loss in iteration 193 : 0.328067952582434
Loss in iteration 194 : 0.3280396441696074
Loss in iteration 195 : 0.32801162931906425
Loss in iteration 196 : 0.32798390371504416
Loss in iteration 197 : 0.3279564631256474
Loss in iteration 198 : 0.327929303400816
Loss in iteration 199 : 0.32790242047032836
Loss in iteration 200 : 0.3278758103418943
Testing accuracy  of updater 2 on alg 0 with rate 0.1 = 0.8519746944290891, training accuracy 0.8463452088452088, time elapsed: 3733 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6846388386617934
Loss in iteration 3 : 0.6731240165844562
Loss in iteration 4 : 0.6594848769938566
Loss in iteration 5 : 0.6445614737076061
Loss in iteration 6 : 0.6291059506777609
Loss in iteration 7 : 0.6137520051344977
Loss in iteration 8 : 0.5989989544767212
Loss in iteration 9 : 0.5852088581274241
Loss in iteration 10 : 0.5726143064579406
Loss in iteration 11 : 0.561333918771112
Loss in iteration 12 : 0.551392427142333
Loss in iteration 13 : 0.5427424702566672
Loss in iteration 14 : 0.5352857778914277
Loss in iteration 15 : 0.5288921338843783
Loss in iteration 16 : 0.5234152099556141
Loss in iteration 17 : 0.5187049562094731
Loss in iteration 18 : 0.5146166651147983
Loss in iteration 19 : 0.5110170908755896
Loss in iteration 20 : 0.5077881318309376
Loss in iteration 21 : 0.5048286083468755
Loss in iteration 22 : 0.5020546304827557
Loss in iteration 23 : 0.4993989792701399
Loss in iteration 24 : 0.496809844376472
Loss in iteration 25 : 0.49424918257251044
Loss in iteration 26 : 0.4916908927199242
Loss in iteration 27 : 0.48911894654842775
Loss in iteration 28 : 0.48652557029605603
Loss in iteration 29 : 0.4839095389838657
Loss in iteration 30 : 0.4812746207926262
Loss in iteration 31 : 0.4786281917527795
Loss in iteration 32 : 0.47598002899084974
Loss in iteration 33 : 0.47334128263078845
Loss in iteration 34 : 0.4707236209772433
Loss in iteration 35 : 0.46813853994988186
Loss in iteration 36 : 0.4655968252827856
Loss in iteration 37 : 0.46310815434021513
Loss in iteration 38 : 0.46068082327459997
Loss in iteration 39 : 0.45832158452064026
Loss in iteration 40 : 0.45603557921460885
Loss in iteration 41 : 0.4538263490286679
Loss in iteration 42 : 0.4516959121176076
Loss in iteration 43 : 0.4496448883936271
Loss in iteration 44 : 0.44767266017000085
Loss in iteration 45 : 0.44577755532629904
Loss in iteration 46 : 0.44395704150920867
Loss in iteration 47 : 0.4422079214392496
Loss in iteration 48 : 0.44052652107897183
Loss in iteration 49 : 0.4389088641593158
Loss in iteration 50 : 0.43735082828437827
Loss in iteration 51 : 0.43584827947303045
Loss in iteration 52 : 0.43439718349049355
Loss in iteration 53 : 0.43299369362958284
Loss in iteration 54 : 0.43163421569035587
Loss in iteration 55 : 0.4303154517637855
Loss in iteration 56 : 0.4290344250496628
Loss in iteration 57 : 0.4277884883430068
Loss in iteration 58 : 0.42657531902791807
Loss in iteration 59 : 0.4253929034513256
Loss in iteration 60 : 0.4242395134424665
Loss in iteration 61 : 0.42311367753080337
Loss in iteration 62 : 0.42201414912688295
Loss in iteration 63 : 0.4209398735969666
Loss in iteration 64 : 0.41988995580893373
Loss in iteration 65 : 0.41886362937524335
Loss in iteration 66 : 0.41786022848521853
Loss in iteration 67 : 0.41687916291599597
Loss in iteration 68 : 0.41591989654687395
Loss in iteration 69 : 0.4149819294799775
Loss in iteration 70 : 0.41406478369190053
Loss in iteration 71 : 0.4131679920055654
Loss in iteration 72 : 0.41229109007517717
Loss in iteration 73 : 0.4114336110161002
Loss in iteration 74 : 0.410595082280285
Loss in iteration 75 : 0.40977502437126156
Loss in iteration 76 : 0.4089729510057138
Loss in iteration 77 : 0.40818837035565825
Loss in iteration 78 : 0.40742078704238377
Loss in iteration 79 : 0.40666970459624313
Loss in iteration 80 : 0.40593462814208087
Loss in iteration 81 : 0.40521506711560057
Loss in iteration 82 : 0.40451053785987534
Loss in iteration 83 : 0.40382056599113114
Loss in iteration 84 : 0.40314468845901
Loss in iteration 85 : 0.4024824552571533
Loss in iteration 86 : 0.4018334307658377
Loss in iteration 87 : 0.40119719472887116
Loss in iteration 88 : 0.40057334288289737
Loss in iteration 89 : 0.3999614872687361
Loss in iteration 90 : 0.39936125626188224
Loss in iteration 91 : 0.3987722943637919
Loss in iteration 92 : 0.3981942617969644
Loss in iteration 93 : 0.39762683394634174
Loss in iteration 94 : 0.3970697006872556
Loss in iteration 95 : 0.39652256563670424
Loss in iteration 96 : 0.3959851453604516
Loss in iteration 97 : 0.39545716856384927
Loss in iteration 98 : 0.3949383752894023
Loss in iteration 99 : 0.39442851613945445
Loss in iteration 100 : 0.3939273515378131
Loss in iteration 101 : 0.3934346510401399
Loss in iteration 102 : 0.3929501926992998
Loss in iteration 103 : 0.3924737624888128
Loss in iteration 104 : 0.39200515378487527
Loss in iteration 105 : 0.3915441669055429
Loss in iteration 106 : 0.3910906087039185
Loss in iteration 107 : 0.39064429221124325
Loss in iteration 108 : 0.39020503632492076
Loss in iteration 109 : 0.3897726655362126
Loss in iteration 110 : 0.3893470096921651
Loss in iteration 111 : 0.388927903786523
Loss in iteration 112 : 0.3885151877745719
Loss in iteration 113 : 0.3881087064073667
Loss in iteration 114 : 0.3877083090812094
Loss in iteration 115 : 0.38731384969884164
Loss in iteration 116 : 0.3869251865393206
Loss in iteration 117 : 0.38654218213414776
Loss in iteration 118 : 0.3861647031476384
Loss in iteration 119 : 0.38579262026011496
Loss in iteration 120 : 0.3854258080527893
Loss in iteration 121 : 0.3850641448937118
Loss in iteration 122 : 0.38470751282429827
Loss in iteration 123 : 0.3843557974463492
Loss in iteration 124 : 0.3840088878095909
Loss in iteration 125 : 0.38366667629986745
Loss in iteration 126 : 0.38332905852834603
Loss in iteration 127 : 0.3829959332220035
Loss in iteration 128 : 0.38266720211579786
Loss in iteration 129 : 0.38234276984687865
Loss in iteration 130 : 0.38202254385117534
Loss in iteration 131 : 0.3817064342626871
Loss in iteration 132 : 0.3813943538157563
Loss in iteration 133 : 0.3810862177504841
Loss in iteration 134 : 0.3807819437215361
Loss in iteration 135 : 0.3804814517103621
Loss in iteration 136 : 0.38018466394095296
Loss in iteration 137 : 0.37989150479912814
Loss in iteration 138 : 0.37960190075529504
Loss in iteration 139 : 0.37931578029065716
Loss in iteration 140 : 0.3790330738267702
Loss in iteration 141 : 0.3787537136583106
Loss in iteration 142 : 0.3784776338889158
Loss in iteration 143 : 0.3782047703699869
Loss in iteration 144 : 0.37793506064226634
Loss in iteration 145 : 0.37766844388005594
Loss in iteration 146 : 0.3774048608378791
Loss in iteration 147 : 0.3771442537994949
Loss in iteration 148 : 0.37688656652907104
Loss in iteration 149 : 0.37663174422438256
Loss in iteration 150 : 0.37637973347192244
Loss in iteration 151 : 0.3761304822037961
Loss in iteration 152 : 0.3758839396562857
Loss in iteration 153 : 0.3756400563299651
Loss in iteration 154 : 0.3753987839513257
Loss in iteration 155 : 0.37516007543576624
Loss in iteration 156 : 0.37492388485191047
Loss in iteration 157 : 0.37469016738718197
Loss in iteration 158 : 0.3744588793145674
Loss in iteration 159 : 0.37422997796050544
Loss in iteration 160 : 0.37400342167388545
Loss in iteration 161 : 0.3737791697960571
Loss in iteration 162 : 0.37355718263187526
Loss in iteration 163 : 0.373337421421695
Loss in iteration 164 : 0.37311984831427186
Loss in iteration 165 : 0.3729044263406046
Loss in iteration 166 : 0.372691119388613
Loss in iteration 167 : 0.3724798921786459
Loss in iteration 168 : 0.3722707102398205
Loss in iteration 169 : 0.37206353988712665
Loss in iteration 170 : 0.37185834819928965
Loss in iteration 171 : 0.3716551029973532
Loss in iteration 172 : 0.37145377282397873
Loss in iteration 173 : 0.37125432692341764
Loss in iteration 174 : 0.37105673522213195
Loss in iteration 175 : 0.3708609683100521
Loss in iteration 176 : 0.3706669974224531
Loss in iteration 177 : 0.3704747944223933
Loss in iteration 178 : 0.3702843317837505
Loss in iteration 179 : 0.3700955825747755
Loss in iteration 180 : 0.3699085204421857
Loss in iteration 181 : 0.3697231195957536
Loss in iteration 182 : 0.36953935479340877
Loss in iteration 183 : 0.3693572013267538
Loss in iteration 184 : 0.3691766350070993
Loss in iteration 185 : 0.3689976321518824
Loss in iteration 186 : 0.36882016957152675
Loss in iteration 187 : 0.36864422455669854
Loss in iteration 188 : 0.3684697748659555
Loss in iteration 189 : 0.36829679871377335
Loss in iteration 190 : 0.3681252747589225
Loss in iteration 191 : 0.3679551820932125
Loss in iteration 192 : 0.3677865002305438
Loss in iteration 193 : 0.3676192090963148
Loss in iteration 194 : 0.3674532890171249
Loss in iteration 195 : 0.3672887207107925
Loss in iteration 196 : 0.36712548527663963
Loss in iteration 197 : 0.366963564186101
Loss in iteration 198 : 0.366802939273558
Loss in iteration 199 : 0.36664359272748354
Loss in iteration 200 : 0.366485507081808
Testing accuracy  of updater 2 on alg 0 with rate 0.009999999999999995 = 0.832626988514219, training accuracy 0.8326781326781327, time elapsed: 4326 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 30.621201081645385
Loss in iteration 3 : 6.017710250263125
Loss in iteration 4 : 10.383262877188017
Loss in iteration 5 : 10.795089090590423
Loss in iteration 6 : 2.783209025947943
Loss in iteration 7 : 2.397427933185929
Loss in iteration 8 : 2.1292715933724176
Loss in iteration 9 : 2.005691457437177
Loss in iteration 10 : 2.3607210338027738
Loss in iteration 11 : 2.8786945500024985
Loss in iteration 12 : 4.439245846588998
Loss in iteration 13 : 2.9850932606247245
Loss in iteration 14 : 3.8699334424813814
Loss in iteration 15 : 2.4525998493801473
Loss in iteration 16 : 2.8523534183287027
Loss in iteration 17 : 2.265799295909092
Loss in iteration 18 : 2.788669109527586
Loss in iteration 19 : 2.292255479303191
Loss in iteration 20 : 2.970905641159883
Loss in iteration 21 : 2.2894093609454136
Loss in iteration 22 : 2.9004142385502134
Loss in iteration 23 : 2.157710621042467
Loss in iteration 24 : 2.634904810959594
Loss in iteration 25 : 2.038068051463638
Loss in iteration 26 : 2.5138730257741306
Loss in iteration 27 : 2.006887933938567
Loss in iteration 28 : 2.549247542878578
Loss in iteration 29 : 1.9999783350182063
Loss in iteration 30 : 2.53817761734194
Loss in iteration 31 : 1.934473831981241
Loss in iteration 32 : 2.401380349161876
Loss in iteration 33 : 1.8530296664239547
Loss in iteration 34 : 2.2981597799912405
Loss in iteration 35 : 1.8136683389940365
Loss in iteration 36 : 2.289892548014699
Loss in iteration 37 : 1.7999715281462487
Loss in iteration 38 : 2.2850857815385215
Loss in iteration 39 : 1.7609783659296272
Loss in iteration 40 : 2.2089132990638816
Loss in iteration 41 : 1.7054247221699375
Loss in iteration 42 : 2.1311209651686536
Loss in iteration 43 : 1.668833015882941
Loss in iteration 44 : 2.1064293397861955
Loss in iteration 45 : 1.650490062850581
Loss in iteration 46 : 2.0958948372293777
Loss in iteration 47 : 1.6228409276775702
Loss in iteration 48 : 2.049698912409664
Loss in iteration 49 : 1.5841216182590439
Loss in iteration 50 : 1.9942123827236147
Loss in iteration 51 : 1.5532929580642305
Loss in iteration 52 : 1.9656765440988861
Loss in iteration 53 : 1.5334237122601477
Loss in iteration 54 : 1.9496033795082635
Loss in iteration 55 : 1.5109994823375126
Loss in iteration 56 : 1.9176686381411754
Loss in iteration 57 : 1.4827217729439448
Loss in iteration 58 : 1.878254443794553
Loss in iteration 59 : 1.4575815243873391
Loss in iteration 60 : 1.8513632755556628
Loss in iteration 61 : 1.4383535947877433
Loss in iteration 62 : 1.8325865850357994
Loss in iteration 63 : 1.4189752969071943
Loss in iteration 64 : 1.8076276476911575
Loss in iteration 65 : 1.3970871968064036
Loss in iteration 66 : 1.7785177061573225
Loss in iteration 67 : 1.3765332366363947
Loss in iteration 68 : 1.7548954574979096
Loss in iteration 69 : 1.3589868449703766
Loss in iteration 70 : 1.7358168163668795
Loss in iteration 71 : 1.3419414998974055
Loss in iteration 72 : 1.7147699539177512
Loss in iteration 73 : 1.3241106588515705
Loss in iteration 74 : 1.6919581413846203
Loss in iteration 75 : 1.3069910042578547
Loss in iteration 76 : 1.671544540201839
Loss in iteration 77 : 1.2914031968809991
Loss in iteration 78 : 1.6534983465003048
Loss in iteration 79 : 1.2763813249811489
Loss in iteration 80 : 1.6351073376632508
Loss in iteration 81 : 1.2613201586431562
Loss in iteration 82 : 1.6162179790814222
Loss in iteration 83 : 1.2467511435913676
Loss in iteration 84 : 1.5984671799011634
Loss in iteration 85 : 1.233020933929631
Loss in iteration 86 : 1.581955778987743
Loss in iteration 87 : 1.2197760462326062
Loss in iteration 88 : 1.565635775774687
Loss in iteration 89 : 1.2067560528682666
Loss in iteration 90 : 1.5493659465495533
Loss in iteration 91 : 1.1941298194955898
Loss in iteration 92 : 1.5337358354691712
Loss in iteration 93 : 1.1820246323204384
Loss in iteration 94 : 1.518811598014127
Loss in iteration 95 : 1.1703132803146767
Loss in iteration 96 : 1.5042105465633322
Loss in iteration 97 : 1.1588879593467372
Loss in iteration 98 : 1.4898501867873732
Loss in iteration 99 : 1.1477926525216877
Loss in iteration 100 : 1.4759233328142825
Loss in iteration 101 : 1.137065661235886
Loss in iteration 102 : 1.4624564550348063
Loss in iteration 103 : 1.1266587947446864
Loss in iteration 104 : 1.449313430175126
Loss in iteration 105 : 1.1165262540714793
Loss in iteration 106 : 1.4364513601482094
Loss in iteration 107 : 1.1066730646224698
Loss in iteration 108 : 1.423924609177639
Loss in iteration 109 : 1.097105429430462
Loss in iteration 110 : 1.4117383506173606
Loss in iteration 111 : 1.0878020025509847
Loss in iteration 112 : 1.3998425744219012
Loss in iteration 113 : 1.07874108849926
Loss in iteration 114 : 1.3882145738538572
Loss in iteration 115 : 1.0699173428263569
Loss in iteration 116 : 1.3768643111284762
Loss in iteration 117 : 1.061326967506796
Loss in iteration 118 : 1.3657883954807033
Loss in iteration 119 : 1.0529580197242971
Loss in iteration 120 : 1.3549658343359365
Loss in iteration 121 : 1.0447982679262162
Loss in iteration 122 : 1.3443832779947014
Loss in iteration 123 : 1.036840667024364
Loss in iteration 124 : 1.334037983268308
Loss in iteration 125 : 1.0290791724138733
Loss in iteration 126 : 1.3239240402672832
Loss in iteration 127 : 1.021505605966987
Loss in iteration 128 : 1.3140303216844489
Loss in iteration 129 : 1.014111790134393
Loss in iteration 130 : 1.3043478389411105
Loss in iteration 131 : 1.0068912608773566
Loss in iteration 132 : 1.294871030230182
Loss in iteration 133 : 0.9998381412536556
Loss in iteration 134 : 1.2855938186848523
Loss in iteration 135 : 0.99294616026189
Loss in iteration 136 : 1.2765087681896916
Loss in iteration 137 : 0.9862091903988809
Loss in iteration 138 : 1.2676091242306866
Loss in iteration 139 : 0.9796217570692726
Loss in iteration 140 : 1.258889303357163
Loss in iteration 141 : 0.9731787465832558
Loss in iteration 142 : 1.2503438091080321
Loss in iteration 143 : 0.9668751042725177
Loss in iteration 144 : 1.2419669140564724
Loss in iteration 145 : 0.9607059539955545
Loss in iteration 146 : 1.2337531953112566
Loss in iteration 147 : 0.9546667390471971
Loss in iteration 148 : 1.2256977015062358
Loss in iteration 149 : 0.9487531458691892
Loss in iteration 150 : 1.2177956631894968
Loss in iteration 151 : 0.9429610098014733
Loss in iteration 152 : 1.2100423782805851
Loss in iteration 153 : 0.9372863335877647
Loss in iteration 154 : 1.2024333414974595
Loss in iteration 155 : 0.9317253207026249
Loss in iteration 156 : 1.1949642943575123
Loss in iteration 157 : 0.9262743523893395
Loss in iteration 158 : 1.1876311474643206
Loss in iteration 159 : 0.9209299556748028
Loss in iteration 160 : 1.1804299388095092
Loss in iteration 161 : 0.9156888005996577
Loss in iteration 162 : 1.17335685958073
Loss in iteration 163 : 0.9105477041634059
Loss in iteration 164 : 1.1664082653832264
Loss in iteration 165 : 0.9055036205122228
Loss in iteration 166 : 1.1595806531673638
Loss in iteration 167 : 0.9005536277828873
Loss in iteration 168 : 1.1528706444803782
Loss in iteration 169 : 0.8956949223831474
Loss in iteration 170 : 1.1462749868748137
Loss in iteration 171 : 0.8909248159394847
Loss in iteration 172 : 1.1397905535622308
Loss in iteration 173 : 0.8862407292185447
Loss in iteration 174 : 1.133414334340815
Loss in iteration 175 : 0.8816401850834152
Loss in iteration 176 : 1.1271434273799579
Loss in iteration 177 : 0.8771208034027386
Loss in iteration 178 : 1.1209750358238517
Loss in iteration 179 : 0.8726802970094801
Loss in iteration 180 : 1.1149064646288105
Loss in iteration 181 : 0.8683164671775231
Loss in iteration 182 : 1.108935115386799
Loss in iteration 183 : 0.864027198948823
Loss in iteration 184 : 1.1030584812582933
Loss in iteration 185 : 0.8598104570624444
Loss in iteration 186 : 1.0972741432314166
Loss in iteration 187 : 0.855664282341672
Loss in iteration 188 : 1.091579766713766
Loss in iteration 189 : 0.8515867881372566
Loss in iteration 190 : 1.0859730977793898
Loss in iteration 191 : 0.847576156851314
Loss in iteration 192 : 1.0804519595002826
Loss in iteration 193 : 0.8436306367211678
Loss in iteration 194 : 1.0750142487071987
Loss in iteration 195 : 0.8397485388447742
Loss in iteration 196 : 1.06965793297856
Loss in iteration 197 : 0.8359282343419736
Loss in iteration 198 : 1.0643810476621123
Loss in iteration 199 : 0.8321681516376919
Loss in iteration 200 : 1.0591816930015587
Testing accuracy  of updater 3 on alg 0 with rate 10.0 = 0.776057981696456, training accuracy 0.7792076167076167, time elapsed: 4259 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.45073749901137866
Loss in iteration 3 : 0.3996638827674247
Loss in iteration 4 : 0.3823868207276731
Loss in iteration 5 : 0.37135813889803515
Loss in iteration 6 : 0.36335326357633085
Loss in iteration 7 : 0.3573087577883011
Loss in iteration 8 : 0.35260779428344813
Loss in iteration 9 : 0.34886294147640007
Loss in iteration 10 : 0.3458199202577445
Loss in iteration 11 : 0.34330574148249765
Loss in iteration 12 : 0.34119906337078465
Loss in iteration 13 : 0.33941249632251896
Loss in iteration 14 : 0.33788164043547614
Loss in iteration 15 : 0.33655807056095044
Loss in iteration 16 : 0.33540471781276054
Loss in iteration 17 : 0.33439275136310626
Loss in iteration 18 : 0.3334994255209625
Loss in iteration 19 : 0.3327065632152065
Loss in iteration 20 : 0.3319994683382352
Loss in iteration 21 : 0.33136613284428323
Loss in iteration 22 : 0.3307966500799177
Loss in iteration 23 : 0.3302827747713577
Loss in iteration 24 : 0.3298175888615983
Loss in iteration 25 : 0.32939524479223764
Loss in iteration 26 : 0.3290107661631929
Loss in iteration 27 : 0.3286598913998666
Loss in iteration 28 : 0.32833895000646895
Loss in iteration 29 : 0.3280447637592113
Loss in iteration 30 : 0.32777456716815634
Loss in iteration 31 : 0.32752594295825643
Loss in iteration 32 : 0.3272967693553113
Loss in iteration 33 : 0.32708517672347937
Loss in iteration 34 : 0.3268895116662984
Loss in iteration 35 : 0.3267083071262698
Loss in iteration 36 : 0.32654025733802944
Loss in iteration 37 : 0.3263841967336349
Loss in iteration 38 : 0.326239082085429
Loss in iteration 39 : 0.3261039773164149
Loss in iteration 40 : 0.32597804052056106
Loss in iteration 41 : 0.32586051282348155
Loss in iteration 42 : 0.3257507087833932
Loss in iteration 43 : 0.3256480080873472
Loss in iteration 44 : 0.3255518483416213
Loss in iteration 45 : 0.32546171879040237
Loss in iteration 46 : 0.3253771548252365
Loss in iteration 47 : 0.3252977331708358
Loss in iteration 48 : 0.325223067651464
Loss in iteration 49 : 0.32515280545752423
Loss in iteration 50 : 0.32508662384455844
Loss in iteration 51 : 0.32502422720726826
Loss in iteration 52 : 0.3249653444797782
Loss in iteration 53 : 0.32490972682057395
Loss in iteration 54 : 0.32485714554652706
Loss in iteration 55 : 0.32480739028546574
Loss in iteration 56 : 0.32476026732096436
Loss in iteration 57 : 0.32471559810661527
Loss in iteration 58 : 0.32467321793016307
Loss in iteration 59 : 0.3246329747102506
Loss in iteration 60 : 0.32459472791104294
Loss in iteration 61 : 0.3245583475615634
Loss in iteration 62 : 0.32452371336843994
Loss in iteration 63 : 0.32449071391200884
Loss in iteration 64 : 0.32445924591700065
Loss in iteration 65 : 0.32442921359004717
Loss in iteration 66 : 0.32440052801713615
Loss in iteration 67 : 0.3243731066149869
Loss in iteration 68 : 0.3243468726309411
Loss in iteration 69 : 0.3243217546865792
Loss in iteration 70 : 0.324297686360855
Loss in iteration 71 : 0.3242746058088891
Loss in iteration 72 : 0.32425245541309033
Loss in iteration 73 : 0.3242311814635572
Loss in iteration 74 : 0.3242107338650168
Loss in iteration 75 : 0.3241910658679128
Loss in iteration 76 : 0.32417213382141324
Loss in iteration 77 : 0.3241538969464013
Loss in iteration 78 : 0.3241363171266454
Loss in iteration 79 : 0.32411935871658176
Loss in iteration 80 : 0.3241029883642124
Loss in iteration 81 : 0.3240871748478915
Loss in iteration 82 : 0.32407188892574207
Loss in iteration 83 : 0.32405710319666187
Loss in iteration 84 : 0.3240427919719601
Loss in iteration 85 : 0.3240289311567115
Loss in iteration 86 : 0.32401549814005437
Loss in iteration 87 : 0.32400247169367946
Loss in iteration 88 : 0.32398983187785574
Loss in iteration 89 : 0.32397755995439226
Loss in iteration 90 : 0.3239656383059593
Loss in iteration 91 : 0.3239540503613015
Loss in iteration 92 : 0.32394278052583747
Loss in iteration 93 : 0.32393181411726996
Loss in iteration 94 : 0.32392113730576266
Loss in iteration 95 : 0.32391073705839585
Loss in iteration 96 : 0.323900601087536
Loss in iteration 97 : 0.3238907178027953
Loss in iteration 98 : 0.32388107626638857
Loss in iteration 99 : 0.3238716661515704
Loss in iteration 100 : 0.32386247770392174
Loss in iteration 101 : 0.3238535017053184
Loss in iteration 102 : 0.3238447294403363
Loss in iteration 103 : 0.3238361526649718
Loss in iteration 104 : 0.3238277635774312
Loss in iteration 105 : 0.3238195547909119
Loss in iteration 106 : 0.32381151930820146
Loss in iteration 107 : 0.323803650497946
Loss in iteration 108 : 0.32379594207252355
Loss in iteration 109 : 0.32378838806733756
Loss in iteration 110 : 0.32378098282149664
Loss in iteration 111 : 0.32377372095974005
Loss in iteration 112 : 0.3237665973755379
Loss in iteration 113 : 0.32375960721528824
Loss in iteration 114 : 0.3237527458635249
Loss in iteration 115 : 0.32374600892906863
Loss in iteration 116 : 0.32373939223209125
Loss in iteration 117 : 0.32373289179194725
Loss in iteration 118 : 0.3237265038158088
Loss in iteration 119 : 0.323720224688025
Loss in iteration 120 : 0.3237140509601082
Loss in iteration 121 : 0.32370797934138235
Loss in iteration 122 : 0.32370200669018107
Loss in iteration 123 : 0.3236961300056107
Loss in iteration 124 : 0.32369034641981276
Loss in iteration 125 : 0.32368465319068335
Loss in iteration 126 : 0.32367904769506856
Loss in iteration 127 : 0.32367352742233646
Loss in iteration 128 : 0.32366808996836927
Loss in iteration 129 : 0.32366273302988896
Loss in iteration 130 : 0.3236574543991407
Loss in iteration 131 : 0.3236522519588745
Loss in iteration 132 : 0.32364712367765847
Loss in iteration 133 : 0.3236420676054152
Loss in iteration 134 : 0.32363708186927237
Loss in iteration 135 : 0.3236321646696231
Loss in iteration 136 : 0.3236273142764305
Loss in iteration 137 : 0.3236225290257348
Loss in iteration 138 : 0.323617807316362
Loss in iteration 139 : 0.32361314760684656
Loss in iteration 140 : 0.32360854841248293
Loss in iteration 141 : 0.3236040083025949
Loss in iteration 142 : 0.32359952589791047
Loss in iteration 143 : 0.32359509986812995
Loss in iteration 144 : 0.32359072892958946
Loss in iteration 145 : 0.32358641184308967
Loss in iteration 146 : 0.3235821474118081
Loss in iteration 147 : 0.3235779344793635
Loss in iteration 148 : 0.3235737719279527
Loss in iteration 149 : 0.32356965867662013
Loss in iteration 150 : 0.3235655936795837
Loss in iteration 151 : 0.3235615759246939
Loss in iteration 152 : 0.323557604431936
Loss in iteration 153 : 0.3235536782520375
Loss in iteration 154 : 0.323549796465136
Loss in iteration 155 : 0.32354595817952286
Loss in iteration 156 : 0.3235421625304523
Loss in iteration 157 : 0.32353840867901057
Loss in iteration 158 : 0.323534695811039
Loss in iteration 159 : 0.3235310231361212
Loss in iteration 160 : 0.3235273898866184
Loss in iteration 161 : 0.3235237953167493
Loss in iteration 162 : 0.3235202387017232
Loss in iteration 163 : 0.32351671933690446
Loss in iteration 164 : 0.3235132365370354
Loss in iteration 165 : 0.3235097896354881
Loss in iteration 166 : 0.32350637798354626
Loss in iteration 167 : 0.3235030009497296
Loss in iteration 168 : 0.32349965791915847
Loss in iteration 169 : 0.3234963482929312
Loss in iteration 170 : 0.3234930714875435
Loss in iteration 171 : 0.3234898269343309
Loss in iteration 172 : 0.32348661407894
Loss in iteration 173 : 0.323483432380817
Loss in iteration 174 : 0.3234802813127355
Loss in iteration 175 : 0.32347716036032137
Loss in iteration 176 : 0.32347406902162357
Loss in iteration 177 : 0.3234710068066899
Loss in iteration 178 : 0.32346797323715953
Loss in iteration 179 : 0.3234649678459017
Loss in iteration 180 : 0.32346199017661387
Loss in iteration 181 : 0.32345903978349577
Loss in iteration 182 : 0.32345611623091447
Loss in iteration 183 : 0.3234532190930621
Loss in iteration 184 : 0.3234503479536676
Loss in iteration 185 : 0.3234475024056955
Loss in iteration 186 : 0.3234446820510546
Loss in iteration 187 : 0.3234418865003382
Loss in iteration 188 : 0.32343911537256065
Loss in iteration 189 : 0.32343636829490297
Loss in iteration 190 : 0.3234336449024777
Loss in iteration 191 : 0.3234309448380932
Loss in iteration 192 : 0.3234282677520418
Loss in iteration 193 : 0.3234256133018769
Testing accuracy  of updater 3 on alg 0 with rate 1.0 = 0.8500706344819114, training accuracy 0.8493243243243244, time elapsed: 3340 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6307287333043426
Loss in iteration 3 : 0.5878337519739268
Loss in iteration 4 : 0.5570876324086254
Loss in iteration 5 : 0.5342382653738091
Loss in iteration 6 : 0.5167048389955241
Loss in iteration 7 : 0.5028591837419891
Loss in iteration 8 : 0.49164034008287427
Loss in iteration 9 : 0.48233735417797635
Loss in iteration 10 : 0.4744620204581519
Loss in iteration 11 : 0.4676717873901151
Loss in iteration 12 : 0.4617216512623102
Loss in iteration 13 : 0.45643333661701974
Loss in iteration 14 : 0.45167508083238544
Loss in iteration 15 : 0.44734809168595946
Loss in iteration 16 : 0.44337730207651965
Loss in iteration 17 : 0.439704950334094
Loss in iteration 18 : 0.4362860539593968
Loss in iteration 19 : 0.4330851740966709
Loss in iteration 20 : 0.43007407369064093
Loss in iteration 21 : 0.42723000323184196
Loss in iteration 22 : 0.4245344329332655
Loss in iteration 23 : 0.42197210621767
Loss in iteration 24 : 0.41953032694771936
Loss in iteration 25 : 0.4171984183647934
Loss in iteration 26 : 0.4149673092975198
Loss in iteration 27 : 0.4128292154781813
Loss in iteration 28 : 0.4107773924696417
Loss in iteration 29 : 0.40880594288668315
Loss in iteration 30 : 0.40690966504845627
Loss in iteration 31 : 0.4050839334362985
Loss in iteration 32 : 0.4033246037048455
Loss in iteration 33 : 0.4016279367486802
Loss in iteration 34 : 0.39999053763224873
Loss in iteration 35 : 0.39840930616960046
Loss in iteration 36 : 0.3968813966779683
Loss in iteration 37 : 0.39540418498896296
Loss in iteration 38 : 0.3939752412276933
Loss in iteration 39 : 0.3925923071968613
Loss in iteration 40 : 0.39125327745461375
Loss in iteration 41 : 0.38995618336919907
Loss in iteration 42 : 0.3886991795845055
Loss in iteration 43 : 0.3874805324479427
Loss in iteration 44 : 0.38629861004412364
Loss in iteration 45 : 0.3851518735497089
Loss in iteration 46 : 0.3840388696815022
Loss in iteration 47 : 0.38295822405458974
Loss in iteration 48 : 0.38190863530262026
Loss in iteration 49 : 0.38088886984052545
Loss in iteration 50 : 0.37989775717214563
Loss in iteration 51 : 0.3789341856632433
Loss in iteration 52 : 0.37799709871444226
Loss in iteration 53 : 0.37708549128030944
Loss in iteration 54 : 0.37619840668978227
Loss in iteration 55 : 0.3753349337308295
Loss in iteration 56 : 0.3744942039679985
Loss in iteration 57 : 0.37367538926667915
Loss in iteration 58 : 0.37287769950168465
Loss in iteration 59 : 0.37210038043118665
Loss in iteration 60 : 0.3713427117196253
Loss in iteration 61 : 0.3706040050955435
Loss in iteration 62 : 0.3698836026320565
Loss in iteration 63 : 0.3691808751392525
Loss in iteration 64 : 0.3684952206591245
Loss in iteration 65 : 0.367826063054658
Loss in iteration 66 : 0.36717285068574923
Loss in iteration 67 : 0.3665350551652343
Loss in iteration 68 : 0.36591217018916206
Loss in iteration 69 : 0.36530371043593207
Loss in iteration 70 : 0.3647092105293898
Loss in iteration 71 : 0.3641282240615599
Loss in iteration 72 : 0.36356032267087357
Loss in iteration 73 : 0.363005095172297
Loss in iteration 74 : 0.3624621467358954
Loss in iteration 75 : 0.3619310981108005
Loss in iteration 76 : 0.361411584891608
Loss in iteration 77 : 0.3609032568246047
Loss in iteration 78 : 0.36040577715137817
Loss in iteration 79 : 0.35991882198747693
Loss in iteration 80 : 0.3594420797339903
Loss in iteration 81 : 0.35897525052016094
Loss in iteration 82 : 0.3585180456750635
Loss in iteration 83 : 0.3580701872267415
Loss in iteration 84 : 0.3576314074271308
Loss in iteration 85 : 0.35720144830130923
Loss in iteration 86 : 0.3567800612196523
Loss in iteration 87 : 0.35636700649160924
Loss in iteration 88 : 0.3559620529798375
Loss in iteration 89 : 0.3555649777335733
Loss in iteration 90 : 0.3551755656401781
Loss in iteration 91 : 0.35479360909377267
Loss in iteration 92 : 0.3544189076801276
Loss in iteration 93 : 0.35405126787682234
Loss in iteration 94 : 0.35369050276785735
Loss in iteration 95 : 0.35333643177200114
Loss in iteration 96 : 0.35298888038402354
Loss in iteration 97 : 0.3526476799282124
Loss in iteration 98 : 0.35231266732347155
Loss in iteration 99 : 0.3519836848594044
Loss in iteration 100 : 0.35166057998279543
Loss in iteration 101 : 0.35134320509395184
Loss in iteration 102 : 0.3510314173523794
Loss in iteration 103 : 0.35072507849133244
Loss in iteration 104 : 0.3504240546407391
Loss in iteration 105 : 0.35012821615812906
Loss in iteration 106 : 0.34983743746711077
Loss in iteration 107 : 0.34955159690301474
Loss in iteration 108 : 0.34927057656539173
Loss in iteration 109 : 0.34899426217697616
Loss in iteration 110 : 0.34872254294878424
Loss in iteration 111 : 0.34845531145110503
Loss in iteration 112 : 0.34819246349001126
Loss in iteration 113 : 0.34793389798920876
Loss in iteration 114 : 0.34767951687687904
Loss in iteration 115 : 0.3474292249773287
Loss in iteration 116 : 0.34718292990722077
Loss in iteration 117 : 0.34694054197609536
Loss in iteration 118 : 0.34670197409109704
Loss in iteration 119 : 0.3464671416655681
Loss in iteration 120 : 0.34623596253145655
Loss in iteration 121 : 0.34600835685527304
Loss in iteration 122 : 0.34578424705746486
Loss in iteration 123 : 0.34556355773505415
Loss in iteration 124 : 0.3453462155873698
Loss in iteration 125 : 0.3451321493447492
Loss in iteration 126 : 0.34492128970007224
Loss in iteration 127 : 0.3447135692429903
Loss in iteration 128 : 0.344508922396741
Loss in iteration 129 : 0.34430728535741056
Loss in iteration 130 : 0.3441085960355774
Loss in iteration 131 : 0.34391279400018676
Loss in iteration 132 : 0.3437198204245827
Loss in iteration 133 : 0.34352961803459015
Loss in iteration 134 : 0.3433421310585849
Loss in iteration 135 : 0.34315730517942084
Loss in iteration 136 : 0.34297508748818023
Loss in iteration 137 : 0.34279542643963584
Loss in iteration 138 : 0.3426182718093648
Loss in iteration 139 : 0.3424435746524515
Loss in iteration 140 : 0.3422712872636991
Loss in iteration 141 : 0.3421013631392987
Loss in iteration 142 : 0.34193375693986916
Loss in iteration 143 : 0.3417684244548676
Loss in iteration 144 : 0.3416053225682365
Loss in iteration 145 : 0.3414444092253051
Loss in iteration 146 : 0.34128564340085155
Loss in iteration 147 : 0.34112898506830025
Loss in iteration 148 : 0.3409743951700062
Loss in iteration 149 : 0.3408218355885595
Loss in iteration 150 : 0.3406712691191126
Loss in iteration 151 : 0.34052265944263366
Loss in iteration 152 : 0.34037597110011764
Loss in iteration 153 : 0.3402311694676521
Loss in iteration 154 : 0.34008822073233386
Loss in iteration 155 : 0.33994709186902217
Loss in iteration 156 : 0.3398077506178549
Loss in iteration 157 : 0.3396701654625297
Loss in iteration 158 : 0.33953430560930214
Loss in iteration 159 : 0.33940014096668913
Loss in iteration 160 : 0.33926764212583665
Loss in iteration 161 : 0.3391367803415443
Loss in iteration 162 : 0.3390075275138961
Loss in iteration 163 : 0.33887985617049043
Loss in iteration 164 : 0.33875373944926185
Loss in iteration 165 : 0.33862915108184133
Loss in iteration 166 : 0.3385060653774493
Loss in iteration 167 : 0.3383844572073243
Loss in iteration 168 : 0.3382643019896074
Loss in iteration 169 : 0.3381455756747471
Loss in iteration 170 : 0.3380282547313385
Loss in iteration 171 : 0.33791231613240924
Loss in iteration 172 : 0.337797737342132
Loss in iteration 173 : 0.3376844963029688
Loss in iteration 174 : 0.3375725714231772
Loss in iteration 175 : 0.3374619415647315
Loss in iteration 176 : 0.33735258603159674
Loss in iteration 177 : 0.33724448455837
Loss in iteration 178 : 0.33713761729925024
Loss in iteration 179 : 0.3370319648173633
Loss in iteration 180 : 0.336927508074393
Loss in iteration 181 : 0.3368242284205128
Loss in iteration 182 : 0.3367221075846526
Loss in iteration 183 : 0.33662112766500885
Loss in iteration 184 : 0.33652127111986463
Loss in iteration 185 : 0.33642252075867574
Loss in iteration 186 : 0.33632485973340526
Loss in iteration 187 : 0.336228271530126
Loss in iteration 188 : 0.3361327399608429
Loss in iteration 189 : 0.33603824915558733
Loss in iteration 190 : 0.3359447835546978
Loss in iteration 191 : 0.33585232790135794
Loss in iteration 192 : 0.335760867234319
Loss in iteration 193 : 0.3356703868808401
Loss in iteration 194 : 0.3355808724498377
Loss in iteration 195 : 0.33549230982520056
Loss in iteration 196 : 0.3354046851593302
Loss in iteration 197 : 0.3353179848668163
Loss in iteration 198 : 0.3352321956183341
Loss in iteration 199 : 0.3351473043346623
Loss in iteration 200 : 0.33506329818091407
Testing accuracy  of updater 3 on alg 0 with rate 0.09999999999999998 = 0.8460782507217002, training accuracy 0.8448402948402949, time elapsed: 3547 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6920044749122338
Loss in iteration 3 : 0.6908389442411161
Loss in iteration 4 : 0.6896609571526451
Loss in iteration 5 : 0.6884756519129271
Loss in iteration 6 : 0.6872860809116603
Loss in iteration 7 : 0.6860942527576904
Loss in iteration 8 : 0.6849015780586383
Loss in iteration 9 : 0.6837090923707456
Loss in iteration 10 : 0.6825175802631033
Loss in iteration 11 : 0.6813276499489451
Loss in iteration 12 : 0.680139780933271
Loss in iteration 13 : 0.6789543558969313
Loss in iteration 14 : 0.6777716828619428
Loss in iteration 15 : 0.6765920110976974
Loss in iteration 16 : 0.6754155428517176
Loss in iteration 17 : 0.6742424422178139
Loss in iteration 18 : 0.6730728420042023
Loss in iteration 19 : 0.6719068491910072
Loss in iteration 20 : 0.6707445493895094
Loss in iteration 21 : 0.6695860105843084
Loss in iteration 22 : 0.668431286323087
Loss in iteration 23 : 0.6672804184135043
Loss in iteration 24 : 0.6661334391203165
Loss in iteration 25 : 0.6649903728603587
Loss in iteration 26 : 0.6638512374534761
Loss in iteration 27 : 0.6627160450432952
Loss in iteration 28 : 0.6615848028091574
Loss in iteration 29 : 0.6604575135582188
Loss in iteration 30 : 0.6593341762475324
Loss in iteration 31 : 0.6582147864581975
Loss in iteration 32 : 0.6570993368292505
Loss in iteration 33 : 0.6559878174524506
Loss in iteration 34 : 0.654880216226413
Loss in iteration 35 : 0.6537765191682473
Loss in iteration 36 : 0.6526767106824045
Loss in iteration 37 : 0.6515807737893057
Loss in iteration 38 : 0.6504886903183353
Loss in iteration 39 : 0.6494004410707997
Loss in iteration 40 : 0.6483160059580525
Loss in iteration 41 : 0.6472353641186898
Loss in iteration 42 : 0.6461584940178311
Loss in iteration 43 : 0.6450853735303794
Loss in iteration 44 : 0.6440159800096135
Loss in iteration 45 : 0.642950290342046
Loss in iteration 46 : 0.6418882809892223
Loss in iteration 47 : 0.6408299280168186
Loss in iteration 48 : 0.6397752071113075
Loss in iteration 49 : 0.6387240935841674
Loss in iteration 50 : 0.6376765623635855
Loss in iteration 51 : 0.6366325879733169
Loss in iteration 52 : 0.6355921444984665
Loss in iteration 53 : 0.6345552055378624
Loss in iteration 54 : 0.6335217441429143
Loss in iteration 55 : 0.6324917327434713
Loss in iteration 56 : 0.6314651430619954
Loss in iteration 57 : 0.6304419460189089
Loss in iteration 58 : 0.6294221116344763
Loss in iteration 59 : 0.6284056089355885
Loss in iteration 60 : 0.6273924058800184
Loss in iteration 61 : 0.6263824693144625
Loss in iteration 62 : 0.6253757649856996
Loss in iteration 63 : 0.6243722576233541
Loss in iteration 64 : 0.6233719111069737
Loss in iteration 65 : 0.6223746887184484
Loss in iteration 66 : 0.6213805534654893
Loss in iteration 67 : 0.6203894684471366
Loss in iteration 68 : 0.6194013972237408
Loss in iteration 69 : 0.6184163041541714
Loss in iteration 70 : 0.6174341546717994
Loss in iteration 71 : 0.6164549154851751
Loss in iteration 72 : 0.6154785547035507
Loss in iteration 73 : 0.6145050418975496
Loss in iteration 74 : 0.6135343481098231
Loss in iteration 75 : 0.6125664458302517
Loss in iteration 76 : 0.6116013089478357
Loss in iteration 77 : 0.6106389126884997
Loss in iteration 78 : 0.6096792335458747
Loss in iteration 79 : 0.6087222492099575
Loss in iteration 80 : 0.6077679384971209
Loss in iteration 81 : 0.606816281283499
Loss in iteration 82 : 0.6058672584428179
Loss in iteration 83 : 0.6049208517889685
Loss in iteration 84 : 0.6039770440231856
Loss in iteration 85 : 0.6030358186854408
Loss in iteration 86 : 0.6020971601096271
Loss in iteration 87 : 0.6011610533819803
Loss in iteration 88 : 0.6002274843023214
Loss in iteration 89 : 0.5992964393477673
Loss in iteration 90 : 0.5983679056384295
Loss in iteration 91 : 0.5974418709049751
Loss in iteration 92 : 0.5965183234576569
Loss in iteration 93 : 0.5955972521566846
Loss in iteration 94 : 0.5946786463836935
Loss in iteration 95 : 0.5937624960141452
Loss in iteration 96 : 0.5928487913905571
Loss in iteration 97 : 0.5919375232963259
Loss in iteration 98 : 0.5910286829300948
Loss in iteration 99 : 0.5901222618804254
Loss in iteration 100 : 0.5892182521007379
Loss in iteration 101 : 0.5883166458843548
Loss in iteration 102 : 0.5874174358394019
Loss in iteration 103 : 0.5865206148636659
Loss in iteration 104 : 0.5856261761190386
Loss in iteration 105 : 0.5847341130057089
Loss in iteration 106 : 0.5838444191358916
Loss in iteration 107 : 0.5829570883072933
Loss in iteration 108 : 0.5820721144765173
Loss in iteration 109 : 0.5811894917329004
Loss in iteration 110 : 0.5803092142735574
Loss in iteration 111 : 0.5794312763809286
Loss in iteration 112 : 0.5785556724045388
Loss in iteration 113 : 0.5776823967492463
Loss in iteration 114 : 0.5768114438723613
Loss in iteration 115 : 0.5759428082920378
Loss in iteration 116 : 0.5750764846080214
Loss in iteration 117 : 0.5742124675342717
Loss in iteration 118 : 0.5733507519404601
Loss in iteration 119 : 0.5724913328970135
Loss in iteration 120 : 0.5716342057176004
Loss in iteration 121 : 0.5707793659933522
Loss in iteration 122 : 0.5699268096154323
Loss in iteration 123 : 0.569076532785209
Loss in iteration 124 : 0.5682285320135033
Loss in iteration 125 : 0.5673828041116644
Loss in iteration 126 : 0.5665393461773153
Loss in iteration 127 : 0.5656981555773102
Loss in iteration 128 : 0.5648592299296376
Loss in iteration 129 : 0.5640225670854444
Loss in iteration 130 : 0.5631881651117736
Loss in iteration 131 : 0.5623560222753404
Loss in iteration 132 : 0.5615261370274102
Loss in iteration 133 : 0.5606985079897522
Loss in iteration 134 : 0.5598731339415587
Loss in iteration 135 : 0.5590500138073115
Loss in iteration 136 : 0.5582291466454027
Loss in iteration 137 : 0.5574105316374722
Loss in iteration 138 : 0.5565941680783917
Loss in iteration 139 : 0.5557800553667587
Loss in iteration 140 : 0.5549681929959523
Loss in iteration 141 : 0.5541585805455759
Loss in iteration 142 : 0.5533512176733577
Loss in iteration 143 : 0.5525461041073889
Loss in iteration 144 : 0.5517432396387117
Loss in iteration 145 : 0.5509426241142216
Loss in iteration 146 : 0.5501442574298517
Loss in iteration 147 : 0.549348139524017
Loss in iteration 148 : 0.5485542703713274
Loss in iteration 149 : 0.547762649976496
Loss in iteration 150 : 0.5469732783685051
Loss in iteration 151 : 0.5461861555949376
Loss in iteration 152 : 0.5454012817165461
Loss in iteration 153 : 0.5446186568019948
Loss in iteration 154 : 0.543838280922814
Loss in iteration 155 : 0.5430601541485744
Loss in iteration 156 : 0.5422842765422862
Loss in iteration 157 : 0.5415106481560711
Loss in iteration 158 : 0.5407392690271591
Loss in iteration 159 : 0.5399701391742363
Loss in iteration 160 : 0.5392032585942401
Loss in iteration 161 : 0.5384386272596354
Loss in iteration 162 : 0.5376762451162971
Loss in iteration 163 : 0.5369161120819791
Loss in iteration 164 : 0.5361582280453652
Loss in iteration 165 : 0.535402592865724
Loss in iteration 166 : 0.5346492063729482
Loss in iteration 167 : 0.5338980683679033
Loss in iteration 168 : 0.5331491786227651
Loss in iteration 169 : 0.5324025368812696
Loss in iteration 170 : 0.5316581428585718
Loss in iteration 171 : 0.5309159962407234
Loss in iteration 172 : 0.5301760966836945
Loss in iteration 173 : 0.5294384438119903
Loss in iteration 174 : 0.5287030372169462
Loss in iteration 175 : 0.5279698764548258
Loss in iteration 176 : 0.5272389610447541
Loss in iteration 177 : 0.5265102904666583
Loss in iteration 178 : 0.5257838641591901
Loss in iteration 179 : 0.5250596815177029
Loss in iteration 180 : 0.5243377418922975
Loss in iteration 181 : 0.5236180445859466
Loss in iteration 182 : 0.5229005888527296
Loss in iteration 183 : 0.5221853738960786
Loss in iteration 184 : 0.521472398867174
Loss in iteration 185 : 0.5207616628633334
Loss in iteration 186 : 0.5200531649264982
Loss in iteration 187 : 0.5193469040417347
Loss in iteration 188 : 0.5186428791357846
Loss in iteration 189 : 0.5179410890756367
Loss in iteration 190 : 0.5172415326670853
Loss in iteration 191 : 0.51654420865333
Loss in iteration 192 : 0.5158491157135419
Loss in iteration 193 : 0.5151562524614075
Loss in iteration 194 : 0.5144656174436308
Loss in iteration 195 : 0.5137772091384095
Loss in iteration 196 : 0.5130910259538194
Loss in iteration 197 : 0.5124070662261155
Loss in iteration 198 : 0.5117253282179425
Loss in iteration 199 : 0.5110458101164064
Loss in iteration 200 : 0.5103685100309776
Testing accuracy  of updater 4 on alg 0 with rate 1000.0 = 0.7985995946194951, training accuracy 0.7963452088452089, time elapsed: 4008 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6920044749122338
Loss in iteration 3 : 0.6908389442411161
Loss in iteration 4 : 0.6896609571526451
Loss in iteration 5 : 0.6884756519129271
Loss in iteration 6 : 0.6872860809116603
Loss in iteration 7 : 0.6860942527576904
Loss in iteration 8 : 0.6849015780586383
Loss in iteration 9 : 0.6837090923707456
Loss in iteration 10 : 0.6825175802631033
Loss in iteration 11 : 0.6813276499489451
Loss in iteration 12 : 0.680139780933271
Loss in iteration 13 : 0.6789543558969313
Loss in iteration 14 : 0.6777716828619428
Loss in iteration 15 : 0.6765920110976974
Loss in iteration 16 : 0.6754155428517176
Loss in iteration 17 : 0.6742424422178139
Loss in iteration 18 : 0.6730728420042023
Loss in iteration 19 : 0.6719068491910072
Loss in iteration 20 : 0.6707445493895094
Loss in iteration 21 : 0.6695860105843084
Loss in iteration 22 : 0.668431286323087
Loss in iteration 23 : 0.6672804184135043
Loss in iteration 24 : 0.6661334391203165
Loss in iteration 25 : 0.6649903728603587
Loss in iteration 26 : 0.6638512374534761
Loss in iteration 27 : 0.6627160450432952
Loss in iteration 28 : 0.6615848028091574
Loss in iteration 29 : 0.6604575135582188
Loss in iteration 30 : 0.6593341762475324
Loss in iteration 31 : 0.6582147864581975
Loss in iteration 32 : 0.6570993368292505
Loss in iteration 33 : 0.6559878174524506
Loss in iteration 34 : 0.654880216226413
Loss in iteration 35 : 0.6537765191682473
Loss in iteration 36 : 0.6526767106824045
Loss in iteration 37 : 0.6515807737893057
Loss in iteration 38 : 0.6504886903183353
Loss in iteration 39 : 0.6494004410707997
Loss in iteration 40 : 0.6483160059580525
Loss in iteration 41 : 0.6472353641186898
Loss in iteration 42 : 0.6461584940178311
Loss in iteration 43 : 0.6450853735303794
Loss in iteration 44 : 0.6440159800096135
Loss in iteration 45 : 0.642950290342046
Loss in iteration 46 : 0.6418882809892223
Loss in iteration 47 : 0.6408299280168186
Loss in iteration 48 : 0.6397752071113075
Loss in iteration 49 : 0.6387240935841674
Loss in iteration 50 : 0.6376765623635855
Loss in iteration 51 : 0.6366325879733169
Loss in iteration 52 : 0.6355921444984665
Loss in iteration 53 : 0.6345552055378624
Loss in iteration 54 : 0.6335217441429143
Loss in iteration 55 : 0.6324917327434713
Loss in iteration 56 : 0.6314651430619954
Loss in iteration 57 : 0.6304419460189089
Loss in iteration 58 : 0.6294221116344763
Loss in iteration 59 : 0.6284056089355885
Loss in iteration 60 : 0.6273924058800184
Loss in iteration 61 : 0.6263824693144625
Loss in iteration 62 : 0.6253757649856996
Loss in iteration 63 : 0.6243722576233541
Loss in iteration 64 : 0.6233719111069737
Loss in iteration 65 : 0.6223746887184484
Loss in iteration 66 : 0.6213805534654893
Loss in iteration 67 : 0.6203894684471366
Loss in iteration 68 : 0.6194013972237408
Loss in iteration 69 : 0.6184163041541714
Loss in iteration 70 : 0.6174341546717994
Loss in iteration 71 : 0.6164549154851751
Loss in iteration 72 : 0.6154785547035507
Loss in iteration 73 : 0.6145050418975496
Loss in iteration 74 : 0.6135343481098231
Loss in iteration 75 : 0.6125664458302517
Loss in iteration 76 : 0.6116013089478357
Loss in iteration 77 : 0.6106389126884997
Loss in iteration 78 : 0.6096792335458747
Loss in iteration 79 : 0.6087222492099575
Loss in iteration 80 : 0.6077679384971209
Loss in iteration 81 : 0.606816281283499
Loss in iteration 82 : 0.6058672584428179
Loss in iteration 83 : 0.6049208517889685
Loss in iteration 84 : 0.6039770440231856
Loss in iteration 85 : 0.6030358186854408
Loss in iteration 86 : 0.6020971601096271
Loss in iteration 87 : 0.6011610533819803
Loss in iteration 88 : 0.6002274843023214
Loss in iteration 89 : 0.5992964393477673
Loss in iteration 90 : 0.5983679056384295
Loss in iteration 91 : 0.5974418709049751
Loss in iteration 92 : 0.5965183234576569
Loss in iteration 93 : 0.5955972521566846
Loss in iteration 94 : 0.5946786463836935
Loss in iteration 95 : 0.5937624960141452
Loss in iteration 96 : 0.5928487913905571
Loss in iteration 97 : 0.5919375232963259
Loss in iteration 98 : 0.5910286829300948
Loss in iteration 99 : 0.5901222618804254
Loss in iteration 100 : 0.5892182521007379
Loss in iteration 101 : 0.5883166458843548
Loss in iteration 102 : 0.5874174358394019
Loss in iteration 103 : 0.5865206148636659
Loss in iteration 104 : 0.5856261761190386
Loss in iteration 105 : 0.5847341130057089
Loss in iteration 106 : 0.5838444191358916
Loss in iteration 107 : 0.5829570883072933
Loss in iteration 108 : 0.5820721144765173
Loss in iteration 109 : 0.5811894917329004
Loss in iteration 110 : 0.5803092142735574
Loss in iteration 111 : 0.5794312763809286
Loss in iteration 112 : 0.5785556724045388
Loss in iteration 113 : 0.5776823967492463
Loss in iteration 114 : 0.5768114438723613
Loss in iteration 115 : 0.5759428082920378
Loss in iteration 116 : 0.5750764846080214
Loss in iteration 117 : 0.5742124675342717
Loss in iteration 118 : 0.5733507519404601
Loss in iteration 119 : 0.5724913328970135
Loss in iteration 120 : 0.5716342057176004
Loss in iteration 121 : 0.5707793659933522
Loss in iteration 122 : 0.5699268096154323
Loss in iteration 123 : 0.569076532785209
Loss in iteration 124 : 0.5682285320135033
Loss in iteration 125 : 0.5673828041116644
Loss in iteration 126 : 0.5665393461773153
Loss in iteration 127 : 0.5656981555773102
Loss in iteration 128 : 0.5648592299296376
Loss in iteration 129 : 0.5640225670854444
Loss in iteration 130 : 0.5631881651117736
Loss in iteration 131 : 0.5623560222753404
Loss in iteration 132 : 0.5615261370274102
Loss in iteration 133 : 0.5606985079897522
Loss in iteration 134 : 0.5598731339415587
Loss in iteration 135 : 0.5590500138073115
Loss in iteration 136 : 0.5582291466454027
Loss in iteration 137 : 0.5574105316374722
Loss in iteration 138 : 0.5565941680783917
Loss in iteration 139 : 0.5557800553667587
Loss in iteration 140 : 0.5549681929959523
Loss in iteration 141 : 0.5541585805455759
Loss in iteration 142 : 0.5533512176733577
Loss in iteration 143 : 0.5525461041073889
Loss in iteration 144 : 0.5517432396387117
Loss in iteration 145 : 0.5509426241142216
Loss in iteration 146 : 0.5501442574298517
Loss in iteration 147 : 0.549348139524017
Loss in iteration 148 : 0.5485542703713274
Loss in iteration 149 : 0.547762649976496
Loss in iteration 150 : 0.5469732783685051
Loss in iteration 151 : 0.5461861555949376
Loss in iteration 152 : 0.5454012817165461
Loss in iteration 153 : 0.5446186568019948
Loss in iteration 154 : 0.543838280922814
Loss in iteration 155 : 0.5430601541485744
Loss in iteration 156 : 0.5422842765422862
Loss in iteration 157 : 0.5415106481560711
Loss in iteration 158 : 0.5407392690271591
Loss in iteration 159 : 0.5399701391742363
Loss in iteration 160 : 0.5392032585942401
Loss in iteration 161 : 0.5384386272596354
Loss in iteration 162 : 0.5376762451162971
Loss in iteration 163 : 0.5369161120819791
Loss in iteration 164 : 0.5361582280453652
Loss in iteration 165 : 0.535402592865724
Loss in iteration 166 : 0.5346492063729482
Loss in iteration 167 : 0.5338980683679033
Loss in iteration 168 : 0.5331491786227651
Loss in iteration 169 : 0.5324025368812696
Loss in iteration 170 : 0.5316581428585718
Loss in iteration 171 : 0.5309159962407234
Loss in iteration 172 : 0.5301760966836945
Loss in iteration 173 : 0.5294384438119903
Loss in iteration 174 : 0.5287030372169462
Loss in iteration 175 : 0.5279698764548258
Loss in iteration 176 : 0.5272389610447541
Loss in iteration 177 : 0.5265102904666583
Loss in iteration 178 : 0.5257838641591901
Loss in iteration 179 : 0.5250596815177029
Loss in iteration 180 : 0.5243377418922975
Loss in iteration 181 : 0.5236180445859466
Loss in iteration 182 : 0.5229005888527296
Loss in iteration 183 : 0.5221853738960786
Loss in iteration 184 : 0.521472398867174
Loss in iteration 185 : 0.5207616628633334
Loss in iteration 186 : 0.5200531649264982
Loss in iteration 187 : 0.5193469040417347
Loss in iteration 188 : 0.5186428791357846
Loss in iteration 189 : 0.5179410890756367
Loss in iteration 190 : 0.5172415326670853
Loss in iteration 191 : 0.51654420865333
Loss in iteration 192 : 0.5158491157135419
Loss in iteration 193 : 0.5151562524614075
Loss in iteration 194 : 0.5144656174436308
Loss in iteration 195 : 0.5137772091384095
Loss in iteration 196 : 0.5130910259538194
Loss in iteration 197 : 0.5124070662261155
Loss in iteration 198 : 0.5117253282179425
Loss in iteration 199 : 0.5110458101164064
Loss in iteration 200 : 0.5103685100309776
Testing accuracy  of updater 4 on alg 0 with rate 100.0 = 0.7985995946194951, training accuracy 0.7963452088452089, time elapsed: 4266 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6920044749122338
Loss in iteration 3 : 0.6908389442411161
Loss in iteration 4 : 0.6896609571526451
Loss in iteration 5 : 0.6884756519129271
Loss in iteration 6 : 0.6872860809116603
Loss in iteration 7 : 0.6860942527576904
Loss in iteration 8 : 0.6849015780586383
Loss in iteration 9 : 0.6837090923707456
Loss in iteration 10 : 0.6825175802631033
Loss in iteration 11 : 0.6813276499489451
Loss in iteration 12 : 0.680139780933271
Loss in iteration 13 : 0.6789543558969313
Loss in iteration 14 : 0.6777716828619428
Loss in iteration 15 : 0.6765920110976974
Loss in iteration 16 : 0.6754155428517176
Loss in iteration 17 : 0.6742424422178139
Loss in iteration 18 : 0.6730728420042023
Loss in iteration 19 : 0.6719068491910072
Loss in iteration 20 : 0.6707445493895094
Loss in iteration 21 : 0.6695860105843084
Loss in iteration 22 : 0.668431286323087
Loss in iteration 23 : 0.6672804184135043
Loss in iteration 24 : 0.6661334391203165
Loss in iteration 25 : 0.6649903728603587
Loss in iteration 26 : 0.6638512374534761
Loss in iteration 27 : 0.6627160450432952
Loss in iteration 28 : 0.6615848028091574
Loss in iteration 29 : 0.6604575135582188
Loss in iteration 30 : 0.6593341762475324
Loss in iteration 31 : 0.6582147864581975
Loss in iteration 32 : 0.6570993368292505
Loss in iteration 33 : 0.6559878174524506
Loss in iteration 34 : 0.654880216226413
Loss in iteration 35 : 0.6537765191682473
Loss in iteration 36 : 0.6526767106824045
Loss in iteration 37 : 0.6515807737893057
Loss in iteration 38 : 0.6504886903183353
Loss in iteration 39 : 0.6494004410707997
Loss in iteration 40 : 0.6483160059580525
Loss in iteration 41 : 0.6472353641186898
Loss in iteration 42 : 0.6461584940178311
Loss in iteration 43 : 0.6450853735303794
Loss in iteration 44 : 0.6440159800096135
Loss in iteration 45 : 0.642950290342046
Loss in iteration 46 : 0.6418882809892223
Loss in iteration 47 : 0.6408299280168186
Loss in iteration 48 : 0.6397752071113075
Loss in iteration 49 : 0.6387240935841674
Loss in iteration 50 : 0.6376765623635855
Loss in iteration 51 : 0.6366325879733169
Loss in iteration 52 : 0.6355921444984665
Loss in iteration 53 : 0.6345552055378624
Loss in iteration 54 : 0.6335217441429143
Loss in iteration 55 : 0.6324917327434713
Loss in iteration 56 : 0.6314651430619954
Loss in iteration 57 : 0.6304419460189089
Loss in iteration 58 : 0.6294221116344763
Loss in iteration 59 : 0.6284056089355885
Loss in iteration 60 : 0.6273924058800184
Loss in iteration 61 : 0.6263824693144625
Loss in iteration 62 : 0.6253757649856996
Loss in iteration 63 : 0.6243722576233541
Loss in iteration 64 : 0.6233719111069737
Loss in iteration 65 : 0.6223746887184484
Loss in iteration 66 : 0.6213805534654893
Loss in iteration 67 : 0.6203894684471366
Loss in iteration 68 : 0.6194013972237408
Loss in iteration 69 : 0.6184163041541714
Loss in iteration 70 : 0.6174341546717994
Loss in iteration 71 : 0.6164549154851751
Loss in iteration 72 : 0.6154785547035507
Loss in iteration 73 : 0.6145050418975496
Loss in iteration 74 : 0.6135343481098231
Loss in iteration 75 : 0.6125664458302517
Loss in iteration 76 : 0.6116013089478357
Loss in iteration 77 : 0.6106389126884997
Loss in iteration 78 : 0.6096792335458747
Loss in iteration 79 : 0.6087222492099575
Loss in iteration 80 : 0.6077679384971209
Loss in iteration 81 : 0.606816281283499
Loss in iteration 82 : 0.6058672584428179
Loss in iteration 83 : 0.6049208517889685
Loss in iteration 84 : 0.6039770440231856
Loss in iteration 85 : 0.6030358186854408
Loss in iteration 86 : 0.6020971601096271
Loss in iteration 87 : 0.6011610533819803
Loss in iteration 88 : 0.6002274843023214
Loss in iteration 89 : 0.5992964393477673
Loss in iteration 90 : 0.5983679056384295
Loss in iteration 91 : 0.5974418709049751
Loss in iteration 92 : 0.5965183234576569
Loss in iteration 93 : 0.5955972521566846
Loss in iteration 94 : 0.5946786463836935
Loss in iteration 95 : 0.5937624960141452
Loss in iteration 96 : 0.5928487913905571
Loss in iteration 97 : 0.5919375232963259
Loss in iteration 98 : 0.5910286829300948
Loss in iteration 99 : 0.5901222618804254
Loss in iteration 100 : 0.5892182521007379
Loss in iteration 101 : 0.5883166458843548
Loss in iteration 102 : 0.5874174358394019
Loss in iteration 103 : 0.5865206148636659
Loss in iteration 104 : 0.5856261761190386
Loss in iteration 105 : 0.5847341130057089
Loss in iteration 106 : 0.5838444191358916
Loss in iteration 107 : 0.5829570883072933
Loss in iteration 108 : 0.5820721144765173
Loss in iteration 109 : 0.5811894917329004
Loss in iteration 110 : 0.5803092142735574
Loss in iteration 111 : 0.5794312763809286
Loss in iteration 112 : 0.5785556724045388
Loss in iteration 113 : 0.5776823967492463
Loss in iteration 114 : 0.5768114438723613
Loss in iteration 115 : 0.5759428082920378
Loss in iteration 116 : 0.5750764846080214
Loss in iteration 117 : 0.5742124675342717
Loss in iteration 118 : 0.5733507519404601
Loss in iteration 119 : 0.5724913328970135
Loss in iteration 120 : 0.5716342057176004
Loss in iteration 121 : 0.5707793659933522
Loss in iteration 122 : 0.5699268096154323
Loss in iteration 123 : 0.569076532785209
Loss in iteration 124 : 0.5682285320135033
Loss in iteration 125 : 0.5673828041116644
Loss in iteration 126 : 0.5665393461773153
Loss in iteration 127 : 0.5656981555773102
Loss in iteration 128 : 0.5648592299296376
Loss in iteration 129 : 0.5640225670854444
Loss in iteration 130 : 0.5631881651117736
Loss in iteration 131 : 0.5623560222753404
Loss in iteration 132 : 0.5615261370274102
Loss in iteration 133 : 0.5606985079897522
Loss in iteration 134 : 0.5598731339415587
Loss in iteration 135 : 0.5590500138073115
Loss in iteration 136 : 0.5582291466454027
Loss in iteration 137 : 0.5574105316374722
Loss in iteration 138 : 0.5565941680783917
Loss in iteration 139 : 0.5557800553667587
Loss in iteration 140 : 0.5549681929959523
Loss in iteration 141 : 0.5541585805455759
Loss in iteration 142 : 0.5533512176733577
Loss in iteration 143 : 0.5525461041073889
Loss in iteration 144 : 0.5517432396387117
Loss in iteration 145 : 0.5509426241142216
Loss in iteration 146 : 0.5501442574298517
Loss in iteration 147 : 0.549348139524017
Loss in iteration 148 : 0.5485542703713274
Loss in iteration 149 : 0.547762649976496
Loss in iteration 150 : 0.5469732783685051
Loss in iteration 151 : 0.5461861555949376
Loss in iteration 152 : 0.5454012817165461
Loss in iteration 153 : 0.5446186568019948
Loss in iteration 154 : 0.543838280922814
Loss in iteration 155 : 0.5430601541485744
Loss in iteration 156 : 0.5422842765422862
Loss in iteration 157 : 0.5415106481560711
Loss in iteration 158 : 0.5407392690271591
Loss in iteration 159 : 0.5399701391742363
Loss in iteration 160 : 0.5392032585942401
Loss in iteration 161 : 0.5384386272596354
Loss in iteration 162 : 0.5376762451162971
Loss in iteration 163 : 0.5369161120819791
Loss in iteration 164 : 0.5361582280453652
Loss in iteration 165 : 0.535402592865724
Loss in iteration 166 : 0.5346492063729482
Loss in iteration 167 : 0.5338980683679033
Loss in iteration 168 : 0.5331491786227651
Loss in iteration 169 : 0.5324025368812696
Loss in iteration 170 : 0.5316581428585718
Loss in iteration 171 : 0.5309159962407234
Loss in iteration 172 : 0.5301760966836945
Loss in iteration 173 : 0.5294384438119903
Loss in iteration 174 : 0.5287030372169462
Loss in iteration 175 : 0.5279698764548258
Loss in iteration 176 : 0.5272389610447541
Loss in iteration 177 : 0.5265102904666583
Loss in iteration 178 : 0.5257838641591901
Loss in iteration 179 : 0.5250596815177029
Loss in iteration 180 : 0.5243377418922975
Loss in iteration 181 : 0.5236180445859466
Loss in iteration 182 : 0.5229005888527296
Loss in iteration 183 : 0.5221853738960786
Loss in iteration 184 : 0.521472398867174
Loss in iteration 185 : 0.5207616628633334
Loss in iteration 186 : 0.5200531649264982
Loss in iteration 187 : 0.5193469040417347
Loss in iteration 188 : 0.5186428791357846
Loss in iteration 189 : 0.5179410890756367
Loss in iteration 190 : 0.5172415326670853
Loss in iteration 191 : 0.51654420865333
Loss in iteration 192 : 0.5158491157135419
Loss in iteration 193 : 0.5151562524614075
Loss in iteration 194 : 0.5144656174436308
Loss in iteration 195 : 0.5137772091384095
Loss in iteration 196 : 0.5130910259538194
Loss in iteration 197 : 0.5124070662261155
Loss in iteration 198 : 0.5117253282179425
Loss in iteration 199 : 0.5110458101164064
Loss in iteration 200 : 0.5103685100309776
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.7985995946194951, training accuracy 0.7963452088452089, time elapsed: 4269 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 9.682506281916961
Loss in iteration 3 : 1.8061207798208114
Loss in iteration 4 : 3.754921483208247
Loss in iteration 5 : 3.926788294894692
Loss in iteration 6 : 0.9562615305698622
Loss in iteration 7 : 0.8383314594966591
Loss in iteration 8 : 0.7817432413206891
Loss in iteration 9 : 0.8196903382617693
Loss in iteration 10 : 1.2853440500515152
Loss in iteration 11 : 1.7092266319834253
Loss in iteration 12 : 2.6443856712940854
Loss in iteration 13 : 0.8989133273629202
Loss in iteration 14 : 1.0002743594777594
Loss in iteration 15 : 1.0671801492468667
Loss in iteration 16 : 1.7093931926103791
Loss in iteration 17 : 1.3060997861670345
Loss in iteration 18 : 1.9107196179680854
Loss in iteration 19 : 1.0395452295624676
Loss in iteration 20 : 1.2763452796499606
Loss in iteration 21 : 1.100396699550756
Loss in iteration 22 : 1.575908278429676
Loss in iteration 23 : 1.1718582170102654
Loss in iteration 24 : 1.63506046163849
Loss in iteration 25 : 1.1002272619294329
Loss in iteration 26 : 1.4386717440437908
Loss in iteration 27 : 1.1099230405700797
Loss in iteration 28 : 1.520095262717777
Loss in iteration 29 : 1.1323758951913947
Loss in iteration 30 : 1.5484868918583412
Loss in iteration 31 : 1.1151817833936066
Loss in iteration 32 : 1.4936502652138113
Loss in iteration 33 : 1.1149491999198307
Loss in iteration 34 : 1.5112636185678632
Loss in iteration 35 : 1.122374683837667
Loss in iteration 36 : 1.5242160450402968
Loss in iteration 37 : 1.1186848472731277
Loss in iteration 38 : 1.5091364498258653
Loss in iteration 39 : 1.117829133694196
Loss in iteration 40 : 1.5119238082262292
Loss in iteration 41 : 1.1204288858311136
Loss in iteration 42 : 1.5173318563570477
Loss in iteration 43 : 1.1199395277723636
Loss in iteration 44 : 1.5133149103627486
Loss in iteration 45 : 1.119597442676035
Loss in iteration 46 : 1.5133372071947104
Loss in iteration 47 : 1.1206162086732117
Loss in iteration 48 : 1.5154540793607834
Loss in iteration 49 : 1.1208097283711915
Loss in iteration 50 : 1.514474644479648
Loss in iteration 51 : 1.120806742812173
Loss in iteration 52 : 1.514206774849124
Loss in iteration 53 : 1.1212908741397911
Loss in iteration 54 : 1.5149838621461709
Loss in iteration 55 : 1.1215706432804495
Loss in iteration 56 : 1.5147864897748489
Loss in iteration 57 : 1.1217013067794466
Loss in iteration 58 : 1.514600938713917
Loss in iteration 59 : 1.1219873994318659
Loss in iteration 60 : 1.5148558268476569
Loss in iteration 61 : 1.1222268730025935
Loss in iteration 62 : 1.5148241498521808
Loss in iteration 63 : 1.1223842010173617
Loss in iteration 64 : 1.5147147086815393
Loss in iteration 65 : 1.1225804483386046
Loss in iteration 66 : 1.514772404333631
Loss in iteration 67 : 1.1227643523845454
Loss in iteration 68 : 1.5147550342095806
Loss in iteration 69 : 1.1229058651172383
Loss in iteration 70 : 1.5146828853367367
Loss in iteration 71 : 1.1230494905876531
Loss in iteration 72 : 1.5146687146387587
Loss in iteration 73 : 1.123186175936441
Loss in iteration 74 : 1.5146393288734683
Loss in iteration 75 : 1.1233003410475857
Loss in iteration 76 : 1.5145822556897384
Loss in iteration 77 : 1.123407351009667
Loss in iteration 78 : 1.5145435582653477
Loss in iteration 79 : 1.1235079462059936
Loss in iteration 80 : 1.5145042669857682
Loss in iteration 81 : 1.1235956068855115
Loss in iteration 82 : 1.5144532038141008
Loss in iteration 83 : 1.1236754157935456
Loss in iteration 84 : 1.5144075180117613
Loss in iteration 85 : 1.1237493371706453
Loss in iteration 86 : 1.514363734565048
Loss in iteration 87 : 1.1238151396268987
Loss in iteration 88 : 1.5143157657103987
Loss in iteration 89 : 1.1238745274688333
Loss in iteration 90 : 1.5142694380086847
Loss in iteration 91 : 1.1239289603597997
Loss in iteration 92 : 1.5142248708769481
Loss in iteration 93 : 1.1239779260721787
Loss in iteration 94 : 1.514179319627889
Loss in iteration 95 : 1.1240220865370263
Loss in iteration 96 : 1.5141345528169725
Loss in iteration 97 : 1.1240623360813329
Loss in iteration 98 : 1.5140911193604845
Loss in iteration 99 : 1.1240987420464688
Loss in iteration 100 : 1.514047916682429
Loss in iteration 101 : 1.1241316495077278
Loss in iteration 102 : 1.5140053900330366
Loss in iteration 103 : 1.1241615877766533
Loss in iteration 104 : 1.5139639004871694
Loss in iteration 105 : 1.1241887638434
Loss in iteration 106 : 1.5139230397284105
Loss in iteration 107 : 1.124213410984854
Loss in iteration 108 : 1.5138828716016484
Loss in iteration 109 : 1.1242358517462054
Loss in iteration 110 : 1.5138435658233387
Loss in iteration 111 : 1.1242562876365052
Loss in iteration 112 : 1.5138049769277446
Loss in iteration 113 : 1.124274894211924
Loss in iteration 114 : 1.5137670801424643
Loss in iteration 115 : 1.1242918785812797
Loss in iteration 116 : 1.5137299363581311
Loss in iteration 117 : 1.124307401464178
Loss in iteration 118 : 1.5136934902435895
Loss in iteration 119 : 1.1243215969971454
Loss in iteration 120 : 1.5136577070624673
Loss in iteration 121 : 1.1243346049755767
Loss in iteration 122 : 1.5136225979624858
Loss in iteration 123 : 1.124346545272073
Loss in iteration 124 : 1.5135881357606598
Loss in iteration 125 : 1.124357519178971
Loss in iteration 126 : 1.5135542915673144
Loss in iteration 127 : 1.1243676245429555
Loss in iteration 128 : 1.513521057295938
Loss in iteration 129 : 1.1243769487627324
Loss in iteration 130 : 1.513488414111505
Loss in iteration 131 : 1.1243855673182837
Loss in iteration 132 : 1.513456339476211
Loss in iteration 133 : 1.1243935502471518
Loss in iteration 134 : 1.5134248194849527
Loss in iteration 135 : 1.1244009609495944
Loss in iteration 136 : 1.5133938382090648
Loss in iteration 137 : 1.124407855179416
Loss in iteration 138 : 1.5133633776377513
Loss in iteration 139 : 1.1244142837061324
Loss in iteration 140 : 1.5133334232503493
Loss in iteration 141 : 1.1244202925962208
Loss in iteration 142 : 1.5133039608338994
Loss in iteration 143 : 1.1244259228532112
Loss in iteration 144 : 1.51327497547046
Loss in iteration 145 : 1.124431211533481
Loss in iteration 146 : 1.5132464537510197
Loss in iteration 147 : 1.1244361922246837
Loss in iteration 148 : 1.513218382985303
Loss in iteration 149 : 1.1244408950370803
Loss in iteration 150 : 1.5131907504880602
Loss in iteration 151 : 1.1244453471181308
Loss in iteration 152 : 1.5131635443604476
Loss in iteration 153 : 1.1244495730350228
Loss in iteration 154 : 1.5131367533587718
Loss in iteration 155 : 1.1244535948944945
Loss in iteration 156 : 1.5131103665217644
Loss in iteration 157 : 1.1244574326204877
Loss in iteration 158 : 1.5130843734063082
Loss in iteration 159 : 1.1244611042191737
Loss in iteration 160 : 1.513058764093426
Loss in iteration 161 : 1.124464625920037
Loss in iteration 162 : 1.5130335290152686
Loss in iteration 163 : 1.124468012350679
Loss in iteration 164 : 1.513008659004888
Loss in iteration 165 : 1.1244712767158047
Loss in iteration 166 : 1.5129841453102995
Loss in iteration 167 : 1.1244744309207795
Loss in iteration 168 : 1.512959979516195
Loss in iteration 169 : 1.1244774856931063
Loss in iteration 170 : 1.5129361535393064
Loss in iteration 171 : 1.1244804507052586
Loss in iteration 172 : 1.512912659630714
Loss in iteration 173 : 1.1244833346720458
Loss in iteration 174 : 1.5128894903380425
Loss in iteration 175 : 1.1244861454393282
Loss in iteration 176 : 1.5128666384894487
Loss in iteration 177 : 1.1244888900702599
Loss in iteration 178 : 1.5128440971876658
Loss in iteration 179 : 1.1244915749191122
Loss in iteration 180 : 1.5128218597888503
Loss in iteration 181 : 1.1244942056970388
Loss in iteration 182 : 1.5127999198871023
Loss in iteration 183 : 1.1244967875342355
Loss in iteration 184 : 1.5127782713054576
Loss in iteration 185 : 1.1244993250348916
Loss in iteration 186 : 1.5127569080816088
Loss in iteration 187 : 1.124501822326438
Loss in iteration 188 : 1.5127358244555769
Loss in iteration 189 : 1.1245042831049468
Loss in iteration 190 : 1.5127150148601272
Loss in iteration 191 : 1.1245067106761037
Loss in iteration 192 : 1.5126944739103647
Loss in iteration 193 : 1.124509107992
Loss in iteration 194 : 1.5126741963937613
Loss in iteration 195 : 1.1245114776848593
Loss in iteration 196 : 1.5126541772617037
Loss in iteration 197 : 1.1245138220974682
Loss in iteration 198 : 1.5126344116211823
Loss in iteration 199 : 1.124516143310887
Loss in iteration 200 : 1.5126148947268445
Testing accuracy  of updater 5 on alg 0 with rate 1.0 = 0.772188440513482, training accuracy 0.7750921375921376, time elapsed: 4007 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.4946357794844829
Loss in iteration 3 : 0.4077379289571959
Loss in iteration 4 : 0.3862556224370212
Loss in iteration 5 : 0.3721577406122656
Loss in iteration 6 : 0.362301438965908
Loss in iteration 7 : 0.35507755913628863
Loss in iteration 8 : 0.34958915061046963
Loss in iteration 9 : 0.3453020002736896
Loss in iteration 10 : 0.34188003311418325
Loss in iteration 11 : 0.33910194618379286
Loss in iteration 12 : 0.3368162120618135
Loss in iteration 13 : 0.3349155353728375
Loss in iteration 14 : 0.3333216783944511
Loss in iteration 15 : 0.33197610466256877
Loss in iteration 16 : 0.330834013598126
Loss in iteration 17 : 0.32986042771166907
Loss in iteration 18 : 0.3290275586545535
Loss in iteration 19 : 0.32831299319177415
Loss in iteration 20 : 0.3276984165627829
Loss in iteration 21 : 0.32716869621333583
Loss in iteration 22 : 0.32671121143669757
Loss in iteration 23 : 0.32631535433339726
Loss in iteration 24 : 0.32597215186797235
Loss in iteration 25 : 0.3256739754375947
Loss in iteration 26 : 0.325414314265957
Loss in iteration 27 : 0.32518759666676783
Loss in iteration 28 : 0.32498904680367124
Loss in iteration 29 : 0.32481456932372077
Loss in iteration 30 : 0.32466065383424053
Loss in iteration 31 : 0.3245242980204756
Loss in iteration 32 : 0.32440294421688065
Loss in iteration 33 : 0.32429448586109505
Loss in iteration 34 : 0.3241977900812687
Loss in iteration 35 : 0.3241195570445397
Loss in iteration 36 : 0.3241642723967477
Loss in iteration 37 : 0.3258707568033301
Loss in iteration 38 : 0.352045809516563
Loss in iteration 39 : 0.543263360562661
Loss in iteration 40 : 0.4845912960503532
Loss in iteration 41 : 0.3348847567755869
Loss in iteration 42 : 0.3282360838184535
Loss in iteration 43 : 0.3258034989979464
Loss in iteration 44 : 0.3249805257278104
Loss in iteration 45 : 0.32455653632727544
Loss in iteration 46 : 0.32438287612596434
Loss in iteration 47 : 0.324350917519206
Loss in iteration 48 : 0.3245426372256013
Loss in iteration 49 : 0.325089797304112
Loss in iteration 50 : 0.3267014655456366
Loss in iteration 51 : 0.33085575695564884
Loss in iteration 52 : 0.34357228108370497
Loss in iteration 53 : 0.36794937719534554
Loss in iteration 54 : 0.40642613487497636
Loss in iteration 55 : 0.3763555668813642
Loss in iteration 56 : 0.370833448246183
Loss in iteration 57 : 0.3446538151411626
Loss in iteration 58 : 0.34015870232154166
Loss in iteration 59 : 0.3343136881434292
Loss in iteration 60 : 0.33404271865563084
Loss in iteration 61 : 0.33316461219958327
Loss in iteration 62 : 0.3358421016329733
Loss in iteration 63 : 0.3379173554677129
Loss in iteration 64 : 0.34552165319347794
Loss in iteration 65 : 0.3495238303675327
Loss in iteration 66 : 0.36129445122307274
Loss in iteration 67 : 0.35658179834194315
Loss in iteration 68 : 0.36177331228678583
Loss in iteration 69 : 0.34944027355427687
Loss in iteration 70 : 0.3498242473925851
Loss in iteration 71 : 0.3422178200139621
Loss in iteration 72 : 0.343316180499501
Loss in iteration 73 : 0.3400032442452235
Loss in iteration 74 : 0.3430865190936737
Loss in iteration 75 : 0.3419691995144723
Loss in iteration 76 : 0.3473525392047499
Loss in iteration 77 : 0.34633179729729857
Loss in iteration 78 : 0.35282817603352196
Loss in iteration 79 : 0.34900648239362614
Loss in iteration 80 : 0.3539266079394211
Loss in iteration 81 : 0.3475013618610642
Loss in iteration 82 : 0.3505100142888493
Loss in iteration 83 : 0.34465083088916665
Loss in iteration 84 : 0.34736900733465714
Loss in iteration 85 : 0.343260708987981
Loss in iteration 86 : 0.3467452063968755
Loss in iteration 87 : 0.3438214333495074
Loss in iteration 88 : 0.3482869235150619
Loss in iteration 89 : 0.34543042651941774
Loss in iteration 90 : 0.35033270338921363
Loss in iteration 91 : 0.3465147836510402
Loss in iteration 92 : 0.35097815506985763
Loss in iteration 93 : 0.3462100683083778
Loss in iteration 94 : 0.34998002279972823
Loss in iteration 95 : 0.34518733021497217
Loss in iteration 96 : 0.34871229748525134
Loss in iteration 97 : 0.3445199371453814
Loss in iteration 98 : 0.34828188276007194
Loss in iteration 99 : 0.3446200595320215
Loss in iteration 100 : 0.3487698997423456
Loss in iteration 101 : 0.3451925235651791
Loss in iteration 102 : 0.3495378278050623
Loss in iteration 103 : 0.3456366979787912
Loss in iteration 104 : 0.34986165483022147
Loss in iteration 105 : 0.3455993741296934
Loss in iteration 106 : 0.34957394368686023
Loss in iteration 107 : 0.3452419210610727
Loss in iteration 108 : 0.3490895156740861
Loss in iteration 109 : 0.3449511372830114
Loss in iteration 110 : 0.3488612122151008
Loss in iteration 111 : 0.344939749482816
Loss in iteration 112 : 0.34899507773218175
Loss in iteration 113 : 0.34513349854937153
Loss in iteration 114 : 0.3492760618746412
Loss in iteration 115 : 0.34531073517006794
Loss in iteration 116 : 0.3494244717609386
Loss in iteration 117 : 0.34532157169920863
Loss in iteration 118 : 0.34934573112312617
Loss in iteration 119 : 0.34519792272161937
Loss in iteration 120 : 0.3491643394965999
Loss in iteration 121 : 0.345075968659067
Loss in iteration 122 : 0.34905496492547095
Loss in iteration 123 : 0.3450510188328632
Loss in iteration 124 : 0.34908215069000464
Loss in iteration 125 : 0.34511149945169195
Loss in iteration 126 : 0.3491801428753703
Loss in iteration 127 : 0.34517864095038003
Loss in iteration 128 : 0.3492427273396448
Loss in iteration 129 : 0.34518970329856935
Loss in iteration 130 : 0.34922230943346344
Loss in iteration 131 : 0.34514629546157716
Loss in iteration 132 : 0.3491541744751287
Loss in iteration 133 : 0.34509521929018094
Loss in iteration 134 : 0.3491035851307087
Loss in iteration 135 : 0.34507647430578403
Loss in iteration 136 : 0.3491028071590105
Loss in iteration 137 : 0.34509206917662916
Loss in iteration 138 : 0.34913387801928353
Loss in iteration 139 : 0.3451150624968654
Loss in iteration 140 : 0.3491574867408578
Loss in iteration 141 : 0.3451199034745295
Loss in iteration 142 : 0.3491515046540532
Loss in iteration 143 : 0.34510335286473753
Loss in iteration 144 : 0.3491247324317624
Loss in iteration 145 : 0.3450809887293172
Loss in iteration 146 : 0.3491009112153904
Loss in iteration 147 : 0.34506892559773245
Loss in iteration 148 : 0.3490947334028804
Loss in iteration 149 : 0.3450702701345273
Loss in iteration 150 : 0.3491021961063546
Loss in iteration 151 : 0.3450761916257161
Loss in iteration 152 : 0.34910915263866393
Loss in iteration 153 : 0.3450765810786057
Loss in iteration 154 : 0.3491058894560996
Loss in iteration 155 : 0.3450688194154429
Loss in iteration 156 : 0.349093983383531
Loss in iteration 157 : 0.3450578711447365
Loss in iteration 158 : 0.3490818346071071
Loss in iteration 159 : 0.3450500257692171
Loss in iteration 160 : 0.34907575944320113
Loss in iteration 161 : 0.34504729338805884
Loss in iteration 162 : 0.34907532166018845
Loss in iteration 163 : 0.3450469305473041
Loss in iteration 164 : 0.3490755822387988
Loss in iteration 165 : 0.3450450468272189
Loss in iteration 166 : 0.34907246097240724
Loss in iteration 167 : 0.3450401600870283
Loss in iteration 168 : 0.34906589105826935
Loss in iteration 169 : 0.34503375673647496
Loss in iteration 170 : 0.34905875253081786
Loss in iteration 171 : 0.3450282234598013
Loss in iteration 172 : 0.34905364980760234
Loss in iteration 173 : 0.3450246084891349
Loss in iteration 174 : 0.34905082104278684
Loss in iteration 175 : 0.34502212284633654
Loss in iteration 176 : 0.34904860023589557
Loss in iteration 177 : 0.345019308556153
Loss in iteration 178 : 0.34904533337300847
Loss in iteration 179 : 0.3450154371505213
Loss in iteration 180 : 0.34904074990505524
Loss in iteration 181 : 0.34501091094012626
Loss in iteration 182 : 0.3490358001918976
Loss in iteration 183 : 0.34500661117765236
Loss in iteration 184 : 0.3490315223038842
Loss in iteration 185 : 0.34500302993710646
Loss in iteration 186 : 0.3490281527526465
Loss in iteration 187 : 0.34499996850554576
Loss in iteration 188 : 0.349025152632271
Loss in iteration 189 : 0.3449968946552086
Loss in iteration 190 : 0.34902187141942115
Loss in iteration 191 : 0.34499347756914034
Loss in iteration 192 : 0.34901811826398615
Loss in iteration 193 : 0.34498980584777394
Loss in iteration 194 : 0.3490141913211643
Loss in iteration 195 : 0.34498619568190164
Loss in iteration 196 : 0.3490104920517157
Loss in iteration 197 : 0.34498286330097516
Loss in iteration 198 : 0.3490071613977072
Loss in iteration 199 : 0.34497977214556297
Loss in iteration 200 : 0.34900403438686556
Testing accuracy  of updater 5 on alg 0 with rate 0.1 = 0.8329340949573122, training accuracy 0.8373771498771498, time elapsed: 4267 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6101998638028461
Loss in iteration 3 : 0.5691388314835146
Loss in iteration 4 : 0.5429824779780066
Loss in iteration 5 : 0.5239461530686823
Loss in iteration 6 : 0.50890869124488
Loss in iteration 7 : 0.49639108729746884
Loss in iteration 8 : 0.48560652266429893
Loss in iteration 9 : 0.47609609178675055
Loss in iteration 10 : 0.46757102162505554
Loss in iteration 11 : 0.45983784805263805
Loss in iteration 12 : 0.45276013269633364
Loss in iteration 13 : 0.4462375270736141
Loss in iteration 14 : 0.440193633364104
Loss in iteration 15 : 0.43456859850303986
Loss in iteration 16 : 0.4293143941128143
Loss in iteration 17 : 0.4243916935459409
Loss in iteration 18 : 0.41976773809390755
Loss in iteration 19 : 0.4154148376867055
Loss in iteration 20 : 0.41130929104076214
Loss in iteration 21 : 0.4074305904497465
Loss in iteration 22 : 0.4037608242559146
Loss in iteration 23 : 0.4002842195065635
Loss in iteration 24 : 0.39698678596310794
Loss in iteration 25 : 0.39385603473595737
Loss in iteration 26 : 0.39088075283114526
Loss in iteration 27 : 0.388050820290593
Loss in iteration 28 : 0.3853570602956427
Loss in iteration 29 : 0.3827911151567303
Loss in iteration 30 : 0.38034534290150224
Loss in iteration 31 : 0.3780127304427614
Loss in iteration 32 : 0.3757868202196784
Loss in iteration 33 : 0.373661647870775
Loss in iteration 34 : 0.3716316889906257
Loss in iteration 35 : 0.3696918133958633
Loss in iteration 36 : 0.36783724561602327
Loss in iteration 37 : 0.3660635305557133
Loss in iteration 38 : 0.3643665034641665
Loss in iteration 39 : 0.36274226350811856
Loss in iteration 40 : 0.3611871503815028
Loss in iteration 41 : 0.35969772350511436
Loss in iteration 42 : 0.3582707434727312
Loss in iteration 43 : 0.3569031554872484
Loss in iteration 44 : 0.35559207459985964
Loss in iteration 45 : 0.354334772616362
Loss in iteration 46 : 0.35312866656616676
Loss in iteration 47 : 0.35197130864239246
Loss in iteration 48 : 0.3508603775176789
Loss in iteration 49 : 0.3497936709235372
Loss in iteration 50 : 0.3487690993563249
Loss in iteration 51 : 0.3477846807458466
Loss in iteration 52 : 0.3468385358986346
Loss in iteration 53 : 0.34592888451233833
Loss in iteration 54 : 0.3450540415535652
Loss in iteration 55 : 0.34421241380125733
Loss in iteration 56 : 0.34340249638110165
Loss in iteration 57 : 0.3426228691521089
Loss in iteration 58 : 0.3418721928508622
Loss in iteration 59 : 0.3411492049482902
Loss in iteration 60 : 0.3404527152230556
Loss in iteration 61 : 0.33978160110084477
Loss in iteration 62 : 0.3391348028457163
Loss in iteration 63 : 0.3385113187159876
Loss in iteration 64 : 0.3379102002113607
Loss in iteration 65 : 0.33733054753967434
Loss in iteration 66 : 0.33677150542225565
Loss in iteration 67 : 0.3362322593374964
Loss in iteration 68 : 0.33571203227566554
Loss in iteration 69 : 0.33521008204648917
Loss in iteration 70 : 0.3347256991474714
Loss in iteration 71 : 0.33425820516783983
Loss in iteration 72 : 0.3338069516732188
Loss in iteration 73 : 0.33337131949139803
Loss in iteration 74 : 0.33295071830222667
Loss in iteration 75 : 0.3325445864256901
Loss in iteration 76 : 0.3321523907021504
Loss in iteration 77 : 0.33177362636730084
Loss in iteration 78 : 0.33140781683985177
Loss in iteration 79 : 0.3310545133600392
Loss in iteration 80 : 0.3307132944385971
Loss in iteration 81 : 0.3303837650956821
Loss in iteration 82 : 0.33006555588493075
Loss in iteration 83 : 0.3297583217087363
Loss in iteration 84 : 0.3294617404366646
Loss in iteration 85 : 0.3291755113423251
Loss in iteration 86 : 0.3288993533762887
Loss in iteration 87 : 0.3286330032962327
Loss in iteration 88 : 0.3283762136807242
Loss in iteration 89 : 0.3281287508597321
Loss in iteration 90 : 0.32789039280153887
Loss in iteration 91 : 0.3276609270005449
Loss in iteration 92 : 0.32744014841267743
Loss in iteration 93 : 0.3272278574849466
Loss in iteration 94 : 0.3270238583349491
Loss in iteration 95 : 0.32682795836172424
Loss in iteration 96 : 0.3266400118714029
Loss in iteration 97 : 0.32646189999842234
Loss in iteration 98 : 0.3263870012964027
Loss in iteration 99 : 0.3282217274590561
Loss in iteration 100 : 0.32791039908365344
Loss in iteration 101 : 0.3264952049445975
Loss in iteration 102 : 0.3259615762052183
Loss in iteration 103 : 0.32576965171015776
Loss in iteration 104 : 0.32564646653306867
Loss in iteration 105 : 0.32554338036035424
Loss in iteration 106 : 0.3254478274042195
Loss in iteration 107 : 0.32535676277017667
Loss in iteration 108 : 0.32526911166684697
Loss in iteration 109 : 0.32518469006957523
Loss in iteration 110 : 0.32510362292087497
Loss in iteration 111 : 0.32502681443113446
Loss in iteration 112 : 0.3249565983596317
Loss in iteration 113 : 0.3249003919526479
Loss in iteration 114 : 0.32487913519583583
Loss in iteration 115 : 0.32495284798350366
Loss in iteration 116 : 0.3251962542439181
Loss in iteration 117 : 0.3254623830198515
Loss in iteration 118 : 0.32527847490138134
Loss in iteration 119 : 0.32495383680910167
Loss in iteration 120 : 0.3246845497329063
Loss in iteration 121 : 0.3245364663823347
Loss in iteration 122 : 0.3244387790663442
Loss in iteration 123 : 0.32437205826863247
Loss in iteration 124 : 0.3243189910740902
Loss in iteration 125 : 0.324277785664765
Loss in iteration 126 : 0.324246231989359
Loss in iteration 127 : 0.324230463048751
Loss in iteration 128 : 0.3242354200569663
Loss in iteration 129 : 0.32427666185271087
Loss in iteration 130 : 0.3243461556871674
Loss in iteration 131 : 0.3244251273734775
Loss in iteration 132 : 0.32441968260933857
Loss in iteration 133 : 0.32434867414905144
Loss in iteration 134 : 0.3242173188871107
Loss in iteration 135 : 0.3241129618939481
Loss in iteration 136 : 0.32402368859175246
Loss in iteration 137 : 0.32396577073745175
Loss in iteration 138 : 0.32392116862223796
Loss in iteration 139 : 0.32389481470123005
Loss in iteration 140 : 0.32387778416265717
Loss in iteration 141 : 0.32387738884515066
Loss in iteration 142 : 0.32388640853031425
Loss in iteration 143 : 0.3239127885550119
Loss in iteration 144 : 0.3239359598288965
Loss in iteration 145 : 0.32395828006538535
Loss in iteration 146 : 0.3239436343753434
Loss in iteration 147 : 0.3239154308484511
Loss in iteration 148 : 0.32385723850300463
Loss in iteration 149 : 0.3238084184663408
Loss in iteration 150 : 0.3237562498988148
Loss in iteration 151 : 0.3237223917951064
Loss in iteration 152 : 0.3236925189120364
Loss in iteration 153 : 0.32367860380128705
Loss in iteration 154 : 0.32366835120927673
Loss in iteration 155 : 0.3236719960034186
Loss in iteration 156 : 0.3236759345978271
Loss in iteration 157 : 0.32369007816641915
Loss in iteration 158 : 0.3236944452024852
Loss in iteration 159 : 0.32370103350756535
Loss in iteration 160 : 0.3236870401283557
Loss in iteration 161 : 0.32367363360621315
Loss in iteration 162 : 0.32364327597060827
Loss in iteration 163 : 0.323621054251687
Loss in iteration 164 : 0.3235919831366544
Loss in iteration 165 : 0.3235756107441675
Loss in iteration 166 : 0.32355734746191034
Loss in iteration 167 : 0.3235518576794017
Loss in iteration 168 : 0.32354471955293324
Loss in iteration 169 : 0.323548790365347
Loss in iteration 170 : 0.3235481603863726
Loss in iteration 171 : 0.3235558054019577
Loss in iteration 172 : 0.3235532268496465
Loss in iteration 173 : 0.32355576746701387
Loss in iteration 174 : 0.3235444172350259
Loss in iteration 175 : 0.3235383395948957
Loss in iteration 176 : 0.32352032233177647
Loss in iteration 177 : 0.3235105912588447
Loss in iteration 178 : 0.3234932550021102
Loss in iteration 179 : 0.3234863507308803
Loss in iteration 180 : 0.3234744211466932
Loss in iteration 181 : 0.3234731578300874
Loss in iteration 182 : 0.3234669003381414
Loss in iteration 183 : 0.3234702875006096
Loss in iteration 184 : 0.32346670746015166
Loss in iteration 185 : 0.3234711041733479
Loss in iteration 186 : 0.32346583634422116
Loss in iteration 187 : 0.3234673261347197
Loss in iteration 188 : 0.3234578708525005
Loss in iteration 189 : 0.32345550151790625
Loss in iteration 190 : 0.3234432568221079
Loss in iteration 191 : 0.3234394414983374
Loss in iteration 192 : 0.32342774529829704
Loss in iteration 193 : 0.3234254706071772
Loss in iteration 194 : 0.3234165359917382
Loss in iteration 195 : 0.3234171198773099
Loss in iteration 196 : 0.3234109380959583
Loss in iteration 197 : 0.3234136693467552
Loss in iteration 198 : 0.32340852057151015
Loss in iteration 199 : 0.3234114416726188
Loss in iteration 200 : 0.3234052261206628
Testing accuracy  of updater 5 on alg 0 with rate 0.009999999999999995 = 0.8499477919046742, training accuracy 0.8487407862407862, time elapsed: 4240 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 4.558338093070672
Loss in iteration 3 : 3.420373956978546
Loss in iteration 4 : 1.0891996810329483
Loss in iteration 5 : 1.71917630573738
Loss in iteration 6 : 2.574567951644458
Loss in iteration 7 : 1.7759037379769917
Loss in iteration 8 : 1.1998181584694345
Loss in iteration 9 : 1.4489895617874364
Loss in iteration 10 : 1.8445920318056113
Loss in iteration 11 : 1.9472190787945591
Loss in iteration 12 : 1.697477125931318
Loss in iteration 13 : 1.3273248894498957
Loss in iteration 14 : 1.1761044166767676
Loss in iteration 15 : 1.3589217777721923
Loss in iteration 16 : 1.5377252094064267
Loss in iteration 17 : 1.3920210275461489
Loss in iteration 18 : 1.078927111540717
Loss in iteration 19 : 0.9904626102614347
Loss in iteration 20 : 1.1577729846735823
Loss in iteration 21 : 1.2012107734427795
Loss in iteration 22 : 0.9752721857607818
Loss in iteration 23 : 0.7754466340874143
Loss in iteration 24 : 0.9336925957764937
Loss in iteration 25 : 0.9368576892008489
Loss in iteration 26 : 0.6464656991082987
Loss in iteration 27 : 0.7135193061415162
Loss in iteration 28 : 0.7836267378370965
Loss in iteration 29 : 0.5605783588654455
Loss in iteration 30 : 0.5926881330160825
Loss in iteration 31 : 0.6357709434145304
Loss in iteration 32 : 0.44489169351677277
Loss in iteration 33 : 0.6002703548150616
Loss in iteration 34 : 0.43383488885662597
Loss in iteration 35 : 0.553665156367759
Loss in iteration 36 : 0.43699139222248745
Loss in iteration 37 : 0.5273419912353253
Loss in iteration 38 : 0.44581003753031284
Loss in iteration 39 : 0.5046846982579067
Loss in iteration 40 : 0.39836180875497124
Loss in iteration 41 : 0.4475483683198381
Loss in iteration 42 : 0.4136451879546083
Loss in iteration 43 : 0.4024145989285246
Loss in iteration 44 : 0.4177366905003852
Loss in iteration 45 : 0.3771690516343653
Loss in iteration 46 : 0.4093196316733064
Loss in iteration 47 : 0.353934384070767
Loss in iteration 48 : 0.40350631651304614
Loss in iteration 49 : 0.3492720280196072
Loss in iteration 50 : 0.3849864386467188
Loss in iteration 51 : 0.3407526149105748
Loss in iteration 52 : 0.37412367404924124
Loss in iteration 53 : 0.34613944934467106
Loss in iteration 54 : 0.3594969333034863
Loss in iteration 55 : 0.3518940222910812
Loss in iteration 56 : 0.34207590374236063
Loss in iteration 57 : 0.3553246851437186
Loss in iteration 58 : 0.3299972749166587
Loss in iteration 59 : 0.3540719996268564
Loss in iteration 60 : 0.33782530177071923
Loss in iteration 61 : 0.3350403069850248
Loss in iteration 62 : 0.3470763827119289
Loss in iteration 63 : 0.32837807413396863
Loss in iteration 64 : 0.34080553267225466
Loss in iteration 65 : 0.3295863752434683
Loss in iteration 66 : 0.3332713148172966
Loss in iteration 67 : 0.3332045701629659
Loss in iteration 68 : 0.3270551627838944
Loss in iteration 69 : 0.33612539421243176
Loss in iteration 70 : 0.32739723152203376
Loss in iteration 71 : 0.3297820311715747
Loss in iteration 72 : 0.33202570945262644
Loss in iteration 73 : 0.3249462053673752
Loss in iteration 74 : 0.3287454231686223
Loss in iteration 75 : 0.32697877845242873
Loss in iteration 76 : 0.32392572769602423
Loss in iteration 77 : 0.3272076675800412
Loss in iteration 78 : 0.3245005119447359
Loss in iteration 79 : 0.32425094366378493
Loss in iteration 80 : 0.32654873213591434
Loss in iteration 81 : 0.32410487457136516
Loss in iteration 82 : 0.3242806466078964
Loss in iteration 83 : 0.3256108073393792
Loss in iteration 84 : 0.3235687673447542
Loss in iteration 85 : 0.3238461909302086
Loss in iteration 86 : 0.3249005754173837
Loss in iteration 87 : 0.3234195996605998
Loss in iteration 88 : 0.32344429516509154
Loss in iteration 89 : 0.32433551626138674
Loss in iteration 90 : 0.32335410379249674
Loss in iteration 91 : 0.3230023017038565
Loss in iteration 92 : 0.32377740127559484
Loss in iteration 93 : 0.32337820635926107
Loss in iteration 94 : 0.3228310077829435
Loss in iteration 95 : 0.3233289903714961
Loss in iteration 96 : 0.3234133230509145
Loss in iteration 97 : 0.32289844982235666
Loss in iteration 98 : 0.32295219091664573
Loss in iteration 99 : 0.32324900526926337
Loss in iteration 100 : 0.3230171279651322
Loss in iteration 101 : 0.3227299966420158
Loss in iteration 102 : 0.3229073016097975
Loss in iteration 103 : 0.3230578977622268
Loss in iteration 104 : 0.3228371321558211
Loss in iteration 105 : 0.32269798993976073
Loss in iteration 106 : 0.3228475901656892
Loss in iteration 107 : 0.3229249047402488
Loss in iteration 108 : 0.32278510042654374
Loss in iteration 109 : 0.32269125610167243
Loss in iteration 110 : 0.3227762140280207
Loss in iteration 111 : 0.32284148696865494
Loss in iteration 112 : 0.3227584398867903
Loss in iteration 113 : 0.3226676679585492
Loss in iteration 114 : 0.3226882215637201
Loss in iteration 115 : 0.32275445051905294
Loss in iteration 116 : 0.3227554864004026
Loss in iteration 117 : 0.3226905357082838
Loss in iteration 118 : 0.3226508568553547
Loss in iteration 119 : 0.3226756388438501
Loss in iteration 120 : 0.3227168842584773
Loss in iteration 121 : 0.3227178677553256
Loss in iteration 122 : 0.3226780258856631
Loss in iteration 123 : 0.32264528360950545
Loss in iteration 124 : 0.32264853627305085
Loss in iteration 125 : 0.3226734303572546
Loss in iteration 126 : 0.3226880012556351
Loss in iteration 127 : 0.32267687548767593
Loss in iteration 128 : 0.3226536188954459
Loss in iteration 129 : 0.3226378673632785
Loss in iteration 130 : 0.32263873116464525
Loss in iteration 131 : 0.322650902559222
Loss in iteration 132 : 0.3226634756523932
Loss in iteration 133 : 0.32266872672065405
Loss in iteration 134 : 0.3226637440988803
Loss in iteration 135 : 0.3226527804731084
Loss in iteration 136 : 0.3226416130247775
Loss in iteration 137 : 0.32263479415920354
Loss in iteration 138 : 0.3226329356932018
Loss in iteration 139 : 0.322634975207346
Loss in iteration 140 : 0.3226397236513316
Loss in iteration 141 : 0.3226456734321664
Loss in iteration 142 : 0.32265152465534785
Loss in iteration 143 : 0.3226566639613022
Loss in iteration 144 : 0.3226622473178175
Loss in iteration 145 : 0.3226687012042982
Loss in iteration 146 : 0.32267786281288796
Loss in iteration 147 : 0.32269050480041345
Loss in iteration 148 : 0.3227115858374727
Loss in iteration 149 : 0.3227448645003518
Loss in iteration 150 : 0.32280547056974734
Loss in iteration 151 : 0.3229077431691018
Loss in iteration 152 : 0.32310850322975176
Loss in iteration 153 : 0.3234620485692865
Loss in iteration 154 : 0.32421460546646447
Loss in iteration 155 : 0.3255386310260224
Loss in iteration 156 : 0.32861091686590493
Loss in iteration 157 : 0.3334425360127653
Loss in iteration 158 : 0.3453733547965215
Loss in iteration 159 : 0.35604882173564445
Loss in iteration 160 : 0.38037060401387474
Loss in iteration 161 : 0.36538270001151557
Loss in iteration 162 : 0.3502439389310651
Loss in iteration 163 : 0.3283323549499604
Loss in iteration 164 : 0.32926548828460284
Loss in iteration 165 : 0.35032623139326946
Loss in iteration 166 : 0.37312770616126123
Loss in iteration 167 : 0.4282953338238471
Loss in iteration 168 : 0.39331805327891717
Loss in iteration 169 : 0.3642678529919618
Loss in iteration 170 : 0.33192931954018234
Loss in iteration 171 : 0.3561274168159134
Loss in iteration 172 : 0.39660172833957874
Loss in iteration 173 : 0.36597798823888483
Loss in iteration 174 : 0.34262511417200625
Loss in iteration 175 : 0.32565227850893336
Loss in iteration 176 : 0.3265955494352503
Loss in iteration 177 : 0.34237534626149724
Loss in iteration 178 : 0.3644194205277862
Loss in iteration 179 : 0.413885813167208
Loss in iteration 180 : 0.39572088285710344
Loss in iteration 181 : 0.3769582758720853
Loss in iteration 182 : 0.3327048202237228
Loss in iteration 183 : 0.3511781132346002
Loss in iteration 184 : 0.39424485676153975
Loss in iteration 185 : 0.36087028380098446
Loss in iteration 186 : 0.3336507845396202
Loss in iteration 187 : 0.325429454482833
Loss in iteration 188 : 0.34016574791372634
Loss in iteration 189 : 0.3766552941773187
Loss in iteration 190 : 0.39144301812883053
Loss in iteration 191 : 0.4212572128120691
Loss in iteration 192 : 0.3565784936026847
Loss in iteration 193 : 0.33107816066459006
Loss in iteration 194 : 0.3567992612921466
Loss in iteration 195 : 0.3691456068964903
Loss in iteration 196 : 0.3696660257325636
Loss in iteration 197 : 0.3385808220257871
Loss in iteration 198 : 0.3250833843810685
Loss in iteration 199 : 0.3281802830981227
Loss in iteration 200 : 0.3419916047107004
Testing accuracy  of updater 6 on alg 0 with rate 2.0 = 0.8306615072784227, training accuracy 0.8282555282555283, time elapsed: 3871 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.48611992717097613
Loss in iteration 3 : 0.49046953643226726
Loss in iteration 4 : 0.48780451156888005
Loss in iteration 5 : 0.4178743260711607
Loss in iteration 6 : 0.35090899587098784
Loss in iteration 7 : 0.35138260975320396
Loss in iteration 8 : 0.39279071925067677
Loss in iteration 9 : 0.4139893317646586
Loss in iteration 10 : 0.39852836678171377
Loss in iteration 11 : 0.3729996200452122
Loss in iteration 12 : 0.3644199097967758
Loss in iteration 13 : 0.37491451235556644
Loss in iteration 14 : 0.3891345024757927
Loss in iteration 15 : 0.3927311317010911
Loss in iteration 16 : 0.3827090776315869
Loss in iteration 17 : 0.3671456730801952
Loss in iteration 18 : 0.35755755682142587
Loss in iteration 19 : 0.3589409551667693
Loss in iteration 20 : 0.365461218945943
Loss in iteration 21 : 0.3669443885048423
Loss in iteration 22 : 0.3597362546102002
Loss in iteration 23 : 0.3499252414354823
Loss in iteration 24 : 0.34575712043191537
Loss in iteration 25 : 0.34839946966502516
Loss in iteration 26 : 0.3516610317915974
Loss in iteration 27 : 0.34972771563803373
Loss in iteration 28 : 0.3432658618614929
Loss in iteration 29 : 0.3379715517947595
Loss in iteration 30 : 0.33763723025395365
Loss in iteration 31 : 0.3395559375731429
Loss in iteration 32 : 0.338462496084065
Loss in iteration 33 : 0.3338549987903592
Loss in iteration 34 : 0.3301188425499157
Loss in iteration 35 : 0.3299857839875145
Loss in iteration 36 : 0.3313642645204898
Loss in iteration 37 : 0.3309707692192198
Loss in iteration 38 : 0.32861209034975364
Loss in iteration 39 : 0.32683405987798586
Loss in iteration 40 : 0.32718794022391945
Loss in iteration 41 : 0.3282451354075534
Loss in iteration 42 : 0.32789989129872876
Loss in iteration 43 : 0.3263455518573623
Loss in iteration 44 : 0.32541634616169757
Loss in iteration 45 : 0.3257908144110094
Loss in iteration 46 : 0.32631448900948756
Loss in iteration 47 : 0.32591001519310947
Loss in iteration 48 : 0.32500628624389344
Loss in iteration 49 : 0.32469893862122057
Loss in iteration 50 : 0.32512302179664143
Loss in iteration 51 : 0.3253890077374576
Loss in iteration 52 : 0.3249890795674867
Loss in iteration 53 : 0.3244419329320256
Loss in iteration 54 : 0.32435894175078334
Loss in iteration 55 : 0.3245652988214864
Loss in iteration 56 : 0.32450669145452177
Loss in iteration 57 : 0.3240988613825311
Loss in iteration 58 : 0.32376327646389313
Loss in iteration 59 : 0.3237591301460458
Loss in iteration 60 : 0.3238569585180633
Loss in iteration 61 : 0.32375388612784045
Loss in iteration 62 : 0.32351592687595304
Loss in iteration 63 : 0.3234114417414857
Loss in iteration 64 : 0.3234903308554769
Loss in iteration 65 : 0.3235527553193603
Loss in iteration 66 : 0.32346924918328507
Loss in iteration 67 : 0.3233408765220524
Loss in iteration 68 : 0.3233125598709513
Loss in iteration 69 : 0.32336073430742024
Loss in iteration 70 : 0.323356295698734
Loss in iteration 71 : 0.32326772523430586
Loss in iteration 72 : 0.32319031585132646
Loss in iteration 73 : 0.3231896249664481
Loss in iteration 74 : 0.3232159054461493
Loss in iteration 75 : 0.3231969552170402
Loss in iteration 76 : 0.32314428682313107
Loss in iteration 77 : 0.32312054697295944
Loss in iteration 78 : 0.32313938555071287
Loss in iteration 79 : 0.32315279229401883
Loss in iteration 80 : 0.32312987929555387
Loss in iteration 81 : 0.32309753837070254
Loss in iteration 82 : 0.3230905030476635
Loss in iteration 83 : 0.3230997380658065
Loss in iteration 84 : 0.32309414637210776
Loss in iteration 85 : 0.3230702669391697
Loss in iteration 86 : 0.3230527812379264
Loss in iteration 87 : 0.323053874066315
Loss in iteration 88 : 0.3230577818764217
Loss in iteration 89 : 0.32304876541720795
Loss in iteration 90 : 0.32303387299941005
Loss in iteration 91 : 0.3230279578747171
Loss in iteration 92 : 0.3230299838456042
Loss in iteration 93 : 0.32302721900200926
Loss in iteration 94 : 0.32301602425771675
Loss in iteration 95 : 0.32300558419163683
Loss in iteration 96 : 0.32300213941100614
Loss in iteration 97 : 0.3230004844920564
Loss in iteration 98 : 0.3229940296188248
Loss in iteration 99 : 0.32298507995459474
Loss in iteration 100 : 0.3229798769265081
Loss in iteration 101 : 0.32297857048902745
Loss in iteration 102 : 0.3229760796964494
Loss in iteration 103 : 0.32297057197379453
Loss in iteration 104 : 0.3229654854570284
Loss in iteration 105 : 0.3229632629163583
Loss in iteration 106 : 0.32296174177561815
Loss in iteration 107 : 0.3229581824307453
Loss in iteration 108 : 0.3229534697690258
Loss in iteration 109 : 0.3229500835197977
Loss in iteration 110 : 0.32294804623556017
Loss in iteration 111 : 0.3229453613329323
Loss in iteration 112 : 0.3229414400985946
Loss in iteration 113 : 0.32293775634924593
Loss in iteration 114 : 0.32293523681916425
Loss in iteration 115 : 0.32293295680392486
Loss in iteration 116 : 0.3229298978120028
Loss in iteration 117 : 0.3229265227670906
Loss in iteration 118 : 0.32292378669927535
Loss in iteration 119 : 0.3229215714788836
Loss in iteration 120 : 0.32291905824981343
Loss in iteration 121 : 0.32291611121608366
Loss in iteration 122 : 0.3229133564081989
Loss in iteration 123 : 0.3229110709497333
Loss in iteration 124 : 0.32290881596551024
Loss in iteration 125 : 0.32290624744236335
Loss in iteration 126 : 0.32290363157270174
Loss in iteration 127 : 0.3229013152319124
Loss in iteration 128 : 0.3228991666567828
Loss in iteration 129 : 0.3228968649500359
Loss in iteration 130 : 0.32289443188980094
Loss in iteration 131 : 0.3228921305794317
Loss in iteration 132 : 0.3228900055135849
Loss in iteration 133 : 0.3228878514713455
Loss in iteration 134 : 0.3228855822268141
Loss in iteration 135 : 0.3228833451697135
Loss in iteration 136 : 0.32288124668023166
Loss in iteration 137 : 0.32287919248314273
Loss in iteration 138 : 0.3228770739778713
Loss in iteration 139 : 0.3228749421487296
Loss in iteration 140 : 0.322872895774305
Loss in iteration 141 : 0.3228709138707684
Loss in iteration 142 : 0.3228689094078485
Loss in iteration 143 : 0.3228668788032001
Loss in iteration 144 : 0.322864891423648
Loss in iteration 145 : 0.3228629650275967
Loss in iteration 146 : 0.3228610470702702
Loss in iteration 147 : 0.3228591122149129
Loss in iteration 148 : 0.3228571981249825
Loss in iteration 149 : 0.32285533308045417
Loss in iteration 150 : 0.3228534915185285
Loss in iteration 151 : 0.32285164412487427
Loss in iteration 152 : 0.32284980455425527
Loss in iteration 153 : 0.3228479980673659
Loss in iteration 154 : 0.322846217297564
Loss in iteration 155 : 0.32284443933852414
Loss in iteration 156 : 0.3228426652670198
Loss in iteration 157 : 0.3228409137312798
Loss in iteration 158 : 0.32283918733339156
Loss in iteration 159 : 0.32283747144735764
Loss in iteration 160 : 0.3228357613638574
Loss in iteration 161 : 0.3228340681237617
Loss in iteration 162 : 0.322832397338487
Loss in iteration 163 : 0.3228307406552369
Loss in iteration 164 : 0.322829091600763
Loss in iteration 165 : 0.3228274553609471
Loss in iteration 166 : 0.3228258375681479
Loss in iteration 167 : 0.3228242344624228
Loss in iteration 168 : 0.3228226404463904
Loss in iteration 169 : 0.32282105737798605
Loss in iteration 170 : 0.3228194898918494
Loss in iteration 171 : 0.32281793703511696
Loss in iteration 172 : 0.32281639474471696
Loss in iteration 173 : 0.32281486303826723
Loss in iteration 174 : 0.32281334506854964
Loss in iteration 175 : 0.32281184109403194
Loss in iteration 176 : 0.322810348356323
Loss in iteration 177 : 0.32280886599808667
Loss in iteration 178 : 0.32280739588463714
Loss in iteration 179 : 0.3228059387176123
Loss in iteration 180 : 0.32280449283530266
Loss in iteration 181 : 0.3228030572303902
Loss in iteration 182 : 0.3228016329375137
Loss in iteration 183 : 0.3228002207468339
Loss in iteration 184 : 0.3227988197633422
Loss in iteration 185 : 0.32279742908026154
Testing accuracy  of updater 6 on alg 0 with rate 0.2 = 0.8501934770591487, training accuracy 0.8491400491400491, time elapsed: 3082 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.65156239844323
Loss in iteration 3 : 0.5926579139640611
Loss in iteration 4 : 0.537264140055939
Loss in iteration 5 : 0.4969597489946177
Loss in iteration 6 : 0.47333393776720345
Loss in iteration 7 : 0.46168361002107866
Loss in iteration 8 : 0.4556668308312267
Loss in iteration 9 : 0.45018814528330936
Loss in iteration 10 : 0.44229606565096685
Loss in iteration 11 : 0.4310041057192866
Loss in iteration 12 : 0.4167676538121479
Loss in iteration 13 : 0.4009288136656456
Loss in iteration 14 : 0.38520685839841073
Loss in iteration 15 : 0.37124957910179873
Loss in iteration 16 : 0.3602751068617597
Loss in iteration 17 : 0.3528525817023476
Loss in iteration 18 : 0.34885781315912734
Loss in iteration 19 : 0.3475979985246883
Loss in iteration 20 : 0.3480521216274788
Loss in iteration 21 : 0.3491462768240602
Loss in iteration 22 : 0.34998663091792515
Loss in iteration 23 : 0.35000079505623943
Loss in iteration 24 : 0.3489750947373589
Loss in iteration 25 : 0.3470054449652794
Loss in iteration 26 : 0.3443956662926613
Loss in iteration 27 : 0.3415393931263237
Loss in iteration 28 : 0.33881473863208655
Loss in iteration 29 : 0.3365097018075508
Loss in iteration 30 : 0.33478484901262623
Loss in iteration 31 : 0.3336704989751842
Loss in iteration 32 : 0.33308966231764353
Loss in iteration 33 : 0.33289551694913544
Loss in iteration 34 : 0.3329126643713347
Loss in iteration 35 : 0.33297378585390247
Loss in iteration 36 : 0.3329464954470999
Loss in iteration 37 : 0.3327482806205707
Loss in iteration 38 : 0.33234990198174463
Loss in iteration 39 : 0.33176930264086846
Loss in iteration 40 : 0.3310589954701066
Loss in iteration 41 : 0.3302901939927718
Loss in iteration 42 : 0.3295367810497159
Loss in iteration 43 : 0.32886169113433744
Loss in iteration 44 : 0.32830751533218483
Loss in iteration 45 : 0.3278922198538896
Loss in iteration 46 : 0.3276099178135815
Loss in iteration 47 : 0.32743578686095487
Loss in iteration 48 : 0.32733361768134367
Loss in iteration 49 : 0.3272642075655078
Loss in iteration 50 : 0.3271929078498174
Loss in iteration 51 : 0.3270950426597744
Loss in iteration 52 : 0.3269585216075746
Loss in iteration 53 : 0.3267836209350776
Loss in iteration 54 : 0.3265804637886009
Loss in iteration 55 : 0.3263650896214784
Loss in iteration 56 : 0.32615512265475727
Loss in iteration 57 : 0.3259659462360512
Loss in iteration 58 : 0.32580802494678446
Loss in iteration 59 : 0.32568567348822436
Loss in iteration 60 : 0.3255972349521143
Loss in iteration 61 : 0.32553636721318846
Loss in iteration 62 : 0.32549398258846474
Loss in iteration 63 : 0.32546035007251606
Loss in iteration 64 : 0.325426933875562
Loss in iteration 65 : 0.3253876735481185
Loss in iteration 66 : 0.3253395712022286
Loss in iteration 67 : 0.32528260500183054
Loss in iteration 68 : 0.3252191088070793
Loss in iteration 69 : 0.32515283054959765
Loss in iteration 70 : 0.32508790277348554
Loss in iteration 71 : 0.3250279334910191
Loss in iteration 72 : 0.3249753665710626
Loss in iteration 73 : 0.32493118448982894
Loss in iteration 74 : 0.3248949491376767
Loss in iteration 75 : 0.3248651128519416
Loss in iteration 76 : 0.32483949172408155
Loss in iteration 77 : 0.32481578049919513
Loss in iteration 78 : 0.3247920011933531
Loss in iteration 79 : 0.32476680931025537
Loss in iteration 80 : 0.3247396228775842
Loss in iteration 81 : 0.3247105806098189
Loss in iteration 82 : 0.32468036814241424
Loss in iteration 83 : 0.32464997036791143
Loss in iteration 84 : 0.3246204119871806
Loss in iteration 85 : 0.324592539346343
Loss in iteration 86 : 0.32456687868408374
Loss in iteration 87 : 0.32454358431940644
Loss in iteration 88 : 0.32452247007096857
Loss in iteration 89 : 0.32450310209069
Loss in iteration 90 : 0.3244849233348308
Loss in iteration 91 : 0.32446737928953095
Loss in iteration 92 : 0.3244500200284088
Loss in iteration 93 : 0.32443256298284173
Loss in iteration 94 : 0.3244149114097223
Loss in iteration 95 : 0.32439713314277424
Loss in iteration 96 : 0.3243794111388653
Loss in iteration 97 : 0.3243619807354576
Loss in iteration 98 : 0.3243450684061791
Loss in iteration 99 : 0.3243288438061825
Loss in iteration 100 : 0.3243133921367633
Loss in iteration 101 : 0.3242987085746643
Loss in iteration 102 : 0.32428471183322305
Loss in iteration 103 : 0.32427127066244243
Loss in iteration 104 : 0.3242582356462475
Loss in iteration 105 : 0.32424546898482753
Loss in iteration 106 : 0.32423286667760703
Loss in iteration 107 : 0.32422037005556736
Loss in iteration 108 : 0.3242079662990269
Loss in iteration 109 : 0.32419567985178005
Loss in iteration 110 : 0.32418355812084704
Loss in iteration 111 : 0.32417165537875114
Loss in iteration 112 : 0.3241600184280429
Loss in iteration 113 : 0.3241486765787004
Loss in iteration 114 : 0.32413763715090127
Loss in iteration 115 : 0.32412688637872783
Loss in iteration 116 : 0.3241163945243444
Loss in iteration 117 : 0.3241061233825372
Loss in iteration 118 : 0.32409603420872335
Loss in iteration 119 : 0.3240860943808203
Loss in iteration 120 : 0.32407628167278296
Loss in iteration 121 : 0.3240665857074082
Loss in iteration 122 : 0.3240570068070525
Loss in iteration 123 : 0.3240475529483414
Loss in iteration 124 : 0.32403823578273283
Loss in iteration 125 : 0.3240290666987068
Loss in iteration 126 : 0.32402005371490217
Loss in iteration 127 : 0.32401119968043096
Loss in iteration 128 : 0.3240025019054969
Loss in iteration 129 : 0.32399395303122214
Loss in iteration 130 : 0.3239855427294545
Loss in iteration 131 : 0.32397725972840385
Loss in iteration 132 : 0.3239690936854334
Loss in iteration 133 : 0.3239610365476428
Loss in iteration 134 : 0.3239530832119553
Loss in iteration 135 : 0.32394523147561965
Loss in iteration 136 : 0.32393748141608925
Loss in iteration 137 : 0.32392983443197776
Loss in iteration 138 : 0.32392229220452196
Loss in iteration 139 : 0.3239148558081753
Loss in iteration 140 : 0.32390752512628035
Loss in iteration 141 : 0.3239002986360276
Loss in iteration 142 : 0.3238931735387181
Loss in iteration 143 : 0.32388614614398525
Loss in iteration 144 : 0.32387921238075285
Loss in iteration 145 : 0.3238723683046931
Loss in iteration 146 : 0.32386561049661616
Loss in iteration 147 : 0.3238589362884114
Loss in iteration 148 : 0.32385234380076705
Loss in iteration 149 : 0.32384583181922444
Loss in iteration 150 : 0.3238393995640354
Loss in iteration 151 : 0.32383304642142957
Loss in iteration 152 : 0.32382677169959795
Loss in iteration 153 : 0.323820574455942
Loss in iteration 154 : 0.3238144534185643
Loss in iteration 155 : 0.32380840700102326
Loss in iteration 156 : 0.32380243338981796
Loss in iteration 157 : 0.32379653067248126
Loss in iteration 158 : 0.32379069697153057
Loss in iteration 159 : 0.3237849305545033
Loss in iteration 160 : 0.32377922990097213
Loss in iteration 161 : 0.32377359371989195
Loss in iteration 162 : 0.3237680209225263
Loss in iteration 163 : 0.32376251056457994
Loss in iteration 164 : 0.32375706177523267
Loss in iteration 165 : 0.32375167369040875
Loss in iteration 166 : 0.32374634540336605
Loss in iteration 167 : 0.323741075939821
Loss in iteration 168 : 0.32373586425805567
Loss in iteration 169 : 0.32373070926920866
Loss in iteration 170 : 0.32372560986941445
Loss in iteration 171 : 0.3237205649745168
Loss in iteration 172 : 0.32371557354919117
Loss in iteration 173 : 0.32371063462508104
Loss in iteration 174 : 0.32370574730585944
Loss in iteration 175 : 0.3237009107603809
Loss in iteration 176 : 0.32369612420741156
Loss in iteration 177 : 0.3236913868966628
Loss in iteration 178 : 0.3236866980907252
Loss in iteration 179 : 0.3236820570515739
Loss in iteration 180 : 0.32367746303353284
Loss in iteration 181 : 0.32367291528296005
Loss in iteration 182 : 0.3236684130433791
Loss in iteration 183 : 0.3236639555637587
Loss in iteration 184 : 0.3236595421075529
Loss in iteration 185 : 0.32365517196022714
Loss in iteration 186 : 0.32365084443381986
Loss in iteration 187 : 0.3236465588680139
Loss in iteration 188 : 0.32364231462802967
Loss in iteration 189 : 0.32363811110030155
Loss in iteration 190 : 0.3236339476871899
Loss in iteration 191 : 0.32362982380202165
Loss in iteration 192 : 0.323625738865364
Loss in iteration 193 : 0.3236216923030881
Loss in iteration 194 : 0.3236176835462071
Loss in iteration 195 : 0.32361371203216427
Loss in iteration 196 : 0.3236097772069043
Loss in iteration 197 : 0.32360587852709966
Loss in iteration 198 : 0.3236020154619385
Loss in iteration 199 : 0.3235981874940698
Loss in iteration 200 : 0.3235943941196306
Testing accuracy  of updater 6 on alg 0 with rate 0.01999999999999999 = 0.8501320557705301, training accuracy 0.8492936117936118, time elapsed: 3352 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 32.24039829303136
Loss in iteration 3 : 25.461149513710687
Loss in iteration 4 : 7.055606148155897
Loss in iteration 5 : 12.753591897201826
Loss in iteration 6 : 18.261194902938946
Loss in iteration 7 : 10.936951907375047
Loss in iteration 8 : 7.43276280830362
Loss in iteration 9 : 9.810652066799923
Loss in iteration 10 : 12.408087782228723
Loss in iteration 11 : 12.655697031142102
Loss in iteration 12 : 10.686156225734681
Loss in iteration 13 : 8.221158206682745
Loss in iteration 14 : 7.27860426557539
Loss in iteration 15 : 8.274599293286549
Loss in iteration 16 : 9.529135671687797
Loss in iteration 17 : 9.170357400496849
Loss in iteration 18 : 7.449186221839996
Loss in iteration 19 : 6.159358900300887
Loss in iteration 20 : 6.372170382716482
Loss in iteration 21 : 7.197794880019199
Loss in iteration 22 : 7.187791627820735
Loss in iteration 23 : 6.049376447245913
Loss in iteration 24 : 4.884129246622061
Loss in iteration 25 : 5.124605022964602
Loss in iteration 26 : 5.698130998127907
Loss in iteration 27 : 5.124621836412529
Loss in iteration 28 : 3.9101653347084735
Loss in iteration 29 : 3.9494745703285465
Loss in iteration 30 : 4.3827867449820905
Loss in iteration 31 : 3.768380854798267
Loss in iteration 32 : 2.900915719790044
Loss in iteration 33 : 3.433455998044336
Loss in iteration 34 : 3.212901137277427
Loss in iteration 35 : 2.347511012612317
Loss in iteration 36 : 2.8934925189783165
Loss in iteration 37 : 2.335321324528548
Loss in iteration 38 : 2.1384558795387307
Loss in iteration 39 : 2.216449339696478
Loss in iteration 40 : 1.8876907090835577
Loss in iteration 41 : 1.7551248602827503
Loss in iteration 42 : 2.0211450486623566
Loss in iteration 43 : 1.3904786493049202
Loss in iteration 44 : 1.8943651718412382
Loss in iteration 45 : 1.1396133781671418
Loss in iteration 46 : 1.8399639485883437
Loss in iteration 47 : 1.576220064430402
Loss in iteration 48 : 1.0959841649685083
Loss in iteration 49 : 2.0988506938236315
Loss in iteration 50 : 1.192900557373038
Loss in iteration 51 : 1.4419987088681678
Loss in iteration 52 : 1.4591639261918117
Loss in iteration 53 : 1.1703164785376867
Loss in iteration 54 : 1.5223542182732952
Loss in iteration 55 : 0.9728177325438837
Loss in iteration 56 : 1.5580954265401303
Loss in iteration 57 : 1.2796365550373772
Loss in iteration 58 : 0.8371089202229969
Loss in iteration 59 : 1.8937356619205528
Loss in iteration 60 : 1.9127389017496388
Loss in iteration 61 : 1.0076309509533163
Loss in iteration 62 : 2.5262724373378154
Loss in iteration 63 : 0.9620896299214226
Loss in iteration 64 : 1.8649011404413167
Loss in iteration 65 : 0.9927900105346636
Loss in iteration 66 : 1.7835304118201967
Loss in iteration 67 : 0.9166155816823693
Loss in iteration 68 : 1.552008793259291
Loss in iteration 69 : 0.8450011103653149
Loss in iteration 70 : 1.6354140221626583
Loss in iteration 71 : 1.0942024700343398
Loss in iteration 72 : 0.966787414252553
Loss in iteration 73 : 1.5596788307384932
Loss in iteration 74 : 0.8512168799181019
Loss in iteration 75 : 0.9391491016196353
Loss in iteration 76 : 1.2185988286544698
Loss in iteration 77 : 0.7389521904036598
Loss in iteration 78 : 0.6984776791464178
Loss in iteration 79 : 1.4618730854528286
Loss in iteration 80 : 2.4520460668925117
Loss in iteration 81 : 1.0628907931802314
Loss in iteration 82 : 4.0237158445293915
Loss in iteration 83 : 0.876960041114169
Loss in iteration 84 : 2.294295813097507
Loss in iteration 85 : 2.1879134477666633
Loss in iteration 86 : 1.3082468418924427
Loss in iteration 87 : 2.152050791363518
Loss in iteration 88 : 2.051463615808681
Loss in iteration 89 : 1.3867241224893891
Loss in iteration 90 : 1.8911967042212523
Loss in iteration 91 : 1.8380168711032931
Loss in iteration 92 : 1.1991348333040552
Loss in iteration 93 : 1.6402558933554425
Loss in iteration 94 : 1.3388321308148936
Loss in iteration 95 : 1.0823814460420325
Loss in iteration 96 : 1.3216909737167828
Loss in iteration 97 : 0.7208359727206566
Loss in iteration 98 : 1.253793327118517
Loss in iteration 99 : 1.0861477768956762
Loss in iteration 100 : 0.6805940687152482
Loss in iteration 101 : 1.944702841732066
Loss in iteration 102 : 1.2878400802775427
Loss in iteration 103 : 0.9282878840181177
Loss in iteration 104 : 1.2922926298291812
Loss in iteration 105 : 0.9211799675844715
Loss in iteration 106 : 1.0947833515110974
Loss in iteration 107 : 1.0067592657466728
Loss in iteration 108 : 0.8716777831369277
Loss in iteration 109 : 0.8892515811500799
Loss in iteration 110 : 0.9450573695468004
Loss in iteration 111 : 0.6721145163256836
Loss in iteration 112 : 1.3992916623761702
Loss in iteration 113 : 1.2463565538490413
Loss in iteration 114 : 0.8424431376919572
Loss in iteration 115 : 1.608735985470115
Loss in iteration 116 : 0.6718704712619741
Loss in iteration 117 : 1.2109899511211324
Loss in iteration 118 : 0.6765204491746111
Loss in iteration 119 : 1.1117915776692218
Loss in iteration 120 : 0.694425666043893
Loss in iteration 121 : 0.8162146781701803
Loss in iteration 122 : 0.919365110003813
Loss in iteration 123 : 0.5131057194756878
Loss in iteration 124 : 0.6005355947438271
Loss in iteration 125 : 1.0514272201958035
Loss in iteration 126 : 1.6333582471457475
Loss in iteration 127 : 0.6101817191828374
Loss in iteration 128 : 2.1609900009298064
Loss in iteration 129 : 0.9230259239971462
Loss in iteration 130 : 1.543137741454157
Loss in iteration 131 : 0.8494240016585395
Loss in iteration 132 : 1.5139912976476235
Loss in iteration 133 : 0.8954010401387218
Loss in iteration 134 : 1.2647229959949304
Loss in iteration 135 : 0.9686738268102809
Loss in iteration 136 : 1.0726517723223679
Loss in iteration 137 : 0.8078095623988867
Loss in iteration 138 : 1.130374712442687
Loss in iteration 139 : 0.60990553759321
Loss in iteration 140 : 1.6939860273911873
Loss in iteration 141 : 1.6081127748498096
Loss in iteration 142 : 1.062720702907158
Loss in iteration 143 : 1.7434183376916117
Loss in iteration 144 : 0.8762178841380314
Loss in iteration 145 : 1.3642002411486736
Loss in iteration 146 : 1.1091305270938359
Loss in iteration 147 : 1.0839158024645645
Loss in iteration 148 : 1.08225811091619
Loss in iteration 149 : 0.9776541809319647
Loss in iteration 150 : 0.9415627216094384
Loss in iteration 151 : 0.9611030207487669
Loss in iteration 152 : 0.5959132975569507
Loss in iteration 153 : 1.0115587725729807
Loss in iteration 154 : 0.554506950892047
Loss in iteration 155 : 0.444979399076877
Loss in iteration 156 : 0.8984872370874233
Loss in iteration 157 : 2.0172471858934413
Loss in iteration 158 : 2.383072232988747
Loss in iteration 159 : 2.0617340152226733
Loss in iteration 160 : 1.2431158831578972
Loss in iteration 161 : 2.5583689141442947
Loss in iteration 162 : 1.5401287854387469
Loss in iteration 163 : 1.7801811453178922
Loss in iteration 164 : 2.1525761227797218
Loss in iteration 165 : 1.6520135703234344
Loss in iteration 166 : 1.453240239279791
Loss in iteration 167 : 1.8768571104532965
Loss in iteration 168 : 1.3221873175014203
Loss in iteration 169 : 1.5163692843347087
Loss in iteration 170 : 1.4658801498669574
Loss in iteration 171 : 1.0270132602158972
Loss in iteration 172 : 1.3600627590518357
Loss in iteration 173 : 0.825803880642457
Loss in iteration 174 : 1.1306656766414498
Loss in iteration 175 : 0.682544511300129
Loss in iteration 176 : 1.0543171999389895
Loss in iteration 177 : 1.590545938626831
Loss in iteration 178 : 0.5585283112897858
Loss in iteration 179 : 1.7387834435830252
Loss in iteration 180 : 1.154904989796408
Loss in iteration 181 : 0.8771277677235658
Loss in iteration 182 : 1.1706738616801466
Loss in iteration 183 : 0.7782480739665829
Loss in iteration 184 : 1.0187993617970044
Loss in iteration 185 : 0.7089306876298169
Loss in iteration 186 : 1.0788800158994512
Loss in iteration 187 : 0.6314078410575507
Loss in iteration 188 : 0.7743863596195858
Loss in iteration 189 : 0.8641855549463342
Loss in iteration 190 : 0.673979245532241
Loss in iteration 191 : 0.4732456572945576
Loss in iteration 192 : 0.6882287119215517
Loss in iteration 193 : 1.5442459411559692
Loss in iteration 194 : 0.5079117940603372
Loss in iteration 195 : 0.6505636010733307
Loss in iteration 196 : 1.3759355985627448
Loss in iteration 197 : 0.5724735939145672
Loss in iteration 198 : 1.319367573999449
Loss in iteration 199 : 1.397533332052672
Loss in iteration 200 : 1.098032506946162
Testing accuracy  of updater 7 on alg 0 with rate 20.0 = 0.7459001289847061, training accuracy 0.7475737100737101, time elapsed: 3336 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.4571286174805818
Loss in iteration 3 : 0.3848914265043542
Loss in iteration 4 : 0.3742091597686339
Loss in iteration 5 : 0.3525417492152412
Loss in iteration 6 : 0.36062246617714383
Loss in iteration 7 : 0.38733622898428494
Loss in iteration 8 : 0.3994647621879247
Loss in iteration 9 : 0.39550348469709506
Loss in iteration 10 : 0.3937623878750975
Loss in iteration 11 : 0.40030773180241946
Loss in iteration 12 : 0.4054509718590098
Loss in iteration 13 : 0.40126966289581506
Loss in iteration 14 : 0.39229854547923326
Loss in iteration 15 : 0.38805737632193593
Loss in iteration 16 : 0.3890075480484392
Loss in iteration 17 : 0.3868349205498553
Loss in iteration 18 : 0.3785439096113423
Loss in iteration 19 : 0.37089787175146766
Loss in iteration 20 : 0.36813136014490283
Loss in iteration 21 : 0.365012140222842
Loss in iteration 22 : 0.3575974363035819
Loss in iteration 23 : 0.35094257922971317
Loss in iteration 24 : 0.34927321936270544
Loss in iteration 25 : 0.34779040095841207
Loss in iteration 26 : 0.3432131115953464
Loss in iteration 27 : 0.3403926058628921
Loss in iteration 28 : 0.34070787187668755
Loss in iteration 29 : 0.3391953584847295
Loss in iteration 30 : 0.3357714166054552
Loss in iteration 31 : 0.3347087773582046
Loss in iteration 32 : 0.334672174066353
Loss in iteration 33 : 0.3326284953204358
Loss in iteration 34 : 0.33119093776156905
Loss in iteration 35 : 0.33166912058089987
Loss in iteration 36 : 0.3311561532965925
Loss in iteration 37 : 0.32973627023732344
Loss in iteration 38 : 0.32951922484396334
Loss in iteration 39 : 0.3293180871499061
Loss in iteration 40 : 0.32800272746952386
Loss in iteration 41 : 0.3271513130586997
Loss in iteration 42 : 0.3270575657506846
Loss in iteration 43 : 0.3264600695372311
Loss in iteration 44 : 0.32571126799022854
Loss in iteration 45 : 0.3256887664543014
Loss in iteration 46 : 0.32574257110275306
Loss in iteration 47 : 0.3253975007926151
Loss in iteration 48 : 0.3253248779182698
Loss in iteration 49 : 0.32554952171412954
Loss in iteration 50 : 0.3254258986456285
Loss in iteration 51 : 0.3251158215868079
Loss in iteration 52 : 0.3250288970518576
Loss in iteration 53 : 0.32485487797698415
Loss in iteration 54 : 0.32443645300199286
Loss in iteration 55 : 0.32416850902778466
Loss in iteration 56 : 0.32407356035803137
Loss in iteration 57 : 0.3238681173472884
Loss in iteration 58 : 0.32367077677427863
Loss in iteration 59 : 0.3236446549939223
Loss in iteration 60 : 0.32359261118733185
Loss in iteration 61 : 0.3234465544218126
Loss in iteration 62 : 0.3233867965744876
Loss in iteration 63 : 0.32338605378642415
Loss in iteration 64 : 0.3233158833123295
Loss in iteration 65 : 0.32326039478989843
Loss in iteration 66 : 0.3232794107614997
Loss in iteration 67 : 0.3232634811579196
Loss in iteration 68 : 0.32320392216761445
Loss in iteration 69 : 0.3231862946085535
Loss in iteration 70 : 0.3231752295493018
Loss in iteration 71 : 0.32312427751307615
Loss in iteration 72 : 0.32309253179119435
Loss in iteration 73 : 0.3230943599577211
Loss in iteration 74 : 0.3230759757258256
Loss in iteration 75 : 0.32304969401083666
Loss in iteration 76 : 0.32304864091321195
Loss in iteration 77 : 0.3230427941422301
Loss in iteration 78 : 0.3230184578297907
Loss in iteration 79 : 0.32300708833599284
Loss in iteration 80 : 0.32300569862265904
Loss in iteration 81 : 0.3229913118028904
Loss in iteration 82 : 0.32297796755620717
Loss in iteration 83 : 0.3229767174976131
Loss in iteration 84 : 0.32296976679088174
Loss in iteration 85 : 0.3229563403371262
Loss in iteration 86 : 0.3229511761101385
Loss in iteration 87 : 0.322947873800438
Loss in iteration 88 : 0.322938031224519
Loss in iteration 89 : 0.32293115887219603
Loss in iteration 90 : 0.32292892795270617
Loss in iteration 91 : 0.32292221042684177
Loss in iteration 92 : 0.3229138627136988
Loss in iteration 93 : 0.32290971540713415
Loss in iteration 94 : 0.3229046370009463
Loss in iteration 95 : 0.3228969877639652
Loss in iteration 96 : 0.32289222315652255
Loss in iteration 97 : 0.32288901140926624
Loss in iteration 98 : 0.3228836249889438
Loss in iteration 99 : 0.322879000385954
Loss in iteration 100 : 0.3228764396058029
Loss in iteration 101 : 0.3228727524188406
Loss in iteration 102 : 0.3228685470935865
Loss in iteration 103 : 0.3228660237301132
Loss in iteration 104 : 0.3228633860016914
Loss in iteration 105 : 0.32285965700962155
Loss in iteration 106 : 0.32285665770316585
Loss in iteration 107 : 0.3228540951758143
Loss in iteration 108 : 0.32285065882471786
Loss in iteration 109 : 0.3228474002254224
Loss in iteration 110 : 0.32284493870900893
Loss in iteration 111 : 0.3228421740659525
Loss in iteration 112 : 0.3228392228546627
Loss in iteration 113 : 0.32283685697877207
Loss in iteration 114 : 0.32283446586483716
Loss in iteration 115 : 0.322831691478536
Loss in iteration 116 : 0.32282920981302893
Loss in iteration 117 : 0.32282696169059516
Loss in iteration 118 : 0.3228244775268856
Loss in iteration 119 : 0.32282210022044905
Loss in iteration 120 : 0.3228200136217066
Loss in iteration 121 : 0.32281779805012234
Loss in iteration 122 : 0.3228154929203578
Loss in iteration 123 : 0.3228133789967539
Loss in iteration 124 : 0.3228112515262166
Loss in iteration 125 : 0.3228090114540958
Loss in iteration 126 : 0.3228069055687412
Loss in iteration 127 : 0.3228048925302259
Loss in iteration 128 : 0.3228028046166099
Loss in iteration 129 : 0.3228007708864434
Loss in iteration 130 : 0.32279884631597144
Loss in iteration 131 : 0.32279689048141763
Loss in iteration 132 : 0.32279494275934095
Loss in iteration 133 : 0.3227930982757536
Loss in iteration 134 : 0.3227912701140456
Loss in iteration 135 : 0.32278942646215053
Loss in iteration 136 : 0.3227876439090893
Loss in iteration 137 : 0.3227858870020883
Loss in iteration 138 : 0.32278410173511984
Loss in iteration 139 : 0.32278234336781125
Loss in iteration 140 : 0.32278062479501346
Loss in iteration 141 : 0.3227789010679857
Loss in iteration 142 : 0.32277719804457167
Loss in iteration 143 : 0.32277554374814116
Loss in iteration 144 : 0.32277390347398577
Loss in iteration 145 : 0.3227722752504781
Loss in iteration 146 : 0.32277068590110425
Loss in iteration 147 : 0.32276911726533786
Loss in iteration 148 : 0.3227675555165464
Loss in iteration 149 : 0.32276602119016756
Loss in iteration 150 : 0.32276451051574506
Loss in iteration 151 : 0.32276300721090373
Loss in iteration 152 : 0.32276152303156075
Loss in iteration 153 : 0.32276006312351746
Loss in iteration 154 : 0.3227586145559764
Loss in iteration 155 : 0.3227571813681002
Loss in iteration 156 : 0.3227557715176496
Loss in iteration 157 : 0.32275437626276404
Loss in iteration 158 : 0.32275299376163297
Loss in iteration 159 : 0.32275163120687733
Loss in iteration 160 : 0.32275028462904864
Loss in iteration 161 : 0.32274895020896255
Loss in iteration 162 : 0.32274763345784857
Loss in iteration 163 : 0.32274633388716867
Loss in iteration 164 : 0.3227450470620305
Loss in iteration 165 : 0.3227437756015333
Loss in iteration 166 : 0.32274252049413743
Loss in iteration 167 : 0.3227412779926949
Loss in iteration 168 : 0.32274004894141306
Testing accuracy  of updater 7 on alg 0 with rate 2.0 = 0.8500706344819114, training accuracy 0.8491707616707617, time elapsed: 2993 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6404678613530544
Loss in iteration 3 : 0.5688447374537989
Loss in iteration 4 : 0.5120586166122061
Loss in iteration 5 : 0.48164242183770345
Loss in iteration 6 : 0.469173252193568
Loss in iteration 7 : 0.4612995352832713
Loss in iteration 8 : 0.44914130183382334
Loss in iteration 9 : 0.4300527205709386
Loss in iteration 10 : 0.4061955870467355
Loss in iteration 11 : 0.38227801749235
Loss in iteration 12 : 0.3631048869961205
Loss in iteration 13 : 0.35155125929864395
Loss in iteration 14 : 0.34769643147114937
Loss in iteration 15 : 0.3493016689447494
Loss in iteration 16 : 0.3531014585462178
Loss in iteration 17 : 0.3561490270034401
Loss in iteration 18 : 0.3566834201021195
Loss in iteration 19 : 0.3543543113705268
Loss in iteration 20 : 0.34992705152167075
Loss in iteration 21 : 0.3447211491414679
Loss in iteration 22 : 0.3400366595275776
Loss in iteration 23 : 0.33674816466796553
Loss in iteration 24 : 0.335141339938998
Loss in iteration 25 : 0.33497072627953134
Loss in iteration 26 : 0.33565725677790365
Loss in iteration 27 : 0.3365289636550058
Loss in iteration 28 : 0.33702704229838815
Loss in iteration 29 : 0.33683281450791314
Loss in iteration 30 : 0.33590353724735944
Loss in iteration 31 : 0.33442900107370493
Loss in iteration 32 : 0.33273540591057005
Loss in iteration 33 : 0.331169577230261
Loss in iteration 34 : 0.32999628598213643
Loss in iteration 35 : 0.32933442942839436
Loss in iteration 36 : 0.3291448470439541
Loss in iteration 37 : 0.3292664985948335
Loss in iteration 38 : 0.32948357142396045
Loss in iteration 39 : 0.32959863919796184
Loss in iteration 40 : 0.3294884410508158
Loss in iteration 41 : 0.32912774021623
Loss in iteration 42 : 0.32857902718539966
Loss in iteration 43 : 0.32795696095557525
Loss in iteration 44 : 0.3273829611538893
Loss in iteration 45 : 0.32694589527886897
Loss in iteration 46 : 0.3266801202987207
Loss in iteration 47 : 0.32656461669359654
Loss in iteration 48 : 0.3265395733380093
Loss in iteration 49 : 0.32653196831750636
Loss in iteration 50 : 0.326480515102331
Loss in iteration 51 : 0.32635241047117053
Loss in iteration 52 : 0.3261482727790933
Loss in iteration 53 : 0.3258958993477532
Loss in iteration 54 : 0.32563671732294264
Loss in iteration 55 : 0.32541033418868154
Loss in iteration 56 : 0.32524227944604245
Loss in iteration 57 : 0.3251382377903041
Loss in iteration 58 : 0.3250855348461011
Loss in iteration 59 : 0.3250602215987897
Loss in iteration 60 : 0.32503656318490715
Loss in iteration 61 : 0.3249954613562203
Loss in iteration 62 : 0.32492924875580564
Loss in iteration 63 : 0.3248419116073161
Loss in iteration 64 : 0.32474546924681125
Loss in iteration 65 : 0.32465438656967827
Loss in iteration 66 : 0.32458021534850146
Loss in iteration 67 : 0.3245281875411498
Loss in iteration 68 : 0.32449652150587044
Loss in iteration 69 : 0.3244781661131735
Loss in iteration 70 : 0.3244639587228359
Loss in iteration 71 : 0.32444590300488446
Loss in iteration 72 : 0.3244194758323028
Loss in iteration 73 : 0.3243843903088847
Loss in iteration 74 : 0.32434384751092943
Loss in iteration 75 : 0.32430279329077205
Loss in iteration 76 : 0.3242659270760892
Loss in iteration 77 : 0.3242361583320031
Loss in iteration 78 : 0.3242139379205288
Loss in iteration 79 : 0.32419752911340544
Loss in iteration 80 : 0.3241839605751431
Loss in iteration 81 : 0.3241702214381152
Loss in iteration 82 : 0.32415425461743996
Loss in iteration 83 : 0.3241354506265071
Loss in iteration 84 : 0.32411456556329793
Loss in iteration 85 : 0.3240931952998568
Loss in iteration 86 : 0.3240730642951042
Loss in iteration 87 : 0.32405540318379283
Loss in iteration 88 : 0.32404060896662856
Loss in iteration 89 : 0.32402825025695914
Loss in iteration 90 : 0.32401735182932845
Loss in iteration 91 : 0.3240068106104899
Loss in iteration 92 : 0.3239957783878363
Loss in iteration 93 : 0.32398388831610875
Loss in iteration 94 : 0.3239712777215996
Loss in iteration 95 : 0.3239584371444722
Loss in iteration 96 : 0.3239459686662644
Loss in iteration 97 : 0.3239343519730631
Loss in iteration 98 : 0.3239237959748731
Loss in iteration 99 : 0.3239142105858633
Loss in iteration 100 : 0.32390528613062786
Loss in iteration 101 : 0.3238966334775979
Loss in iteration 102 : 0.32388792583857134
Loss in iteration 103 : 0.3238789934031752
Loss in iteration 104 : 0.3238698470791308
Loss in iteration 105 : 0.3238606362817754
Loss in iteration 106 : 0.3238515674469737
Loss in iteration 107 : 0.32384281851066865
Loss in iteration 108 : 0.32383447946460076
Loss in iteration 109 : 0.32382653455476096
Loss in iteration 110 : 0.3238188844851862
Loss in iteration 111 : 0.32381139354024796
Loss in iteration 112 : 0.3238039407714333
Loss in iteration 113 : 0.3237964568645758
Loss in iteration 114 : 0.32378893663435404
Loss in iteration 115 : 0.3237814273387852
Loss in iteration 116 : 0.3237740012958163
Loss in iteration 117 : 0.3237667251280504
Loss in iteration 118 : 0.3237596368529083
Loss in iteration 119 : 0.3237527372601593
Loss in iteration 120 : 0.3237459958919689
Loss in iteration 121 : 0.32373936682559595
Loss in iteration 122 : 0.3237328069428493
Loss in iteration 123 : 0.32372628985492685
Loss in iteration 124 : 0.32371981142274364
Loss in iteration 125 : 0.3237133864910092
Loss in iteration 126 : 0.3237070395917789
Loss in iteration 127 : 0.3237007939735368
Loss in iteration 128 : 0.3236946630967581
Loss in iteration 129 : 0.3236886471136729
Loss in iteration 130 : 0.32368273464422226
Loss in iteration 131 : 0.3236769082505489
Loss in iteration 132 : 0.3236711510129771
Loss in iteration 133 : 0.32366545170421346
Loss in iteration 134 : 0.3236598070119133
Loss in iteration 135 : 0.3236542205812849
Loss in iteration 136 : 0.3236486998078967
Loss in iteration 137 : 0.32364325192954313
Loss in iteration 138 : 0.32363788092641216
Loss in iteration 139 : 0.3236325861746943
Loss in iteration 140 : 0.32362736300519995
Loss in iteration 141 : 0.3236222046166545
Loss in iteration 142 : 0.32361710441495173
Loss in iteration 143 : 0.3236120578690378
Loss in iteration 144 : 0.32360706331258426
Loss in iteration 145 : 0.32360212159926083
Loss in iteration 146 : 0.32359723494354603
Loss in iteration 147 : 0.32359240550742774
Loss in iteration 148 : 0.32358763428056475
Loss in iteration 149 : 0.32358292059650784
Loss in iteration 150 : 0.3235782623380536
Loss in iteration 151 : 0.32357365662961385
Loss in iteration 152 : 0.3235691006783759
Loss in iteration 153 : 0.3235645924354734
Loss in iteration 154 : 0.32356013087375063
Loss in iteration 155 : 0.3235557158534966
Loss in iteration 156 : 0.3235513477007412
Loss in iteration 157 : 0.3235470267026752
Loss in iteration 158 : 0.3235427527170474
Loss in iteration 159 : 0.32353852501506414
Loss in iteration 160 : 0.3235343423713202
Loss in iteration 161 : 0.32353020332286925
Loss in iteration 162 : 0.3235261064727602
Loss in iteration 163 : 0.32352205072028356
Loss in iteration 164 : 0.3235180353482115
Loss in iteration 165 : 0.3235140599617035
Loss in iteration 166 : 0.32351012432825255
Loss in iteration 167 : 0.32350622819454944
Loss in iteration 168 : 0.32350237115070923
Loss in iteration 169 : 0.323498552581942
Loss in iteration 170 : 0.32349477170877533
Loss in iteration 171 : 0.32349102768445914
Loss in iteration 172 : 0.32348731970325234
Loss in iteration 173 : 0.3234836470779095
Loss in iteration 174 : 0.3234800092635451
Loss in iteration 175 : 0.3234764058289737
Loss in iteration 176 : 0.3234728363955254
Loss in iteration 177 : 0.32346930057167755
Loss in iteration 178 : 0.3234657979080783
Loss in iteration 179 : 0.3234623278856426
Loss in iteration 180 : 0.32345888993493205
Loss in iteration 181 : 0.3234554834738631
Loss in iteration 182 : 0.3234521079464887
Loss in iteration 183 : 0.3234487628483421
Loss in iteration 184 : 0.3234454477314775
Loss in iteration 185 : 0.3234421621910394
Loss in iteration 186 : 0.3234389058416794
Loss in iteration 187 : 0.3234356782943786
Loss in iteration 188 : 0.32343247914205403
Loss in iteration 189 : 0.323429307957592
Loss in iteration 190 : 0.3234261643026873
Loss in iteration 191 : 0.32342304774210495
Loss in iteration 192 : 0.323419957857006
Loss in iteration 193 : 0.32341689425249714
Loss in iteration 194 : 0.32341385655757054
Loss in iteration 195 : 0.3234108444188289
Loss in iteration 196 : 0.3234078574913762
Loss in iteration 197 : 0.3234048954307725
Loss in iteration 198 : 0.3234019578887733
Loss in iteration 199 : 0.323399044513699
Loss in iteration 200 : 0.3233961549543931
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999996 = 0.8502548983477674, training accuracy 0.8491707616707617, time elapsed: 3387 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 7.434632541737809
Loss in iteration 3 : 3.4293777692983247
Loss in iteration 4 : 0.843095364075567
Loss in iteration 5 : 1.5572374051871414
Loss in iteration 6 : 0.9957749837758284
Loss in iteration 7 : 0.9732734920910787
Loss in iteration 8 : 0.9540648747848232
Loss in iteration 9 : 0.919120455123847
Loss in iteration 10 : 0.8768635421362201
Loss in iteration 11 : 0.8340752742016637
Loss in iteration 12 : 0.7918099695777105
Loss in iteration 13 : 0.7501836445704618
Loss in iteration 14 : 0.7077694907683769
Loss in iteration 15 : 0.6625450537381578
Loss in iteration 16 : 0.6146116914048152
Loss in iteration 17 : 0.568233518925555
Loss in iteration 18 : 0.5291097243139521
Loss in iteration 19 : 0.49713161227633773
Loss in iteration 20 : 0.46737305167202453
Loss in iteration 21 : 0.4391157193888731
Loss in iteration 22 : 0.4149682339921083
Loss in iteration 23 : 0.39632281712622863
Loss in iteration 24 : 0.38450667511854375
Loss in iteration 25 : 0.4184583511522844
Loss in iteration 26 : 1.0274605974579636
Loss in iteration 27 : 1.8099463304721022
Loss in iteration 28 : 1.863933187897511
Loss in iteration 29 : 0.8591600233672467
Loss in iteration 30 : 0.6510896048973653
Loss in iteration 31 : 0.696766460113187
Loss in iteration 32 : 0.6518190696888404
Loss in iteration 33 : 0.6251139761349255
Loss in iteration 34 : 0.5930439339012371
Loss in iteration 35 : 0.5567906871008059
Loss in iteration 36 : 0.5210997932489461
Loss in iteration 37 : 0.4870964363192364
Loss in iteration 38 : 0.45459657666076114
Loss in iteration 39 : 0.4239684010434744
Loss in iteration 40 : 0.39762359060168023
Loss in iteration 41 : 0.3784681683529053
Loss in iteration 42 : 0.3651461119884303
Loss in iteration 43 : 0.35347763057501275
Loss in iteration 44 : 0.34410046839101827
Loss in iteration 45 : 0.33950791902955296
Loss in iteration 46 : 0.34154043335485246
Loss in iteration 47 : 0.38733867503694314
Loss in iteration 48 : 0.9076912599680053
Loss in iteration 49 : 2.3221169368670784
Loss in iteration 50 : 0.9557740682865956
Loss in iteration 51 : 0.736144762501751
Loss in iteration 52 : 0.5329236512472694
Loss in iteration 53 : 0.5276910880249291
Loss in iteration 54 : 0.5029667615715885
Loss in iteration 55 : 0.4739582175580983
Loss in iteration 56 : 0.4451817882445111
Loss in iteration 57 : 0.42032516102182665
Loss in iteration 58 : 0.3996483321791606
Loss in iteration 59 : 0.3818396490224425
Loss in iteration 60 : 0.36674058951600824
Loss in iteration 61 : 0.3546698926867317
Loss in iteration 62 : 0.34493040232580985
Loss in iteration 63 : 0.33750322353360046
Loss in iteration 64 : 0.334206703370556
Loss in iteration 65 : 0.34543864672585634
Loss in iteration 66 : 0.5078790789338953
Loss in iteration 67 : 1.5272369287811014
Loss in iteration 68 : 0.40012865849584256
Loss in iteration 69 : 0.37649889355649696
Loss in iteration 70 : 0.41141839472649583
Loss in iteration 71 : 0.49080775922821535
Loss in iteration 72 : 0.6436818853996796
Loss in iteration 73 : 1.0707840394703674
Loss in iteration 74 : 0.46122045838170384
Loss in iteration 75 : 0.4267073040029695
Loss in iteration 76 : 0.43559244959512344
Loss in iteration 77 : 0.4473806498958046
Loss in iteration 78 : 0.4617718373628205
Loss in iteration 79 : 0.5809945822029534
Loss in iteration 80 : 0.6097032991631269
Loss in iteration 81 : 0.9100692081724354
Loss in iteration 82 : 0.4918789926287348
Loss in iteration 83 : 0.48374451213263664
Loss in iteration 84 : 0.47436741591432624
Loss in iteration 85 : 0.5100832304203068
Loss in iteration 86 : 0.4986298031966374
Loss in iteration 87 : 0.6159094743882477
Loss in iteration 88 : 0.5737011643743383
Loss in iteration 89 : 0.7469350582295713
Loss in iteration 90 : 0.5251210522583583
Loss in iteration 91 : 0.5492857061803054
Loss in iteration 92 : 0.4837175109402753
Loss in iteration 93 : 0.5067128121362732
Loss in iteration 94 : 0.47952251084080266
Loss in iteration 95 : 0.5601388251342396
Loss in iteration 96 : 0.5425790309072341
Loss in iteration 97 : 0.7141715286007833
Loss in iteration 98 : 0.5306476391757772
Loss in iteration 99 : 0.5914420798463706
Loss in iteration 100 : 0.4942035517437783
Loss in iteration 101 : 0.5159891720651443
Loss in iteration 102 : 0.4692809695869222
Loss in iteration 103 : 0.5126760616203839
Loss in iteration 104 : 0.4965977919311918
Loss in iteration 105 : 0.6167156500461679
Loss in iteration 106 : 0.543105393429546
Loss in iteration 107 : 0.6735205823464088
Loss in iteration 108 : 0.5053874823451167
Loss in iteration 109 : 0.5286667982290949
Loss in iteration 110 : 0.46784953526529505
Loss in iteration 111 : 0.4900446870218632
Loss in iteration 112 : 0.46679804841844436
Loss in iteration 113 : 0.5437424752048108
Loss in iteration 114 : 0.5269960565679578
Loss in iteration 115 : 0.6884706485326862
Loss in iteration 116 : 0.5193926469593386
Loss in iteration 117 : 0.5759474814210267
Loss in iteration 118 : 0.4818352382321202
Loss in iteration 119 : 0.49810365768301235
Loss in iteration 120 : 0.45594399163101246
Loss in iteration 121 : 0.4939690339562062
Loss in iteration 122 : 0.48343041527372865
Loss in iteration 123 : 0.6022553809749226
Loss in iteration 124 : 0.5374538292680853
Loss in iteration 125 : 0.675019691636333
Loss in iteration 126 : 0.4957721812308084
Loss in iteration 127 : 0.5111448137789145
Loss in iteration 128 : 0.455611401134427
Loss in iteration 129 : 0.47081143149160676
Loss in iteration 130 : 0.4518151549403625
Loss in iteration 131 : 0.5219206699443811
Loss in iteration 132 : 0.5187758480786329
Loss in iteration 133 : 0.6934938511760977
Loss in iteration 134 : 0.5156430598545291
Loss in iteration 135 : 0.5735280862751807
Loss in iteration 136 : 0.4760161186018992
Loss in iteration 137 : 0.4855261619546988
Loss in iteration 138 : 0.4443687160072684
Loss in iteration 139 : 0.47217181068400244
Loss in iteration 140 : 0.4668633314070357
Loss in iteration 141 : 0.5787896072976596
Loss in iteration 142 : 0.5389880015108506
Loss in iteration 143 : 0.7045027713808604
Loss in iteration 144 : 0.4898070199035759
Loss in iteration 145 : 0.49764102136698307
Loss in iteration 146 : 0.4475531345999014
Loss in iteration 147 : 0.4563586281734032
Loss in iteration 148 : 0.4391009474046082
Loss in iteration 149 : 0.5001519990667334
Loss in iteration 150 : 0.5097229554461695
Loss in iteration 151 : 0.6963943518161603
Loss in iteration 152 : 0.5178195282111165
Loss in iteration 153 : 0.5864977317512603
Loss in iteration 154 : 0.475011858238935
Loss in iteration 155 : 0.47805148226119276
Loss in iteration 156 : 0.4357841455872953
Loss in iteration 157 : 0.452520639125504
Loss in iteration 158 : 0.448990030326445
Loss in iteration 159 : 0.5471699073207578
Loss in iteration 160 : 0.5397142293601448
Loss in iteration 161 : 0.7429188610155651
Loss in iteration 162 : 0.48536450585897833
Loss in iteration 163 : 0.4903432117426649
Loss in iteration 164 : 0.44516563386492025
Loss in iteration 165 : 0.45140152829489
Loss in iteration 166 : 0.43243290713944
Loss in iteration 167 : 0.4846710055881992
Loss in iteration 168 : 0.49751771188432703
Loss in iteration 169 : 0.6788196257575663
Loss in iteration 170 : 0.5237493417218356
Loss in iteration 171 : 0.6124412556446237
Loss in iteration 172 : 0.47540494640811853
Loss in iteration 173 : 0.4726066008603672
Loss in iteration 174 : 0.4300606772849849
Loss in iteration 175 : 0.4384019156139723
Loss in iteration 176 : 0.43443321790363576
Loss in iteration 177 : 0.5182996511417468
Loss in iteration 178 : 0.5356326486868066
Loss in iteration 179 : 0.7688451017383315
Loss in iteration 180 : 0.484148418236935
Loss in iteration 181 : 0.4928410361471698
Loss in iteration 182 : 0.4479999524958731
Loss in iteration 183 : 0.4540826998729291
Loss in iteration 184 : 0.43037381989831974
Loss in iteration 185 : 0.473429176123869
Loss in iteration 186 : 0.4827819915575846
Loss in iteration 187 : 0.6438333225629113
Loss in iteration 188 : 0.530414216903277
Loss in iteration 189 : 0.6470683993815648
Loss in iteration 190 : 0.4755384492787565
Loss in iteration 191 : 0.4686280251726216
Loss in iteration 192 : 0.42720387360008544
Loss in iteration 193 : 0.4306336264640975
Loss in iteration 194 : 0.4251331490937028
Loss in iteration 195 : 0.49754111729506884
Loss in iteration 196 : 0.5274173549361955
Loss in iteration 197 : 0.7736339407419279
Loss in iteration 198 : 0.48804878476204355
Loss in iteration 199 : 0.5070994191270758
Loss in iteration 200 : 0.4541684349399916
Testing accuracy  of updater 8 on alg 0 with rate 2.0 = 0.834653891038634, training accuracy 0.8328316953316953, time elapsed: 3361 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.46322741321081484
Loss in iteration 3 : 0.4425958530140575
Loss in iteration 4 : 0.4179458895658843
Loss in iteration 5 : 0.3828888065489943
Loss in iteration 6 : 0.3540250669677502
Loss in iteration 7 : 0.34012309394453305
Loss in iteration 8 : 0.33745453096906525
Loss in iteration 9 : 0.3386425541058038
Loss in iteration 10 : 0.33934365497123026
Loss in iteration 11 : 0.33876946827618226
Loss in iteration 12 : 0.3377165932829056
Loss in iteration 13 : 0.33696036690977665
Loss in iteration 14 : 0.33669966500149284
Loss in iteration 15 : 0.3366974197462846
Loss in iteration 16 : 0.336615367117594
Loss in iteration 17 : 0.33624747920977016
Loss in iteration 18 : 0.3355790546732943
Loss in iteration 19 : 0.33472332099315527
Loss in iteration 20 : 0.3338211821475537
Loss in iteration 21 : 0.3329695213582111
Loss in iteration 22 : 0.33220228953400915
Loss in iteration 23 : 0.33151177318201136
Loss in iteration 24 : 0.33088078773454965
Loss in iteration 25 : 0.3303021990026082
Loss in iteration 26 : 0.32977950468640116
Loss in iteration 27 : 0.32931675725146575
Loss in iteration 28 : 0.3289097602434072
Loss in iteration 29 : 0.3285449023044346
Loss in iteration 30 : 0.32820436284607524
Loss in iteration 31 : 0.3278725964954115
Loss in iteration 32 : 0.3275400383689214
Loss in iteration 33 : 0.3272032374114264
Loss in iteration 34 : 0.3268630365373617
Loss in iteration 35 : 0.3265227249532554
Loss in iteration 36 : 0.3261869998398595
Loss in iteration 37 : 0.32586147640615576
Loss in iteration 38 : 0.32555217311546564
Loss in iteration 39 : 0.3252647412638461
Loss in iteration 40 : 0.3250036354673632
Loss in iteration 41 : 0.3247715474992877
Loss in iteration 42 : 0.32456926407302056
Loss in iteration 43 : 0.3243958868498705
Loss in iteration 44 : 0.324249243750234
Loss in iteration 45 : 0.32412634096643067
Loss in iteration 46 : 0.3240237769612465
Loss in iteration 47 : 0.3239380926071744
Loss in iteration 48 : 0.32386604793065876
Loss in iteration 49 : 0.32380481498474234
Loss in iteration 50 : 0.3237520795102668
Loss in iteration 51 : 0.32370605639901534
Loss in iteration 52 : 0.32366543810680687
Loss in iteration 53 : 0.3236293028301782
Loss in iteration 54 : 0.3235970083368752
Loss in iteration 55 : 0.32356809102731743
Loss in iteration 56 : 0.3235421823647058
Loss in iteration 57 : 0.3235189486277492
Loss in iteration 58 : 0.3234980553283668
Loss in iteration 59 : 0.3234791541280745
Loss in iteration 60 : 0.32346188746384164
Loss in iteration 61 : 0.32344590453831434
Loss in iteration 62 : 0.3234308820380722
Loss in iteration 63 : 0.32341654381447227
Loss in iteration 64 : 0.32340267539271866
Loss in iteration 65 : 0.3233891310709126
Loss in iteration 66 : 0.32337583313349955
Loss in iteration 67 : 0.3233627640988978
Loss in iteration 68 : 0.32334995385082715
Loss in iteration 69 : 0.3233374639545487
Loss in iteration 70 : 0.3233253714762531
Loss in iteration 71 : 0.32331375429461784
Loss in iteration 72 : 0.32330267933921547
Loss in iteration 73 : 0.3232921945431409
Loss in iteration 74 : 0.3232823246750284
Loss in iteration 75 : 0.32327307070456235
Loss in iteration 76 : 0.32326441200197675
Loss in iteration 77 : 0.3232563104876562
Loss in iteration 78 : 0.32324871581672193
Loss in iteration 79 : 0.32324157077337473
Loss in iteration 80 : 0.32323481622014366
Loss in iteration 81 : 0.3232283951571819
Loss in iteration 82 : 0.32322225565985746
Loss in iteration 83 : 0.32321635265210263
Loss in iteration 84 : 0.32321064862002646
Loss in iteration 85 : 0.323205113467868
Loss in iteration 86 : 0.3231997237664858
Loss in iteration 87 : 0.32319446164998394
Loss in iteration 88 : 0.32318931358850705
Loss in iteration 89 : 0.3231842692168126
Loss in iteration 90 : 0.3231793203398089
Loss in iteration 91 : 0.32317446017812806
Loss in iteration 92 : 0.3231696828660094
Loss in iteration 93 : 0.3231649831751138
Loss in iteration 94 : 0.3231603564133997
Loss in iteration 95 : 0.32315579843746084
Loss in iteration 96 : 0.32315130571770845
Loss in iteration 97 : 0.3231468754055585
Loss in iteration 98 : 0.3231425053665407
Loss in iteration 99 : 0.32313819415997114
Loss in iteration 100 : 0.3231339409612399
Loss in iteration 101 : 0.32312974543533673
Loss in iteration 102 : 0.3231256075784407
Loss in iteration 103 : 0.32312152754808027
Loss in iteration 104 : 0.323117505502228
Loss in iteration 105 : 0.32311354146406
Loss in iteration 106 : 0.3231096352238397
Loss in iteration 107 : 0.3231057862830799
Loss in iteration 108 : 0.3231019938402904
Loss in iteration 109 : 0.3230982568126921
Loss in iteration 110 : 0.32309457388504975
Loss in iteration 111 : 0.32309094357510976
Loss in iteration 112 : 0.32308736430525326
Loss in iteration 113 : 0.3230838344711301
Loss in iteration 114 : 0.3230803525004255
Loss in iteration 115 : 0.3230769168972958
Loss in iteration 116 : 0.32307352627080954
Loss in iteration 117 : 0.32307017934782467
Loss in iteration 118 : 0.32306687497259134
Loss in iteration 119 : 0.3230636120963347
Loss in iteration 120 : 0.3230603897605595
Loss in iteration 121 : 0.3230572070776116
Loss in iteration 122 : 0.3230540632115889
Loss in iteration 123 : 0.3230509573618014
Loss in iteration 124 : 0.3230478887501079
Loss in iteration 125 : 0.32304485661269544
Loss in iteration 126 : 0.3230418601960416
Loss in iteration 127 : 0.32303889875641206
Loss in iteration 128 : 0.323035971561879
Loss in iteration 129 : 0.32303307789580654
Loss in iteration 130 : 0.32303021706081675
Loss in iteration 131 : 0.32302738838239015
Loss in iteration 132 : 0.3230245912115936
Loss in iteration 133 : 0.32302182492659015
Loss in iteration 134 : 0.32301908893288067
Loss in iteration 135 : 0.3230163826623748
Loss in iteration 136 : 0.32301370557156445
Loss in iteration 137 : 0.32301105713906103
Loss in iteration 138 : 0.3230084368628218
Loss in iteration 139 : 0.3230058442573647
Loss in iteration 140 : 0.3230032788511514
Loss in iteration 141 : 0.32300074018431996
Loss in iteration 142 : 0.3229982278068536
Loss in iteration 143 : 0.3229957412771873
Loss in iteration 144 : 0.3229932801612322
Loss in iteration 145 : 0.32299084403177303
Loss in iteration 146 : 0.32298843246815834
Loss in iteration 147 : 0.3229860450562013
Loss in iteration 148 : 0.3229836813882347
Loss in iteration 149 : 0.32298134106324905
Loss in iteration 150 : 0.32297902368702563
Loss in iteration 151 : 0.3229767288723445
Loss in iteration 152 : 0.3229744562390938
Loss in iteration 153 : 0.32297220541439875
Loss in iteration 154 : 0.3229699760326619
Loss in iteration 155 : 0.32296776773561897
Loss in iteration 156 : 0.32296558017229793
Loss in iteration 157 : 0.32296341299899106
Loss in iteration 158 : 0.32296126587916046
Loss in iteration 159 : 0.32295913848335744
Loss in iteration 160 : 0.3229570304890907
Loss in iteration 161 : 0.32295494158069304
Loss in iteration 162 : 0.32295287144919305
Loss in iteration 163 : 0.3229508197921497
Loss in iteration 164 : 0.32294878631351914
Loss in iteration 165 : 0.322946770723492
Loss in iteration 166 : 0.32294477273835237
Loss in iteration 167 : 0.3229427920803359
Loss in iteration 168 : 0.322940828477482
Loss in iteration 169 : 0.32293888166351076
Loss in iteration 170 : 0.32293695137768375
Loss in iteration 171 : 0.3229350373646794
Loss in iteration 172 : 0.32293313937448687
Testing accuracy  of updater 8 on alg 0 with rate 0.2 = 0.8501320557705301, training accuracy 0.8488022113022113, time elapsed: 3028 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6399329623377626
Loss in iteration 3 : 0.5849330982073004
Loss in iteration 4 : 0.5373353275567981
Loss in iteration 5 : 0.5008639005754826
Loss in iteration 6 : 0.4752202048237372
Loss in iteration 7 : 0.4580401850042155
Loss in iteration 8 : 0.44645791359772213
Loss in iteration 9 : 0.43797204782324645
Loss in iteration 10 : 0.4307609339195016
Loss in iteration 11 : 0.42369264488874525
Loss in iteration 12 : 0.4162084281173235
Loss in iteration 13 : 0.4081731936805715
Loss in iteration 14 : 0.3997322463597172
Loss in iteration 15 : 0.3911873819011953
Loss in iteration 16 : 0.38289582718010334
Loss in iteration 17 : 0.3751930802604457
Loss in iteration 18 : 0.36834023604379834
Loss in iteration 19 : 0.36249550330041835
Loss in iteration 20 : 0.35770784148640383
Loss in iteration 21 : 0.35392851540164255
Loss in iteration 22 : 0.35103471693763105
Loss in iteration 23 : 0.3488588193095707
Loss in iteration 24 : 0.3472174202797986
Loss in iteration 25 : 0.3459358023937289
Loss in iteration 26 : 0.3448653051190829
Loss in iteration 27 : 0.343892900782803
Loss in iteration 28 : 0.3429436744680884
Loss in iteration 29 : 0.34197778862179695
Loss in iteration 30 : 0.3409838769792777
Loss in iteration 31 : 0.33997076373340024
Loss in iteration 32 : 0.33895908439539346
Loss in iteration 33 : 0.3379739328617952
Loss in iteration 34 : 0.3370391864636366
Loss in iteration 35 : 0.3361737434440637
Loss in iteration 36 : 0.3353895870801009
Loss in iteration 37 : 0.3346913808351193
Loss in iteration 38 : 0.3340771925622487
Loss in iteration 39 : 0.3335399238288654
Loss in iteration 40 : 0.3330690586970786
Loss in iteration 41 : 0.33265242062615674
Loss in iteration 42 : 0.33227771579716076
Loss in iteration 43 : 0.3319337303505562
Loss in iteration 44 : 0.3316111273514371
Loss in iteration 45 : 0.33130285109030205
Loss in iteration 46 : 0.3310041896809399
Loss in iteration 47 : 0.3307125725609402
Loss in iteration 48 : 0.3304271897985834
Loss in iteration 49 : 0.33014851825707353
Loss in iteration 50 : 0.32987782911377433
Loss in iteration 51 : 0.32961673530095614
Loss in iteration 52 : 0.3293668190816157
Loss in iteration 53 : 0.329129361652562
Loss in iteration 54 : 0.3289051802523426
Loss in iteration 55 : 0.32869456501931915
Loss in iteration 56 : 0.328497298483355
Loss in iteration 57 : 0.3283127352368208
Loss in iteration 58 : 0.32813991772095136
Loss in iteration 59 : 0.32797770554769606
Loss in iteration 60 : 0.32782489951288546
Loss in iteration 61 : 0.3276803465224255
Loss in iteration 62 : 0.3275430171690205
Loss in iteration 63 : 0.32741205291640785
Loss in iteration 64 : 0.3272867842204809
Loss in iteration 65 : 0.32716672411244563
Loss in iteration 66 : 0.32705154367378003
Loss in iteration 67 : 0.32694103651375794
Loss in iteration 68 : 0.3268350790194933
Loss in iteration 69 : 0.32673359206626323
Loss in iteration 70 : 0.326636508358463
Loss in iteration 71 : 0.3265437479071308
Loss in iteration 72 : 0.32645520257618243
Loss in iteration 73 : 0.3263707293170301
Loss in iteration 74 : 0.32629015075694595
Loss in iteration 75 : 0.326213261241337
Loss in iteration 76 : 0.32613983622881687
Loss in iteration 77 : 0.32606964303812636
Loss in iteration 78 : 0.326002451263049
Loss in iteration 79 : 0.3259380416153624
Loss in iteration 80 : 0.32587621244286663
Loss in iteration 81 : 0.3258167836306946
Loss in iteration 82 : 0.3257595979805325
Loss in iteration 83 : 0.32570452044525344
Loss in iteration 84 : 0.3256514357670092
Loss in iteration 85 : 0.32560024513069885
Loss in iteration 86 : 0.3255508624201948
Loss in iteration 87 : 0.3255032105751092
Loss in iteration 88 : 0.3254572184177174
Loss in iteration 89 : 0.32541281817716056
Loss in iteration 90 : 0.3253699438018581
Loss in iteration 91 : 0.3253285300361811
Loss in iteration 92 : 0.3252885121523612
Loss in iteration 93 : 0.32524982617721415
Loss in iteration 94 : 0.32521240943303387
Loss in iteration 95 : 0.3251762012189702
Loss in iteration 96 : 0.3251411434852421
Loss in iteration 97 : 0.3251071813906479
Loss in iteration 98 : 0.3250742636760766
Loss in iteration 99 : 0.32504234282725347
Loss in iteration 100 : 0.3250113750339361
Loss in iteration 101 : 0.32498131997782853
Loss in iteration 102 : 0.32495214049650223
Loss in iteration 103 : 0.3249238021761553
Loss in iteration 104 : 0.32489627292361584
Loss in iteration 105 : 0.3248695225599813
Loss in iteration 106 : 0.3248435224667012
Loss in iteration 107 : 0.3248182453022642
Loss in iteration 108 : 0.3247936647957329
Loss in iteration 109 : 0.3247697556132794
Loss in iteration 110 : 0.32474649328678035
Loss in iteration 111 : 0.32472385418924277
Loss in iteration 112 : 0.3247018155403964
Loss in iteration 113 : 0.32468035542685625
Loss in iteration 114 : 0.3246594528236518
Loss in iteration 115 : 0.3246390876076197
Loss in iteration 116 : 0.32461924055687363
Loss in iteration 117 : 0.32459989333419464
Loss in iteration 118 : 0.32458102845515163
Loss in iteration 119 : 0.3245626292437704
Loss in iteration 120 : 0.3245446797799454
Loss in iteration 121 : 0.3245271648429835
Loss in iteration 122 : 0.32451006985548686
Loss in iteration 123 : 0.32449338083101176
Loss in iteration 124 : 0.3244770843277335
Loss in iteration 125 : 0.32446116740951
Loss in iteration 126 : 0.3244456176144389
Loss in iteration 127 : 0.32443042293038254
Loss in iteration 128 : 0.3244155717762459
Loss in iteration 129 : 0.32440105298756977
Loss in iteration 130 : 0.3243868558048877
Loss in iteration 131 : 0.32437296986346553
Loss in iteration 132 : 0.3243593851833071
Loss in iteration 133 : 0.32434609215866916
Loss in iteration 134 : 0.32433308154657736
Loss in iteration 135 : 0.32432034445432806
Loss in iteration 136 : 0.32430787232601
Loss in iteration 137 : 0.324295656928408
Loss in iteration 138 : 0.324283690336609
Loss in iteration 139 : 0.3242719649197511
Loss in iteration 140 : 0.32426047332725194
Loss in iteration 141 : 0.32424920847574384
Loss in iteration 142 : 0.324238163536951
Loss in iteration 143 : 0.3242273319265133
Loss in iteration 144 : 0.3242167072937789
Loss in iteration 145 : 0.32420628351247494
Loss in iteration 146 : 0.3241960546720856
Loss in iteration 147 : 0.3241860150698863
Loss in iteration 148 : 0.3241761592033698
Loss in iteration 149 : 0.3241664817630533
Loss in iteration 150 : 0.3241569776254902
Loss in iteration 151 : 0.32414764184647415
Loss in iteration 152 : 0.32413846965436244
Loss in iteration 153 : 0.32412945644353586
Loss in iteration 154 : 0.3241205977680154
Loss in iteration 155 : 0.3241118893352288
Loss in iteration 156 : 0.3241033269999666
Loss in iteration 157 : 0.32409490675858166
Loss in iteration 158 : 0.32408662474341504
Loss in iteration 159 : 0.32407847721746863
Loss in iteration 160 : 0.32407046056936245
Loss in iteration 161 : 0.32406257130850813
Loss in iteration 162 : 0.3240548060605475
Loss in iteration 163 : 0.3240471615630073
Loss in iteration 164 : 0.32403963466114777
Loss in iteration 165 : 0.32403222230400786
Loss in iteration 166 : 0.3240249215406061
Loss in iteration 167 : 0.32401772951630264
Loss in iteration 168 : 0.324010643469281
Loss in iteration 169 : 0.32400366072716974
Loss in iteration 170 : 0.32399677870380206
Loss in iteration 171 : 0.32398999489604186
Loss in iteration 172 : 0.3239833068808059
Loss in iteration 173 : 0.3239767123121035
Loss in iteration 174 : 0.3239702089182765
Loss in iteration 175 : 0.32396379449928325
Loss in iteration 176 : 0.3239574669241364
Loss in iteration 177 : 0.32395122412841704
Loss in iteration 178 : 0.32394506411188917
Loss in iteration 179 : 0.3239389849362449
Loss in iteration 180 : 0.3239329847228924
Loss in iteration 181 : 0.32392706165086965
Loss in iteration 182 : 0.32392121395482226
Loss in iteration 183 : 0.3239154399230661
Loss in iteration 184 : 0.32390973789571836
Loss in iteration 185 : 0.3239041062629085
Loss in iteration 186 : 0.32389854346303576
Loss in iteration 187 : 0.32389304798112306
Loss in iteration 188 : 0.3238876183472038
Loss in iteration 189 : 0.32388225313477587
Loss in iteration 190 : 0.3238769509593242
Loss in iteration 191 : 0.32387171047687685
Loss in iteration 192 : 0.32386653038263463
Loss in iteration 193 : 0.3238614094096427
Loss in iteration 194 : 0.3238563463275136
Loss in iteration 195 : 0.3238513399411838
Loss in iteration 196 : 0.32384638908974583
Loss in iteration 197 : 0.32384149264529444
Loss in iteration 198 : 0.32383664951183183
Loss in iteration 199 : 0.3238318586241969
Loss in iteration 200 : 0.3238271189470579
Testing accuracy  of updater 8 on alg 0 with rate 0.01999999999999999 = 0.8501320557705301, training accuracy 0.8492014742014742, time elapsed: 4082 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 19.370822584528653
Loss in iteration 3 : 13.741824437046205
Loss in iteration 4 : 3.5982748803758895
Loss in iteration 5 : 12.279619196380287
Loss in iteration 6 : 11.510445664214062
Loss in iteration 7 : 6.068500112873534
Loss in iteration 8 : 7.0991080788984835
Loss in iteration 9 : 9.914449891531314
Loss in iteration 10 : 10.369821537127784
Loss in iteration 11 : 8.371464732113738
Loss in iteration 12 : 6.3880441735299955
Loss in iteration 13 : 6.8053391423911
Loss in iteration 14 : 8.40094547912981
Loss in iteration 15 : 8.012728416643872
Loss in iteration 16 : 6.056005138710548
Loss in iteration 17 : 5.412710536453648
Loss in iteration 18 : 6.485414954767418
Loss in iteration 19 : 6.6616511353501755
Loss in iteration 20 : 5.156236036980289
Loss in iteration 21 : 4.216842960571525
Loss in iteration 22 : 5.342104519261457
Loss in iteration 23 : 4.740943641370825
Loss in iteration 24 : 3.2325643370774784
Loss in iteration 25 : 4.060265958264196
Loss in iteration 26 : 3.734260241685279
Loss in iteration 27 : 2.378023970230913
Loss in iteration 28 : 3.596947857538603
Loss in iteration 29 : 2.0987732722639554
Loss in iteration 30 : 2.816308965373309
Loss in iteration 31 : 2.2679706212080464
Loss in iteration 32 : 2.296405389432482
Loss in iteration 33 : 1.72525740982227
Loss in iteration 34 : 2.3439398829832565
Loss in iteration 35 : 1.301786665010121
Loss in iteration 36 : 3.04448844027972
Loss in iteration 37 : 2.2149124814043804
Loss in iteration 38 : 2.139160921114832
Loss in iteration 39 : 1.6623640935503738
Loss in iteration 40 : 2.1693046871143156
Loss in iteration 41 : 1.4942745781137994
Loss in iteration 42 : 1.9905235362958151
Loss in iteration 43 : 1.4991041552469078
Loss in iteration 44 : 1.5192980118975699
Loss in iteration 45 : 1.4698430145389365
Loss in iteration 46 : 1.3534049492489892
Loss in iteration 47 : 1.2959613921879876
Loss in iteration 48 : 1.4027101515260005
Loss in iteration 49 : 0.7712513199152435
Loss in iteration 50 : 0.9359242827309284
Loss in iteration 51 : 1.0284033515962399
Loss in iteration 52 : 0.8454301335876145
Loss in iteration 53 : 0.6318538902520395
Loss in iteration 54 : 1.0153949244167393
Loss in iteration 55 : 3.0676995153112365
Loss in iteration 56 : 1.7622483663042061
Loss in iteration 57 : 4.363350692994798
Loss in iteration 58 : 1.1393375402492079
Loss in iteration 59 : 2.575736882810778
Loss in iteration 60 : 2.5647527187880286
Loss in iteration 61 : 1.9864305862474272
Loss in iteration 62 : 2.4556386268401327
Loss in iteration 63 : 2.835638048673168
Loss in iteration 64 : 2.30055191016773
Loss in iteration 65 : 2.160766935831657
Loss in iteration 66 : 2.4346790272066494
Loss in iteration 67 : 2.304975868616549
Loss in iteration 68 : 1.8349417262322847
Loss in iteration 69 : 1.8421498348957528
Loss in iteration 70 : 1.9451202548762274
Loss in iteration 71 : 1.4315675955322889
Loss in iteration 72 : 1.5912762362947417
Loss in iteration 73 : 1.4470347567945367
Loss in iteration 74 : 1.2034253399293053
Loss in iteration 75 : 1.113082589817701
Loss in iteration 76 : 1.1955190214789886
Loss in iteration 77 : 0.7718661351590564
Loss in iteration 78 : 1.4674055191672106
Loss in iteration 79 : 1.0115325828782062
Loss in iteration 80 : 0.9582117213994911
Loss in iteration 81 : 1.2733631806440715
Loss in iteration 82 : 0.731928186467755
Loss in iteration 83 : 1.015617527237584
Loss in iteration 84 : 0.8444207212734257
Loss in iteration 85 : 0.5821831935476492
Loss in iteration 86 : 0.7250352016026352
Loss in iteration 87 : 0.6880948822302468
Loss in iteration 88 : 0.5745877437366151
Loss in iteration 89 : 0.5245444206345878
Loss in iteration 90 : 0.5778048295016625
Loss in iteration 91 : 0.7919446249686124
Loss in iteration 92 : 0.6922638977728445
Loss in iteration 93 : 0.60489383696699
Loss in iteration 94 : 0.4983217892865238
Loss in iteration 95 : 0.5976826298793747
Loss in iteration 96 : 0.8776460349146292
Loss in iteration 97 : 0.5798409634383778
Loss in iteration 98 : 0.4675457538304897
Loss in iteration 99 : 0.4060410689567976
Loss in iteration 100 : 0.4655258612369058
Loss in iteration 101 : 0.8543346766883728
Loss in iteration 102 : 0.8237770603320363
Loss in iteration 103 : 1.1401134623985942
Loss in iteration 104 : 0.5780317930922748
Loss in iteration 105 : 1.210021312583576
Loss in iteration 106 : 0.6535863767833787
Loss in iteration 107 : 0.9906004750513567
Loss in iteration 108 : 0.6103045991736468
Loss in iteration 109 : 0.9640473402024743
Loss in iteration 110 : 0.559878479381982
Loss in iteration 111 : 0.7988133788795976
Loss in iteration 112 : 0.6169011873477551
Loss in iteration 113 : 0.4464862288597038
Loss in iteration 114 : 0.528440236304386
Loss in iteration 115 : 0.7323185894799403
Loss in iteration 116 : 1.2431557754895426
Loss in iteration 117 : 0.4687163423396638
Loss in iteration 118 : 1.3086070757891786
Loss in iteration 119 : 0.9433839108992582
Loss in iteration 120 : 1.009940493026005
Loss in iteration 121 : 0.7897346619531623
Loss in iteration 122 : 0.9886493395044956
Loss in iteration 123 : 0.7329862328670166
Loss in iteration 124 : 0.9471452810792979
Loss in iteration 125 : 0.5747776438610172
Loss in iteration 126 : 0.9163668489622607
Loss in iteration 127 : 0.8075600943424467
Loss in iteration 128 : 0.6821620566874934
Loss in iteration 129 : 1.0817989846540539
Loss in iteration 130 : 0.5920004424630785
Loss in iteration 131 : 0.6876897085178385
Loss in iteration 132 : 0.7200332799353149
Loss in iteration 133 : 0.507336998203824
Loss in iteration 134 : 0.7281439523524772
Loss in iteration 135 : 0.4175947590113034
Loss in iteration 136 : 0.5243989694018899
Loss in iteration 137 : 1.1813500072195147
Loss in iteration 138 : 0.3832928802104286
Loss in iteration 139 : 1.4403704661938548
Loss in iteration 140 : 1.9479996034489113
Loss in iteration 141 : 2.1959883487170098
Loss in iteration 142 : 0.9520956027382983
Loss in iteration 143 : 1.7833822687447347
Loss in iteration 144 : 1.9241750026165503
Loss in iteration 145 : 1.229542970831823
Loss in iteration 146 : 1.5492415576896408
Loss in iteration 147 : 1.813812779536341
Loss in iteration 148 : 1.3726368216782505
Loss in iteration 149 : 1.1233025677738262
Loss in iteration 150 : 1.5231738221505542
Loss in iteration 151 : 1.170597078961037
Loss in iteration 152 : 0.9870554713064853
Loss in iteration 153 : 1.277762439100854
Loss in iteration 154 : 0.8005384755427807
Loss in iteration 155 : 1.1658472237378663
Loss in iteration 156 : 0.5904449544580782
Loss in iteration 157 : 0.9453288418933352
Loss in iteration 158 : 0.5475014381948616
Loss in iteration 159 : 1.131490817660152
Loss in iteration 160 : 0.6084857555906407
Loss in iteration 161 : 0.7531627643131437
Loss in iteration 162 : 0.8625205996237862
Loss in iteration 163 : 0.4932448015141634
Loss in iteration 164 : 0.8009061760600944
Loss in iteration 165 : 0.4593246527335201
Loss in iteration 166 : 0.6399780937293421
Loss in iteration 167 : 0.9140175674186954
Loss in iteration 168 : 0.5567999763617861
Loss in iteration 169 : 1.1924473029410225
Loss in iteration 170 : 0.5779415032730001
Loss in iteration 171 : 0.8603998407246386
Loss in iteration 172 : 0.5554273862539861
Loss in iteration 173 : 0.85282762240883
Loss in iteration 174 : 0.5490026075606091
Loss in iteration 175 : 0.7661895618586102
Loss in iteration 176 : 0.49614971489812065
Loss in iteration 177 : 0.7867793300341249
Loss in iteration 178 : 0.5192961028680934
Loss in iteration 179 : 0.4835995539417265
Loss in iteration 180 : 0.8201927412428442
Loss in iteration 181 : 0.7030630824434673
Loss in iteration 182 : 0.4689909127006339
Loss in iteration 183 : 0.8686536397028354
Loss in iteration 184 : 0.5471336744535695
Loss in iteration 185 : 0.6309280608260471
Loss in iteration 186 : 0.6061935142717028
Loss in iteration 187 : 0.457785416597872
Loss in iteration 188 : 0.6438664972528368
Loss in iteration 189 : 0.3872455929464147
Loss in iteration 190 : 0.6927086914625847
Loss in iteration 191 : 0.9619233033075257
Loss in iteration 192 : 0.5632460160602977
Loss in iteration 193 : 1.194657906822457
Loss in iteration 194 : 0.5044691139582901
Loss in iteration 195 : 0.8977152755919966
Loss in iteration 196 : 0.5563315935734473
Loss in iteration 197 : 0.8274822147141684
Loss in iteration 198 : 0.5626596057714811
Loss in iteration 199 : 0.7214559245970359
Loss in iteration 200 : 0.5541586954431673
Testing accuracy  of updater 9 on alg 0 with rate 2.0 = 0.7407407407407407, training accuracy 0.7462837837837838, time elapsed: 3812 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.474994204713273
Loss in iteration 3 : 0.4216083739216144
Loss in iteration 4 : 0.4150717602219207
Loss in iteration 5 : 0.3762625908089739
Loss in iteration 6 : 0.34695702522710203
Loss in iteration 7 : 0.3569239434032528
Loss in iteration 8 : 0.3823234232701628
Loss in iteration 9 : 0.3918949900105127
Loss in iteration 10 : 0.38322484009875984
Loss in iteration 11 : 0.373395965169223
Loss in iteration 12 : 0.37405118352163536
Loss in iteration 13 : 0.3817885222874165
Loss in iteration 14 : 0.3865531339425374
Loss in iteration 15 : 0.3827328585565054
Loss in iteration 16 : 0.37316136742277145
Loss in iteration 17 : 0.36537877261578583
Loss in iteration 18 : 0.3641091403667079
Loss in iteration 19 : 0.3667550748700852
Loss in iteration 20 : 0.3668368909131561
Loss in iteration 21 : 0.3617191022266566
Loss in iteration 22 : 0.3553682535600401
Loss in iteration 23 : 0.35284594805544045
Loss in iteration 24 : 0.35387422980670225
Loss in iteration 25 : 0.3537821269485372
Loss in iteration 26 : 0.3497938342160434
Loss in iteration 27 : 0.3442542636222458
Loss in iteration 28 : 0.3411778262268904
Loss in iteration 29 : 0.3407740476342662
Loss in iteration 30 : 0.3395713566481495
Loss in iteration 31 : 0.33601784586815464
Loss in iteration 32 : 0.33267359983932204
Loss in iteration 33 : 0.3318782850446533
Loss in iteration 34 : 0.3324210345278873
Loss in iteration 35 : 0.33189463860308305
Loss in iteration 36 : 0.3301860032734983
Loss in iteration 37 : 0.3291009507271119
Loss in iteration 38 : 0.32935288094298704
Loss in iteration 39 : 0.3295913762797441
Loss in iteration 40 : 0.32869761982198875
Loss in iteration 41 : 0.32744093677471875
Loss in iteration 42 : 0.32701750829617987
Loss in iteration 43 : 0.32721559251184723
Loss in iteration 44 : 0.3270263255726586
Loss in iteration 45 : 0.32629379173084566
Loss in iteration 46 : 0.3257786601379791
Loss in iteration 47 : 0.32582758677755175
Loss in iteration 48 : 0.3258785948157025
Loss in iteration 49 : 0.32545429338974347
Loss in iteration 50 : 0.3248723171463564
Loss in iteration 51 : 0.32461055702068703
Loss in iteration 52 : 0.3245675302682185
Loss in iteration 53 : 0.32435873350716304
Loss in iteration 54 : 0.3239676042865619
Loss in iteration 55 : 0.323710913537486
Loss in iteration 56 : 0.3237109304114905
Loss in iteration 57 : 0.32374769875084297
Loss in iteration 58 : 0.323642843118235
Loss in iteration 59 : 0.32350397664704583
Loss in iteration 60 : 0.32349340213724814
Loss in iteration 61 : 0.32356480271807314
Loss in iteration 62 : 0.32356346338669756
Loss in iteration 63 : 0.32346717431127625
Loss in iteration 64 : 0.32339026573137525
Loss in iteration 65 : 0.323390682245782
Loss in iteration 66 : 0.32339713348218463
Loss in iteration 67 : 0.3233419278622971
Loss in iteration 68 : 0.3232647339819358
Loss in iteration 69 : 0.32323625496146635
Loss in iteration 70 : 0.3232486578720847
Loss in iteration 71 : 0.3232420761669565
Loss in iteration 72 : 0.3232018696278805
Loss in iteration 73 : 0.3231698521800316
Loss in iteration 74 : 0.3231694685000588
Loss in iteration 75 : 0.32317231619205433
Loss in iteration 76 : 0.3231492003036254
Loss in iteration 77 : 0.3231139868882423
Loss in iteration 78 : 0.32309529312766955
Loss in iteration 79 : 0.3230918267929763
Loss in iteration 80 : 0.32308117591978425
Loss in iteration 81 : 0.3230582554946652
Loss in iteration 82 : 0.3230401332191298
Loss in iteration 83 : 0.32303631810429695
Loss in iteration 84 : 0.3230354654885272
Loss in iteration 85 : 0.3230256588071753
Loss in iteration 86 : 0.32301176374588103
Loss in iteration 87 : 0.32300435755530066
Loss in iteration 88 : 0.3230023679354842
Loss in iteration 89 : 0.3229967981708268
Loss in iteration 90 : 0.32298587222091313
Loss in iteration 91 : 0.32297658688660913
Loss in iteration 92 : 0.3229727874911635
Loss in iteration 93 : 0.3229701493396127
Loss in iteration 94 : 0.32296432644563594
Loss in iteration 95 : 0.32295749451042904
Loss in iteration 96 : 0.32295369508121097
Loss in iteration 97 : 0.32295218210152504
Loss in iteration 98 : 0.32294926293869575
Loss in iteration 99 : 0.3229442703441279
Loss in iteration 100 : 0.3229398850030777
Loss in iteration 101 : 0.3229373946719334
Loss in iteration 102 : 0.32293499746587356
Loss in iteration 103 : 0.322931130308451
Loss in iteration 104 : 0.32292681988582955
Loss in iteration 105 : 0.3229236627427479
Loss in iteration 106 : 0.32292131621139974
Loss in iteration 107 : 0.3229183921308732
Loss in iteration 108 : 0.3229147565626827
Loss in iteration 109 : 0.32291149364047383
Loss in iteration 110 : 0.32290903205078064
Loss in iteration 111 : 0.32290661863525016
Testing accuracy  of updater 9 on alg 0 with rate 0.2 = 0.8502548983477674, training accuracy 0.8492014742014742, time elapsed: 2095 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6502184487748475
Loss in iteration 3 : 0.5876647011768411
Loss in iteration 4 : 0.5283052834683687
Loss in iteration 5 : 0.48532284965301925
Loss in iteration 6 : 0.4604742507297145
Loss in iteration 7 : 0.4485966168850317
Loss in iteration 8 : 0.4429990079539205
Loss in iteration 9 : 0.438429174795921
Loss in iteration 10 : 0.4318274379974707
Loss in iteration 11 : 0.4220628017270517
Loss in iteration 12 : 0.40940288405153286
Loss in iteration 13 : 0.3949870838140765
Loss in iteration 14 : 0.38035511723690635
Loss in iteration 15 : 0.36703648094438673
Loss in iteration 16 : 0.356223005688473
Loss in iteration 17 : 0.3485625522114751
Loss in iteration 18 : 0.34410062153912585
Loss in iteration 19 : 0.3423629285456447
Loss in iteration 20 : 0.3425367968260645
Loss in iteration 21 : 0.3436904889685071
Loss in iteration 22 : 0.34497234620172434
Loss in iteration 23 : 0.345750047362015
Loss in iteration 24 : 0.3456743648703606
Loss in iteration 25 : 0.34467287849556816
Loss in iteration 26 : 0.34289263324911956
Loss in iteration 27 : 0.34061600516356816
Loss in iteration 28 : 0.3381727161020312
Loss in iteration 29 : 0.3358654683882324
Loss in iteration 30 : 0.3339194005389681
Loss in iteration 31 : 0.3324583853058414
Loss in iteration 32 : 0.3315054237702755
Loss in iteration 33 : 0.3310007482785914
Loss in iteration 34 : 0.3308298417683209
Loss in iteration 35 : 0.3308540249254778
Loss in iteration 36 : 0.33093788923089484
Loss in iteration 37 : 0.33096996409931473
Loss in iteration 38 : 0.33087505190762184
Loss in iteration 39 : 0.3306183232511744
Loss in iteration 40 : 0.33020241879618883
Loss in iteration 41 : 0.32965947194443623
Loss in iteration 42 : 0.32904022893664564
Loss in iteration 43 : 0.32840239152049344
Loss in iteration 44 : 0.32780002100796685
Loss in iteration 45 : 0.32727538728520955
Loss in iteration 46 : 0.326854084689225
Loss in iteration 47 : 0.32654363744732634
Loss in iteration 48 : 0.326335257173301
Loss in iteration 49 : 0.3262079697070183
Loss in iteration 50 : 0.326134058877536
Loss in iteration 51 : 0.3260847107840254
Loss in iteration 52 : 0.3260348755730937
Loss in iteration 53 : 0.32596664933620095
Loss in iteration 54 : 0.3258708462027988
Loss in iteration 55 : 0.32574680130556055
Loss in iteration 56 : 0.3256007497645467
Loss in iteration 57 : 0.32544331863582
Loss in iteration 58 : 0.32528673050169327
Loss in iteration 59 : 0.3251422598136776
Loss in iteration 60 : 0.32501833825778026
Loss in iteration 61 : 0.324919516320689
Loss in iteration 62 : 0.3248462982233663
Loss in iteration 63 : 0.32479571139570945
Loss in iteration 64 : 0.32476237108301237
Loss in iteration 65 : 0.32473976210424754
Loss in iteration 66 : 0.3247214770986913
Loss in iteration 67 : 0.32470220905155606
Loss in iteration 68 : 0.3246783768455625
Loss in iteration 69 : 0.3246483477364167
Loss in iteration 70 : 0.324612295035735
Loss in iteration 71 : 0.32457178292205324
Loss in iteration 72 : 0.32452919859042206
Loss in iteration 73 : 0.32448715518390653
Loss in iteration 74 : 0.32444797125328817
Loss in iteration 75 : 0.3244133004912456
Loss in iteration 76 : 0.3243839468994958
Loss in iteration 77 : 0.32435986284451157
Loss in iteration 78 : 0.3243402967590054
Loss in iteration 79 : 0.3243240375457154
Loss in iteration 80 : 0.32430969555372297
Loss in iteration 81 : 0.3242959644628563
Loss in iteration 82 : 0.32428182178035414
Loss in iteration 83 : 0.3242666440558926
Loss in iteration 84 : 0.32425023223265864
Loss in iteration 85 : 0.3242327591835466
Loss in iteration 86 : 0.3242146629483743
Loss in iteration 87 : 0.3241965143797167
Loss in iteration 88 : 0.3241788870491962
Loss in iteration 89 : 0.3241622516437325
Loss in iteration 90 : 0.32414690859504613
Loss in iteration 91 : 0.3241329633909235
Loss in iteration 92 : 0.32412034071707463
Loss in iteration 93 : 0.32410882754646625
Loss in iteration 94 : 0.3240981321490265
Loss in iteration 95 : 0.32408794572545924
Loss in iteration 96 : 0.3240779954613966
Loss in iteration 97 : 0.32406808142870924
Loss in iteration 98 : 0.32405809400340163
Loss in iteration 99 : 0.3240480124746089
Loss in iteration 100 : 0.3240378886440274
Loss in iteration 101 : 0.3240278210917861
Loss in iteration 102 : 0.32401792632849363
Loss in iteration 103 : 0.3240083124223459
Loss in iteration 104 : 0.32399905921315636
Loss in iteration 105 : 0.3239902073135758
Loss in iteration 106 : 0.32398175616133185
Loss in iteration 107 : 0.32397366976379177
Loss in iteration 108 : 0.32396588769024015
Loss in iteration 109 : 0.32395833840786337
Loss in iteration 110 : 0.3239509521890086
Loss in iteration 111 : 0.3239436714083361
Loss in iteration 112 : 0.32393645691121586
Loss in iteration 113 : 0.3239292900679851
Loss in iteration 114 : 0.32392217095601566
Loss in iteration 115 : 0.3239151137069873
Loss in iteration 116 : 0.3239081403595262
Loss in iteration 117 : 0.32390127456796863
Loss in iteration 118 : 0.3238945362899108
Loss in iteration 119 : 0.3238879381927575
Loss in iteration 120 : 0.32388148407910106
Loss in iteration 121 : 0.32387516922066656
Loss in iteration 122 : 0.3238689821769851
Loss in iteration 123 : 0.32386290749487945
Loss in iteration 124 : 0.3238569286444654
Loss in iteration 125 : 0.3238510306276627
Loss in iteration 126 : 0.3238452018591757
Loss in iteration 127 : 0.32383943512307883
Loss in iteration 128 : 0.3238337276077556
Loss in iteration 129 : 0.3238280801835087
Loss in iteration 130 : 0.3238224961895194
Loss in iteration 131 : 0.3238169800334178
Loss in iteration 132 : 0.323811535883008
Loss in iteration 133 : 0.3238061666614433
Loss in iteration 134 : 0.3238008734644397
Loss in iteration 135 : 0.3237956554218043
Loss in iteration 136 : 0.3237905099433983
Loss in iteration 137 : 0.3237854332330799
Loss in iteration 138 : 0.32378042092904513
Loss in iteration 139 : 0.32377546873300556
Loss in iteration 140 : 0.3237705729186579
Loss in iteration 141 : 0.32376573065160386
Loss in iteration 142 : 0.3237609400991305
Loss in iteration 143 : 0.32375620034939473
Loss in iteration 144 : 0.3237515111897762
Loss in iteration 145 : 0.3237468728097544
Loss in iteration 146 : 0.3237422854948677
Loss in iteration 147 : 0.3237377493674092
Loss in iteration 148 : 0.3237332642108966
Loss in iteration 149 : 0.323728829393433
Loss in iteration 150 : 0.3237244438848887
Loss in iteration 151 : 0.32372010634696585
Loss in iteration 152 : 0.32371581526632826
Loss in iteration 153 : 0.323711569098836
Loss in iteration 154 : 0.3237073663970281
Loss in iteration 155 : 0.3237032059010972
Loss in iteration 156 : 0.3236990865839174
Loss in iteration 157 : 0.32369500765051795
Loss in iteration 158 : 0.3236909685005097
Loss in iteration 159 : 0.3236869686669573
Loss in iteration 160 : 0.32368300774684416
Loss in iteration 161 : 0.32367908533696055
Loss in iteration 162 : 0.3236752009855116
Loss in iteration 163 : 0.32367135416495046
Loss in iteration 164 : 0.32366754426682076
Loss in iteration 165 : 0.3236637706152235
Loss in iteration 166 : 0.32366003249290887
Loss in iteration 167 : 0.3236563291728248
Loss in iteration 168 : 0.3236526599483099
Loss in iteration 169 : 0.3236490241567121
Loss in iteration 170 : 0.32364542119327644
Loss in iteration 171 : 0.323641850514545
Loss in iteration 172 : 0.3236383116325301
Loss in iteration 173 : 0.323634804102344
Loss in iteration 174 : 0.32363132750659285
Loss in iteration 175 : 0.32362788143991433
Loss in iteration 176 : 0.32362446549628227
Loss in iteration 177 : 0.3236210792607607
Loss in iteration 178 : 0.3236177223063117
Loss in iteration 179 : 0.3236143941951617
Loss in iteration 180 : 0.32361109448358694
Loss in iteration 181 : 0.3236078227285653
Loss in iteration 182 : 0.32360457849463053
Loss in iteration 183 : 0.3236013613596716
Loss in iteration 184 : 0.3235981709187076
Loss in iteration 185 : 0.32359500678533903
Loss in iteration 186 : 0.32359186859097366
Loss in iteration 187 : 0.3235887559823454
Loss in iteration 188 : 0.3235856686180616
Loss in iteration 189 : 0.3235826061649278
Loss in iteration 190 : 0.32357956829473505
Loss in iteration 191 : 0.3235765546819628
Loss in iteration 192 : 0.32357356500260653
Loss in iteration 193 : 0.3235705989340802
Loss in iteration 194 : 0.32356765615603245
Loss in iteration 195 : 0.32356473635165384
Loss in iteration 196 : 0.3235618392091872
Testing accuracy  of updater 9 on alg 0 with rate 0.01999999999999999 = 0.8498863706160555, training accuracy 0.8492321867321867, time elapsed: 3568 millisecond.
