objc[859]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x10ba504c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10bad44e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 18:13:46 INFO SparkContext: Running Spark version 2.0.0
18/02/26 18:13:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 18:13:46 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 18:13:46 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 18:13:46 INFO SecurityManager: Changing view acls groups to: 
18/02/26 18:13:46 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 18:13:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 18:13:48 INFO Utils: Successfully started service 'sparkDriver' on port 49826.
18/02/26 18:13:48 INFO SparkEnv: Registering MapOutputTracker
18/02/26 18:13:48 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 18:13:48 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-05e52778-5d05-4f69-b989-1d0a18f0e3db
18/02/26 18:13:48 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 18:13:48 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 18:13:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 18:13:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 18:13:49 INFO Executor: Starting executor ID driver on host localhost
18/02/26 18:13:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49827.
18/02/26 18:13:49 INFO NettyBlockTransferService: Server created on 192.168.2.140:49827
18/02/26 18:13:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 49827)
18/02/26 18:13:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:49827 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 49827)
18/02/26 18:13:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 49827)
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 2.141549710106488
Loss in iteration 3 : 22.853611911924023
Loss in iteration 4 : 7.21335341680556
Loss in iteration 5 : 16.31191673601085
Loss in iteration 6 : 11.442165808524548
Loss in iteration 7 : 9.853829878915363
Loss in iteration 8 : 14.82284181691413
Loss in iteration 9 : 5.208611633251837
Loss in iteration 10 : 14.418368881409089
Loss in iteration 11 : 5.027456977489407
Loss in iteration 12 : 13.16121341794933
Loss in iteration 13 : 5.731375784753711
Loss in iteration 14 : 12.185442371765923
Loss in iteration 15 : 6.159044453681352
Loss in iteration 16 : 11.112710431471651
Loss in iteration 17 : 6.636997211211101
Loss in iteration 18 : 10.102649849043829
Loss in iteration 19 : 6.9678892811581665
Loss in iteration 20 : 9.149213153490695
Loss in iteration 21 : 7.157181689747246
Loss in iteration 22 : 8.298333441163864
Loss in iteration 23 : 7.163533922187165
Loss in iteration 24 : 7.5925123345224526
Loss in iteration 25 : 7.02406964037165
Loss in iteration 26 : 7.001492905922772
Loss in iteration 27 : 6.8358197263641465
Loss in iteration 28 : 6.46063891285564
Loss in iteration 29 : 6.638765314460299
Loss in iteration 30 : 6.033601126197938
Loss in iteration 31 : 6.4398244301918455
Loss in iteration 32 : 5.710873207592277
Loss in iteration 33 : 6.245350939543627
Loss in iteration 34 : 5.443091679953739
Loss in iteration 35 : 6.05940758122028
Loss in iteration 36 : 5.200301852161027
Loss in iteration 37 : 5.885788329523017
Loss in iteration 38 : 5.004616699027921
Loss in iteration 39 : 5.736806682123505
Loss in iteration 40 : 4.851267538800062
Loss in iteration 41 : 5.613576091966786
Loss in iteration 42 : 4.729931568486472
Loss in iteration 43 : 5.512083071690393
Loss in iteration 44 : 4.630276631813032
Loss in iteration 45 : 5.427013934654445
Loss in iteration 46 : 4.545162113838059
Loss in iteration 47 : 5.353566656837553
Loss in iteration 48 : 4.469785388997436
Loss in iteration 49 : 5.2881638570347524
Loss in iteration 50 : 4.4014101221878414
Loss in iteration 51 : 5.2290228656008715
Loss in iteration 52 : 4.338588314309602
Loss in iteration 53 : 5.174854409074381
Loss in iteration 54 : 4.280386792480614
Loss in iteration 55 : 5.124470984959149
Loss in iteration 56 : 4.226226279031746
Loss in iteration 57 : 5.077069142260412
Loss in iteration 58 : 4.175914895832702
Loss in iteration 59 : 5.032397573236789
Loss in iteration 60 : 4.129519093742629
Loss in iteration 61 : 4.990574392627231
Loss in iteration 62 : 4.0871386719404335
Loss in iteration 63 : 4.9520648480287015
Loss in iteration 64 : 4.048840182490215
Loss in iteration 65 : 4.9162612516531645
Loss in iteration 66 : 4.014409499607035
Loss in iteration 67 : 4.874381138066329
Loss in iteration 68 : 3.981769288192354
Loss in iteration 69 : 4.834482774596378
Loss in iteration 70 : 3.952872989644929
Loss in iteration 71 : 4.80226914262368
Loss in iteration 72 : 3.928628877456003
Loss in iteration 73 : 4.773430204168923
Loss in iteration 74 : 3.907532614081969
Loss in iteration 75 : 4.747192746517256
Loss in iteration 76 : 3.8890180694357186
Loss in iteration 77 : 4.723103048137378
Loss in iteration 78 : 3.8726655838956083
Loss in iteration 79 : 4.700407225364924
Loss in iteration 80 : 3.8580813833123844
Loss in iteration 81 : 4.678727245758651
Loss in iteration 82 : 3.84499079525849
Loss in iteration 83 : 4.658333625544983
Loss in iteration 84 : 3.8332396266691875
Loss in iteration 85 : 4.639642696867704
Loss in iteration 86 : 3.822704852484356
Loss in iteration 87 : 4.622707303622456
Loss in iteration 88 : 3.8132326192603303
Loss in iteration 89 : 4.607271357472206
Loss in iteration 90 : 3.804653931589664
Loss in iteration 91 : 4.59300364378205
Loss in iteration 92 : 3.7968150376857235
Loss in iteration 93 : 4.579613750471645
Loss in iteration 94 : 3.7895880692322192
Loss in iteration 95 : 4.566871186531451
Loss in iteration 96 : 3.782868997575615
Loss in iteration 97 : 4.554595405577422
Loss in iteration 98 : 3.7765727914623675
Loss in iteration 99 : 4.54264740050369
Loss in iteration 100 : 3.7706296498440652
Loss in iteration 101 : 4.530928520818157
Loss in iteration 102 : 3.7649828747257943
Loss in iteration 103 : 4.519380758973132
Loss in iteration 104 : 3.759587608339996
Loss in iteration 105 : 4.50798127566556
Loss in iteration 106 : 3.7544094130874313
Loss in iteration 107 : 4.496731860714972
Loss in iteration 108 : 3.7494225376554624
Loss in iteration 109 : 4.485651338605162
Loss in iteration 110 : 3.74460872698373
Loss in iteration 111 : 4.474774423623616
Loss in iteration 112 : 3.739957026183869
Loss in iteration 113 : 4.464150638117977
Loss in iteration 114 : 3.7354635858278264
Loss in iteration 115 : 4.453836230867908
Loss in iteration 116 : 3.7311300825786446
Loss in iteration 117 : 4.4438806884904185
Loss in iteration 118 : 3.72696067226151
Loss in iteration 119 : 4.434315458348565
Loss in iteration 120 : 3.7229587921521317
Loss in iteration 121 : 4.425150136896224
Loss in iteration 122 : 3.7191251801056007
Loss in iteration 123 : 4.416375643569683
Loss in iteration 124 : 3.7154575145051947
Loss in iteration 125 : 4.407970755457076
Loss in iteration 126 : 3.7119512108707644
Loss in iteration 127 : 4.399908642092712
Loss in iteration 128 : 3.70860064494953
Loss in iteration 129 : 4.39216167437689
Loss in iteration 130 : 3.705400237130026
Loss in iteration 131 : 4.3847041783818055
Loss in iteration 132 : 3.70234511522714
Loss in iteration 133 : 4.37751351939515
Loss in iteration 134 : 3.6994313007040422
Loss in iteration 135 : 4.370570095459711
Loss in iteration 136 : 3.6966555014015703
Loss in iteration 137 : 4.363856757756025
Loss in iteration 138 : 3.6940146601231763
Loss in iteration 139 : 4.357358037644811
Loss in iteration 140 : 3.691505424864636
Loss in iteration 141 : 4.351059427891867
Loss in iteration 142 : 3.6891236862679007
Loss in iteration 143 : 4.34494685813272
Loss in iteration 144 : 3.68686428136045
Loss in iteration 145 : 4.339006419114514
Loss in iteration 146 : 3.6847209036573214
Loss in iteration 147 : 4.333224323761524
Loss in iteration 148 : 3.6826862048130953
Loss in iteration 149 : 4.327587047880171
Loss in iteration 150 : 3.680752035595314
Loss in iteration 151 : 4.32208157221888
Loss in iteration 152 : 3.6789097592330062
Loss in iteration 153 : 4.316695649062863
Loss in iteration 154 : 3.677150574991686
Loss in iteration 155 : 4.3114180337981995
Loss in iteration 156 : 3.675465806223042
Loss in iteration 157 : 4.306238645869296
Loss in iteration 158 : 3.6738471269411055
Loss in iteration 159 : 4.301148646516094
Loss in iteration 160 : 3.672286718494077
Loss in iteration 161 : 4.296140438085905
Loss in iteration 162 : 3.670777360526271
Loss in iteration 163 : 4.291207600256283
Loss in iteration 164 : 3.6693124678706273
Loss in iteration 165 : 4.286344782975205
Loss in iteration 166 : 3.6678860881866866
Loss in iteration 167 : 4.281547575994307
Loss in iteration 168 : 3.6664928753025214
Loss in iteration 169 : 4.27681237224576
Loss in iteration 170 : 3.6651280514464997
Loss in iteration 171 : 4.272136238343614
Loss in iteration 172 : 3.6637873687050666
Loss in iteration 173 : 4.267516801100013
Loss in iteration 174 : 3.662467076706321
Loss in iteration 175 : 4.262952154707882
Loss in iteration 176 : 3.6611639001111995
Loss in iteration 177 : 4.258440789492557
Loss in iteration 178 : 3.6598750263065214
Loss in iteration 179 : 4.253981540076023
Loss in iteration 180 : 3.65859810099108
Loss in iteration 181 : 4.249573548557088
Loss in iteration 182 : 3.6573312273540695
Loss in iteration 183 : 4.24521623697674
Loss in iteration 184 : 3.656072963451229
Loss in iteration 185 : 4.240909282953672
Loss in iteration 186 : 3.6548223122986694
Loss in iteration 187 : 4.236652592923428
Loss in iteration 188 : 3.6535787001259092
Loss in iteration 189 : 4.232446268784105
Loss in iteration 190 : 3.652341939997897
Loss in iteration 191 : 4.228290565720401
Loss in iteration 192 : 3.6511121803217663
Loss in iteration 193 : 4.224185841217948
Loss in iteration 194 : 3.64988984016604
Loss in iteration 195 : 4.2201324974019
Loss in iteration 196 : 3.648675535383165
Loss in iteration 197 : 4.2161309204633275
Loss in iteration 198 : 3.647470000862182
Loss in iteration 199 : 4.212181421802627
Loss in iteration 200 : 3.6462740146408303
Loss in iteration 201 : 4.208284185517716
Loss in iteration 202 : 3.645088329089257
Loss in iteration 203 : 4.2044392260842285
Loss in iteration 204 : 3.643913613149095
Loss in iteration 205 : 4.200646358759137
Loss in iteration 206 : 3.6427504080024726
Loss in iteration 207 : 4.19690518370865
Loss in iteration 208 : 3.6415990969083123
Loss in iteration 209 : 4.193215083425861
Loss in iteration 210 : 3.6404598885573534
Loss in iteration 211 : 4.189575231890601
Loss in iteration 212 : 3.639332812329759
Loss in iteration 213 : 4.185984613238528
Loss in iteration 214 : 3.6382177233236415
Loss in iteration 215 : 4.182442047447535
Loss in iteration 216 : 3.637114314900761
Loss in iteration 217 : 4.178946220629478
Loss in iteration 218 : 3.6360221366512095
Loss in iteration 219 : 4.175495717817943
Loss in iteration 220 : 3.6349406159878637
Loss in iteration 221 : 4.172089056545523
Loss in iteration 222 : 3.6338690819380908
Loss in iteration 223 : 4.168724719918424
Loss in iteration 224 : 3.632806790032263
Loss in iteration 225 : 4.165401188261345
Loss in iteration 226 : 3.6317529474594896
Loss in iteration 227 : 4.162116968696684
Loss in iteration 228 : 3.630706737860289
Loss in iteration 229 : 4.158870622236607
Loss in iteration 230 : 3.6296673452604393
Loss in iteration 231 : 4.155660788114266
Loss in iteration 232 : 3.6286339767366007
Loss in iteration 233 : 4.152486205177093
Loss in iteration 234 : 3.627605883456734
Loss in iteration 235 : 4.149345730225275
Loss in iteration 236 : 3.626582379772638
Loss in iteration 237 : 4.146238353213257
Loss in iteration 238 : 3.625562860067961
Loss in iteration 239 : 4.1431632092502
Loss in iteration 240 : 3.62454681308838
Loss in iteration 241 : 4.140119587341929
Loss in iteration 242 : 3.6235338335084717
Loss in iteration 243 : 4.137106935816736
Loss in iteration 244 : 3.6225236305219704
Loss in iteration 245 : 4.134124864375086
Loss in iteration 246 : 3.6215160332843324
Loss in iteration 247 : 4.131173142704252
Loss in iteration 248 : 3.620510993087797
Loss in iteration 249 : 4.128251695608596
Loss in iteration 250 : 3.619508582213916
Loss in iteration 251 : 4.125360594632132
Loss in iteration 252 : 3.6185089894852775
Loss in iteration 253 : 4.122500046197112
Loss in iteration 254 : 3.6175125126280316
Loss in iteration 255 : 4.1196703763553595
Loss in iteration 256 : 3.6165195476567407
Loss in iteration 257 : 4.116872012348837
Loss in iteration 258 : 3.615530575599816
Loss in iteration 259 : 4.114105461300891
Loss in iteration 260 : 3.614546146990018
Loss in iteration 261 : 4.1113712865012335
Loss in iteration 262 : 3.6135668646445964
Loss in iteration 263 : 4.108670081896606
Loss in iteration 264 : 3.6125933653426654
Loss in iteration 265 : 4.10600244553923
Loss in iteration 266 : 3.6116263010680427
Loss in iteration 267 : 4.1033689528623345
Loss in iteration 268 : 3.610666320513792
Loss in iteration 269 : 4.100770130729162
Loss in iteration 270 : 3.6097140515380155
Loss in iteration 271 : 4.098206433228411
Loss in iteration 272 : 3.6087700852150215
Loss in iteration 273 : 4.095678220156644
Loss in iteration 274 : 3.6078349620455175
Loss in iteration 275 : 4.09318573903621
Loss in iteration 276 : 3.6069091607774664
Loss in iteration 277 : 4.090729111371614
Loss in iteration 278 : 3.6059930901551014
Loss in iteration 279 : 4.088308323658588
Loss in iteration 280 : 3.60508708376735
Loss in iteration 281 : 4.085923223444066
Loss in iteration 282 : 3.6041913980177798
Loss in iteration 283 : 4.083573520507995
Loss in iteration 284 : 3.60330621309908
Loss in iteration 285 : 4.0812587930167314
Loss in iteration 286 : 3.6024316367312235
Loss in iteration 287 : 4.078978498294361
Loss in iteration 288 : 3.6015677103221706
Loss in iteration 289 : 4.0767319876843855
Loss in iteration 290 : 3.600714417135504
Loss in iteration 291 : 4.074518524833543
Loss in iteration 292 : 3.5998716920001854
Loss in iteration 293 : 4.072337306625073
Loss in iteration 294 : 3.5990394320742314
Loss in iteration 295 : 4.070187485917973
Loss in iteration 296 : 3.5982175081705314
Loss in iteration 297 : 4.068068195210206
Loss in iteration 298 : 3.5974057761680105
Loss in iteration 299 : 4.0659785703332325
Loss in iteration 300 : 3.5966040880592574
Loss in iteration 301 : 4.0639177733005
Loss in iteration 302 : 3.595812302224891
Loss in iteration 303 : 4.06188501347323
Loss in iteration 304 : 3.5950302925715585
Loss in iteration 305 : 4.059879566271955
Loss in iteration 306 : 3.594257956225869
Loss in iteration 307 : 4.057900788754301
Loss in iteration 308 : 3.5934952195374654
Loss in iteration 309 : 4.055948131498661
Loss in iteration 310 : 3.5927420422131497
Loss in iteration 311 : 4.054021146379425
Loss in iteration 312 : 3.591998419478875
Loss in iteration 313 : 4.052119489991428
Loss in iteration 314 : 3.591264382246869
Loss in iteration 315 : 4.05024292267123
Loss in iteration 316 : 3.5905399953490953
Loss in iteration 317 : 4.0483913032647845
Loss in iteration 318 : 3.5898253539823237
Loss in iteration 319 : 4.046564579990998
Loss in iteration 320 : 3.5891205785893905
Loss in iteration 321 : 4.044762777935828
Loss in iteration 322 : 3.588425808471211
Loss in iteration 323 : 4.0429859838678945
Loss in iteration 324 : 3.5877411944789275
Loss in iteration 325 : 4.041234329180948
Loss in iteration 326 : 3.5870668911705654
Loss in iteration 327 : 4.039507971831803
Loss in iteration 328 : 3.58640304882903
Loss in iteration 329 : 4.037807078150109
Loss in iteration 330 : 3.585749805726824
Loss in iteration 331 : 4.036131805348677
Loss in iteration 332 : 3.585107280988026
Loss in iteration 333 : 4.0344822854664155
Loss in iteration 334 : 3.5844755683435543
Loss in iteration 335 : 4.032858611340443
Loss in iteration 336 : 3.5838547310072046
Loss in iteration 337 : 4.031260825042988
Loss in iteration 338 : 3.583244797821594
Loss in iteration 339 : 4.029688909046982
Loss in iteration 340 : 3.5826457607438615
Loss in iteration 341 : 4.028142780215783
Loss in iteration 342 : 3.5820575736651916
Loss in iteration 343 : 4.026622286559125
Loss in iteration 344 : 3.5814801524914164
Loss in iteration 345 : 4.0251272065688735
Loss in iteration 346 : 3.580913376358629
Loss in iteration 347 : 4.023657250848922
Loss in iteration 348 : 3.5803570898184858
Loss in iteration 349 : 4.022212065687053
Loss in iteration 350 : 3.5798111058044007
Loss in iteration 351 : 4.020791238180441
Loss in iteration 352 : 3.579275209180416
Loss in iteration 353 : 4.019394302518752
Loss in iteration 354 : 3.5787491606780866
Loss in iteration 355 : 4.018020747043091
Loss in iteration 356 : 3.5782327010391626
Loss in iteration 357 : 4.016670021731621
Loss in iteration 358 : 3.5777255552033234
Loss in iteration 359 : 4.015341545806286
Loss in iteration 360 : 3.577227436403763
Loss in iteration 361 : 4.014034715204965
Loss in iteration 362 : 3.576738050061459
Loss in iteration 363 : 4.012748909716323
Loss in iteration 364 : 3.576257097394713
Loss in iteration 365 : 4.011483499624715
Loss in iteration 366 : 3.5757842786868888
Loss in iteration 367 : 4.010237851759911
Loss in iteration 368 : 3.575319296177059
Loss in iteration 369 : 4.009011334887304
Loss in iteration 370 : 3.574861856558182
Loss in iteration 371 : 4.00780332440939
Loss in iteration 372 : 3.574411673083468
Loss in iteration 373 : 4.006613206377575
Loss in iteration 374 : 3.5739684672937266
Loss in iteration 375 : 4.005440380835369
Loss in iteration 376 : 3.573531970388024
Loss in iteration 377 : 4.004284264529793
Loss in iteration 378 : 3.5731019242661617
Loss in iteration 379 : 4.0031442930394805
Loss in iteration 380 : 3.57267808227548
Loss in iteration 381 : 4.002019922374044
Loss in iteration 382 : 3.572260209696241
Loss in iteration 383 : 4.000910630103179
Loss in iteration 384 : 3.571848084000234
Loss in iteration 385 : 3.999815916073753
Loss in iteration 386 : 3.5714414949158626
Loss in iteration 387 : 3.998735302772431
Loss in iteration 388 : 3.5710402443319462
Loss in iteration 389 : 3.997668335387827
Loss in iteration 390 : 3.570644146069061
Loss in iteration 391 : 3.996614581622478
Loss in iteration 392 : 3.5702530255453016
Loss in iteration 393 : 3.995573631300329
Loss in iteration 394 : 3.5698667193600615
Loss in iteration 395 : 3.9945450958108384
Loss in iteration 396 : 3.5694850748164937
Loss in iteration 397 : 3.993528607425822
Loss in iteration 398 : 3.5691079494007165
Loss in iteration 399 : 3.9925238185204144
Loss in iteration 400 : 3.5687352102329815
Testing accuracy  of updater 0 on alg 0 with rate 10.0 = 0.722375, training accuracy 0.722375, time elapsed: 9743 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6826496441898315
Loss in iteration 3 : 0.725167450273125
Loss in iteration 4 : 0.9043540796081131
Loss in iteration 5 : 0.9965633061226778
Loss in iteration 6 : 1.2081521260774444
Loss in iteration 7 : 0.8731585215684704
Loss in iteration 8 : 1.073014754531772
Loss in iteration 9 : 0.8494648798849441
Loss in iteration 10 : 1.0034789436340166
Loss in iteration 11 : 0.8103927207765956
Loss in iteration 12 : 0.926232059609523
Loss in iteration 13 : 0.7768120276425868
Loss in iteration 14 : 0.8607201840207828
Loss in iteration 15 : 0.744377769829368
Loss in iteration 16 : 0.8036105906410181
Loss in iteration 17 : 0.7136515407463072
Loss in iteration 18 : 0.7544269284821339
Loss in iteration 19 : 0.6849606934737429
Loss in iteration 20 : 0.7122413323717436
Loss in iteration 21 : 0.65856187768203
Loss in iteration 22 : 0.6761260965331404
Loss in iteration 23 : 0.6345720640813586
Loss in iteration 24 : 0.6452250038763838
Loss in iteration 25 : 0.612982572124985
Loss in iteration 26 : 0.6187857076040149
Loss in iteration 27 : 0.5936977934166677
Loss in iteration 28 : 0.5961615356064669
Loss in iteration 29 : 0.5765719973701614
Loss in iteration 30 : 0.5768031478058391
Loss in iteration 31 : 0.5614411483053
Loss in iteration 32 : 0.5602505572244704
Loss in iteration 33 : 0.5481516058199746
Loss in iteration 34 : 0.5461293625914495
Loss in iteration 35 : 0.5365757609816606
Loss in iteration 36 : 0.5341429191740229
Loss in iteration 37 : 0.5266014870814502
Loss in iteration 38 : 0.524048343243742
Loss in iteration 39 : 0.5181100333716702
Loss in iteration 40 : 0.5156273972185385
Loss in iteration 41 : 0.5109670373527165
Loss in iteration 42 : 0.5086719651262644
Loss in iteration 43 : 0.5050273743713423
Loss in iteration 44 : 0.5029833242899197
Loss in iteration 45 : 0.5001430085720845
Loss in iteration 46 : 0.4983750962674798
Loss in iteration 47 : 0.49616863274259093
Loss in iteration 48 : 0.4946751998761718
Loss in iteration 49 : 0.49296513038157924
Loss in iteration 50 : 0.4917267397637677
Loss in iteration 51 : 0.49040204575618407
Loss in iteration 52 : 0.4893886385832387
Loss in iteration 53 : 0.48835975704582474
Loss in iteration 54 : 0.48753634037645566
Loss in iteration 55 : 0.4867314363284446
Loss in iteration 56 : 0.4860624333064284
Loss in iteration 57 : 0.4854245479649703
Loss in iteration 58 : 0.4848768597073078
Loss in iteration 59 : 0.4843615883623385
Loss in iteration 60 : 0.4839064482816865
Loss in iteration 61 : 0.4834799069309339
Loss in iteration 62 : 0.48309369286384074
Loss in iteration 63 : 0.48273065202844134
Loss in iteration 64 : 0.4823948985864859
Loss in iteration 65 : 0.4820770578169879
Loss in iteration 66 : 0.48177794402331814
Loss in iteration 67 : 0.481492373927718
Loss in iteration 68 : 0.48121994232734266
Loss in iteration 69 : 0.4809577343122768
Loss in iteration 70 : 0.4807050421351248
Loss in iteration 71 : 0.48046019288457753
Loss in iteration 72 : 0.48022252631115814
Loss in iteration 73 : 0.47999106030261485
Loss in iteration 74 : 0.47976527878439323
Loss in iteration 75 : 0.47954459001548877
Loss in iteration 76 : 0.4793286196210813
Loss in iteration 77 : 0.4791169993370011
Loss in iteration 78 : 0.47890946459118233
Loss in iteration 79 : 0.47870577621154825
Loss in iteration 80 : 0.47850574595656564
Loss in iteration 81 : 0.4783092097067878
Loss in iteration 82 : 0.47811602922303964
Loss in iteration 83 : 0.4779260843467569
Loss in iteration 84 : 0.4777392686075833
Loss in iteration 85 : 0.4775554879529196
Loss in iteration 86 : 0.477374655955149
Loss in iteration 87 : 0.477196694478557
Loss in iteration 88 : 0.4770215299500116
Loss in iteration 89 : 0.4768490943783651
Loss in iteration 90 : 0.47667932277115815
Loss in iteration 91 : 0.4765121540244468
Loss in iteration 92 : 0.47634752922361784
Loss in iteration 93 : 0.47618539230456447
Loss in iteration 94 : 0.4760256889616746
Loss in iteration 95 : 0.47586836710102876
Loss in iteration 96 : 0.4757133761436962
Loss in iteration 97 : 0.47556066731927604
Loss in iteration 98 : 0.47541019321988315
Loss in iteration 99 : 0.47526190798027346
Loss in iteration 100 : 0.47511576698885183
Loss in iteration 101 : 0.4749717269909188
Loss in iteration 102 : 0.47482974589764143
Loss in iteration 103 : 0.47468978283899615
Loss in iteration 104 : 0.4745517980339206
Loss in iteration 105 : 0.4744157528113855
Loss in iteration 106 : 0.4742816095189929
Loss in iteration 107 : 0.47414933152450567
Loss in iteration 108 : 0.4740188831488743
Loss in iteration 109 : 0.4738902296562818
Loss in iteration 110 : 0.47376333720292757
Loss in iteration 111 : 0.4736381728207311
Loss in iteration 112 : 0.4735147043765021
Loss in iteration 113 : 0.47339290055250455
Loss in iteration 114 : 0.47327273081267185
Loss in iteration 115 : 0.47315416538197697
Loss in iteration 116 : 0.47303717521758654
Loss in iteration 117 : 0.47292173198820514
Loss in iteration 118 : 0.4728078080488255
Loss in iteration 119 : 0.472695376420671
Loss in iteration 120 : 0.4725844107687235
Loss in iteration 121 : 0.47247488538257704
Loss in iteration 122 : 0.4723667751561804
Loss in iteration 123 : 0.47226005556976847
Loss in iteration 124 : 0.47215470267141735
Loss in iteration 125 : 0.4720506930601139
Loss in iteration 126 : 0.4719480038688481
Loss in iteration 127 : 0.4718466127488097
Loss in iteration 128 : 0.47174649785380735
Loss in iteration 129 : 0.4716476378255505
Loss in iteration 130 : 0.4715500117792553
Loss in iteration 131 : 0.47145359928993846
Loss in iteration 132 : 0.4713583803790802
Loss in iteration 133 : 0.47126433550187163
Loss in iteration 134 : 0.47117144553483203
Loss in iteration 135 : 0.47107969176393844
Loss in iteration 136 : 0.47098905587311646
Loss in iteration 137 : 0.4708995199331739
Loss in iteration 138 : 0.470811066391099
Loss in iteration 139 : 0.47072367805974014
Loss in iteration 140 : 0.4706373381078321
Loss in iteration 141 : 0.47055203005036145
Loss in iteration 142 : 0.47046773773927475
Loss in iteration 143 : 0.4703844453544815
Loss in iteration 144 : 0.4703021373951681
Loss in iteration 145 : 0.470220798671401
Loss in iteration 146 : 0.470140414295997
Loss in iteration 147 : 0.4700609696766787
Loss in iteration 148 : 0.4699824505084695
Loss in iteration 149 : 0.4699048427663432
Loss in iteration 150 : 0.4698281326981096
Loss in iteration 151 : 0.46975230681752556
Loss in iteration 152 : 0.4696773518976273
Loss in iteration 153 : 0.46960325496427424
Loss in iteration 154 : 0.4695300032898887
Loss in iteration 155 : 0.4694575843874035
Loss in iteration 156 : 0.46938598600438436
Loss in iteration 157 : 0.46931519611734124
Loss in iteration 158 : 0.4692452029262125
Loss in iteration 159 : 0.46917599484900957
Loss in iteration 160 : 0.46910756051663943
Loss in iteration 161 : 0.469039888767864
Loss in iteration 162 : 0.46897296864442933
Loss in iteration 163 : 0.46890678938631186
Loss in iteration 164 : 0.46884134042713993
Loss in iteration 165 : 0.4687766113897211
Loss in iteration 166 : 0.46871259208171007
Loss in iteration 167 : 0.46864927249140603
Loss in iteration 168 : 0.4685866427836605
Loss in iteration 169 : 0.4685246932959151
Loss in iteration 170 : 0.4684634145343398
Loss in iteration 171 : 0.4684027971700843
Loss in iteration 172 : 0.4683428320356364
Loss in iteration 173 : 0.4682835101212834
Loss in iteration 174 : 0.4682248225716677
Loss in iteration 175 : 0.46816676068243546
Loss in iteration 176 : 0.4681093158969842
Loss in iteration 177 : 0.46805247980328907
Loss in iteration 178 : 0.4679962441308315
Loss in iteration 179 : 0.4679406007475826
Loss in iteration 180 : 0.4678855416571021
Loss in iteration 181 : 0.4678310589956819
Loss in iteration 182 : 0.46777714502958884
Loss in iteration 183 : 0.4677237921523634
Loss in iteration 184 : 0.46767099288220254
Loss in iteration 185 : 0.4676187398593964
Loss in iteration 186 : 0.46756702584384585
Loss in iteration 187 : 0.4675158437126321
Loss in iteration 188 : 0.467465186457649
Loss in iteration 189 : 0.4674150471833084
Loss in iteration 190 : 0.4673654191042805
Loss in iteration 191 : 0.4673162955433194
Loss in iteration 192 : 0.46726766992911256
Loss in iteration 193 : 0.46721953579420533
Loss in iteration 194 : 0.46717188677297394
Loss in iteration 195 : 0.4671247165996276
Loss in iteration 196 : 0.46707801910629604
Loss in iteration 197 : 0.4670317882211298
Loss in iteration 198 : 0.4669860179664579
Loss in iteration 199 : 0.46694070245699754
Loss in iteration 200 : 0.4668958358980934
Loss in iteration 201 : 0.4668514125840088
Loss in iteration 202 : 0.46680742689625687
Loss in iteration 203 : 0.46676387330195174
Loss in iteration 204 : 0.4667207463522286
Loss in iteration 205 : 0.4666780406806829
Loss in iteration 206 : 0.4666357510018372
Loss in iteration 207 : 0.4665938721096695
Loss in iteration 208 : 0.46655239887614075
Loss in iteration 209 : 0.4665113262497943
Loss in iteration 210 : 0.4664706492543494
Loss in iteration 211 : 0.46643036298735263
Loss in iteration 212 : 0.46639046261885003
Loss in iteration 213 : 0.466350943390085
Loss in iteration 214 : 0.46631180061223604
Loss in iteration 215 : 0.4662730296651661
Loss in iteration 216 : 0.4662346259962181
Loss in iteration 217 : 0.4661965851190194
Loss in iteration 218 : 0.4661589026123238
Loss in iteration 219 : 0.4661215741188721
Loss in iteration 220 : 0.4660845953442831
Loss in iteration 221 : 0.4660479620559593
Loss in iteration 222 : 0.46601167008202365
Loss in iteration 223 : 0.4659757153102789
Loss in iteration 224 : 0.46594009368718103
Loss in iteration 225 : 0.46590480121684197
Loss in iteration 226 : 0.46586983396004783
Loss in iteration 227 : 0.46583518803330864
Loss in iteration 228 : 0.4658008596079039
Loss in iteration 229 : 0.46576684490897546
Loss in iteration 230 : 0.4657331402146206
Loss in iteration 231 : 0.4656997418550106
Loss in iteration 232 : 0.4656666462115246
Loss in iteration 233 : 0.46563384971590516
Loss in iteration 234 : 0.4656013488494224
Loss in iteration 235 : 0.46556914014206496
Loss in iteration 236 : 0.4655372201717403
Loss in iteration 237 : 0.46550558556348925
Loss in iteration 238 : 0.46547423298872553
Loss in iteration 239 : 0.4654431591644814
Loss in iteration 240 : 0.4654123608526692
Loss in iteration 241 : 0.4653818348593593
Loss in iteration 242 : 0.46535157803407334
Loss in iteration 243 : 0.4653215872690894
Loss in iteration 244 : 0.4652918594987557
Loss in iteration 245 : 0.4652623916988276
Loss in iteration 246 : 0.4652331808858091
Loss in iteration 247 : 0.46520422411631057
Loss in iteration 248 : 0.4651755184864152
Loss in iteration 249 : 0.46514706113106274
Loss in iteration 250 : 0.4651188492234381
Loss in iteration 251 : 0.46509087997438053
Loss in iteration 252 : 0.4650631506317877
Loss in iteration 253 : 0.4650356584800578
Loss in iteration 254 : 0.4650084008395075
Loss in iteration 255 : 0.4649813750658315
Loss in iteration 256 : 0.46495457854955036
Loss in iteration 257 : 0.46492800871548545
Loss in iteration 258 : 0.46490166302222835
Loss in iteration 259 : 0.464875538961627
Loss in iteration 260 : 0.464849634058286
Loss in iteration 261 : 0.4648239458690616
Loss in iteration 262 : 0.4647984719825848
Loss in iteration 263 : 0.4647732100187765
Loss in iteration 264 : 0.46474815762837696
Loss in iteration 265 : 0.46472331249248644
Loss in iteration 266 : 0.4646986723221124
Loss in iteration 267 : 0.46467423485772336
Loss in iteration 268 : 0.4646499978688088
Loss in iteration 269 : 0.4646259591534499
Loss in iteration 270 : 0.46460211653790184
Loss in iteration 271 : 0.46457846787617113
Loss in iteration 272 : 0.46455501104961183
Loss in iteration 273 : 0.4645317439665222
Loss in iteration 274 : 0.4645086645617531
Loss in iteration 275 : 0.4644857707963206
Loss in iteration 276 : 0.4644630606570229
Loss in iteration 277 : 0.464440532156063
Loss in iteration 278 : 0.4644181833306881
Testing accuracy  of updater 0 on alg 0 with rate 1.0 = 0.790625, training accuracy 0.790625, time elapsed: 4961 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6878658918511898
Loss in iteration 3 : 0.6839999898353142
Loss in iteration 4 : 0.6808139087951758
Loss in iteration 5 : 0.6779759015464742
Loss in iteration 6 : 0.6753351733505979
Loss in iteration 7 : 0.6728218611194686
Loss in iteration 8 : 0.6704024793367385
Loss in iteration 9 : 0.6680600344706835
Loss in iteration 10 : 0.6657851055363639
Loss in iteration 11 : 0.6635718203046845
Loss in iteration 12 : 0.6614160283793032
Loss in iteration 13 : 0.6593144648356358
Loss in iteration 14 : 0.6572643624102471
Loss in iteration 15 : 0.6552632676432818
Loss in iteration 16 : 0.6533089502150735
Loss in iteration 17 : 0.6513993551912489
Loss in iteration 18 : 0.6495325752975566
Loss in iteration 19 : 0.6477068327923036
Loss in iteration 20 : 0.6459204661632976
Loss in iteration 21 : 0.6441719194500578
Loss in iteration 22 : 0.6424597331622375
Loss in iteration 23 : 0.6407825362970553
Loss in iteration 24 : 0.6391390392002929
Loss in iteration 25 : 0.6375280271260977
Loss in iteration 26 : 0.6359483544020409
Loss in iteration 27 : 0.6343989391308689
Loss in iteration 28 : 0.6328787583733315
Loss in iteration 29 : 0.63138684376433
Loss in iteration 30 : 0.6299222775200337
Loss in iteration 31 : 0.6284841887979914
Loss in iteration 32 : 0.6270717503759338
Loss in iteration 33 : 0.6256841756183785
Loss in iteration 34 : 0.6243207157031229
Loss in iteration 35 : 0.6229806570825679
Loss in iteration 36 : 0.6216633191572712
Loss in iteration 37 : 0.6203680521414728
Loss in iteration 38 : 0.6190942351023505
Loss in iteration 39 : 0.6178412741567088
Loss in iteration 40 : 0.6166086008103466
Loss in iteration 41 : 0.6153956704270135
Loss in iteration 42 : 0.6142019608150303
Loss in iteration 43 : 0.6130269709209849
Loss in iteration 44 : 0.6118702196209101
Loss in iteration 45 : 0.6107312446003315
Loss in iteration 46 : 0.6096096013154189
Loss in iteration 47 : 0.6085048620282412
Loss in iteration 48 : 0.6074166149098146
Loss in iteration 49 : 0.606344463205223
Loss in iteration 50 : 0.6052880244556702
Loss in iteration 51 : 0.6042469297727403
Loss in iteration 52 : 0.6032208231607084
Loss in iteration 53 : 0.6022093608829873
Loss in iteration 54 : 0.6012122108692547
Loss in iteration 55 : 0.600229052160073
Loss in iteration 56 : 0.5992595743861374
Loss in iteration 57 : 0.5983034772794824
Loss in iteration 58 : 0.5973604702142854
Loss in iteration 59 : 0.5964302717750442
Loss in iteration 60 : 0.5955126093501358
Loss in iteration 61 : 0.5946072187489426
Loss in iteration 62 : 0.5937138438408323
Loss in iteration 63 : 0.5928322362144686
Loss in iteration 64 : 0.5919621548560601
Loss in iteration 65 : 0.5911033658451945
Loss in iteration 66 : 0.5902556420671362
Loss in iteration 67 : 0.5894187629404285
Loss in iteration 68 : 0.5885925141588142
Loss in iteration 69 : 0.5877766874465385
Loss in iteration 70 : 0.5869710803261741
Loss in iteration 71 : 0.5861754958981621
Loss in iteration 72 : 0.5853897426313479
Loss in iteration 73 : 0.5846136341638047
Loss in iteration 74 : 0.5838469891133456
Loss in iteration 75 : 0.5830896308971142
Loss in iteration 76 : 0.5823413875597203
Loss in iteration 77 : 0.5816020916094078
Loss in iteration 78 : 0.580871579861795
Loss in iteration 79 : 0.5801496932907491
Loss in iteration 80 : 0.579436276885978
Loss in iteration 81 : 0.578731179516969
Loss in iteration 82 : 0.5780342538029286
Loss in iteration 83 : 0.5773453559883697
Loss in iteration 84 : 0.5766643458240654
Loss in iteration 85 : 0.5759910864530619
Loss in iteration 86 : 0.5753254443014858
Loss in iteration 87 : 0.5746672889739062
Loss in iteration 88 : 0.5740164931529833
Loss in iteration 89 : 0.5733729325032358
Loss in iteration 90 : 0.572736485578649
Loss in iteration 91 : 0.5721070337339967
Loss in iteration 92 : 0.5714844610396388
Loss in iteration 93 : 0.5708686541996695
Loss in iteration 94 : 0.5702595024732089
Loss in iteration 95 : 0.5696568975987187
Loss in iteration 96 : 0.5690607337211947
Loss in iteration 97 : 0.5684709073220674
Loss in iteration 98 : 0.5678873171517357
Loss in iteration 99 : 0.5673098641645661
Loss in iteration 100 : 0.5667384514562779
Loss in iteration 101 : 0.5661729842035761
Loss in iteration 102 : 0.5656133696059705
Loss in iteration 103 : 0.5650595168296256
Loss in iteration 104 : 0.5645113369532251
Loss in iteration 105 : 0.5639687429156889
Loss in iteration 106 : 0.563431649465731
Loss in iteration 107 : 0.5628999731131152
Loss in iteration 108 : 0.5623736320815962
Loss in iteration 109 : 0.561852546263424
Loss in iteration 110 : 0.561336637175388
Loss in iteration 111 : 0.5608258279163042
Loss in iteration 112 : 0.5603200431259152
Loss in iteration 113 : 0.5598192089451138
Loss in iteration 114 : 0.5593232529774745
Loss in iteration 115 : 0.5588321042520125
Loss in iteration 116 : 0.5583456931871228
Loss in iteration 117 : 0.5578639515556751
Loss in iteration 118 : 0.5573868124511995
Loss in iteration 119 : 0.5569142102551149
Loss in iteration 120 : 0.5564460806049967
Loss in iteration 121 : 0.5559823603637962
Loss in iteration 122 : 0.5555229875900161
Loss in iteration 123 : 0.5550679015087795
Loss in iteration 124 : 0.5546170424837766
Loss in iteration 125 : 0.5541703519900384
Loss in iteration 126 : 0.5537277725875431
Loss in iteration 127 : 0.5532892478955703
Loss in iteration 128 : 0.5528547225678234
Loss in iteration 129 : 0.552424142268282
Loss in iteration 130 : 0.5519974536477373
Loss in iteration 131 : 0.5515746043210106
Loss in iteration 132 : 0.5511555428448187
Loss in iteration 133 : 0.5507402186962724
Loss in iteration 134 : 0.5503285822519699
Loss in iteration 135 : 0.5499205847676966
Loss in iteration 136 : 0.5495161783586641
Loss in iteration 137 : 0.5491153159803134
Loss in iteration 138 : 0.5487179514096426
Loss in iteration 139 : 0.5483240392270542
Loss in iteration 140 : 0.5479335347986641
Loss in iteration 141 : 0.5475463942591394
Loss in iteration 142 : 0.5471625744949411
Loss in iteration 143 : 0.5467820331280697
Loss in iteration 144 : 0.5464047285001964
Loss in iteration 145 : 0.5460306196572451
Loss in iteration 146 : 0.5456596663343598
Loss in iteration 147 : 0.5452918289412833
Loss in iteration 148 : 0.5449270685480945
Loss in iteration 149 : 0.5445653468713367
Loss in iteration 150 : 0.5442066262604823
Loss in iteration 151 : 0.5438508696847607
Loss in iteration 152 : 0.5434980407203079
Loss in iteration 153 : 0.543148103537654
Loss in iteration 154 : 0.5428010228895156
Loss in iteration 155 : 0.5424567640989
Loss in iteration 156 : 0.5421152930474964
Loss in iteration 157 : 0.5417765761643755
Loss in iteration 158 : 0.541440580414948
Loss in iteration 159 : 0.5411072732901961
Loss in iteration 160 : 0.5407766227961857
Loss in iteration 161 : 0.5404485974438105
Loss in iteration 162 : 0.5401231662388015
Loss in iteration 163 : 0.5398002986719714
Loss in iteration 164 : 0.5394799647096882
Loss in iteration 165 : 0.5391621347845819
Loss in iteration 166 : 0.5388467797864687
Loss in iteration 167 : 0.5385338710534981
Loss in iteration 168 : 0.5382233803634981
Loss in iteration 169 : 0.5379152799255184
Loss in iteration 170 : 0.5376095423715903
Loss in iteration 171 : 0.5373061407486655
Loss in iteration 172 : 0.537005048510732
Loss in iteration 173 : 0.5367062395111318
Loss in iteration 174 : 0.5364096879950401
Loss in iteration 175 : 0.5361153685921249
Loss in iteration 176 : 0.5358232563093577
Loss in iteration 177 : 0.5355333265240092
Loss in iteration 178 : 0.5352455549767818
Loss in iteration 179 : 0.5349599177651111
Loss in iteration 180 : 0.5346763913366124
Loss in iteration 181 : 0.5343949524826606
Loss in iteration 182 : 0.534115578332136
Loss in iteration 183 : 0.5338382463452856
Loss in iteration 184 : 0.5335629343077333
Loss in iteration 185 : 0.5332896203246098
Loss in iteration 186 : 0.5330182828148268
Loss in iteration 187 : 0.5327489005054538
Loss in iteration 188 : 0.5324814524262329
Loss in iteration 189 : 0.5322159179042092
Loss in iteration 190 : 0.531952276558467
Loss in iteration 191 : 0.5316905082949829
Loss in iteration 192 : 0.5314305933016
Loss in iteration 193 : 0.5311725120430877
Loss in iteration 194 : 0.5309162452563184
Loss in iteration 195 : 0.530661773945549
Loss in iteration 196 : 0.5304090793777873
Loss in iteration 197 : 0.5301581430782684
Loss in iteration 198 : 0.5299089468260186
Loss in iteration 199 : 0.5296614726495069
Loss in iteration 200 : 0.5294157028224009
Loss in iteration 201 : 0.529171619859389
Loss in iteration 202 : 0.5289292065121094
Loss in iteration 203 : 0.5286884457651447
Loss in iteration 204 : 0.528449320832105
Loss in iteration 205 : 0.5282118151517964
Loss in iteration 206 : 0.5279759123844546
Loss in iteration 207 : 0.527741596408059
Loss in iteration 208 : 0.5275088513147268
Loss in iteration 209 : 0.5272776614071684
Loss in iteration 210 : 0.5270480111952226
Loss in iteration 211 : 0.5268198853924541
Loss in iteration 212 : 0.5265932689128179
Loss in iteration 213 : 0.5263681468673931
Loss in iteration 214 : 0.5261445045611867
Loss in iteration 215 : 0.5259223274899767
Loss in iteration 216 : 0.525701601337246
Loss in iteration 217 : 0.5254823119711537
Loss in iteration 218 : 0.525264445441576
Loss in iteration 219 : 0.5250479879772034
Loss in iteration 220 : 0.5248329259826839
Loss in iteration 221 : 0.5246192460358357
Loss in iteration 222 : 0.5244069348849016
Loss in iteration 223 : 0.5241959794458578
Loss in iteration 224 : 0.5239863667997806
Loss in iteration 225 : 0.5237780841902514
Loss in iteration 226 : 0.5235711190208238
Loss in iteration 227 : 0.5233654588525203
Loss in iteration 228 : 0.5231610914013951
Loss in iteration 229 : 0.5229580045361306
Loss in iteration 230 : 0.5227561862756805
Loss in iteration 231 : 0.5225556247869576
Loss in iteration 232 : 0.5223563083825657
Loss in iteration 233 : 0.522158225518568
Loss in iteration 234 : 0.521961364792302
Loss in iteration 235 : 0.5217657149402345
Loss in iteration 236 : 0.5215712648358486
Loss in iteration 237 : 0.5213780034875749
Loss in iteration 238 : 0.5211859200367606
Loss in iteration 239 : 0.5209950037556766
Loss in iteration 240 : 0.5208052440455458
Loss in iteration 241 : 0.5206166304346289
Loss in iteration 242 : 0.5204291525763342
Loss in iteration 243 : 0.5202428002473548
Loss in iteration 244 : 0.5200575633458454
Loss in iteration 245 : 0.5198734318896359
Loss in iteration 246 : 0.5196903960144641
Loss in iteration 247 : 0.519508445972253
Loss in iteration 248 : 0.5193275721294079
Loss in iteration 249 : 0.5191477649651509
Loss in iteration 250 : 0.5189690150698778
Loss in iteration 251 : 0.5187913131435484
Loss in iteration 252 : 0.5186146499941027
Loss in iteration 253 : 0.5184390165359016
Loss in iteration 254 : 0.5182644037882066
Loss in iteration 255 : 0.5180908028736669
Loss in iteration 256 : 0.5179182050168456
Loss in iteration 257 : 0.5177466015427711
Loss in iteration 258 : 0.5175759838755042
Loss in iteration 259 : 0.5174063435367389
Loss in iteration 260 : 0.5172376721444212
Loss in iteration 261 : 0.5170699614113944
Loss in iteration 262 : 0.5169032031440651
Loss in iteration 263 : 0.516737389241093
Loss in iteration 264 : 0.5165725116921001
Loss in iteration 265 : 0.5164085625764037
Loss in iteration 266 : 0.5162455340617771
Loss in iteration 267 : 0.5160834184032118
Loss in iteration 268 : 0.5159222079417234
Loss in iteration 269 : 0.5157618951031566
Loss in iteration 270 : 0.5156024723970286
Loss in iteration 271 : 0.5154439324153743
Loss in iteration 272 : 0.5152862678316273
Loss in iteration 273 : 0.5151294713994994
Loss in iteration 274 : 0.5149735359518952
Loss in iteration 275 : 0.5148184543998424
Loss in iteration 276 : 0.5146642197314285
Loss in iteration 277 : 0.5145108250107636
Loss in iteration 278 : 0.5143582633769569
Loss in iteration 279 : 0.5142065280431145
Loss in iteration 280 : 0.5140556122953451
Loss in iteration 281 : 0.5139055094917906
Loss in iteration 282 : 0.5137562130616563
Loss in iteration 283 : 0.5136077165042855
Loss in iteration 284 : 0.5134600133882141
Loss in iteration 285 : 0.5133130973502673
Loss in iteration 286 : 0.5131669620946526
Loss in iteration 287 : 0.5130216013920829
Loss in iteration 288 : 0.5128770090788973
Loss in iteration 289 : 0.5127331790562093
Loss in iteration 290 : 0.5125901052890552
Loss in iteration 291 : 0.5124477818055727
Loss in iteration 292 : 0.5123062026961755
Loss in iteration 293 : 0.5121653621127491
Loss in iteration 294 : 0.5120252542678584
Loss in iteration 295 : 0.5118858734339649
Loss in iteration 296 : 0.5117472139426605
Loss in iteration 297 : 0.5116092701839083
Loss in iteration 298 : 0.5114720366052993
Loss in iteration 299 : 0.5113355077113143
Loss in iteration 300 : 0.511199678062603
Loss in iteration 301 : 0.5110645422752719
Loss in iteration 302 : 0.5109300950201827
Loss in iteration 303 : 0.5107963310222609
Loss in iteration 304 : 0.5106632450598184
Loss in iteration 305 : 0.5105308319638783
Loss in iteration 306 : 0.5103990866175199
Loss in iteration 307 : 0.5102680039552241
Loss in iteration 308 : 0.5101375789622351
Loss in iteration 309 : 0.5100078066739298
Loss in iteration 310 : 0.5098786821751958
Loss in iteration 311 : 0.5097502005998161
Loss in iteration 312 : 0.5096223571298691
Loss in iteration 313 : 0.5094951469951343
Loss in iteration 314 : 0.5093685654725054
Loss in iteration 315 : 0.5092426078854125
Loss in iteration 316 : 0.5091172696032511
Loss in iteration 317 : 0.5089925460408267
Loss in iteration 318 : 0.5088684326578015
Loss in iteration 319 : 0.5087449249581436
Loss in iteration 320 : 0.5086220184895993
Loss in iteration 321 : 0.5084997088431618
Loss in iteration 322 : 0.5083779916525455
Loss in iteration 323 : 0.5082568625936771
Loss in iteration 324 : 0.5081363173841915
Loss in iteration 325 : 0.5080163517829221
Loss in iteration 326 : 0.5078969615894225
Loss in iteration 327 : 0.5077781426434751
Loss in iteration 328 : 0.5076598908246112
Loss in iteration 329 : 0.5075422020516441
Loss in iteration 330 : 0.5074250722822028
Loss in iteration 331 : 0.5073084975122709
Loss in iteration 332 : 0.5071924737757435
Loss in iteration 333 : 0.507076997143971
Loss in iteration 334 : 0.5069620637253294
Loss in iteration 335 : 0.506847669664787
Loss in iteration 336 : 0.5067338111434632
Loss in iteration 337 : 0.5066204843782344
Loss in iteration 338 : 0.5065076856212933
Loss in iteration 339 : 0.5063954111597535
Loss in iteration 340 : 0.50628365731524
Loss in iteration 341 : 0.5061724204434996
Loss in iteration 342 : 0.5060616969339983
Loss in iteration 343 : 0.5059514832095385
Loss in iteration 344 : 0.5058417757258781
Loss in iteration 345 : 0.5057325709713524
Loss in iteration 346 : 0.5056238654665051
Loss in iteration 347 : 0.505515655763716
Loss in iteration 348 : 0.5054079384468495
Loss in iteration 349 : 0.5053007101308836
Loss in iteration 350 : 0.5051939674615757
Loss in iteration 351 : 0.5050877071151011
Loss in iteration 352 : 0.5049819257977182
Loss in iteration 353 : 0.5048766202454271
Loss in iteration 354 : 0.5047717872236386
Loss in iteration 355 : 0.5046674235268466
Loss in iteration 356 : 0.5045635259783002
Loss in iteration 357 : 0.5044600914296863
Loss in iteration 358 : 0.5043571167608127
Loss in iteration 359 : 0.5042545988792987
Loss in iteration 360 : 0.5041525347202612
Loss in iteration 361 : 0.5040509212460225
Loss in iteration 362 : 0.503949755445797
Loss in iteration 363 : 0.5038490343354094
Loss in iteration 364 : 0.5037487549569932
Loss in iteration 365 : 0.5036489143787058
Loss in iteration 366 : 0.5035495096944476
Loss in iteration 367 : 0.5034505380235759
Loss in iteration 368 : 0.503351996510635
Loss in iteration 369 : 0.5032538823250764
Loss in iteration 370 : 0.5031561926609942
Loss in iteration 371 : 0.5030589247368574
Loss in iteration 372 : 0.5029620757952432
Loss in iteration 373 : 0.5028656431025849
Loss in iteration 374 : 0.5027696239489137
Loss in iteration 375 : 0.5026740156476
Loss in iteration 376 : 0.5025788155351144
Loss in iteration 377 : 0.502484020970772
Loss in iteration 378 : 0.5023896293364948
Loss in iteration 379 : 0.5022956380365675
Loss in iteration 380 : 0.5022020444974052
Loss in iteration 381 : 0.502108846167318
Loss in iteration 382 : 0.5020160405162775
Loss in iteration 383 : 0.5019236250356887
Loss in iteration 384 : 0.5018315972381701
Loss in iteration 385 : 0.5017399546573251
Loss in iteration 386 : 0.5016486948475241
Loss in iteration 387 : 0.5015578153836907
Loss in iteration 388 : 0.5014673138610828
Loss in iteration 389 : 0.5013771878950828
Loss in iteration 390 : 0.5012874351209897
Loss in iteration 391 : 0.5011980531938175
Loss in iteration 392 : 0.501109039788076
Loss in iteration 393 : 0.5010203925975857
Loss in iteration 394 : 0.5009321093352747
Loss in iteration 395 : 0.5008441877329762
Loss in iteration 396 : 0.5007566255412409
Loss in iteration 397 : 0.5006694205291455
Loss in iteration 398 : 0.5005825704840983
Loss in iteration 399 : 0.5004960732116569
Loss in iteration 400 : 0.5004099265353421
Testing accuracy  of updater 0 on alg 0 with rate 0.09999999999999998 = 0.776625, training accuracy 0.776625, time elapsed: 6453 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 2.141549710106488
Loss in iteration 3 : 19.912833318335288
Loss in iteration 4 : 7.26329565976292
Loss in iteration 5 : 24.584270937815585
Loss in iteration 6 : 31.901257938297874
Loss in iteration 7 : 15.571335992746048
Loss in iteration 8 : 13.779093573662813
Loss in iteration 9 : 23.582129583930527
Loss in iteration 10 : 8.618430102979152
Loss in iteration 11 : 10.918571670793302
Loss in iteration 12 : 17.659501645415308
Loss in iteration 13 : 7.160537080861416
Loss in iteration 14 : 8.200299065885332
Loss in iteration 15 : 14.427973212444378
Loss in iteration 16 : 9.454683713401053
Loss in iteration 17 : 5.318414726782044
Loss in iteration 18 : 11.046389048206672
Loss in iteration 19 : 9.955899554541949
Loss in iteration 20 : 5.466558546742433
Loss in iteration 21 : 8.305500239795158
Loss in iteration 22 : 9.74469786013283
Loss in iteration 23 : 6.645437470716246
Loss in iteration 24 : 6.001239377838218
Loss in iteration 25 : 8.502556853610052
Loss in iteration 26 : 6.876025234315871
Loss in iteration 27 : 5.391798725838787
Loss in iteration 28 : 7.042042128121348
Loss in iteration 29 : 6.893136347497736
Loss in iteration 30 : 5.133855167160736
Loss in iteration 31 : 5.791644534734023
Loss in iteration 32 : 6.294025807286129
Loss in iteration 33 : 4.758703691352934
Loss in iteration 34 : 5.168414150441388
Loss in iteration 35 : 5.525393191109944
Loss in iteration 36 : 4.392151481039302
Loss in iteration 37 : 4.543210763209445
Loss in iteration 38 : 4.784818901469662
Loss in iteration 39 : 3.8050358843141123
Loss in iteration 40 : 4.250634764019573
Loss in iteration 41 : 4.014342802231846
Loss in iteration 42 : 3.3997963330524255
Loss in iteration 43 : 3.8467454039220934
Loss in iteration 44 : 3.160172225413199
Loss in iteration 45 : 3.3101894563293044
Loss in iteration 46 : 3.163996049870915
Loss in iteration 47 : 2.8055997658705594
Loss in iteration 48 : 3.0097529552486173
Loss in iteration 49 : 2.521242703766333
Loss in iteration 50 : 2.7692677749648236
Loss in iteration 51 : 2.3237423376401694
Loss in iteration 52 : 2.5692828285173634
Loss in iteration 53 : 2.158678859170491
Loss in iteration 54 : 2.377332257454441
Loss in iteration 55 : 2.0380957616833637
Loss in iteration 56 : 2.1626779084226886
Loss in iteration 57 : 2.0730223767231433
Loss in iteration 58 : 1.8608108547270668
Loss in iteration 59 : 2.0778006146006875
Loss in iteration 60 : 1.9286789847298016
Loss in iteration 61 : 1.67475989263265
Loss in iteration 62 : 1.811206436634948
Loss in iteration 63 : 1.9646897769623806
Loss in iteration 64 : 1.7589848327163684
Loss in iteration 65 : 1.5074865543603386
Loss in iteration 66 : 1.3359119160781312
Loss in iteration 67 : 1.287207494471519
Loss in iteration 68 : 1.362842920965161
Loss in iteration 69 : 1.6733350209947684
Loss in iteration 70 : 2.593327549247612
Loss in iteration 71 : 2.943139864469047
Loss in iteration 72 : 2.7794999319680365
Loss in iteration 73 : 1.6589877320217208
Loss in iteration 74 : 1.2420285142861898
Loss in iteration 75 : 1.0752238167409511
Loss in iteration 76 : 1.0156554324751519
Loss in iteration 77 : 0.9769628586304493
Loss in iteration 78 : 0.935036781583341
Loss in iteration 79 : 0.8882470091216371
Loss in iteration 80 : 0.8450223117940671
Loss in iteration 81 : 0.8034707569475906
Loss in iteration 82 : 0.7585552061800782
Loss in iteration 83 : 0.9160433050819802
Loss in iteration 84 : 4.712245396643273
Loss in iteration 85 : 9.4295832536735
Loss in iteration 86 : 0.7199790456102292
Loss in iteration 87 : 5.741364710427388
Loss in iteration 88 : 7.184991107177819
Loss in iteration 89 : 1.11095303237753
Loss in iteration 90 : 3.67399586353951
Loss in iteration 91 : 5.5455282839411595
Loss in iteration 92 : 1.3896223786878754
Loss in iteration 93 : 5.680948323657673
Loss in iteration 94 : 2.102350120655962
Loss in iteration 95 : 3.1540044331402526
Loss in iteration 96 : 2.9165252805932655
Loss in iteration 97 : 1.881645181959208
Loss in iteration 98 : 3.1470744796259686
Loss in iteration 99 : 1.763401801349125
Loss in iteration 100 : 3.335792831976215
Loss in iteration 101 : 2.0127698586558536
Loss in iteration 102 : 2.4350252968594033
Loss in iteration 103 : 2.2273781072046064
Loss in iteration 104 : 1.6129409760977795
Loss in iteration 105 : 2.2208311297357057
Loss in iteration 106 : 1.4963520060607065
Loss in iteration 107 : 1.6828859016998954
Loss in iteration 108 : 2.1344421289335074
Loss in iteration 109 : 1.354671476210652
Loss in iteration 110 : 1.2836146789128648
Loss in iteration 111 : 1.949175367706022
Loss in iteration 112 : 2.304987060726714
Loss in iteration 113 : 2.4047599828981605
Loss in iteration 114 : 1.899597598587554
Loss in iteration 115 : 1.8571026314760248
Loss in iteration 116 : 2.1154346479781525
Loss in iteration 117 : 2.8516122918563327
Loss in iteration 118 : 2.9296340028370635
Loss in iteration 119 : 2.2610562129335157
Loss in iteration 120 : 1.647695699641764
Loss in iteration 121 : 1.3394954643322212
Loss in iteration 122 : 1.1689546310611414
Loss in iteration 123 : 1.0780431814834082
Loss in iteration 124 : 1.023861849493166
Loss in iteration 125 : 1.0033532937248164
Loss in iteration 126 : 1.0582194617358127
Loss in iteration 127 : 1.584854750396372
Loss in iteration 128 : 3.775851553236046
Loss in iteration 129 : 5.670835979643233
Loss in iteration 130 : 1.0474903083843643
Loss in iteration 131 : 2.1629034402580327
Loss in iteration 132 : 6.642006360243778
Loss in iteration 133 : 1.1108875495401862
Loss in iteration 134 : 4.330357352548269
Loss in iteration 135 : 4.52695489639609
Loss in iteration 136 : 1.640144909150356
Loss in iteration 137 : 6.06327907330742
Loss in iteration 138 : 1.5440453943299344
Loss in iteration 139 : 4.31171215497728
Loss in iteration 140 : 1.9015471508322461
Loss in iteration 141 : 2.953414301702348
Loss in iteration 142 : 2.933097538442061
Loss in iteration 143 : 2.067856536250197
Loss in iteration 144 : 3.5956931331611877
Loss in iteration 145 : 1.6175766206276976
Loss in iteration 146 : 2.644504479896867
Loss in iteration 147 : 1.874536089799145
Loss in iteration 148 : 1.6309594985461007
Loss in iteration 149 : 2.356495873808738
Loss in iteration 150 : 1.4143129339087246
Loss in iteration 151 : 1.703699369098662
Loss in iteration 152 : 2.513447015379411
Loss in iteration 153 : 1.4155006908684877
Loss in iteration 154 : 1.177183061004788
Loss in iteration 155 : 1.904073949091236
Loss in iteration 156 : 2.648517352406797
Loss in iteration 157 : 2.8647313187003247
Loss in iteration 158 : 1.8361689634448475
Loss in iteration 159 : 1.4058710764724613
Loss in iteration 160 : 1.3015756090956483
Loss in iteration 161 : 1.4674803454666443
Loss in iteration 162 : 2.201769026640768
Loss in iteration 163 : 3.70785975163971
Loss in iteration 164 : 3.0611857345228364
Loss in iteration 165 : 2.0653408468600585
Loss in iteration 166 : 1.540720617975829
Loss in iteration 167 : 1.4176556527931585
Loss in iteration 168 : 1.3783580064048087
Loss in iteration 169 : 1.496550015024416
Loss in iteration 170 : 1.6721093859796012
Loss in iteration 171 : 2.1121319071474827
Loss in iteration 172 : 2.127877039927662
Loss in iteration 173 : 2.2992328411767287
Loss in iteration 174 : 1.763114224432275
Loss in iteration 175 : 1.63929383988347
Loss in iteration 176 : 1.440104947450238
Loss in iteration 177 : 1.5249318531827576
Loss in iteration 178 : 1.6606492890810707
Loss in iteration 179 : 2.258926485621814
Loss in iteration 180 : 2.4418816180902208
Loss in iteration 181 : 2.6127182742781176
Loss in iteration 182 : 1.9190948923739466
Loss in iteration 183 : 1.6707861886876676
Loss in iteration 184 : 1.4592013903199035
Loss in iteration 185 : 1.519147907752767
Loss in iteration 186 : 1.6595187121153527
Loss in iteration 187 : 2.318720852661679
Loss in iteration 188 : 2.6232490255920937
Loss in iteration 189 : 2.879797992045602
Loss in iteration 190 : 1.8522882911967065
Loss in iteration 191 : 1.5249388259284802
Loss in iteration 192 : 1.337338145822986
Loss in iteration 193 : 1.4045264128117285
Loss in iteration 194 : 1.5489239199187714
Loss in iteration 195 : 2.1635155782107285
Loss in iteration 196 : 2.5315610185857014
Loss in iteration 197 : 2.8992116711685805
Loss in iteration 198 : 1.9042263181620533
Loss in iteration 199 : 1.540518131830815
Loss in iteration 200 : 1.3482201377399328
Loss in iteration 201 : 1.4190982565082062
Loss in iteration 202 : 1.6295238629761046
Loss in iteration 203 : 2.3777007782424984
Loss in iteration 204 : 2.742901360223701
Loss in iteration 205 : 2.8440709136320805
Loss in iteration 206 : 1.819790196101439
Loss in iteration 207 : 1.457143872450547
Loss in iteration 208 : 1.2736452207939821
Loss in iteration 209 : 1.2968929075481765
Loss in iteration 210 : 1.4377226376670746
Loss in iteration 211 : 2.0703038365619935
Loss in iteration 212 : 2.762386035825253
Loss in iteration 213 : 3.3208894746426263
Loss in iteration 214 : 1.8663757047305634
Loss in iteration 215 : 1.3635500272052443
Loss in iteration 216 : 1.1589746131173988
Loss in iteration 217 : 1.1308107540670007
Loss in iteration 218 : 1.1940383052485182
Loss in iteration 219 : 1.6214745256760426
Loss in iteration 220 : 2.7206867027226598
Loss in iteration 221 : 4.185709374486894
Loss in iteration 222 : 1.8910117454020487
Loss in iteration 223 : 1.208056782685708
Loss in iteration 224 : 1.031580398417431
Loss in iteration 225 : 0.9899413295192184
Loss in iteration 226 : 1.0036184578564904
Loss in iteration 227 : 1.2528438771016186
Loss in iteration 228 : 2.5172690781911777
Loss in iteration 229 : 5.400797045146242
Loss in iteration 230 : 1.9289765956796945
Loss in iteration 231 : 1.104717910370171
Loss in iteration 232 : 0.9686418291550571
Loss in iteration 233 : 0.9441917748854368
Loss in iteration 234 : 0.9707931926249944
Loss in iteration 235 : 1.2686297831540596
Loss in iteration 236 : 2.7844003229249275
Loss in iteration 237 : 5.64871538700613
Loss in iteration 238 : 1.630296706144663
Loss in iteration 239 : 0.9137977279982159
Loss in iteration 240 : 1.2277355999105874
Loss in iteration 241 : 2.6322715709003033
Loss in iteration 242 : 4.5994735504957225
Loss in iteration 243 : 1.4701796830893903
Loss in iteration 244 : 1.1104406120215617
Loss in iteration 245 : 2.5753278370400725
Loss in iteration 246 : 2.9841933140742345
Loss in iteration 247 : 2.082494961351215
Loss in iteration 248 : 1.1129428699168675
Loss in iteration 249 : 1.4736222549985
Loss in iteration 250 : 2.712472831554077
Loss in iteration 251 : 2.0053513101516907
Loss in iteration 252 : 1.3713780798440183
Loss in iteration 253 : 1.0424114251319683
Loss in iteration 254 : 1.0899917385494577
Loss in iteration 255 : 1.5955029462373185
Loss in iteration 256 : 2.6772275275954094
Loss in iteration 257 : 4.164031204302175
Loss in iteration 258 : 1.7798214623358342
Loss in iteration 259 : 1.138095058705581
Loss in iteration 260 : 0.9856991670779381
Loss in iteration 261 : 0.9466647929446151
Loss in iteration 262 : 0.9715333420148836
Loss in iteration 263 : 1.3300298622088513
Loss in iteration 264 : 3.3528954427882756
Loss in iteration 265 : 5.94642603629712
Loss in iteration 266 : 1.468978325834299
Loss in iteration 267 : 0.9113013377818836
Loss in iteration 268 : 1.8357341413043973
Loss in iteration 269 : 4.079796055148328
Loss in iteration 270 : 3.5307337382227297
Loss in iteration 271 : 1.1469760920617562
Loss in iteration 272 : 1.758790981665362
Loss in iteration 273 : 3.6978691187217128
Loss in iteration 274 : 1.6639337091757975
Loss in iteration 275 : 1.2656344267301243
Loss in iteration 276 : 2.4643334477798007
Loss in iteration 277 : 2.050260118374392
Loss in iteration 278 : 1.3444035736091662
Loss in iteration 279 : 1.1560052745899747
Loss in iteration 280 : 1.6591067231508043
Loss in iteration 281 : 2.6266847980774126
Loss in iteration 282 : 1.8243728364719414
Loss in iteration 283 : 1.4283295061949344
Loss in iteration 284 : 1.1808211376109525
Loss in iteration 285 : 1.1806835322417273
Loss in iteration 286 : 1.4208339172733868
Loss in iteration 287 : 2.740929561075275
Loss in iteration 288 : 4.006833392473497
Loss in iteration 289 : 3.121847972269257
Loss in iteration 290 : 1.721360051276997
Loss in iteration 291 : 1.3362444398915898
Loss in iteration 292 : 1.2383506809443507
Loss in iteration 293 : 1.2693343505594028
Loss in iteration 294 : 1.4697185251859703
Loss in iteration 295 : 2.241172169449551
Loss in iteration 296 : 3.1887678384637193
Loss in iteration 297 : 3.437416067477167
Loss in iteration 298 : 1.584162569374811
Loss in iteration 299 : 1.108297286496424
Loss in iteration 300 : 0.9940138070029082
Loss in iteration 301 : 0.9966105880517111
Loss in iteration 302 : 1.139758513888253
Loss in iteration 303 : 1.6881750182820618
Loss in iteration 304 : 3.532701508909712
Loss in iteration 305 : 3.0772988694295127
Loss in iteration 306 : 2.4289270021567453
Loss in iteration 307 : 1.6513063746501684
Loss in iteration 308 : 1.4841083070185583
Loss in iteration 309 : 1.3887386136741047
Loss in iteration 310 : 1.4810850891345617
Loss in iteration 311 : 1.6841365466404892
Loss in iteration 312 : 2.259662533937754
Loss in iteration 313 : 2.4913866910296503
Loss in iteration 314 : 2.7019342214734907
Loss in iteration 315 : 1.909102941707819
Loss in iteration 316 : 1.720479079361212
Loss in iteration 317 : 1.513664882963541
Loss in iteration 318 : 1.6476928658360244
Loss in iteration 319 : 1.7540391053544337
Loss in iteration 320 : 2.299275581504014
Loss in iteration 321 : 2.238629470391312
Loss in iteration 322 : 2.292519128376307
Loss in iteration 323 : 1.7660718549025223
Loss in iteration 324 : 1.6176079611207348
Loss in iteration 325 : 1.4659665119477858
Loss in iteration 326 : 1.5550315723330654
Loss in iteration 327 : 1.6753178692586244
Loss in iteration 328 : 2.2397015241958336
Loss in iteration 329 : 2.522994789871721
Loss in iteration 330 : 2.815888509688508
Loss in iteration 331 : 1.984949583901189
Loss in iteration 332 : 1.6794684213848075
Loss in iteration 333 : 1.4943347877844961
Loss in iteration 334 : 1.5894158593544196
Loss in iteration 335 : 1.7095608760769427
Loss in iteration 336 : 2.156053485917859
Loss in iteration 337 : 2.2482840014787433
Loss in iteration 338 : 2.426648985312453
Loss in iteration 339 : 1.9005160495670588
Loss in iteration 340 : 1.7375845366643867
Loss in iteration 341 : 1.5483695876995316
Loss in iteration 342 : 1.6398557753319232
Loss in iteration 343 : 1.7244462251451835
Loss in iteration 344 : 2.154218588433273
Loss in iteration 345 : 2.222345981628708
Loss in iteration 346 : 2.41781033833539
Loss in iteration 347 : 1.9248649772268387
Loss in iteration 348 : 1.766046526010543
Loss in iteration 349 : 1.5646109602785023
Loss in iteration 350 : 1.6302736285617507
Loss in iteration 351 : 1.7013559898135902
Loss in iteration 352 : 2.1088754830886955
Loss in iteration 353 : 2.255100064614531
Loss in iteration 354 : 2.5176209536276617
Loss in iteration 355 : 1.9916210869053386
Loss in iteration 356 : 1.8017642208793598
Loss in iteration 357 : 1.5692507760148915
Loss in iteration 358 : 1.6124613070769027
Loss in iteration 359 : 1.6567584123315882
Loss in iteration 360 : 2.025384164446991
Loss in iteration 361 : 2.2011123404098445
Loss in iteration 362 : 2.527746816306297
Loss in iteration 363 : 2.0240205949526526
Loss in iteration 364 : 1.8359121167583945
Loss in iteration 365 : 1.5828807787237313
Loss in iteration 366 : 1.6119135405821865
Loss in iteration 367 : 1.6330879519713453
Loss in iteration 368 : 1.9690195909668577
Loss in iteration 369 : 2.1665323330062924
Loss in iteration 370 : 2.5443817418758883
Loss in iteration 371 : 2.0773800378211345
Loss in iteration 372 : 1.8910902184366938
Loss in iteration 373 : 1.6148831876995033
Loss in iteration 374 : 1.62924340015018
Loss in iteration 375 : 1.6258443466618093
Loss in iteration 376 : 1.9206201807206875
Loss in iteration 377 : 2.1063603588995488
Loss in iteration 378 : 2.5092655783511932
Loss in iteration 379 : 2.115119665754486
Loss in iteration 380 : 1.963252479123043
Loss in iteration 381 : 1.6593352523032872
Loss in iteration 382 : 1.6556777059606056
Loss in iteration 383 : 1.6160642269023564
Loss in iteration 384 : 1.854387408221269
Loss in iteration 385 : 2.0101801884959554
Loss in iteration 386 : 2.4253941763950277
Loss in iteration 387 : 2.1524881675740573
Loss in iteration 388 : 2.0684070063597284
Loss in iteration 389 : 1.7308487073440537
Loss in iteration 390 : 1.699721744934994
Loss in iteration 391 : 1.613622273708375
Loss in iteration 392 : 1.7897086307223657
Loss in iteration 393 : 1.9131595723773294
Loss in iteration 394 : 2.3268598165385517
Loss in iteration 395 : 2.1871770985859373
Loss in iteration 396 : 2.189444244470667
Loss in iteration 397 : 1.811750826946252
Loss in iteration 398 : 1.7423595018043512
Loss in iteration 399 : 1.6050849508639702
Loss in iteration 400 : 1.720464914561617
Testing accuracy  of updater 1 on alg 0 with rate 10.0 = 0.67725, training accuracy 0.67725, time elapsed: 5604 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6826496441898315
Loss in iteration 3 : 0.6339254231375371
Loss in iteration 4 : 0.5867704951414214
Loss in iteration 5 : 0.5629247813403965
Loss in iteration 6 : 0.5309264422630869
Loss in iteration 7 : 0.5148220298238643
Loss in iteration 8 : 0.5014300866706939
Loss in iteration 9 : 0.49361340845674695
Loss in iteration 10 : 0.4895398353767431
Loss in iteration 11 : 0.4873414015503836
Loss in iteration 12 : 0.48534746038929416
Loss in iteration 13 : 0.48596599650215777
Loss in iteration 14 : 0.4840173168519073
Loss in iteration 15 : 0.4857146609389189
Loss in iteration 16 : 0.48378769976390923
Loss in iteration 17 : 0.48470766070971777
Loss in iteration 18 : 0.483368334567589
Loss in iteration 19 : 0.482548489487799
Loss in iteration 20 : 0.4819658599700555
Loss in iteration 21 : 0.47999165564753715
Loss in iteration 22 : 0.479631916670203
Loss in iteration 23 : 0.4776483370884671
Loss in iteration 24 : 0.47671713059767423
Loss in iteration 25 : 0.47526808608302457
Loss in iteration 26 : 0.4735834702065309
Loss in iteration 27 : 0.47253592833282193
Loss in iteration 28 : 0.4706389269933046
Loss in iteration 29 : 0.46969701289099397
Loss in iteration 30 : 0.46819256438895185
Loss in iteration 31 : 0.4671926091231691
Loss in iteration 32 : 0.46627051259625035
Loss in iteration 33 : 0.4652834041919633
Loss in iteration 34 : 0.46482699453703136
Loss in iteration 35 : 0.4640628105703771
Loss in iteration 36 : 0.4638858293576205
Loss in iteration 37 : 0.46348106477789874
Loss in iteration 38 : 0.46340871264224054
Loss in iteration 39 : 0.46329872825637836
Loss in iteration 40 : 0.4632104743355541
Loss in iteration 41 : 0.4632567117253562
Loss in iteration 42 : 0.463144547139271
Loss in iteration 43 : 0.46324091804174156
Loss in iteration 44 : 0.46313981722026326
Loss in iteration 45 : 0.4632067765776688
Loss in iteration 46 : 0.46312256262487006
Loss in iteration 47 : 0.46310951998571054
Loss in iteration 48 : 0.463032772281511
Loss in iteration 49 : 0.4629412567149818
Loss in iteration 50 : 0.46286719860269726
Loss in iteration 51 : 0.4627243964351826
Loss in iteration 52 : 0.46263978907545183
Loss in iteration 53 : 0.462469779020908
Loss in iteration 54 : 0.4623628086743383
Loss in iteration 55 : 0.46219243478786026
Loss in iteration 56 : 0.4620706673277446
Loss in iteration 57 : 0.46192604452490743
Loss in iteration 58 : 0.46180401697337514
Loss in iteration 59 : 0.4616935354725827
Loss in iteration 60 : 0.4615795874842697
Loss in iteration 61 : 0.4614967689330251
Loss in iteration 62 : 0.46139748605998226
Loss in iteration 63 : 0.46133387465538556
Loss in iteration 64 : 0.46125477348586275
Loss in iteration 65 : 0.4612021818535512
Loss in iteration 66 : 0.46114207350457104
Loss in iteration 67 : 0.46109296267961086
Loss in iteration 68 : 0.46104641306191013
Loss in iteration 69 : 0.46099839924326036
Loss in iteration 70 : 0.4609606847620851
Loss in iteration 71 : 0.46091549269076415
Loss in iteration 72 : 0.46088221312122674
Loss in iteration 73 : 0.46084083016433774
Loss in iteration 74 : 0.4608076279118877
Loss in iteration 75 : 0.4607695808087607
Loss in iteration 76 : 0.4607345057109088
Loss in iteration 77 : 0.46069934420399694
Loss in iteration 78 : 0.4606630076610583
Loss in iteration 79 : 0.4606299671091063
Loss in iteration 80 : 0.46059361545984384
Loss in iteration 81 : 0.46056162310107573
Loss in iteration 82 : 0.4605265930050762
Loss in iteration 83 : 0.46049539722689165
Loss in iteration 84 : 0.4604630641970623
Loss in iteration 85 : 0.4604331470013138
Loss in iteration 86 : 0.46040406424013136
Loss in iteration 87 : 0.460375740001066
Loss in iteration 88 : 0.46034944497340835
Loss in iteration 89 : 0.46032284027084414
Loss in iteration 90 : 0.4602986516056323
Loss in iteration 91 : 0.46027395580339175
Loss in iteration 92 : 0.4602513475455744
Loss in iteration 93 : 0.460228547407536
Loss in iteration 94 : 0.460207051340294
Loss in iteration 95 : 0.46018584877011337
Loss in iteration 96 : 0.46016518049895916
Loss in iteration 97 : 0.46014524774362653
Loss in iteration 98 : 0.46012535856491954
Loss in iteration 99 : 0.4601063894534248
Loss in iteration 100 : 0.460087246968224
Loss in iteration 101 : 0.46006891118800636
Loss in iteration 102 : 0.4600504297269225
Loss in iteration 103 : 0.4600325139243531
Loss in iteration 104 : 0.4600146612598623
Loss in iteration 105 : 0.4599971450034431
Loss in iteration 106 : 0.45997992293334566
Loss in iteration 107 : 0.4599628661744211
Loss in iteration 108 : 0.45994624413583174
Loss in iteration 109 : 0.4599297156539
Loss in iteration 110 : 0.45991366568084263
Loss in iteration 111 : 0.4598977411090762
Loss in iteration 112 : 0.4598822594784118
Loss in iteration 113 : 0.45986697955883826
Loss in iteration 114 : 0.45985205252424216
Loss in iteration 115 : 0.45983739709280996
Loss in iteration 116 : 0.4598230038549574
Loss in iteration 117 : 0.45980893050836086
Loss in iteration 118 : 0.459795066532481
Loss in iteration 119 : 0.4597815360845379
Loss in iteration 120 : 0.4597681967184152
Loss in iteration 121 : 0.45975516566523345
Loss in iteration 122 : 0.4597423294163551
Loss in iteration 123 : 0.4597297573301422
Loss in iteration 124 : 0.45971739659249355
Loss in iteration 125 : 0.4597052581533237
Loss in iteration 126 : 0.4596933453434739
Loss in iteration 127 : 0.45968162299809
Loss in iteration 128 : 0.45967012722093314
Loss in iteration 129 : 0.45965880429873984
Loss in iteration 130 : 0.4596476983046837
Loss in iteration 131 : 0.4596367630065025
Loss in iteration 132 : 0.4596260302952764
Loss in iteration 133 : 0.4596154740197111
Loss in iteration 134 : 0.4596051040551422
Loss in iteration 135 : 0.45959491538139174
Loss in iteration 136 : 0.45958489868767877
Loss in iteration 137 : 0.45957506405102866
Loss in iteration 138 : 0.45956539237170285
Loss in iteration 139 : 0.45955589887160697
Loss in iteration 140 : 0.4595465637686337
Loss in iteration 141 : 0.45953739856109255
Loss in iteration 142 : 0.45952838918876865
Loss in iteration 143 : 0.4595195395413979
Loss in iteration 144 : 0.45951084381986923
Loss in iteration 145 : 0.45950229863170017
Loss in iteration 146 : 0.4594939046917487
Loss in iteration 147 : 0.45948565385833684
Loss in iteration 148 : 0.4594775496148292
Loss in iteration 149 : 0.4594695826326366
Loss in iteration 150 : 0.4594617559580024
Loss in iteration 151 : 0.4594540621120516
Loss in iteration 152 : 0.4594465018539716
Loss in iteration 153 : 0.4594390707841742
Loss in iteration 154 : 0.4594317668911938
Loss in iteration 155 : 0.45942458847416545
Loss in iteration 156 : 0.45941753156845516
Loss in iteration 157 : 0.45941059607613965
Loss in iteration 158 : 0.4594037774666975
Loss in iteration 159 : 0.45939707594713936
Loss in iteration 160 : 0.45939048750302697
Loss in iteration 161 : 0.4593840116448474
Loss in iteration 162 : 0.4593776454078457
Loss in iteration 163 : 0.45937138727704085
Loss in iteration 164 : 0.4593652353831448
Loss in iteration 165 : 0.45935918742784027
Loss in iteration 166 : 0.4593532422862555
Loss in iteration 167 : 0.45934739732215923
Loss in iteration 168 : 0.45934165163035495
Loss in iteration 169 : 0.4593360026686852
Loss in iteration 170 : 0.4593304493670012
Loss in iteration 171 : 0.45932498957594825
Loss in iteration 172 : 0.45931962190424214
Loss in iteration 173 : 0.45931434465709275
Loss in iteration 174 : 0.4593091561452583
Loss in iteration 175 : 0.4593040550063476
Loss in iteration 176 : 0.45929903940031774
Loss in iteration 177 : 0.459294108121367
Loss in iteration 178 : 0.4592892593672367
Loss in iteration 179 : 0.45928449194452764
Loss in iteration 180 : 0.4592798042174524
Loss in iteration 181 : 0.4592751949160037
Loss in iteration 182 : 0.45927066260387084
Loss in iteration 183 : 0.4592662059246129
Loss in iteration 184 : 0.4592618236132372
Loss in iteration 185 : 0.45925751427814365
Loss in iteration 186 : 0.4592532767622797
Loss in iteration 187 : 0.4592491097018753
Loss in iteration 188 : 0.4592450119798326
Loss in iteration 189 : 0.4592409823060669
Loss in iteration 190 : 0.45923701956048035
Loss in iteration 191 : 0.4592331225480771
Loss in iteration 192 : 0.4592292901388856
Loss in iteration 193 : 0.459225521228952
Loss in iteration 194 : 0.4592218146919423
Testing accuracy  of updater 1 on alg 0 with rate 1.0 = 0.7895, training accuracy 0.7895, time elapsed: 2302 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6878658918511898
Loss in iteration 3 : 0.6807027170966673
Loss in iteration 4 : 0.6739294871347788
Loss in iteration 5 : 0.6668886231145068
Loss in iteration 6 : 0.6579684441529692
Loss in iteration 7 : 0.6470866160584787
Loss in iteration 8 : 0.6358501545886254
Loss in iteration 9 : 0.6258176075743548
Loss in iteration 10 : 0.6170799796352974
Loss in iteration 11 : 0.6086149317782455
Loss in iteration 12 : 0.5996947076182616
Loss in iteration 13 : 0.5906248408492144
Loss in iteration 14 : 0.5822230682148332
Loss in iteration 15 : 0.574914685914503
Loss in iteration 16 : 0.5684454167556635
Loss in iteration 17 : 0.5623019911408141
Loss in iteration 18 : 0.5562424100141602
Loss in iteration 19 : 0.5504326885724559
Loss in iteration 20 : 0.5451825083412979
Loss in iteration 21 : 0.5406159468252317
Loss in iteration 22 : 0.5365812551388358
Loss in iteration 23 : 0.5328221106337717
Loss in iteration 24 : 0.5291993831674031
Loss in iteration 25 : 0.525760250628342
Loss in iteration 26 : 0.5226337961602792
Loss in iteration 27 : 0.5198843942263012
Loss in iteration 28 : 0.5174547948874161
Loss in iteration 29 : 0.5152224074099973
Loss in iteration 30 : 0.5130951623791218
Loss in iteration 31 : 0.5110616419569142
Loss in iteration 32 : 0.5091677590336268
Loss in iteration 33 : 0.5074555801347295
Loss in iteration 34 : 0.5059201648144045
Loss in iteration 35 : 0.5045134235986846
Loss in iteration 36 : 0.5031813971044401
Loss in iteration 37 : 0.5018989110339537
Loss in iteration 38 : 0.5006757386534609
Loss in iteration 39 : 0.4995358671741112
Loss in iteration 40 : 0.49849131296566696
Loss in iteration 41 : 0.4975310688101306
Loss in iteration 42 : 0.49662985596631243
Loss in iteration 43 : 0.49576578659868803
Loss in iteration 44 : 0.4949320733439207
Loss in iteration 45 : 0.49413559345821256
Loss in iteration 46 : 0.4933861683257528
Loss in iteration 47 : 0.49268622853532656
Loss in iteration 48 : 0.4920281782595213
Loss in iteration 49 : 0.491399718080306
Loss in iteration 50 : 0.4907916535732544
Loss in iteration 51 : 0.4902020984228692
Loss in iteration 52 : 0.489634821506781
Loss in iteration 53 : 0.4890941049426018
Loss in iteration 54 : 0.4885804409233668
Loss in iteration 55 : 0.4880898936479985
Loss in iteration 56 : 0.4876167941338949
Loss in iteration 57 : 0.487157184537277
Loss in iteration 58 : 0.48671045083526493
Loss in iteration 59 : 0.4862783720903899
Loss in iteration 60 : 0.4858627606230021
Loss in iteration 61 : 0.4854636000986494
Loss in iteration 62 : 0.48507885849478
Loss in iteration 63 : 0.48470576326723647
Loss in iteration 64 : 0.4843423654281744
Loss in iteration 65 : 0.4839882588628247
Loss in iteration 66 : 0.4836441295503578
Loss in iteration 67 : 0.4833106756981422
Loss in iteration 68 : 0.4829877600924281
Loss in iteration 69 : 0.48267432391649207
Loss in iteration 70 : 0.4823689670102038
Loss in iteration 71 : 0.4820706668370649
Loss in iteration 72 : 0.48177912080797397
Loss in iteration 73 : 0.4814945544928545
Loss in iteration 74 : 0.4812172299497165
Loss in iteration 75 : 0.48094704354828277
Loss in iteration 76 : 0.4806834648577373
Loss in iteration 77 : 0.48042578893623544
Loss in iteration 78 : 0.4801734694714035
Loss in iteration 79 : 0.47992629218375016
Loss in iteration 80 : 0.47968430268418705
Loss in iteration 81 : 0.47944758434255375
Loss in iteration 82 : 0.47921606307838194
Loss in iteration 83 : 0.4789894630679938
Loss in iteration 84 : 0.47876741239221454
Loss in iteration 85 : 0.4785495982493625
Loss in iteration 86 : 0.47833585842962967
Loss in iteration 87 : 0.4781261605289943
Loss in iteration 88 : 0.4779205044102549
Loss in iteration 89 : 0.4777188272581826
Loss in iteration 90 : 0.477520973256807
Loss in iteration 91 : 0.47732673503584433
Loss in iteration 92 : 0.4771359252626396
Loss in iteration 93 : 0.47694842504935026
Loss in iteration 94 : 0.4767641815804591
Loss in iteration 95 : 0.47658316619176516
Loss in iteration 96 : 0.47640532792255535
Loss in iteration 97 : 0.4762305736654909
Loss in iteration 98 : 0.4760587824946961
Loss in iteration 99 : 0.4758898378862771
Loss in iteration 100 : 0.4757236530472288
Loss in iteration 101 : 0.47556017395497596
Loss in iteration 102 : 0.47539936231089985
Loss in iteration 103 : 0.47524117331380283
Loss in iteration 104 : 0.4750855436433983
Loss in iteration 105 : 0.47493239539988513
Loss in iteration 106 : 0.4747816502468817
Loss in iteration 107 : 0.47463324249938044
Loss in iteration 108 : 0.474487122789071
Loss in iteration 109 : 0.47434325169896385
Loss in iteration 110 : 0.47420158939315354
Loss in iteration 111 : 0.47406208869283206
Loss in iteration 112 : 0.4739246953657453
Loss in iteration 113 : 0.47378935399279665
Loss in iteration 114 : 0.47365601448764516
Loss in iteration 115 : 0.4735246348838857
Loss in iteration 116 : 0.4733951792712602
Loss in iteration 117 : 0.4732676131132624
Loss in iteration 118 : 0.4731418994350269
Loss in iteration 119 : 0.47301799813244083
Loss in iteration 120 : 0.47289586820643864
Loss in iteration 121 : 0.47277547088256144
Loss in iteration 122 : 0.4726567714038058
Loss in iteration 123 : 0.4725397385685741
Loss in iteration 124 : 0.47242434271593803
Loss in iteration 125 : 0.4723105537225417
Loss in iteration 126 : 0.472198340266217
Loss in iteration 127 : 0.4720876705573992
Loss in iteration 128 : 0.4719785137620186
Loss in iteration 129 : 0.4718708410492527
Loss in iteration 130 : 0.4717646256463809
Loss in iteration 131 : 0.4716598420394223
Loss in iteration 132 : 0.47155646497874903
Loss in iteration 133 : 0.4714544689468322
Loss in iteration 134 : 0.4713538283237596
Loss in iteration 135 : 0.47125451799588747
Loss in iteration 136 : 0.4711565139200753
Loss in iteration 137 : 0.47105979327893216
Loss in iteration 138 : 0.4709643341937726
Loss in iteration 139 : 0.47087011525024797
Loss in iteration 140 : 0.4707771151618119
Loss in iteration 141 : 0.47068531274601366
Loss in iteration 142 : 0.4705946871565271
Loss in iteration 143 : 0.47050521816299434
Loss in iteration 144 : 0.47041688628125566
Loss in iteration 145 : 0.47032967269034587
Loss in iteration 146 : 0.470243559020711
Loss in iteration 147 : 0.47015852716405937
Loss in iteration 148 : 0.47007455921296504
Loss in iteration 149 : 0.4699916375357317
Loss in iteration 150 : 0.46990974490574383
Loss in iteration 151 : 0.4698288645859182
Loss in iteration 152 : 0.46974898031670725
Loss in iteration 153 : 0.46967007622720314
Loss in iteration 154 : 0.46959213673390376
Loss in iteration 155 : 0.46951514648720893
Loss in iteration 156 : 0.46943909038423554
Loss in iteration 157 : 0.4693639536213378
Loss in iteration 158 : 0.469289721740069
Loss in iteration 159 : 0.4692163806335866
Loss in iteration 160 : 0.46914391651204423
Loss in iteration 161 : 0.4690723158517996
Loss in iteration 162 : 0.46900156535885196
Loss in iteration 163 : 0.468931651962332
Loss in iteration 164 : 0.4688625628320087
Loss in iteration 165 : 0.46879428540002965
Loss in iteration 166 : 0.46872680736836786
Loss in iteration 167 : 0.46866011669610413
Loss in iteration 168 : 0.4685942015745544
Loss in iteration 169 : 0.4685290504044641
Loss in iteration 170 : 0.46846465178556795
Loss in iteration 171 : 0.4684009945191629
Loss in iteration 172 : 0.46833806761625035
Loss in iteration 173 : 0.46827586030182744
Loss in iteration 174 : 0.4682143620102679
Loss in iteration 175 : 0.46815356237342104
Loss in iteration 176 : 0.46809345120736545
Loss in iteration 177 : 0.4680340185036229
Loss in iteration 178 : 0.46797525442673793
Loss in iteration 179 : 0.46791714931592504
Loss in iteration 180 : 0.4678596936864535
Loss in iteration 181 : 0.4678028782274632
Loss in iteration 182 : 0.46774669379587247
Loss in iteration 183 : 0.46769113140857677
Loss in iteration 184 : 0.4676361822358418
Loss in iteration 185 : 0.4675818375975605
Loss in iteration 186 : 0.4675280889619976
Loss in iteration 187 : 0.46747492794521817
Loss in iteration 188 : 0.46742234630942503
Loss in iteration 189 : 0.46737033595944844
Loss in iteration 190 : 0.4673188889380541
Loss in iteration 191 : 0.46726799742135416
Loss in iteration 192 : 0.4672176537153769
Loss in iteration 193 : 0.4671678502539586
Loss in iteration 194 : 0.46711857959733244
Loss in iteration 195 : 0.4670698344305013
Loss in iteration 196 : 0.4670216075608561
Loss in iteration 197 : 0.4669738919150709
Loss in iteration 198 : 0.46692668053584374
Loss in iteration 199 : 0.4668799665790122
Loss in iteration 200 : 0.46683374331132077
Loss in iteration 201 : 0.4667880041086926
Loss in iteration 202 : 0.46674274245457614
Loss in iteration 203 : 0.4666979519380557
Loss in iteration 204 : 0.46665362625162915
Loss in iteration 205 : 0.4666097591888233
Loss in iteration 206 : 0.46656634464191143
Loss in iteration 207 : 0.4665233765999286
Loss in iteration 208 : 0.46648084914697574
Loss in iteration 209 : 0.46643875646065713
Loss in iteration 210 : 0.4663970928104739
Loss in iteration 211 : 0.4663558525560854
Loss in iteration 212 : 0.46631503014545195
Loss in iteration 213 : 0.4662746201130017
Loss in iteration 214 : 0.4662346170779198
Loss in iteration 215 : 0.46619501574258343
Loss in iteration 216 : 0.466155810891134
Loss in iteration 217 : 0.46611699738804657
Loss in iteration 218 : 0.4660785701767094
Loss in iteration 219 : 0.46604052427791487
Loss in iteration 220 : 0.4660028547883836
Loss in iteration 221 : 0.4659655568793221
Loss in iteration 222 : 0.4659286257950705
Loss in iteration 223 : 0.46589205685182516
Loss in iteration 224 : 0.4658558454364028
Loss in iteration 225 : 0.4658199870050128
Loss in iteration 226 : 0.46578447708201043
Loss in iteration 227 : 0.4657493112586594
Loss in iteration 228 : 0.46571448519191105
Loss in iteration 229 : 0.46567999460322795
Loss in iteration 230 : 0.46564583527746695
Loss in iteration 231 : 0.46561200306178036
Loss in iteration 232 : 0.4655784938645592
Loss in iteration 233 : 0.4655453036543733
Loss in iteration 234 : 0.4655124284589177
Loss in iteration 235 : 0.46547986436398164
Loss in iteration 236 : 0.46544760751243736
Loss in iteration 237 : 0.46541565410326996
Loss in iteration 238 : 0.46538400039062205
Loss in iteration 239 : 0.46535264268287724
Loss in iteration 240 : 0.4653215773417412
Loss in iteration 241 : 0.4652908007813378
Loss in iteration 242 : 0.4652603094673275
Loss in iteration 243 : 0.46523009991602615
Loss in iteration 244 : 0.46520016869356595
Loss in iteration 245 : 0.46517051241505597
Loss in iteration 246 : 0.46514112774377636
Loss in iteration 247 : 0.4651120113903859
Loss in iteration 248 : 0.4650831601121416
Loss in iteration 249 : 0.4650545707121262
Loss in iteration 250 : 0.46502624003849163
Loss in iteration 251 : 0.4649981649837262
Loss in iteration 252 : 0.4649703424839295
Loss in iteration 253 : 0.46494276951809743
Loss in iteration 254 : 0.4649154431074477
Loss in iteration 255 : 0.46488836031471464
Loss in iteration 256 : 0.46486151824350147
Loss in iteration 257 : 0.4648349140376038
Loss in iteration 258 : 0.46480854488037526
Loss in iteration 259 : 0.4647824079940935
Loss in iteration 260 : 0.46475650063932944
Loss in iteration 261 : 0.46473082011434935
Loss in iteration 262 : 0.46470536375451876
Loss in iteration 263 : 0.46468012893170785
Loss in iteration 264 : 0.46465511305372137
Loss in iteration 265 : 0.4646303135637295
Loss in iteration 266 : 0.46460572793971616
Loss in iteration 267 : 0.4645813536939307
Loss in iteration 268 : 0.46455718837235577
Loss in iteration 269 : 0.4645332295541859
Loss in iteration 270 : 0.46450947485130417
Loss in iteration 271 : 0.46448592190778126
Loss in iteration 272 : 0.46446256839938005
Loss in iteration 273 : 0.4644394120330591
Loss in iteration 274 : 0.4644164505465023
Loss in iteration 275 : 0.4643936817076367
Loss in iteration 276 : 0.46437110331418113
Loss in iteration 277 : 0.46434871319318416
Loss in iteration 278 : 0.46432650920057916
Loss in iteration 279 : 0.46430448922074996
Loss in iteration 280 : 0.4642826511660899
Loss in iteration 281 : 0.46426099297658857
Testing accuracy  of updater 1 on alg 0 with rate 0.09999999999999998 = 0.79075, training accuracy 0.79075, time elapsed: 4048 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 4.016337962504751
Loss in iteration 3 : 42.50079140495103
Loss in iteration 4 : 1.3409879028154255
Loss in iteration 5 : 20.770324756889575
Loss in iteration 6 : 4.661303411900024
Loss in iteration 7 : 19.463252964054096
Loss in iteration 8 : 7.37061480770914
Loss in iteration 9 : 12.116460569775128
Loss in iteration 10 : 11.860532355582269
Loss in iteration 11 : 7.767552345407355
Loss in iteration 12 : 7.8526885767327395
Loss in iteration 13 : 6.041902174972181
Loss in iteration 14 : 5.54703072320268
Loss in iteration 15 : 4.615297343999984
Loss in iteration 16 : 4.258664681104029
Loss in iteration 17 : 3.91475001594677
Loss in iteration 18 : 3.7499132410104066
Loss in iteration 19 : 3.5714106103577907
Loss in iteration 20 : 3.466928672438601
Loss in iteration 21 : 3.3673763127174525
Loss in iteration 22 : 3.2722016231195616
Loss in iteration 23 : 3.1792720495428437
Loss in iteration 24 : 3.0921629230412173
Loss in iteration 25 : 3.010242823125025
Loss in iteration 26 : 2.9700342839289795
Loss in iteration 27 : 2.9863717026569025
Loss in iteration 28 : 3.3054091620550103
Loss in iteration 29 : 3.9039537556999013
Loss in iteration 30 : 6.572198296404988
Loss in iteration 31 : 6.7071852448610585
Loss in iteration 32 : 9.984871282708227
Loss in iteration 33 : 6.4335846766340055
Loss in iteration 34 : 8.869845279207537
Loss in iteration 35 : 7.052267480835764
Loss in iteration 36 : 8.294011505600043
Loss in iteration 37 : 6.55849908979351
Loss in iteration 38 : 7.215849315672143
Loss in iteration 39 : 6.035675146856262
Loss in iteration 40 : 6.297184467435166
Loss in iteration 41 : 5.467627252363796
Loss in iteration 42 : 5.526578377162166
Loss in iteration 43 : 4.9543890494676655
Loss in iteration 44 : 5.067758336459337
Loss in iteration 45 : 4.682253840805415
Loss in iteration 46 : 4.939047091029664
Loss in iteration 47 : 4.646012768924973
Loss in iteration 48 : 5.08526436080914
Loss in iteration 49 : 4.738782098156359
Loss in iteration 50 : 5.407379719812947
Loss in iteration 51 : 5.018141541691384
Loss in iteration 52 : 5.9286447707385
Loss in iteration 53 : 5.338793878847575
Loss in iteration 54 : 6.3497378081065365
Loss in iteration 55 : 5.496226967775113
Loss in iteration 56 : 6.447996614487119
Loss in iteration 57 : 5.485340426203826
Loss in iteration 58 : 6.306572941388068
Loss in iteration 59 : 5.39576126786729
Loss in iteration 60 : 6.0627863111658336
Loss in iteration 61 : 5.238397992947783
Loss in iteration 62 : 5.778797292674799
Loss in iteration 63 : 5.071282968765223
Loss in iteration 64 : 5.545409152424139
Loss in iteration 65 : 4.959385816909771
Loss in iteration 66 : 5.426255087280009
Loss in iteration 67 : 4.912105062255796
Loss in iteration 68 : 5.416511903188551
Loss in iteration 69 : 4.9522651038874885
Loss in iteration 70 : 5.520521279082606
Loss in iteration 71 : 5.062832642475028
Loss in iteration 72 : 5.692361505089935
Loss in iteration 73 : 5.18559026225153
Loss in iteration 74 : 5.852072140972345
Loss in iteration 75 : 5.288263573172415
Loss in iteration 76 : 5.961474864878832
Loss in iteration 77 : 5.339965595904985
Loss in iteration 78 : 5.990958430398514
Loss in iteration 79 : 5.338429377458419
Loss in iteration 80 : 5.9508183552696865
Loss in iteration 81 : 5.29951639655088
Loss in iteration 82 : 5.872041352041496
Loss in iteration 83 : 5.237653662372388
Loss in iteration 84 : 5.781050137386947
Loss in iteration 85 : 5.172940974660581
Loss in iteration 86 : 5.704260510611095
Loss in iteration 87 : 5.124091582887643
Loss in iteration 88 : 5.660332987327769
Loss in iteration 89 : 5.101689436925228
Loss in iteration 90 : 5.655170878424995
Loss in iteration 91 : 5.107646545500014
Loss in iteration 92 : 5.683206390354489
Loss in iteration 93 : 5.134746721547794
Loss in iteration 94 : 5.729230287276209
Loss in iteration 95 : 5.170376008777291
Loss in iteration 96 : 5.774902296910435
Loss in iteration 97 : 5.201816183208776
Loss in iteration 98 : 5.805640310368617
Loss in iteration 99 : 5.220306886645028
Loss in iteration 100 : 5.814596198310314
Loss in iteration 101 : 5.223352214987688
Loss in iteration 102 : 5.803627781259595
Loss in iteration 103 : 5.213918236929661
Loss in iteration 104 : 5.780552908465222
Loss in iteration 105 : 5.198001573171768
Loss in iteration 106 : 5.755067021657724
Loss in iteration 107 : 5.182292221210571
Loss in iteration 108 : 5.735534214301161
Loss in iteration 109 : 5.172111216796624
Loss in iteration 110 : 5.726840190258474
Loss in iteration 111 : 5.16992171254494
Loss in iteration 112 : 5.72943762960154
Loss in iteration 113 : 5.174952362456417
Loss in iteration 114 : 5.739891157404829
Loss in iteration 115 : 5.184046703963547
Loss in iteration 116 : 5.752720994293095
Loss in iteration 117 : 5.193230886740731
Loss in iteration 118 : 5.762679641167436
Loss in iteration 119 : 5.19925026843188
Loss in iteration 120 : 5.766498701615359
Loss in iteration 121 : 5.200536026720276
Loss in iteration 122 : 5.763594972986123
Loss in iteration 123 : 5.197394036559046
Loss in iteration 124 : 5.755720952436009
Loss in iteration 125 : 5.191537706508965
Loss in iteration 126 : 5.745924054704944
Loss in iteration 127 : 5.185275156239433
Loss in iteration 128 : 5.737318946042292
Loss in iteration 129 : 5.180663516218641
Loss in iteration 130 : 5.732079772616395
Loss in iteration 131 : 5.178879851929262
Loss in iteration 132 : 5.730900979807057
Loss in iteration 133 : 5.179975330138413
Loss in iteration 134 : 5.733022805757062
Loss in iteration 135 : 5.183049742345264
Loss in iteration 136 : 5.73674135319436
Loss in iteration 137 : 5.186724925468098
Loss in iteration 138 : 5.740154888714753
Loss in iteration 139 : 5.1896939959889075
Loss in iteration 140 : 5.741837736230929
Loss in iteration 141 : 5.191137001460572
Loss in iteration 142 : 5.741217917027764
Loss in iteration 143 : 5.190889838405355
Loss in iteration 144 : 5.738592009838072
Loss in iteration 145 : 5.189364229716542
Loss in iteration 146 : 5.734850848351207
Loss in iteration 147 : 5.187297503621582
Loss in iteration 148 : 5.731067435221867
Loss in iteration 149 : 5.185446323718646
Loss in iteration 150 : 5.728107930244559
Loss in iteration 151 : 5.184332927323398
Loss in iteration 152 : 5.726385988145973
Loss in iteration 153 : 5.184117429374562
Loss in iteration 154 : 5.725814129548109
Loss in iteration 155 : 5.184616928080299
Loss in iteration 156 : 5.725933225552488
Loss in iteration 157 : 5.185438091675505
Loss in iteration 158 : 5.726143568078327
Loss in iteration 159 : 5.186155374718502
Loss in iteration 160 : 5.725938451771842
Loss in iteration 161 : 5.186464023507924
Loss in iteration 162 : 5.725059110130936
Loss in iteration 163 : 5.186260724307986
Loss in iteration 164 : 5.723533953218652
Loss in iteration 165 : 5.185639554540407
Loss in iteration 166 : 5.721612522629404
Loss in iteration 167 : 5.184821738896699
Loss in iteration 168 : 5.719637716275568
Loss in iteration 169 : 5.184055766498386
Loss in iteration 170 : 5.717911336891485
Loss in iteration 171 : 5.1835271533620455
Loss in iteration 172 : 5.716599359584463
Loss in iteration 173 : 5.183306650815461
Loss in iteration 174 : 5.715701316469705
Loss in iteration 175 : 5.1833474468360015
Loss in iteration 176 : 5.7150821204869935
Loss in iteration 177 : 5.183523160071849
Loss in iteration 178 : 5.714543692176239
Loss in iteration 179 : 5.183685849536643
Loss in iteration 180 : 5.713904301285658
Loss in iteration 181 : 5.183720311640235
Loss in iteration 182 : 5.713056992119213
Loss in iteration 183 : 5.18357683235279
Loss in iteration 184 : 5.711991214130321
Loss in iteration 185 : 5.183275418612215
Loss in iteration 186 : 5.710777480828907
Loss in iteration 187 : 5.1828855778765135
Loss in iteration 188 : 5.709527485943951
Loss in iteration 189 : 5.182493200156754
Loss in iteration 190 : 5.7083479854524795
Loss in iteration 191 : 5.182168250815926
Loss in iteration 192 : 5.707305313307726
Loss in iteration 193 : 5.181944109194647
Loss in iteration 194 : 5.706410558024275
Loss in iteration 195 : 5.18181338028875
Loss in iteration 196 : 5.7056264960799234
Loss in iteration 197 : 5.181738470652643
Loss in iteration 198 : 5.704889748666159
Loss in iteration 199 : 5.18167048523412
Loss in iteration 200 : 5.7041376532704
Loss in iteration 201 : 5.1815683644145105
Loss in iteration 202 : 5.7033297128134866
Loss in iteration 203 : 5.1814116288760115
Loss in iteration 204 : 5.702457272861669
Loss in iteration 205 : 5.181203556641802
Loss in iteration 206 : 5.701540336793962
Loss in iteration 207 : 5.180965513245603
Loss in iteration 208 : 5.700615067087807
Loss in iteration 209 : 5.180726069896728
Loss in iteration 210 : 5.6997180582801885
Loss in iteration 211 : 5.180509660126037
Loss in iteration 212 : 5.698873431142876
Loss in iteration 213 : 5.180328796873999
Loss in iteration 214 : 5.698086687763754
Loss in iteration 215 : 5.180181892807645
Loss in iteration 216 : 5.697346195808146
Loss in iteration 217 : 5.180056407644503
Loss in iteration 218 : 5.6966303686816415
Loss in iteration 219 : 5.179935247331245
Loss in iteration 220 : 5.695916989465021
Loss in iteration 221 : 5.179803587230944
Loss in iteration 222 : 5.69519103351599
Loss in iteration 223 : 5.1796536574441285
Loss in iteration 224 : 5.694448534563982
Loss in iteration 225 : 5.179486185875387
Loss in iteration 226 : 5.69369586791284
Loss in iteration 227 : 5.179308599058269
Loss in iteration 228 : 5.692945539870006
Loss in iteration 229 : 5.179131197562594
Loss in iteration 230 : 5.69221059638459
Loss in iteration 231 : 5.178963013151561
Loss in iteration 232 : 5.691499863520834
Loss in iteration 233 : 5.178808859616581
Loss in iteration 234 : 5.690815540088501
Loss in iteration 235 : 5.178668399384302
Loss in iteration 236 : 5.690153563032678
Loss in iteration 237 : 5.178537192912281
Loss in iteration 238 : 5.689506116286238
Loss in iteration 239 : 5.1784090060289225
Loss in iteration 240 : 5.6888650120968265
Loss in iteration 241 : 5.178278335562301
Loss in iteration 242 : 5.688224593171086
Loss in iteration 243 : 5.178142220631045
Loss in iteration 244 : 5.687583214128414
Loss in iteration 245 : 5.1780008247552765
Loss in iteration 246 : 5.686943031183793
Loss in iteration 247 : 5.177856800838425
Loss in iteration 248 : 5.686308477166769
Loss in iteration 249 : 5.177713881405545
Loss in iteration 250 : 5.685684199193061
Loss in iteration 251 : 5.177575334282766
Loss in iteration 252 : 5.685073289954182
Loss in iteration 253 : 5.177442859333737
Loss in iteration 254 : 5.684476392272684
Loss in iteration 255 : 5.1773162435062385
Loss in iteration 256 : 5.6838918428944565
Loss in iteration 257 : 5.177193764861421
Loss in iteration 258 : 5.6833166209142965
Loss in iteration 259 : 5.177073069621743
Loss in iteration 260 : 5.682747619194357
Loss in iteration 261 : 5.176952124445183
Loss in iteration 262 : 5.682182725276336
Loss in iteration 263 : 5.176829887895955
Loss in iteration 264 : 5.6816213559822
Loss in iteration 265 : 5.176706507554549
Loss in iteration 266 : 5.681064348117329
Loss in iteration 267 : 5.176583053483477
Loss in iteration 268 : 5.680513356506015
Loss in iteration 269 : 5.176460964220133
Loss in iteration 270 : 5.67997006212815
Loss in iteration 271 : 5.176341455079991
Loss in iteration 272 : 5.6794355094489966
Loss in iteration 273 : 5.176225109383904
Loss in iteration 274 : 5.678909790369682
Loss in iteration 275 : 5.176111769274981
Loss in iteration 276 : 5.678392129415901
Loss in iteration 277 : 5.17600071432647
Loss in iteration 278 : 5.677881270071035
Loss in iteration 279 : 5.175891013795511
Loss in iteration 280 : 5.677375970130688
Loss in iteration 281 : 5.175781894805315
Loss in iteration 282 : 5.676875407293766
Loss in iteration 283 : 5.175672989827753
Loss in iteration 284 : 5.676379362858181
Loss in iteration 285 : 5.175564394031782
Loss in iteration 286 : 5.675888154738655
Loss in iteration 287 : 5.175456544157172
Loss in iteration 288 : 5.675402387276134
Loss in iteration 289 : 5.175349993685096
Loss in iteration 290 : 5.674922640611256
Loss in iteration 291 : 5.175245184221219
Loss in iteration 292 : 5.674449223508094
Loss in iteration 293 : 5.175142297396448
Loss in iteration 294 : 5.6739820691921485
Loss in iteration 295 : 5.175041227794993
Loss in iteration 296 : 5.673520787874914
Loss in iteration 297 : 5.174941666200361
Loss in iteration 298 : 5.673064829914901
Loss in iteration 299 : 5.1748432438092715
Loss in iteration 300 : 5.672613680827462
Loss in iteration 301 : 5.17474567404323
Loss in iteration 302 : 5.6721670110513465
Loss in iteration 303 : 5.174648840197885
Loss in iteration 304 : 5.671724733083296
Loss in iteration 305 : 5.174552805842508
Loss in iteration 306 : 5.671286960684426
Loss in iteration 307 : 5.174457757214851
Loss in iteration 308 : 5.67085390178895
Loss in iteration 309 : 5.174363910338978
Loss in iteration 310 : 5.670425735794239
Loss in iteration 311 : 5.174271423063677
Loss in iteration 312 : 5.670002523078354
Loss in iteration 313 : 5.174180343614229
Loss in iteration 314 : 5.669584174605838
Loss in iteration 315 : 5.174090608441994
Loss in iteration 316 : 5.669170482597025
Loss in iteration 317 : 5.17400208179879
Loss in iteration 318 : 5.668761190534291
Loss in iteration 319 : 5.173914615333486
Loss in iteration 320 : 5.668356069933068
Loss in iteration 321 : 5.173828102279693
Loss in iteration 322 : 5.667954974327691
Loss in iteration 323 : 5.1737425071056204
Loss in iteration 324 : 5.667557854357452
Loss in iteration 325 : 5.1736578638143795
Loss in iteration 326 : 5.667164734964174
Loss in iteration 327 : 5.173574248840892
Loss in iteration 328 : 5.666775669568306
Loss in iteration 329 : 5.173491742895212
Loss in iteration 330 : 5.66639069209505
Loss in iteration 331 : 5.173410397765376
Loss in iteration 332 : 5.666009784976653
Loss in iteration 333 : 5.173330219539257
Loss in iteration 334 : 5.665632872256465
Loss in iteration 335 : 5.1732511716702465
Loss in iteration 336 : 5.665259836058713
Loss in iteration 337 : 5.173173193365753
Loss in iteration 338 : 5.664890546312584
Loss in iteration 339 : 5.173096223855969
Loss in iteration 340 : 5.664524890417713
Loss in iteration 341 : 5.173020222521121
Loss in iteration 342 : 5.66416279182452
Loss in iteration 343 : 5.172945178097348
Loss in iteration 344 : 5.663804212499057
Loss in iteration 345 : 5.172871105395934
Loss in iteration 346 : 5.6634491410893135
Loss in iteration 347 : 5.172798032880542
Loss in iteration 348 : 5.663097573617976
Loss in iteration 349 : 5.172725987279248
Loss in iteration 350 : 5.662749495143882
Loss in iteration 351 : 5.172654981460347
Loss in iteration 352 : 5.6624048690345345
Loss in iteration 353 : 5.1725850095241865
Loss in iteration 354 : 5.662063636526633
Loss in iteration 355 : 5.172516049703134
Loss in iteration 356 : 5.661725724964123
Loss in iteration 357 : 5.172448072653027
Loss in iteration 358 : 5.661391060148433
Loss in iteration 359 : 5.172381051124497
Loss in iteration 360 : 5.661059577484503
Loss in iteration 361 : 5.172314967174632
Loss in iteration 362 : 5.660731227965851
Loss in iteration 363 : 5.172249814653585
Loss in iteration 364 : 5.660405977643408
Loss in iteration 365 : 5.172185596850162
Loss in iteration 366 : 5.660083801884247
Loss in iteration 367 : 5.172122321005629
Loss in iteration 368 : 5.6597646774452075
Loss in iteration 369 : 5.172059992279296
Loss in iteration 370 : 5.659448575681641
Loss in iteration 371 : 5.1719986095104105
Loss in iteration 372 : 5.659135459218203
Loss in iteration 373 : 5.171938164048046
Loss in iteration 374 : 5.658825282716436
Loss in iteration 375 : 5.171878641556441
Loss in iteration 376 : 5.658517996737202
Loss in iteration 377 : 5.171820025610159
Loss in iteration 378 : 5.658213552713498
Loss in iteration 379 : 5.171762301428716
Loss in iteration 380 : 5.6579119069788355
Loss in iteration 381 : 5.171705458334697
Loss in iteration 382 : 5.657613022502851
Loss in iteration 383 : 5.171649490239678
Loss in iteration 384 : 5.6573168680765
Loss in iteration 385 : 5.171594394319567
Loss in iteration 386 : 5.657023415685444
Loss in iteration 387 : 5.171540168689656
Loss in iteration 388 : 5.656732637360125
Loss in iteration 389 : 5.171486810122095
Loss in iteration 390 : 5.65644450276264
Loss in iteration 391 : 5.171434312653076
Loss in iteration 392 : 5.656158978277507
Loss in iteration 393 : 5.171382667446221
Loss in iteration 394 : 5.6558760276784135
Loss in iteration 395 : 5.171331863746929
Loss in iteration 396 : 5.655595613842302
Loss in iteration 397 : 5.171281890382846
Loss in iteration 398 : 5.655317700682802
Loss in iteration 399 : 5.1712327371563465
Loss in iteration 400 : 5.65504225453773
Testing accuracy  of updater 2 on alg 0 with rate 10.0 = 0.76675, training accuracy 0.76675, time elapsed: 4788 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.7390243244034626
Loss in iteration 3 : 1.5461235979835175
Loss in iteration 4 : 2.4961351716319125
Loss in iteration 5 : 0.5491945676139464
Loss in iteration 6 : 0.5116117840937401
Loss in iteration 7 : 0.5594905327553612
Loss in iteration 8 : 0.6994699929099957
Loss in iteration 9 : 0.9559049119020779
Loss in iteration 10 : 1.0668534021602731
Loss in iteration 11 : 0.808824983537983
Loss in iteration 12 : 0.7371409817547577
Loss in iteration 13 : 0.6752825726820127
Loss in iteration 14 : 0.6109967122349372
Loss in iteration 15 : 0.5707663607584027
Loss in iteration 16 : 0.543075121035778
Loss in iteration 17 : 0.5282697556481837
Loss in iteration 18 : 0.5200434020403024
Loss in iteration 19 : 0.5156113921810067
Loss in iteration 20 : 0.5126013245511593
Loss in iteration 21 : 0.5099866661947244
Loss in iteration 22 : 0.5073499834642364
Loss in iteration 23 : 0.504566829433719
Loss in iteration 24 : 0.5016376127338616
Loss in iteration 25 : 0.4985886438503925
Loss in iteration 26 : 0.4954598727295745
Loss in iteration 27 : 0.4922913044277051
Loss in iteration 28 : 0.4891266471024051
Loss in iteration 29 : 0.48601204513431456
Loss in iteration 30 : 0.4829957859599715
Loss in iteration 31 : 0.4801269729000892
Loss in iteration 32 : 0.47745189603337895
Loss in iteration 33 : 0.4750119075352789
Loss in iteration 34 : 0.4728386820019561
Loss in iteration 35 : 0.4709530630044385
Loss in iteration 36 : 0.46936168196075406
Loss in iteration 37 : 0.46805877035351534
Loss in iteration 38 : 0.46702534225380454
Loss in iteration 39 : 0.46623403452433104
Loss in iteration 40 : 0.46564976296017596
Loss in iteration 41 : 0.46523579018915506
Loss in iteration 42 : 0.4649538811014268
Loss in iteration 43 : 0.4647698641185812
Loss in iteration 44 : 0.4646523023446003
Loss in iteration 45 : 0.4645770410084978
Loss in iteration 46 : 0.4645250676678275
Loss in iteration 47 : 0.46448695878670276
Loss in iteration 48 : 0.4644632103405315
Loss in iteration 49 : 0.4644725025335746
Loss in iteration 50 : 0.46456707268111813
Loss in iteration 51 : 0.4648597833013292
Loss in iteration 52 : 0.46561439367850477
Loss in iteration 53 : 0.46733371200598456
Loss in iteration 54 : 0.4712436551832381
Loss in iteration 55 : 0.47930623611989126
Loss in iteration 56 : 0.4967595508507202
Loss in iteration 57 : 0.5276572085585572
Loss in iteration 58 : 0.5867717004375643
Loss in iteration 59 : 0.650164560303307
Loss in iteration 60 : 0.7342256375699746
Loss in iteration 61 : 0.7343765515332784
Loss in iteration 62 : 0.7532407557680411
Loss in iteration 63 : 0.7028029576797065
Loss in iteration 64 : 0.6887390378255568
Loss in iteration 65 : 0.6448547639881952
Loss in iteration 66 : 0.6232080658500647
Loss in iteration 67 : 0.5904625848613482
Loss in iteration 68 : 0.5706431804272818
Loss in iteration 69 : 0.548749159105419
Loss in iteration 70 : 0.5342875686250125
Loss in iteration 71 : 0.5207888449437339
Loss in iteration 72 : 0.511423651325227
Loss in iteration 73 : 0.5034977485287155
Loss in iteration 74 : 0.4978302100906427
Loss in iteration 75 : 0.49330918129383294
Loss in iteration 76 : 0.49006088359656247
Loss in iteration 77 : 0.487610466079337
Loss in iteration 78 : 0.48595508427596934
Loss in iteration 79 : 0.48488430580482783
Loss in iteration 80 : 0.484450956273303
Loss in iteration 81 : 0.4845640855489104
Loss in iteration 82 : 0.48543983651906103
Loss in iteration 83 : 0.48701310841335316
Loss in iteration 84 : 0.48982281146528384
Loss in iteration 85 : 0.49364944439362013
Loss in iteration 86 : 0.4996906380670317
Loss in iteration 87 : 0.5070127680266758
Loss in iteration 88 : 0.5181619701814855
Loss in iteration 89 : 0.5298206251667814
Loss in iteration 90 : 0.5471955130557408
Loss in iteration 91 : 0.5610181468901625
Loss in iteration 92 : 0.5817627399628864
Loss in iteration 93 : 0.5907293836366093
Loss in iteration 94 : 0.6077158794506377
Loss in iteration 95 : 0.6058864261498703
Loss in iteration 96 : 0.614465844512486
Loss in iteration 97 : 0.6036152609291762
Loss in iteration 98 : 0.6047515874397354
Loss in iteration 99 : 0.5902758384529267
Loss in iteration 100 : 0.5870478779922684
Loss in iteration 101 : 0.5729236593284488
Loss in iteration 102 : 0.5680283855262608
Loss in iteration 103 : 0.5561369668952153
Loss in iteration 104 : 0.5513180791399116
Loss in iteration 105 : 0.5421774999590553
Loss in iteration 106 : 0.538320701767385
Loss in iteration 107 : 0.5317723690488467
Loss in iteration 108 : 0.5292074222002707
Loss in iteration 109 : 0.5248546649662553
Loss in iteration 110 : 0.523636099779327
Loss in iteration 111 : 0.5210702032117789
Loss in iteration 112 : 0.521161574063039
Loss in iteration 113 : 0.5200460037383705
Loss in iteration 114 : 0.5214111989372909
Loss in iteration 115 : 0.5214784234271799
Loss in iteration 116 : 0.5240944402432269
Loss in iteration 117 : 0.5250849053694812
Loss in iteration 118 : 0.5288892472649979
Loss in iteration 119 : 0.5304600845800344
Loss in iteration 120 : 0.5352607739545834
Loss in iteration 121 : 0.5369174255996118
Loss in iteration 122 : 0.5423255686450366
Loss in iteration 123 : 0.5434522095300653
Loss in iteration 124 : 0.5489127972814161
Loss in iteration 125 : 0.5489367505723408
Loss in iteration 126 : 0.5538798594432637
Loss in iteration 127 : 0.5524871311125694
Loss in iteration 128 : 0.5565153003175144
Loss in iteration 129 : 0.5537592103360665
Loss in iteration 130 : 0.5567472573779498
Loss in iteration 131 : 0.5529779570931721
Loss in iteration 132 : 0.5550433881505938
Loss in iteration 133 : 0.5507349223779546
Loss in iteration 134 : 0.5521341778881428
Loss in iteration 135 : 0.5477278078970361
Loss in iteration 136 : 0.5487517680788367
Loss in iteration 137 : 0.5445727051225678
Loss in iteration 138 : 0.5454787817701693
Loss in iteration 139 : 0.541719255161066
Loss in iteration 140 : 0.5427023104715708
Loss in iteration 141 : 0.5394410873520666
Loss in iteration 142 : 0.540630263624481
Loss in iteration 143 : 0.5378634419172501
Loss in iteration 144 : 0.5393310438452791
Loss in iteration 145 : 0.5370005175716651
Loss in iteration 146 : 0.5387736266581341
Loss in iteration 147 : 0.5367887739394743
Loss in iteration 148 : 0.5388593335596676
Loss in iteration 149 : 0.537112606184505
Loss in iteration 150 : 0.5394454230115113
Loss in iteration 151 : 0.5378244147988032
Loss in iteration 152 : 0.5403643833126631
Loss in iteration 153 : 0.5387626063848604
Loss in iteration 154 : 0.5414425665288581
Loss in iteration 155 : 0.5397695478942983
Loss in iteration 156 : 0.542519100504139
Loss in iteration 157 : 0.540708686928592
Loss in iteration 158 : 0.5434629638492122
Loss in iteration 159 : 0.5414779740800152
Loss in iteration 160 : 0.5441846916505754
Loss in iteration 161 : 0.5420166965622814
Loss in iteration 162 : 0.5446401626017885
Loss in iteration 163 : 0.5423046921191849
Loss in iteration 164 : 0.5448264471891073
Loss in iteration 165 : 0.5423553161723105
Loss in iteration 166 : 0.5447720983530294
Loss in iteration 167 : 0.5422050798275304
Loss in iteration 168 : 0.5445253164105159
Loss in iteration 169 : 0.5419030061932953
Loss in iteration 170 : 0.5441429802903919
Loss in iteration 171 : 0.5415018219356602
Loss in iteration 172 : 0.5436822650822853
Loss in iteration 173 : 0.5410518401674538
Loss in iteration 174 : 0.5431952499200675
Loss in iteration 175 : 0.540597379927439
Loss in iteration 176 : 0.5427260362930666
Loss in iteration 177 : 0.5401750387118355
Loss in iteration 178 : 0.5423095438602054
Loss in iteration 179 : 0.5398130420850851
Loss in iteration 180 : 0.5419711975844579
Loss in iteration 181 : 0.5395310705882492
Loss in iteration 182 : 0.5417269716139486
Loss in iteration 183 : 0.5393402395038388
Loss in iteration 184 : 0.5415835517545067
Loss in iteration 185 : 0.5392431646644733
Loss in iteration 186 : 0.5415386237991843
Loss in iteration 187 : 0.5392342281658834
Loss in iteration 188 : 0.5415814473897083
Loss in iteration 189 : 0.539300242238015
Loss in iteration 190 : 0.5416939241806097
Loss in iteration 191 : 0.5394216987771492
Loss in iteration 192 : 0.5418523199895559
Loss in iteration 193 : 0.5395746976090314
Loss in iteration 194 : 0.5420296694586616
Loss in iteration 195 : 0.5397334906551698
Loss in iteration 196 : 0.5421987095491347
Loss in iteration 197 : 0.5398733993300613
Loss in iteration 198 : 0.5423350039358457
Loss in iteration 199 : 0.5399737106286132
Loss in iteration 200 : 0.5424197939466843
Loss in iteration 201 : 0.5400200873014697
Loss in iteration 202 : 0.5424420944235923
Loss in iteration 203 : 0.540006073692801
Loss in iteration 204 : 0.542399663932865
Loss in iteration 205 : 0.5399334378061946
Loss in iteration 206 : 0.5422986921398925
Loss in iteration 207 : 0.5398113180426827
Loss in iteration 208 : 0.5421522998409971
Loss in iteration 209 : 0.5396543725615306
Loss in iteration 210 : 0.5419781654115322
Loss in iteration 211 : 0.539480297775968
Loss in iteration 212 : 0.5417957208558923
Loss in iteration 213 : 0.539307155399468
Loss in iteration 214 : 0.5416233833630251
Loss in iteration 215 : 0.5391509249661947
Loss in iteration 216 : 0.5414762210528387
Loss in iteration 217 : 0.5390236069617863
Loss in iteration 218 : 0.5413643301402781
Loss in iteration 219 : 0.5389320757596485
Loss in iteration 220 : 0.5412920614742043
Loss in iteration 221 : 0.5388777512924354
Loss in iteration 222 : 0.5412581029849112
Loss in iteration 223 : 0.5388570420529304
Loss in iteration 224 : 0.5412563138840654
Loss in iteration 225 : 0.5388624178782654
Loss in iteration 226 : 0.5412771213097873
Loss in iteration 227 : 0.5388839026984115
Loss in iteration 228 : 0.5413092334021965
Loss in iteration 229 : 0.5389107386633661
Loss in iteration 230 : 0.5413413988500408
Loss in iteration 231 : 0.5389329683056251
Loss in iteration 232 : 0.5413639563581994
Loss in iteration 233 : 0.5389427132740086
Loss in iteration 234 : 0.5413699689585424
Loss in iteration 235 : 0.5389349934752949
Loss in iteration 236 : 0.5413558205770005
Loss in iteration 237 : 0.5389080180370464
Loss in iteration 238 : 0.541321250409224
Loss in iteration 239 : 0.53886297196147
Loss in iteration 240 : 0.5412688943994415
Loss in iteration 241 : 0.5388034009545
Loss in iteration 242 : 0.5412034738973757
Loss in iteration 243 : 0.5387343473792909
Loss in iteration 244 : 0.5411308078949608
Loss in iteration 245 : 0.5386614065237382
Loss in iteration 246 : 0.541056825221381
Loss in iteration 247 : 0.5385898570113625
Loss in iteration 248 : 0.540986723247681
Loss in iteration 249 : 0.5385239811051952
Loss in iteration 250 : 0.5409243714261718
Loss in iteration 251 : 0.5384666412684987
Loss in iteration 252 : 0.5408720036010395
Loss in iteration 253 : 0.5384191293024567
Loss in iteration 254 : 0.540830192614171
Loss in iteration 255 : 0.5383812616293606
Loss in iteration 256 : 0.5407980610528496
Loss in iteration 257 : 0.5383516635598544
Loss in iteration 258 : 0.5407736564499405
Loss in iteration 259 : 0.5383281685494218
Loss in iteration 260 : 0.5407544085636257
Loss in iteration 261 : 0.5383082551893036
Loss in iteration 262 : 0.5407375892668049
Loss in iteration 263 : 0.5382894530549895
Loss in iteration 264 : 0.5407207093698896
Loss in iteration 265 : 0.5382696654573574
Loss in iteration 266 : 0.5407018076918221
Loss in iteration 267 : 0.538247378789408
Loss in iteration 268 : 0.5406796116692912
Loss in iteration 269 : 0.5382217505210922
Loss in iteration 270 : 0.5406535716190451
Loss in iteration 271 : 0.5381925873688567
Loss in iteration 272 : 0.5406237890411233
Loss in iteration 273 : 0.5381602391471151
Loss in iteration 274 : 0.5405908709345368
Loss in iteration 275 : 0.5381254410398683
Loss in iteration 276 : 0.5405557463629789
Loss in iteration 277 : 0.538089137654253
Loss in iteration 278 : 0.5405194792294171
Loss in iteration 279 : 0.538052317511774
Loss in iteration 280 : 0.5404831041675547
Loss in iteration 281 : 0.5380158785449896
Loss in iteration 282 : 0.5404475028650124
Loss in iteration 283 : 0.5379805358026652
Loss in iteration 284 : 0.5404133281598483
Loss in iteration 285 : 0.5379467737560979
Loss in iteration 286 : 0.5403809746083809
Loss in iteration 287 : 0.537914838633056
Loss in iteration 288 : 0.5403505879671232
Loss in iteration 289 : 0.5378847617354622
Loss in iteration 290 : 0.5403221025511125
Loss in iteration 291 : 0.5378564027860228
Loss in iteration 292 : 0.5402952945453954
Loss in iteration 293 : 0.5378295026130118
Loss in iteration 294 : 0.5402698404886656
Loss in iteration 295 : 0.5378037362724447
Loss in iteration 296 : 0.5402453725613987
Loss in iteration 297 : 0.5377787602896762
Loss in iteration 298 : 0.540221525229757
Loss in iteration 299 : 0.5377542504123712
Loss in iteration 300 : 0.5401979705838664
Loss in iteration 301 : 0.5377299286065804
Loss in iteration 302 : 0.5401744419373379
Loss in iteration 303 : 0.5377055797170867
Loss in iteration 304 : 0.5401507467298416
Loss in iteration 305 : 0.5376810591831502
Loss in iteration 306 : 0.5401267705101943
Loss in iteration 307 : 0.5376562935501188
Loss in iteration 308 : 0.5401024739337578
Loss in iteration 309 : 0.5376312754442658
Loss in iteration 310 : 0.5400778845166128
Loss in iteration 311 : 0.5376060544080954
Loss in iteration 312 : 0.5400530845786461
Loss in iteration 313 : 0.5375807247179613
Loss in iteration 314 : 0.5400281965508953
Loss in iteration 315 : 0.5375554111445853
Loss in iteration 316 : 0.5400033667058332
Loss in iteration 317 : 0.5375302536041912
Loss in iteration 318 : 0.5399787483938258
Loss in iteration 319 : 0.5375053917446685
Loss in iteration 320 : 0.5399544859692835
Loss in iteration 321 : 0.5374809506319912
Loss in iteration 322 : 0.5399307006693848
Loss in iteration 323 : 0.5374570287508758
Loss in iteration 324 : 0.5399074796732511
Loss in iteration 325 : 0.5374336894332997
Loss in iteration 326 : 0.5398848693597161
Loss in iteration 327 : 0.5374109565453709
Loss in iteration 328 : 0.5398628733869875
Loss in iteration 329 : 0.537388814812939
Loss in iteration 330 : 0.5398414556788291
Loss in iteration 331 : 0.5373672146117437
Loss in iteration 332 : 0.5398205478015187
Loss in iteration 333 : 0.5373460804805172
Loss in iteration 334 : 0.5398000596567518
Loss in iteration 335 : 0.5373253221357693
Loss in iteration 336 : 0.5397798919980024
Loss in iteration 337 : 0.5373048464608838
Loss in iteration 338 : 0.5397599490764108
Loss in iteration 339 : 0.5372845688640586
Loss in iteration 340 : 0.5397401497730935
Loss in iteration 341 : 0.5372644225611921
Loss in iteration 342 : 0.5397204358662573
Loss in iteration 343 : 0.5372443647097533
Loss in iteration 344 : 0.5397007765608653
Loss in iteration 345 : 0.5372243788309993
Loss in iteration 346 : 0.5396811689915215
Loss in iteration 347 : 0.5372044735221121
Loss in iteration 348 : 0.5396616349977746
Loss in iteration 349 : 0.5371846779845567
Loss in iteration 350 : 0.5396422149724768
Loss in iteration 351 : 0.5371650353002415
Loss in iteration 352 : 0.5396229599256204
Loss in iteration 353 : 0.537145594619064
Loss in iteration 354 : 0.5396039230482331
Loss in iteration 355 : 0.5371264034587026
Loss in iteration 356 : 0.5395851519990377
Loss in iteration 357 : 0.5371075011728692
Loss in iteration 358 : 0.5395666828997684
Loss in iteration 359 : 0.537088914358016
Loss in iteration 360 : 0.5395485366679177
Loss in iteration 361 : 0.5370706545999883
Loss in iteration 362 : 0.5395307179057622
Loss in iteration 363 : 0.5370527185769167
Loss in iteration 364 : 0.5395132161707747
Loss in iteration 365 : 0.5370350901938657
Loss in iteration 366 : 0.5394960091329545
Loss in iteration 367 : 0.5370177441748354
Loss in iteration 368 : 0.5394790669189266
Loss in iteration 369 : 0.537000650405347
Loss in iteration 370 : 0.5394623568673451
Loss in iteration 371 : 0.5369837783088695
Loss in iteration 372 : 0.5394458479696764
Loss in iteration 373 : 0.5369671006375907
Loss in iteration 374 : 0.5394295144200646
Loss in iteration 375 : 0.5369505962330254
Loss in iteration 376 : 0.5394133379110427
Loss in iteration 377 : 0.5369342515268114
Loss in iteration 378 : 0.5393973085470318
Loss in iteration 379 : 0.5369180607684987
Loss in iteration 380 : 0.5393814244656698
Loss in iteration 381 : 0.5369020251520149
Loss in iteration 382 : 0.5393656904281183
Loss in iteration 383 : 0.5368861511428604
Loss in iteration 384 : 0.5393501157446622
Loss in iteration 385 : 0.5368704483728528
Loss in iteration 386 : 0.5393347119362637
Loss in iteration 387 : 0.5368549274693367
Loss in iteration 388 : 0.5393194905024609
Loss in iteration 389 : 0.5368395981320184
Loss in iteration 390 : 0.5393044610867396
Loss in iteration 391 : 0.5368244676803227
Loss in iteration 392 : 0.5392896302224985
Loss in iteration 393 : 0.5368095401873209
Loss in iteration 394 : 0.539275000727385
Loss in iteration 395 : 0.5367948162118548
Loss in iteration 396 : 0.5392605717095682
Loss in iteration 397 : 0.5367802930538133
Loss in iteration 398 : 0.5392463390701759
Loss in iteration 399 : 0.5367659653981266
Loss in iteration 400 : 0.5392322963385713
Testing accuracy  of updater 2 on alg 0 with rate 1.0 = 0.785875, training accuracy 0.785875, time elapsed: 4970 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6839617669630191
Loss in iteration 3 : 0.6760909594261041
Loss in iteration 4 : 0.6680109399659566
Loss in iteration 5 : 0.6586683759720922
Loss in iteration 6 : 0.6484556293033409
Loss in iteration 7 : 0.6381864457992892
Loss in iteration 8 : 0.6282892130227825
Loss in iteration 9 : 0.6187991309855131
Loss in iteration 10 : 0.6096646851465802
Loss in iteration 11 : 0.600907387893033
Loss in iteration 12 : 0.5925951698544829
Loss in iteration 13 : 0.5847773046070243
Loss in iteration 14 : 0.5774643669672366
Loss in iteration 15 : 0.5706436570107116
Loss in iteration 16 : 0.5642971258998939
Loss in iteration 17 : 0.5584076534118918
Loss in iteration 18 : 0.5529573676419242
Loss in iteration 19 : 0.5479252529308157
Loss in iteration 20 : 0.5432870845320394
Loss in iteration 21 : 0.5390169075937247
Loss in iteration 22 : 0.5350884742152318
Loss in iteration 23 : 0.5314759734266667
Loss in iteration 24 : 0.5281542653509892
Loss in iteration 25 : 0.5250990260571523
Loss in iteration 26 : 0.5222869855558226
Loss in iteration 27 : 0.5196962120922339
Loss in iteration 28 : 0.5173063329482663
Loss in iteration 29 : 0.5150986366144183
Loss in iteration 30 : 0.513056069757436
Loss in iteration 31 : 0.5111631709570726
Loss in iteration 32 : 0.5094059755886404
Loss in iteration 33 : 0.5077719080048294
Loss in iteration 34 : 0.5062496658524179
Loss in iteration 35 : 0.5048290994581306
Loss in iteration 36 : 0.5035010913054334
Loss in iteration 37 : 0.5022574416396791
Loss in iteration 38 : 0.501090764794626
Loss in iteration 39 : 0.4999943981699422
Loss in iteration 40 : 0.4989623235302441
Loss in iteration 41 : 0.4979890991577861
Loss in iteration 42 : 0.4970698011760986
Loss in iteration 43 : 0.4961999725443536
Loss in iteration 44 : 0.4953755784264779
Loss in iteration 45 : 0.4945929667656164
Loss in iteration 46 : 0.4938488330045416
Loss in iteration 47 : 0.4931401880546265
Loss in iteration 48 : 0.49246432883621993
Loss in iteration 49 : 0.4918188109479458
Loss in iteration 50 : 0.49120142322182225
Loss in iteration 51 : 0.49061016405933794
Loss in iteration 52 : 0.4900432195222157
Loss in iteration 53 : 0.48949894318571996
Loss in iteration 54 : 0.4889758377682837
Loss in iteration 55 : 0.48847253853965905
Loss in iteration 56 : 0.4879877984866384
Loss in iteration 57 : 0.48752047518506453
Loss in iteration 58 : 0.48706951929384695
Loss in iteration 59 : 0.48663396455599334
Loss in iteration 60 : 0.4862129191672905
Loss in iteration 61 : 0.48580555835774236
Loss in iteration 62 : 0.4854111180245469
Loss in iteration 63 : 0.48502888925769755
Loss in iteration 64 : 0.4846582136084031
Loss in iteration 65 : 0.4842984789647081
Loss in iteration 66 : 0.4839491159162077
Loss in iteration 67 : 0.4836095945087892
Loss in iteration 68 : 0.4832794213096368
Loss in iteration 69 : 0.4829581367207963
Loss in iteration 70 : 0.48264531249588855
Loss in iteration 71 : 0.48234054942807564
Loss in iteration 72 : 0.48204347518826673
Loss in iteration 73 : 0.4817537423005454
Loss in iteration 74 : 0.4814710262473202
Loss in iteration 75 : 0.4811950236999932
Loss in iteration 76 : 0.4809254508725764
Loss in iteration 77 : 0.4806620419959182
Loss in iteration 78 : 0.4804045479097172
Loss in iteration 79 : 0.4801527347684195
Loss in iteration 80 : 0.4799063828558866
Loss in iteration 81 : 0.4796652855026344
Loss in iteration 82 : 0.47942924809847637
Loss in iteration 83 : 0.4791980871928539
Loss in iteration 84 : 0.4789716296748492
Loss in iteration 85 : 0.47874971202499317
Loss in iteration 86 : 0.4785321796312753
Loss in iteration 87 : 0.47831888616241086
Loss in iteration 88 : 0.4781096929920483
Loss in iteration 89 : 0.47790446866844405
Loss in iteration 90 : 0.47770308842489273
Loss in iteration 91 : 0.4775054337269562
Loss in iteration 92 : 0.4773113918531657
Loss in iteration 93 : 0.47712085550651256
Loss in iteration 94 : 0.4769337224544177
Loss in iteration 95 : 0.4767498951952373
Loss in iteration 96 : 0.4765692806496445
Loss in iteration 97 : 0.47639178987532965
Loss in iteration 98 : 0.47621733780359227
Loss in iteration 99 : 0.4760458429964415
Loss in iteration 100 : 0.4758772274228306
Loss in iteration 101 : 0.4757114162526699
Loss in iteration 102 : 0.4755483376672629
Loss in iteration 103 : 0.4753879226848402
Loss in iteration 104 : 0.47523010499986124
Loss in iteration 105 : 0.4750748208348432
Loss in iteration 106 : 0.4749220088035248
Loss in iteration 107 : 0.47477160978421756
Loss in iteration 108 : 0.47462356680235723
Loss in iteration 109 : 0.4744778249212801
Loss in iteration 110 : 0.4743343311404207
Loss in iteration 111 : 0.47419303430018805
Loss in iteration 112 : 0.4740538849928641
Loss in iteration 113 : 0.47391683547900965
Loss in iteration 114 : 0.47378183960883496
Loss in iteration 115 : 0.47364885274821256
Loss in iteration 116 : 0.4735178317089042
Loss in iteration 117 : 0.47338873468274845
Loss in iteration 118 : 0.4732615211795262
Loss in iteration 119 : 0.4731361519682656
Loss in iteration 120 : 0.4730125890217945
Loss in iteration 121 : 0.4728907954643346
Loss in iteration 122 : 0.47277073552195
Loss in iteration 123 : 0.47265237447571656
Loss in iteration 124 : 0.47253567861740453
Loss in iteration 125 : 0.4724206152075789
Loss in iteration 126 : 0.4723071524359192
Loss in iteration 127 : 0.4721952593836588
Loss in iteration 128 : 0.47208490598800845
Loss in iteration 129 : 0.4719760630084187
Loss in iteration 130 : 0.4718687019945929
Loss in iteration 131 : 0.4717627952561237
Loss in iteration 132 : 0.47165831583367124
Loss in iteration 133 : 0.4715552374715488
Loss in iteration 134 : 0.4714535345916915
Loss in iteration 135 : 0.471353182268861
Loss in iteration 136 : 0.47125415620706557
Loss in iteration 137 : 0.4711564327170918
Loss in iteration 138 : 0.4710599886950995
Loss in iteration 139 : 0.4709648016022109
Loss in iteration 140 : 0.4708708494450589
Loss in iteration 141 : 0.470778110757218
Loss in iteration 142 : 0.4706865645814778
Loss in iteration 143 : 0.47059619045292
Loss in iteration 144 : 0.47050696838275813
Loss in iteration 145 : 0.4704188788428767
Loss in iteration 146 : 0.4703319027510553
Loss in iteration 147 : 0.4702460214568375
Loss in iteration 148 : 0.47016121672799793
Loss in iteration 149 : 0.4700774707375754
Loss in iteration 150 : 0.46999476605147744
Loss in iteration 151 : 0.4699130856165592
Loss in iteration 152 : 0.46983241274920934
Loss in iteration 153 : 0.46975273112441207
Loss in iteration 154 : 0.4696740247652057
Loss in iteration 155 : 0.4695962780325922
Loss in iteration 156 : 0.46951947561582313
Loss in iteration 157 : 0.4694436025230535
Loss in iteration 158 : 0.4693686440723702
Loss in iteration 159 : 0.46929458588314255
Loss in iteration 160 : 0.46922141386769123
Loss in iteration 161 : 0.46914911422328137
Loss in iteration 162 : 0.4690776734243875
Loss in iteration 163 : 0.46900707821525384
Loss in iteration 164 : 0.4689373156027067
Loss in iteration 165 : 0.46886837284923105
Loss in iteration 166 : 0.4688002374662824
Loss in iteration 167 : 0.4687328972078367
Loss in iteration 168 : 0.46866634006415697
Loss in iteration 169 : 0.46860055425577557
Loss in iteration 170 : 0.4685355282276776
Loss in iteration 171 : 0.4684712506436754
Loss in iteration 172 : 0.46840771038098283
Loss in iteration 173 : 0.46834489652494643
Loss in iteration 174 : 0.4682827983639663
Loss in iteration 175 : 0.4682214053845703
Loss in iteration 176 : 0.4681607072666543
Loss in iteration 177 : 0.4681006938788599
Loss in iteration 178 : 0.46804135527410995
Loss in iteration 179 : 0.4679826816852784
Loss in iteration 180 : 0.46792466352098916
Loss in iteration 181 : 0.4678672913615517
Loss in iteration 182 : 0.46781055595500787
Loss in iteration 183 : 0.4677544482133169
Loss in iteration 184 : 0.46769895920863136
Loss in iteration 185 : 0.46764408016970016
Loss in iteration 186 : 0.467589802478364
Loss in iteration 187 : 0.4675361176661663
Loss in iteration 188 : 0.4674830174110523
Loss in iteration 189 : 0.46743049353416366
Loss in iteration 190 : 0.46737853799672346
Loss in iteration 191 : 0.46732714289702004
Loss in iteration 192 : 0.46727630046745205
Loss in iteration 193 : 0.4672260030716804
Loss in iteration 194 : 0.4671762432018367
Loss in iteration 195 : 0.4671270134758317
Loss in iteration 196 : 0.4670783066347093
Loss in iteration 197 : 0.4670301155400923
Loss in iteration 198 : 0.4669824331716888
Loss in iteration 199 : 0.4669352526248608
Loss in iteration 200 : 0.4668885671082703
Loss in iteration 201 : 0.4668423699415647
Loss in iteration 202 : 0.46679665455314207
Loss in iteration 203 : 0.4667514144779674
Loss in iteration 204 : 0.46670664335543804
Loss in iteration 205 : 0.4666623349273143
Loss in iteration 206 : 0.4666184830356918
Loss in iteration 207 : 0.46657508162102884
Loss in iteration 208 : 0.4665321247202309
Loss in iteration 209 : 0.46648960646476384
Loss in iteration 210 : 0.4664475210788288
Loss in iteration 211 : 0.46640586287757685
Loss in iteration 212 : 0.4663646262653657
Loss in iteration 213 : 0.4663238057340596
Loss in iteration 214 : 0.46628339586136497
Loss in iteration 215 : 0.4662433913092132
Loss in iteration 216 : 0.46620378682218144
Loss in iteration 217 : 0.46616457722593796
Loss in iteration 218 : 0.4661257574257365
Loss in iteration 219 : 0.46608732240494793
Loss in iteration 220 : 0.46604926722360357
Loss in iteration 221 : 0.466011587017004
Loss in iteration 222 : 0.46597427699433003
Loss in iteration 223 : 0.4659373324373064
Loss in iteration 224 : 0.46590074869888176
Loss in iteration 225 : 0.4658645212019428
Loss in iteration 226 : 0.4658286454380582
Loss in iteration 227 : 0.4657931169662514
Loss in iteration 228 : 0.4657579314117949
Loss in iteration 229 : 0.46572308446503335
Loss in iteration 230 : 0.465688571880237
Loss in iteration 231 : 0.4656543894744717
Loss in iteration 232 : 0.46562053312649926
Loss in iteration 233 : 0.4655869987756986
Loss in iteration 234 : 0.46555378242100814
Loss in iteration 235 : 0.46552088011989806
Loss in iteration 236 : 0.46548828798735337
Loss in iteration 237 : 0.4654560021948852
Loss in iteration 238 : 0.4654240189695636
Loss in iteration 239 : 0.46539233459306556
Loss in iteration 240 : 0.4653609454007485
Loss in iteration 241 : 0.4653298477807298
Loss in iteration 242 : 0.4652990381730083
Loss in iteration 243 : 0.46526851306857864
Loss in iteration 244 : 0.4652382690085797
Loss in iteration 245 : 0.46520830258345575
Loss in iteration 246 : 0.46517861043213227
Loss in iteration 247 : 0.46514918924120907
Loss in iteration 248 : 0.46512003574417543
Loss in iteration 249 : 0.4650911467206266
Loss in iteration 250 : 0.4650625189955132
Loss in iteration 251 : 0.4650341494383907
Loss in iteration 252 : 0.46500603496269405
Loss in iteration 253 : 0.4649781725250199
Loss in iteration 254 : 0.4649505591244225
Loss in iteration 255 : 0.4649231918017306
Loss in iteration 256 : 0.4648960676388684
Loss in iteration 257 : 0.46486918375819436
Loss in iteration 258 : 0.46484253732185254
Loss in iteration 259 : 0.46481612553112994
Loss in iteration 260 : 0.4647899456258448
Loss in iteration 261 : 0.4647639948837153
Loss in iteration 262 : 0.4647382706197677
Loss in iteration 263 : 0.4647127701857486
Loss in iteration 264 : 0.4646874909695365
Loss in iteration 265 : 0.464662430394577
Loss in iteration 266 : 0.4646375859193248
Loss in iteration 267 : 0.4646129550366966
Loss in iteration 268 : 0.46458853527352745
Loss in iteration 269 : 0.46456432419004795
Loss in iteration 270 : 0.46454031937935975
Loss in iteration 271 : 0.46451651846693687
Loss in iteration 272 : 0.4644929191101103
Loss in iteration 273 : 0.46446951899759087
Loss in iteration 274 : 0.4644463158489756
Loss in iteration 275 : 0.46442330741428217
Loss in iteration 276 : 0.46440049147348167
Loss in iteration 277 : 0.46437786583603746
Loss in iteration 278 : 0.46435542834046045
Loss in iteration 279 : 0.4643331768538636
Loss in iteration 280 : 0.46431110927153907
Loss in iteration 281 : 0.46428922351651947
Loss in iteration 282 : 0.46426751753916573
Testing accuracy  of updater 2 on alg 0 with rate 0.09999999999999998 = 0.7905, training accuracy 0.7905, time elapsed: 4088 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 19.998616144859007
Loss in iteration 3 : 27.601506627795963
Loss in iteration 4 : 9.938174287340898
Loss in iteration 5 : 11.106207851814611
Loss in iteration 6 : 7.491720966129548
Loss in iteration 7 : 7.967658525853581
Loss in iteration 8 : 6.552448045373032
Loss in iteration 9 : 5.986946808556069
Loss in iteration 10 : 5.211545294583746
Loss in iteration 11 : 4.834898569203402
Loss in iteration 12 : 4.334936535605953
Loss in iteration 13 : 4.161527587209117
Loss in iteration 14 : 3.8032519955082487
Loss in iteration 15 : 3.74362151702139
Loss in iteration 16 : 3.390202323257717
Loss in iteration 17 : 3.404044311737088
Loss in iteration 18 : 3.090429751891933
Loss in iteration 19 : 3.1406097310943095
Loss in iteration 20 : 2.870714649649095
Loss in iteration 21 : 2.947224550217715
Loss in iteration 22 : 2.7094191522488256
Loss in iteration 23 : 2.802199189347175
Loss in iteration 24 : 2.5873107600849896
Loss in iteration 25 : 2.6920876234776006
Loss in iteration 26 : 2.478476038654413
Loss in iteration 27 : 2.5965133818514667
Loss in iteration 28 : 2.37360415897832
Loss in iteration 29 : 2.513481325158702
Loss in iteration 30 : 2.2641300940057327
Loss in iteration 31 : 2.4459832813554776
Loss in iteration 32 : 2.1297988066688585
Loss in iteration 33 : 2.3306158322750097
Loss in iteration 34 : 1.9622470157320695
Loss in iteration 35 : 2.136412850155245
Loss in iteration 36 : 1.8405640051290457
Loss in iteration 37 : 2.0116993299663526
Loss in iteration 38 : 1.7917621006382847
Loss in iteration 39 : 1.9711821949338282
Loss in iteration 40 : 1.8149027180072308
Loss in iteration 41 : 2.0140150835053845
Loss in iteration 42 : 1.9581126422823705
Loss in iteration 43 : 2.190528991852575
Loss in iteration 44 : 2.1453900501793215
Loss in iteration 45 : 2.363725504207516
Loss in iteration 46 : 2.189652444507524
Loss in iteration 47 : 2.351775287804881
Loss in iteration 48 : 2.1289134208101474
Loss in iteration 49 : 2.251814533174765
Loss in iteration 50 : 2.0398683134989883
Loss in iteration 51 : 2.142767661163953
Loss in iteration 52 : 1.9584896739004147
Loss in iteration 53 : 2.0523024367491822
Loss in iteration 54 : 1.8917626852043208
Loss in iteration 55 : 1.9824165133443503
Loss in iteration 56 : 1.8385842416364675
Loss in iteration 57 : 1.9295434593989507
Loss in iteration 58 : 1.7960843446987607
Loss in iteration 59 : 1.8894395727252058
Loss in iteration 60 : 1.7614441596642367
Loss in iteration 61 : 1.8584296319917923
Loss in iteration 62 : 1.7323844603838894
Loss in iteration 63 : 1.8336631238548804
Loss in iteration 64 : 1.7072415247454449
Loss in iteration 65 : 1.8130679081787522
Loss in iteration 66 : 1.6849206583655596
Loss in iteration 67 : 1.795229632400996
Loss in iteration 68 : 1.664796950720556
Loss in iteration 69 : 1.7792487976036417
Loss in iteration 70 : 1.6465920033211505
Loss in iteration 71 : 1.764598976051926
Loss in iteration 72 : 1.6302509184450542
Loss in iteration 73 : 1.7510049925760736
Loss in iteration 74 : 1.6158324679596203
Loss in iteration 75 : 1.738344968653431
Loss in iteration 76 : 1.6034102789956173
Loss in iteration 77 : 1.726564639391333
Loss in iteration 78 : 1.5929772935759716
Loss in iteration 79 : 1.7155915758345985
Loss in iteration 80 : 1.584355197831326
Loss in iteration 81 : 1.7052532235872249
Loss in iteration 82 : 1.5771281220666606
Loss in iteration 83 : 1.6952231391563886
Loss in iteration 84 : 1.5706345115985068
Loss in iteration 85 : 1.6850289461614136
Loss in iteration 86 : 1.5640494872496897
Loss in iteration 87 : 1.6741416308745765
Loss in iteration 88 : 1.5565575378268255
Loss in iteration 89 : 1.6621255916075675
Loss in iteration 90 : 1.5475568168462568
Loss in iteration 91 : 1.6487825825502895
Loss in iteration 92 : 1.5367975197862394
Loss in iteration 93 : 1.6342128951751058
Loss in iteration 94 : 1.5243877604506013
Loss in iteration 95 : 1.618764685542797
Loss in iteration 96 : 1.510682948745244
Loss in iteration 97 : 1.6029098942643119
Loss in iteration 98 : 1.4961334189717217
Loss in iteration 99 : 1.5871149481843905
Loss in iteration 100 : 1.4811598488588285
Loss in iteration 101 : 1.5717539298439396
Loss in iteration 102 : 1.4660850316346286
Loss in iteration 103 : 1.5570742177114214
Loss in iteration 104 : 1.4511163624913193
Loss in iteration 105 : 1.5432003349521024
Loss in iteration 106 : 1.4363608897039204
Loss in iteration 107 : 1.5301562880666661
Loss in iteration 108 : 1.4218572071947315
Loss in iteration 109 : 1.517891693604308
Loss in iteration 110 : 1.4076156231983155
Loss in iteration 111 : 1.5063047697962242
Loss in iteration 112 : 1.3936637180203815
Loss in iteration 113 : 1.495262719505597
Loss in iteration 114 : 1.3800960403127824
Loss in iteration 115 : 1.4846267031479967
Loss in iteration 116 : 1.3671239854731287
Loss in iteration 117 : 1.4742930299384378
Loss in iteration 118 : 1.3551160595737917
Loss in iteration 119 : 1.4642580639800062
Loss in iteration 120 : 1.3446113626979481
Loss in iteration 121 : 1.4546920938237933
Loss in iteration 122 : 1.3362817959456847
Loss in iteration 123 : 1.4459705418862996
Loss in iteration 124 : 1.3308155626802831
Loss in iteration 125 : 1.4385935012233895
Loss in iteration 126 : 1.3287044870035374
Loss in iteration 127 : 1.4329652647545776
Loss in iteration 128 : 1.329948515314119
Loss in iteration 129 : 1.4290911014788164
Loss in iteration 130 : 1.333754326959089
Loss in iteration 131 : 1.4263292015409297
Loss in iteration 132 : 1.3384163227432415
Loss in iteration 133 : 1.4233870878705412
Loss in iteration 134 : 1.341651666937494
Loss in iteration 135 : 1.418717821982692
Loss in iteration 136 : 1.3414493318178837
Loss in iteration 137 : 1.4112140317856947
Loss in iteration 138 : 1.336929681600964
Loss in iteration 139 : 1.4007288041119632
Loss in iteration 140 : 1.3285230110712278
Loss in iteration 141 : 1.388010947629982
Loss in iteration 142 : 1.3174375104991332
Loss in iteration 143 : 1.3741994531443147
Loss in iteration 144 : 1.3049856600013428
Loss in iteration 145 : 1.360333181431249
Loss in iteration 146 : 1.2921875790662256
Loss in iteration 147 : 1.3471194499423214
Loss in iteration 148 : 1.2796745591300918
Loss in iteration 149 : 1.3349226940796792
Loss in iteration 150 : 1.2677513318300688
Loss in iteration 151 : 1.3238509995181689
Loss in iteration 152 : 1.2565007371746488
Loss in iteration 153 : 1.313856054134877
Loss in iteration 154 : 1.245876818239231
Loss in iteration 155 : 1.304811790372219
Loss in iteration 156 : 1.235772213314284
Loss in iteration 157 : 1.2965651880209257
Loss in iteration 158 : 1.2260625731612114
Loss in iteration 159 : 1.2889635788516112
Loss in iteration 160 : 1.2166354851927186
Loss in iteration 161 : 1.281865418371671
Loss in iteration 162 : 1.207411307285612
Loss in iteration 163 : 1.2751415261438983
Loss in iteration 164 : 1.1983616232406964
Loss in iteration 165 : 1.2686739680539305
Loss in iteration 166 : 1.189528698174015
Loss in iteration 167 : 1.2623607531622985
Loss in iteration 168 : 1.1810461713980667
Loss in iteration 169 : 1.2561346119231842
Loss in iteration 170 : 1.173156630186405
Loss in iteration 171 : 1.2499988279767567
Loss in iteration 172 : 1.1662154766085764
Loss in iteration 173 : 1.2440684406220308
Loss in iteration 174 : 1.1606645730781364
Loss in iteration 175 : 1.2385859507905903
Loss in iteration 176 : 1.1569587721130061
Loss in iteration 177 : 1.2338748744676056
Loss in iteration 178 : 1.155439875206551
Loss in iteration 179 : 1.2302190489104943
Loss in iteration 180 : 1.1561770317067277
Loss in iteration 181 : 1.2277025304477533
Loss in iteration 182 : 1.1588228548657935
Loss in iteration 183 : 1.2260823292996128
Loss in iteration 184 : 1.1625589150893898
Loss in iteration 185 : 1.2247697363604888
Loss in iteration 186 : 1.1662062543701994
Loss in iteration 187 : 1.222963380513594
Loss in iteration 188 : 1.1685243523087565
Loss in iteration 189 : 1.2199089610469742
Loss in iteration 190 : 1.168597444611149
Loss in iteration 191 : 1.2151702577569437
Loss in iteration 192 : 1.1660980079762162
Loss in iteration 193 : 1.208758248024956
Loss in iteration 194 : 1.1612786037420137
Loss in iteration 195 : 1.2010550696718791
Loss in iteration 196 : 1.1547456066979875
Loss in iteration 197 : 1.1926143362513963
Loss in iteration 198 : 1.1471903738205356
Loss in iteration 199 : 1.1839702222019786
Loss in iteration 200 : 1.1392062604558348
Loss in iteration 201 : 1.1755288127324401
Loss in iteration 202 : 1.1312155575842004
Loss in iteration 203 : 1.1675408672122753
Loss in iteration 204 : 1.1234715402969446
Loss in iteration 205 : 1.1601228068577343
Loss in iteration 206 : 1.1160941628060932
Loss in iteration 207 : 1.153294898926832
Loss in iteration 208 : 1.10911242513184
Loss in iteration 209 : 1.1470185423720196
Loss in iteration 210 : 1.1025010712265326
Loss in iteration 211 : 1.1412253930388838
Loss in iteration 212 : 1.0962081863386826
Loss in iteration 213 : 1.1358371680878294
Loss in iteration 214 : 1.0901743417922138
Loss in iteration 215 : 1.1307775727751814
Loss in iteration 216 : 1.0843452642795013
Loss in iteration 217 : 1.1259785194655674
Loss in iteration 218 : 1.0786800684241542
Loss in iteration 219 : 1.1213827329866453
Loss in iteration 220 : 1.0731566647935442
Loss in iteration 221 : 1.1169445038325139
Loss in iteration 222 : 1.0677753730034034
Loss in iteration 223 : 1.112629962280665
Loss in iteration 224 : 1.062561145590023
Loss in iteration 225 : 1.1084178009478585
Loss in iteration 226 : 1.0575641866050258
Loss in iteration 227 : 1.1043007746320859
Loss in iteration 228 : 1.0528581908200465
Loss in iteration 229 : 1.100287485690168
Loss in iteration 230 : 1.0485350623094323
Loss in iteration 231 : 1.096403030250271
Loss in iteration 232 : 1.044694992995528
Loss in iteration 233 : 1.0926864289642422
Loss in iteration 234 : 1.0414314061296286
Loss in iteration 235 : 1.0891829824940968
Loss in iteration 236 : 1.0388116041617963
Loss in iteration 237 : 1.0859311852283982
Loss in iteration 238 : 1.0368558525495069
Loss in iteration 239 : 1.0829463628244358
Loss in iteration 240 : 1.035519567153788
Loss in iteration 241 : 1.0802057346851175
Loss in iteration 242 : 1.0346844197235132
Loss in iteration 243 : 1.077640740934064
Loss in iteration 244 : 1.034163598592639
Loss in iteration 245 : 1.07514122970779
Loss in iteration 246 : 1.033723514975713
Loss in iteration 247 : 1.0725725015954686
Loss in iteration 248 : 1.0331191505181825
Loss in iteration 249 : 1.06980142449888
Loss in iteration 250 : 1.032134660620645
Loss in iteration 251 : 1.0667238928999938
Loss in iteration 252 : 1.030617691185961
Loss in iteration 253 : 1.0632849209911657
Loss in iteration 254 : 1.0284975730345134
Loss in iteration 255 : 1.059485505208988
Loss in iteration 256 : 1.0257837857365006
Loss in iteration 257 : 1.0553757152868175
Loss in iteration 258 : 1.0225483959095205
Loss in iteration 259 : 1.0510383642438936
Loss in iteration 260 : 1.018900644877926
Loss in iteration 261 : 1.046569716575611
Loss in iteration 262 : 1.0149619830139882
Loss in iteration 263 : 1.0420627504188518
Loss in iteration 264 : 1.0108469493519754
Loss in iteration 265 : 1.0375959175556202
Loss in iteration 266 : 1.0066517339227132
Loss in iteration 267 : 1.0332278142414784
Loss in iteration 268 : 1.002449668516075
Loss in iteration 269 : 1.0289965965487704
Loss in iteration 270 : 0.9982916968750342
Loss in iteration 271 : 1.0249224239716628
Loss in iteration 272 : 0.9942097443994841
Loss in iteration 273 : 1.0210113493312778
Loss in iteration 274 : 0.9902213179253788
Loss in iteration 275 : 1.017259502799908
Loss in iteration 276 : 0.9863342192003905
Loss in iteration 277 : 1.0136568803195558
Loss in iteration 278 : 0.9825507410298062
Loss in iteration 279 : 1.0101904180386814
Loss in iteration 280 : 0.9788710622260428
Loss in iteration 281 : 1.0068462833956746
Loss in iteration 282 : 0.9752957697922062
Loss in iteration 283 : 1.003611453574714
Loss in iteration 284 : 0.9718275451245382
Loss in iteration 285 : 1.0004747106433902
Loss in iteration 286 : 0.968472090363327
Loss in iteration 287 : 0.9974271866389688
Loss in iteration 288 : 0.9652383716063315
Loss in iteration 289 : 0.9944625633529024
Loss in iteration 290 : 0.9621382415481012
Loss in iteration 291 : 0.9915769886435253
Loss in iteration 292 : 0.9591854937156743
Loss in iteration 293 : 0.9887687296918539
Loss in iteration 294 : 0.9563944072597828
Loss in iteration 295 : 0.9860375583605112
Loss in iteration 296 : 0.9537778729626023
Loss in iteration 297 : 0.9833838667966542
Loss in iteration 298 : 0.9513452481824465
Loss in iteration 299 : 0.980807548522003
Loss in iteration 300 : 0.9491001622491758
Loss in iteration 301 : 0.9783067467056855
Loss in iteration 302 : 0.9470385659774855
Loss in iteration 303 : 0.9758766490928724
Loss in iteration 304 : 0.9451473632657468
Loss in iteration 305 : 0.9735085690820037
Loss in iteration 306 : 0.9434039508336189
Loss in iteration 307 : 0.9711895628864947
Loss in iteration 308 : 0.9417769029959345
Loss in iteration 309 : 0.9689027717187646
Loss in iteration 310 : 0.9402278695195423
Loss in iteration 311 : 0.9666285457832159
Loss in iteration 312 : 0.9387145306913658
Loss in iteration 313 : 0.964346231504151
Loss in iteration 314 : 0.9371942263952776
Loss in iteration 315 : 0.9620363347481654
Loss in iteration 316 : 0.9356277112151767
Loss in iteration 317 : 0.959682665889778
Loss in iteration 318 : 0.9339824419637136
Loss in iteration 319 : 0.9572740657188702
Loss in iteration 320 : 0.9322348997268173
Loss in iteration 321 : 0.9548054089826655
Loss in iteration 322 : 0.9303716602177238
Loss in iteration 323 : 0.9522777527728845
Loss in iteration 324 : 0.928389189421847
Loss in iteration 325 : 0.9496976848710155
Loss in iteration 326 : 0.9262925788828177
Loss in iteration 327 : 0.9470760773433436
Loss in iteration 328 : 0.9240935870703554
Loss in iteration 329 : 0.9444265284763173
Loss in iteration 330 : 0.9218083974528458
Loss in iteration 331 : 0.9417637761226725
Loss in iteration 332 : 0.9194554544903789
Loss in iteration 333 : 0.939102306518381
Loss in iteration 334 : 0.9170536325724836
Loss in iteration 335 : 0.936455294800213
Loss in iteration 336 : 0.9146208706811026
Loss in iteration 337 : 0.9338339252789268
Loss in iteration 338 : 0.9121732983189977
Loss in iteration 339 : 0.9312470699117824
Loss in iteration 340 : 0.9097248021729684
Loss in iteration 341 : 0.9287012599924648
Loss in iteration 342 : 0.9072869409728705
Loss in iteration 343 : 0.926200867648603
Loss in iteration 344 : 0.9048691025401979
Loss in iteration 345 : 0.9237484142748501
Loss in iteration 346 : 0.9024788031532949
Loss in iteration 347 : 0.9213449351561634
Loss in iteration 348 : 0.9001220462743724
Loss in iteration 349 : 0.9189903467499232
Loss in iteration 350 : 0.8978036785015435
Loss in iteration 351 : 0.9166837807292796
Loss in iteration 352 : 0.8955277008526574
Loss in iteration 353 : 0.9144238642018574
Loss in iteration 354 : 0.8932975108163514
Loss in iteration 355 : 0.9122089373357025
Loss in iteration 356 : 0.8911160642369458
Loss in iteration 357 : 0.9100372078593675
Loss in iteration 358 : 0.888985956216283
Loss in iteration 359 : 0.9079068470913586
Loss in iteration 360 : 0.8869094274703998
Loss in iteration 361 : 0.905816035103225
Loss in iteration 362 : 0.8848883077423438
Loss in iteration 363 : 0.9037629641575307
Loss in iteration 364 : 0.8829239116005776
Loss in iteration 365 : 0.9017458103718642
Loss in iteration 366 : 0.8810169046624391
Loss in iteration 367 : 0.8997626840960151
Loss in iteration 368 : 0.8791671601097122
Loss in iteration 369 : 0.8978115699331786
Loss in iteration 370 : 0.8773736261887759
Loss in iteration 371 : 0.8958902676163722
Loss in iteration 372 : 0.8756342249195175
Loss in iteration 373 : 0.8939963448046748
Loss in iteration 374 : 0.873945800138092
Loss in iteration 375 : 0.8921271119420492
Loss in iteration 376 : 0.872304129034529
Loss in iteration 377 : 0.8902796273239728
Loss in iteration 378 : 0.8707040055299017
Loss in iteration 379 : 0.8884507373228864
Loss in iteration 380 : 0.8691393965289308
Loss in iteration 381 : 0.886637152490312
Loss in iteration 382 : 0.8676036640152929
Loss in iteration 383 : 0.8848355554429447
Loss in iteration 384 : 0.8660898381586933
Loss in iteration 385 : 0.8830427317601017
Loss in iteration 386 : 0.8645909202325406
Loss in iteration 387 : 0.881255711378821
Loss in iteration 388 : 0.8631001902453661
Loss in iteration 389 : 0.8794719058897796
Loss in iteration 390 : 0.8616114934608952
Loss in iteration 391 : 0.877689227153062
Loss in iteration 392 : 0.8601194825798777
Loss in iteration 393 : 0.8759061748088958
Loss in iteration 394 : 0.8586197978485008
Loss in iteration 395 : 0.8741218841743252
Loss in iteration 396 : 0.8571091748037627
Loss in iteration 397 : 0.8723361309801118
Loss in iteration 398 : 0.8555854775288773
Loss in iteration 399 : 0.870549294540425
Loss in iteration 400 : 0.8540476629063533
Testing accuracy  of updater 3 on alg 0 with rate 10.0 = 0.74525, training accuracy 0.74525, time elapsed: 5089 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6798550983478008
Loss in iteration 3 : 0.6726801557198457
Loss in iteration 4 : 0.6664786089291433
Loss in iteration 5 : 0.6608152087797737
Loss in iteration 6 : 0.6555393282246946
Loss in iteration 7 : 0.6505737167850197
Loss in iteration 8 : 0.645872968066036
Loss in iteration 9 : 0.6414071430562825
Loss in iteration 10 : 0.6371543756456369
Loss in iteration 11 : 0.6330973540095853
Loss in iteration 12 : 0.6292215729876873
Loss in iteration 13 : 0.6255144254644102
Loss in iteration 14 : 0.6219647031988174
Loss in iteration 15 : 0.6185623032480158
Loss in iteration 16 : 0.6152980409733371
Loss in iteration 17 : 0.6121635205651821
Loss in iteration 18 : 0.6091510382870351
Loss in iteration 19 : 0.6062535056150044
Loss in iteration 20 : 0.6034643854431049
Loss in iteration 21 : 0.6007776375662573
Loss in iteration 22 : 0.5981876712216126
Loss in iteration 23 : 0.5956893032953877
Loss in iteration 24 : 0.593277721251443
Loss in iteration 25 : 0.5909484500927351
Loss in iteration 26 : 0.5886973228205601
Loss in iteration 27 : 0.5865204539559984
Loss in iteration 28 : 0.5844142157579165
Loss in iteration 29 : 0.5823752168242571
Loss in iteration 30 : 0.5804002828052869
Loss in iteration 31 : 0.5784864389917878
Loss in iteration 32 : 0.5766308945707213
Loss in iteration 33 : 0.5748310283658994
Loss in iteration 34 : 0.5730843759032316
Loss in iteration 35 : 0.5713886176590943
Loss in iteration 36 : 0.5697415683671062
Loss in iteration 37 : 0.5681411672731105
Loss in iteration 38 : 0.5665854692409964
Loss in iteration 39 : 0.5650726366231309
Loss in iteration 40 : 0.5636009318189688
Loss in iteration 41 : 0.5621687104540438
Loss in iteration 42 : 0.5607744151190336
Loss in iteration 43 : 0.5594165696152519
Loss in iteration 44 : 0.5580937736587459
Loss in iteration 45 : 0.5568046980002875
Loss in iteration 46 : 0.5555480799230748
Loss in iteration 47 : 0.5543227190839876
Loss in iteration 48 : 0.5531274736676888
Loss in iteration 49 : 0.5519612568260832
Loss in iteration 50 : 0.5508230333783347
Loss in iteration 51 : 0.5497118167491192
Loss in iteration 52 : 0.5486266661250534
Loss in iteration 53 : 0.5475666838110093
Loss in iteration 54 : 0.5465310127699661
Loss in iteration 55 : 0.5455188343314362
Loss in iteration 56 : 0.5445293660549876
Loss in iteration 57 : 0.5435618597365554
Loss in iteration 58 : 0.5426155995464333
Loss in iteration 59 : 0.5416899002886969
Loss in iteration 60 : 0.5407841057728924
Loss in iteration 61 : 0.5398975872894719
Loss in iteration 62 : 0.539029742181252
Loss in iteration 63 : 0.5381799925038665
Loss in iteration 64 : 0.537347783768699
Loss in iteration 65 : 0.5365325837624005
Loss in iteration 66 : 0.5357338814375148
Loss in iteration 67 : 0.53495118586922
Loss in iteration 68 : 0.5341840252736147
Loss in iteration 69 : 0.5334319460832655
Loss in iteration 70 : 0.5326945120761758
Loss in iteration 71 : 0.5319713035545146
Loss in iteration 72 : 0.5312619165698514
Loss in iteration 73 : 0.5305659621917773
Loss in iteration 74 : 0.5298830658171214
Loss in iteration 75 : 0.5292128665170994
Loss in iteration 76 : 0.5285550164199986
Loss in iteration 77 : 0.5279091801271373
Loss in iteration 78 : 0.5272750341599933
Loss in iteration 79 : 0.5266522664365896
Loss in iteration 80 : 0.5260405757753212
Loss in iteration 81 : 0.5254396714245502
Loss in iteration 82 : 0.5248492726164128
Loss in iteration 83 : 0.5242691081433929
Loss in iteration 84 : 0.5236989159563055
Loss in iteration 85 : 0.5231384427824278
Loss in iteration 86 : 0.522587443762616
Loss in iteration 87 : 0.5220456821062979
Loss in iteration 88 : 0.5215129287633207
Loss in iteration 89 : 0.5209889621117048
Loss in iteration 90 : 0.5204735676604
Loss in iteration 91 : 0.5199665377661958
Loss in iteration 92 : 0.5194676713640345
Loss in iteration 93 : 0.5189767737099547
Loss in iteration 94 : 0.5184936561359965
Loss in iteration 95 : 0.5180181358164331
Loss in iteration 96 : 0.5175500355446807
Loss in iteration 97 : 0.5170891835203681
Loss in iteration 98 : 0.5166354131460014
Loss in iteration 99 : 0.5161885628327254
Loss in iteration 100 : 0.5157484758147114
Loss in iteration 101 : 0.5153149999717305
Loss in iteration 102 : 0.5148879876594916
Loss in iteration 103 : 0.5144672955473402
Loss in iteration 104 : 0.5140527844629647
Loss in iteration 105 : 0.5136443192437448
Loss in iteration 106 : 0.5132417685944217
Loss in iteration 107 : 0.5128450049507701
Loss in iteration 108 : 0.51245390434899
Loss in iteration 109 : 0.5120683463005231
Loss in iteration 110 : 0.5116882136720511
Loss in iteration 111 : 0.5113133925704174
Loss in iteration 112 : 0.5109437722322309
Loss in iteration 113 : 0.5105792449179547
Loss in iteration 114 : 0.5102197058102338
Loss in iteration 115 : 0.5098650529163035
Loss in iteration 116 : 0.5095151869742578
Loss in iteration 117 : 0.5091700113630171
Loss in iteration 118 : 0.5088294320158206
Loss in iteration 119 : 0.5084933573370845
Loss in iteration 120 : 0.5081616981224812
Loss in iteration 121 : 0.5078343674820708
Loss in iteration 122 : 0.5075112807663892
Loss in iteration 123 : 0.507192355495328
Loss in iteration 124 : 0.5068775112896876
Loss in iteration 125 : 0.5065666698053166
Loss in iteration 126 : 0.5062597546696753
Loss in iteration 127 : 0.5059566914207717
Loss in iteration 128 : 0.5056574074483279
Loss in iteration 129 : 0.5053618319371042
Loss in iteration 130 : 0.5050698958122772
Loss in iteration 131 : 0.5047815316867948
Loss in iteration 132 : 0.5044966738106219
Loss in iteration 133 : 0.504215258021801
Loss in iteration 134 : 0.503937221699236
Loss in iteration 135 : 0.5036625037171574
Loss in iteration 136 : 0.5033910444011763
Loss in iteration 137 : 0.503122785485877
Loss in iteration 138 : 0.5028576700738718
Loss in iteration 139 : 0.5025956425962796
Loss in iteration 140 : 0.5023366487745412
Loss in iteration 141 : 0.5020806355835654
Loss in iteration 142 : 0.5018275512160897
Loss in iteration 143 : 0.501577345048272
Loss in iteration 144 : 0.5013299676064222
Loss in iteration 145 : 0.5010853705348488
Loss in iteration 146 : 0.5008435065647625
Loss in iteration 147 : 0.500604329484218
Loss in iteration 148 : 0.5003677941090379
Loss in iteration 149 : 0.5001338562546845
Loss in iteration 150 : 0.4999024727090409
Loss in iteration 151 : 0.49967360120608234
Loss in iteration 152 : 0.4994472004003787
Loss in iteration 153 : 0.4992232298424312
Loss in iteration 154 : 0.49900164995477053
Loss in iteration 155 : 0.49878242200882844
Loss in iteration 156 : 0.4985655081025296
Loss in iteration 157 : 0.4983508711385913
Loss in iteration 158 : 0.49813847480348816
Loss in iteration 159 : 0.4979282835470763
Loss in iteration 160 : 0.4977202625628514
Loss in iteration 161 : 0.4975143777687946
Loss in iteration 162 : 0.49731059578882786
Loss in iteration 163 : 0.4971088839348074
Loss in iteration 164 : 0.49690921018907885
Loss in iteration 165 : 0.49671154318755456
Loss in iteration 166 : 0.49651585220328787
Loss in iteration 167 : 0.4963221071305552
Loss in iteration 168 : 0.49613027846937807
Loss in iteration 169 : 0.49594033731054993
Loss in iteration 170 : 0.4957522553210522
Loss in iteration 171 : 0.49556600472992773
Loss in iteration 172 : 0.49538155831456304
Loss in iteration 173 : 0.4951988893873503
Loss in iteration 174 : 0.49501797178275536
Loss in iteration 175 : 0.4948387798447389
Loss in iteration 176 : 0.49466128841454987
Loss in iteration 177 : 0.49448547281885297
Loss in iteration 178 : 0.4943113088582115
Loss in iteration 179 : 0.49413877279587065
Loss in iteration 180 : 0.49396784134687016
Loss in iteration 181 : 0.4937984916674599
Loss in iteration 182 : 0.4936307013448089
Loss in iteration 183 : 0.4934644483869921
Loss in iteration 184 : 0.49329971121325955
Loss in iteration 185 : 0.49313646864457145
Loss in iteration 186 : 0.4929746998943926
Loss in iteration 187 : 0.49281438455973486
Loss in iteration 188 : 0.4926555026124364
Loss in iteration 189 : 0.4924980343906915
Loss in iteration 190 : 0.4923419605907881
Loss in iteration 191 : 0.49218726225908394
Loss in iteration 192 : 0.49203392078418084
Loss in iteration 193 : 0.4918819178893209
Loss in iteration 194 : 0.4917312356249639
Loss in iteration 195 : 0.4915818563615825
Loss in iteration 196 : 0.49143376278262735
Loss in iteration 197 : 0.49128693787768474
Loss in iteration 198 : 0.49114136493580896
Loss in iteration 199 : 0.49099702753902474
Loss in iteration 200 : 0.4908539095560009
Loss in iteration 201 : 0.490711995135884
Loss in iteration 202 : 0.49057126870229
Loss in iteration 203 : 0.4904317149474423
Loss in iteration 204 : 0.4902933188264706
Loss in iteration 205 : 0.4901560655518395
Loss in iteration 206 : 0.4900199405879297
Loss in iteration 207 : 0.4898849296457413
Loss in iteration 208 : 0.4897510186777426
Loss in iteration 209 : 0.4896181938728355
Loss in iteration 210 : 0.48948644165144917
Loss in iteration 211 : 0.48935574866075804
Loss in iteration 212 : 0.48922610177001274
Loss in iteration 213 : 0.48909748806598097
Loss in iteration 214 : 0.4889698948485121
Loss in iteration 215 : 0.4888433096261926
Loss in iteration 216 : 0.4887177201121187
Loss in iteration 217 : 0.48859311421977053
Loss in iteration 218 : 0.4884694800589724
Loss in iteration 219 : 0.488346805931963
Loss in iteration 220 : 0.48822508032955214
Loss in iteration 221 : 0.48810429192737415
Loss in iteration 222 : 0.4879844295822162
Loss in iteration 223 : 0.48786548232845367
Loss in iteration 224 : 0.4877474393745488
Loss in iteration 225 : 0.48763029009964065
Loss in iteration 226 : 0.4875140240502203
Loss in iteration 227 : 0.4873986309368598
Loss in iteration 228 : 0.4872841006310511
Loss in iteration 229 : 0.48717042316208575
Loss in iteration 230 : 0.4870575887140251
Loss in iteration 231 : 0.48694558762272994
Loss in iteration 232 : 0.48683441037296415
Loss in iteration 233 : 0.4867240475955564
Loss in iteration 234 : 0.4866144900646352
Loss in iteration 235 : 0.4865057286949198
Loss in iteration 236 : 0.4863977545390705
Loss in iteration 237 : 0.4862905587851027
Loss in iteration 238 : 0.48618413275385763
Loss in iteration 239 : 0.48607846789652204
Loss in iteration 240 : 0.48597355579221735
Loss in iteration 241 : 0.48586938814562425
Loss in iteration 242 : 0.4857659567846722
Loss in iteration 243 : 0.485663253658274
Loss in iteration 244 : 0.48556127083411366
Loss in iteration 245 : 0.48546000049647464
Loss in iteration 246 : 0.48535943494412015
Loss in iteration 247 : 0.4852595665882238
Loss in iteration 248 : 0.4851603879503338
Loss in iteration 249 : 0.48506189166038954
Loss in iteration 250 : 0.4849640704547753
Loss in iteration 251 : 0.48486691717442065
Loss in iteration 252 : 0.48477042476293397
Loss in iteration 253 : 0.4846745862647817
Loss in iteration 254 : 0.48457939482349993
Loss in iteration 255 : 0.4844848436799525
Loss in iteration 256 : 0.48439092617061486
Loss in iteration 257 : 0.4842976357259012
Loss in iteration 258 : 0.4842049658685245
Loss in iteration 259 : 0.4841129102118855
Loss in iteration 260 : 0.48402146245850397
Loss in iteration 261 : 0.4839306163984747
Loss in iteration 262 : 0.4838403659079576
Loss in iteration 263 : 0.4837507049477012
Loss in iteration 264 : 0.4836616275615951
Loss in iteration 265 : 0.4835731278752413
Loss in iteration 266 : 0.48348520009457535
Loss in iteration 267 : 0.4833978385044985
Loss in iteration 268 : 0.4833110374675389
Loss in iteration 269 : 0.4832247914225503
Loss in iteration 270 : 0.4831390948834219
Loss in iteration 271 : 0.4830539424378302
Loss in iteration 272 : 0.48296932874600124
Loss in iteration 273 : 0.48288524853949916
Loss in iteration 274 : 0.48280169662005507
Loss in iteration 275 : 0.4827186678583929
Loss in iteration 276 : 0.4826361571930999
Loss in iteration 277 : 0.48255415962950676
Loss in iteration 278 : 0.4824726702386039
Loss in iteration 279 : 0.48239168415594563
Loss in iteration 280 : 0.4823111965806273
Loss in iteration 281 : 0.4822312027742345
Loss in iteration 282 : 0.48215169805983543
Loss in iteration 283 : 0.4820726778209932
Loss in iteration 284 : 0.4819941375007861
Loss in iteration 285 : 0.481916072600857
Loss in iteration 286 : 0.48183847868047813
Loss in iteration 287 : 0.48176135135562476
Loss in iteration 288 : 0.4816846862980819
Loss in iteration 289 : 0.48160847923455646
Loss in iteration 290 : 0.4815327259458085
Loss in iteration 291 : 0.4814574222658023
Loss in iteration 292 : 0.4813825640808666
Loss in iteration 293 : 0.48130814732887406
Loss in iteration 294 : 0.48123416799844676
Loss in iteration 295 : 0.48116062212814653
Loss in iteration 296 : 0.4810875058057212
Loss in iteration 297 : 0.4810148151673286
Loss in iteration 298 : 0.48094254639679346
Loss in iteration 299 : 0.480870695724874
Loss in iteration 300 : 0.48079925942854296
Loss in iteration 301 : 0.4807282338302764
Loss in iteration 302 : 0.48065761529736867
Loss in iteration 303 : 0.48058740024123425
Loss in iteration 304 : 0.4805175851167579
Loss in iteration 305 : 0.48044816642162097
Loss in iteration 306 : 0.4803791406956674
Loss in iteration 307 : 0.48031050452025886
Loss in iteration 308 : 0.48024225451766167
Loss in iteration 309 : 0.48017438735043066
Loss in iteration 310 : 0.48010689972080706
Loss in iteration 311 : 0.4800397883701313
Loss in iteration 312 : 0.4799730500782614
Loss in iteration 313 : 0.4799066816630049
Loss in iteration 314 : 0.4798406799795556
Loss in iteration 315 : 0.47977504191994674
Loss in iteration 316 : 0.4797097644125125
Loss in iteration 317 : 0.47964484442135336
Loss in iteration 318 : 0.4795802789458131
Loss in iteration 319 : 0.479516065019974
Loss in iteration 320 : 0.47945219971214503
Loss in iteration 321 : 0.4793886801243654
Loss in iteration 322 : 0.4793255033919344
Loss in iteration 323 : 0.47926266668290585
Loss in iteration 324 : 0.47920016719764613
Loss in iteration 325 : 0.4791380021683496
Loss in iteration 326 : 0.4790761688585969
Loss in iteration 327 : 0.479014664562907
Loss in iteration 328 : 0.4789534866062927
Loss in iteration 329 : 0.47889263234383644
Loss in iteration 330 : 0.47883209916025704
Loss in iteration 331 : 0.4787718844695012
Loss in iteration 332 : 0.47871198571432844
Loss in iteration 333 : 0.4786524003659059
Loss in iteration 334 : 0.47859312592342024
Loss in iteration 335 : 0.4785341599136712
Loss in iteration 336 : 0.4784754998907093
Loss in iteration 337 : 0.4784171434354388
Loss in iteration 338 : 0.47835908815525885
Loss in iteration 339 : 0.47830133168369443
Loss in iteration 340 : 0.4782438716800315
Loss in iteration 341 : 0.4781867058289777
Loss in iteration 342 : 0.47812983184030095
Loss in iteration 343 : 0.4780732474484958
Loss in iteration 344 : 0.478016950412441
Loss in iteration 345 : 0.4779609385150746
Loss in iteration 346 : 0.4779052095630658
Loss in iteration 347 : 0.47784976138649266
Loss in iteration 348 : 0.4777945918385321
Loss in iteration 349 : 0.47773969879514333
Loss in iteration 350 : 0.47768508015476846
Loss in iteration 351 : 0.4776307338380266
Loss in iteration 352 : 0.4775766577874227
Loss in iteration 353 : 0.4775228499670547
Loss in iteration 354 : 0.47746930836232965
Loss in iteration 355 : 0.4774160309796776
Loss in iteration 356 : 0.4773630158462774
Loss in iteration 357 : 0.4773102610097858
Loss in iteration 358 : 0.47725776453806057
Loss in iteration 359 : 0.47720552451890363
Loss in iteration 360 : 0.47715353905980123
Loss in iteration 361 : 0.47710180628766224
Loss in iteration 362 : 0.47705032434856676
Loss in iteration 363 : 0.4769990914075213
Loss in iteration 364 : 0.4769481056482124
Loss in iteration 365 : 0.47689736527276216
Loss in iteration 366 : 0.47684686850149877
Loss in iteration 367 : 0.47679661357271547
Loss in iteration 368 : 0.47674659874244296
Loss in iteration 369 : 0.47669682228422394
Loss in iteration 370 : 0.47664728248889215
Loss in iteration 371 : 0.4765979776643464
Loss in iteration 372 : 0.4765489061353433
Loss in iteration 373 : 0.4765000662432709
Loss in iteration 374 : 0.4764514563459535
Loss in iteration 375 : 0.4764030748174348
Loss in iteration 376 : 0.47635492004778257
Loss in iteration 377 : 0.4763069904428757
Loss in iteration 378 : 0.4762592844242203
Loss in iteration 379 : 0.4762118004287478
Loss in iteration 380 : 0.4761645369086239
Loss in iteration 381 : 0.47611749233105877
Testing accuracy  of updater 3 on alg 0 with rate 1.0 = 0.782, training accuracy 0.782, time elapsed: 4040 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6914790645951557
Loss in iteration 3 : 0.6899550519844734
Loss in iteration 4 : 0.688554201750434
Loss in iteration 5 : 0.68725876612305
Loss in iteration 6 : 0.686053711159229
Loss in iteration 7 : 0.6849263044800896
Loss in iteration 8 : 0.6838657623739198
Loss in iteration 9 : 0.6828629488863813
Loss in iteration 10 : 0.6819101200611151
Loss in iteration 11 : 0.6810007071240575
Loss in iteration 12 : 0.6801291330671669
Loss in iteration 13 : 0.679290657742469
Loss in iteration 14 : 0.6784812471997966
Loss in iteration 15 : 0.6776974635762314
Loss in iteration 16 : 0.6769363723642084
Loss in iteration 17 : 0.6761954643464055
Loss in iteration 18 : 0.6754725898898769
Loss in iteration 19 : 0.6747659036431255
Loss in iteration 20 : 0.6740738179820492
Loss in iteration 21 : 0.6733949638095933
Loss in iteration 22 : 0.6727281575341872
Loss in iteration 23 : 0.6720723732388904
Loss in iteration 24 : 0.6714267192111615
Loss in iteration 25 : 0.670790418136261
Loss in iteration 26 : 0.6701627903694685
Loss in iteration 27 : 0.6695432397964225
Loss in iteration 28 : 0.668931241870019
Loss in iteration 29 : 0.6683263334786008
Loss in iteration 30 : 0.6677281043557841
Loss in iteration 31 : 0.6671361897888494
Loss in iteration 32 : 0.666550264421729
Loss in iteration 33 : 0.6659700369812181
Loss in iteration 34 : 0.6653952457825878
Loss in iteration 35 : 0.6648256548936265
Loss in iteration 36 : 0.6642610508554379
Loss in iteration 37 : 0.6637012398745108
Loss in iteration 38 : 0.6631460454139911
Loss in iteration 39 : 0.6625953061235557
Loss in iteration 40 : 0.6620488740568065
Loss in iteration 41 : 0.6615066131330039
Loss in iteration 42 : 0.6609683978068347
Loss in iteration 43 : 0.6604341119154722
Loss in iteration 44 : 0.6599036476768995
Loss in iteration 45 : 0.6593769048176075
Loss in iteration 46 : 0.65885378981098
Loss in iteration 47 : 0.6583342152106454
Loss in iteration 48 : 0.6578180990653854
Loss in iteration 49 : 0.6573053644042424
Loss in iteration 50 : 0.6567959387821584
Loss in iteration 51 : 0.6562897538779284
Loss in iteration 52 : 0.6557867451374227
Loss in iteration 53 : 0.6552868514561271
Loss in iteration 54 : 0.6547900148958726
Loss in iteration 55 : 0.6542961804313524
Loss in iteration 56 : 0.653805295722708
Loss in iteration 57 : 0.6533173109109177
Loss in iteration 58 : 0.6528321784332621
Loss in iteration 59 : 0.6523498528564191
Loss in iteration 60 : 0.6518702907251622
Loss in iteration 61 : 0.6513934504248744
Loss in iteration 62 : 0.6509192920562996
Loss in iteration 63 : 0.650447777321212
Loss in iteration 64 : 0.6499788694178256
Loss in iteration 65 : 0.6495125329449085
Loss in iteration 66 : 0.6490487338137234
Loss in iteration 67 : 0.648587439166984
Loss in iteration 68 : 0.6481286173041751
Loss in iteration 69 : 0.6476722376125817
Loss in iteration 70 : 0.6472182705035245
Loss in iteration 71 : 0.6467666873533052
Loss in iteration 72 : 0.646317460448445
Loss in iteration 73 : 0.6458705629348422
Loss in iteration 74 : 0.6454259687704959
Loss in iteration 75 : 0.6449836526815124
Loss in iteration 76 : 0.6445435901211287
Loss in iteration 77 : 0.644105757231461
Loss in iteration 78 : 0.643670130807836
Loss in iteration 79 : 0.6432366882654349
Loss in iteration 80 : 0.6428054076081091
Loss in iteration 81 : 0.6423762673991997
Loss in iteration 82 : 0.641949246734197
Loss in iteration 83 : 0.6415243252151147
Loss in iteration 84 : 0.6411014829264728
Loss in iteration 85 : 0.6406807004127218
Loss in iteration 86 : 0.6402619586570779
Loss in iteration 87 : 0.6398452390616071
Loss in iteration 88 : 0.6394305234285153
Loss in iteration 89 : 0.6390177939425399
Loss in iteration 90 : 0.6386070331543756
Loss in iteration 91 : 0.6381982239650603
Loss in iteration 92 : 0.6377913496112879
Loss in iteration 93 : 0.6373863936515191
Loss in iteration 94 : 0.6369833399529259
Loss in iteration 95 : 0.6365821726790372
Loss in iteration 96 : 0.6361828762781006
Loss in iteration 97 : 0.6357854354720712
Loss in iteration 98 : 0.6353898352462204
Loss in iteration 99 : 0.6349960608392972
Loss in iteration 100 : 0.6346040977342429
Loss in iteration 101 : 0.6342139316493782
Loss in iteration 102 : 0.6338255485300897
Loss in iteration 103 : 0.6334389345409274
Loss in iteration 104 : 0.6330540760581335
Loss in iteration 105 : 0.6326709596625578
Loss in iteration 106 : 0.6322895721329311
Loss in iteration 107 : 0.6319099004394902
Loss in iteration 108 : 0.6315319317379149
Loss in iteration 109 : 0.6311556533635846
Loss in iteration 110 : 0.6307810528260996
Loss in iteration 111 : 0.6304081178041018
Loss in iteration 112 : 0.6300368361403114
Loss in iteration 113 : 0.6296671958368246
Loss in iteration 114 : 0.6292991850506502
Loss in iteration 115 : 0.6289327920894155
Loss in iteration 116 : 0.6285680054073141
Loss in iteration 117 : 0.6282048136012223
Loss in iteration 118 : 0.6278432054069885
Loss in iteration 119 : 0.6274831696959075
Loss in iteration 120 : 0.627124695471339
Loss in iteration 121 : 0.626767771865478
Loss in iteration 122 : 0.6264123881362667
Loss in iteration 123 : 0.6260585336644511
Loss in iteration 124 : 0.6257061979507377
Loss in iteration 125 : 0.6253553706131011
Loss in iteration 126 : 0.6250060413841775
Loss in iteration 127 : 0.6246582001087743
Loss in iteration 128 : 0.6243118367414897
Loss in iteration 129 : 0.623966941344413
Loss in iteration 130 : 0.6236235040849174
Loss in iteration 131 : 0.623281515233552
Loss in iteration 132 : 0.6229409651619999
Loss in iteration 133 : 0.6226018443411131
Loss in iteration 134 : 0.6222641433390277
Loss in iteration 135 : 0.621927852819349
Loss in iteration 136 : 0.6215929635393943
Loss in iteration 137 : 0.6212594663485098
Loss in iteration 138 : 0.6209273521864087
Loss in iteration 139 : 0.6205966120816431
Loss in iteration 140 : 0.6202672371500394
Loss in iteration 141 : 0.6199392185932405
Loss in iteration 142 : 0.6196125476972824
Loss in iteration 143 : 0.6192872158312152
Loss in iteration 144 : 0.6189632144457601
Loss in iteration 145 : 0.6186405350720268
Loss in iteration 146 : 0.6183191693202448
Loss in iteration 147 : 0.6179991088785619
Loss in iteration 148 : 0.6176803455118445
Loss in iteration 149 : 0.6173628710605468
Loss in iteration 150 : 0.617046677439588
Loss in iteration 151 : 0.6167317566372631
Loss in iteration 152 : 0.6164181007142083
Loss in iteration 153 : 0.6161057018023502
Loss in iteration 154 : 0.6157945521039255
Loss in iteration 155 : 0.6154846438904978
Loss in iteration 156 : 0.6151759695020201
Loss in iteration 157 : 0.614868521345905
Loss in iteration 158 : 0.6145622918961167
Loss in iteration 159 : 0.6142572736923039
Loss in iteration 160 : 0.6139534593389377
Loss in iteration 161 : 0.6136508415044646
Loss in iteration 162 : 0.6133494129205033
Loss in iteration 163 : 0.6130491663810239
Loss in iteration 164 : 0.6127500947415851
Loss in iteration 165 : 0.612452190918553
Loss in iteration 166 : 0.6121554478883606
Loss in iteration 167 : 0.611859858686774
Loss in iteration 168 : 0.6115654164081625
Loss in iteration 169 : 0.6112721142048119
Loss in iteration 170 : 0.6109799452862242
Loss in iteration 171 : 0.6106889029184445
Loss in iteration 172 : 0.6103989804233949
Loss in iteration 173 : 0.6101101711782303
Loss in iteration 174 : 0.6098224686147
Loss in iteration 175 : 0.6095358662185104
Loss in iteration 176 : 0.6092503575287276
Loss in iteration 177 : 0.6089659361371623
Loss in iteration 178 : 0.6086825956877813
Loss in iteration 179 : 0.6084003298761236
Loss in iteration 180 : 0.6081191324487307
Loss in iteration 181 : 0.6078389972025784
Loss in iteration 182 : 0.6075599179845341
Loss in iteration 183 : 0.6072818886907991
Loss in iteration 184 : 0.6070049032663848
Loss in iteration 185 : 0.6067289557045863
Loss in iteration 186 : 0.6064540400464553
Loss in iteration 187 : 0.6061801503803004
Loss in iteration 188 : 0.6059072808411782
Loss in iteration 189 : 0.6056354256104094
Loss in iteration 190 : 0.6053645789150778
Loss in iteration 191 : 0.6050947350275615
Loss in iteration 192 : 0.6048258882650643
Loss in iteration 193 : 0.604558032989135
Loss in iteration 194 : 0.6042911636052334
Loss in iteration 195 : 0.6040252745622526
Loss in iteration 196 : 0.6037603603520966
Loss in iteration 197 : 0.6034964155092268
Loss in iteration 198 : 0.6032334346102348
Loss in iteration 199 : 0.6029714122734174
Loss in iteration 200 : 0.6027103431583551
Loss in iteration 201 : 0.602450221965498
Loss in iteration 202 : 0.6021910434357535
Loss in iteration 203 : 0.6019328023500898
Loss in iteration 204 : 0.6016754935291276
Loss in iteration 205 : 0.601419111832757
Loss in iteration 206 : 0.6011636521597472
Loss in iteration 207 : 0.6009091094473592
Loss in iteration 208 : 0.6006554786709738
Loss in iteration 209 : 0.600402754843721
Loss in iteration 210 : 0.6001509330161006
Loss in iteration 211 : 0.5999000082756306
Loss in iteration 212 : 0.5996499757464874
Loss in iteration 213 : 0.5994008305891498
Loss in iteration 214 : 0.5991525680000416
Loss in iteration 215 : 0.5989051832111987
Loss in iteration 216 : 0.598658671489926
Loss in iteration 217 : 0.5984130281384528
Loss in iteration 218 : 0.5981682484936071
Loss in iteration 219 : 0.5979243279264849
Loss in iteration 220 : 0.5976812618421276
Loss in iteration 221 : 0.597439045679198
Loss in iteration 222 : 0.5971976749096637
Loss in iteration 223 : 0.5969571450384878
Loss in iteration 224 : 0.5967174516033149
Loss in iteration 225 : 0.596478590174174
Loss in iteration 226 : 0.5962405563531613
Loss in iteration 227 : 0.596003345774161
Loss in iteration 228 : 0.5957669541025278
Loss in iteration 229 : 0.5955313770348201
Loss in iteration 230 : 0.5952966102984916
Loss in iteration 231 : 0.5950626496516175
Loss in iteration 232 : 0.5948294908826087
Loss in iteration 233 : 0.5945971298099343
Loss in iteration 234 : 0.5943655622818499
Loss in iteration 235 : 0.5941347841761163
Loss in iteration 236 : 0.5939047913997412
Loss in iteration 237 : 0.593675579888713
Loss in iteration 238 : 0.59344714560772
Loss in iteration 239 : 0.5932194845499201
Loss in iteration 240 : 0.5929925927366583
Loss in iteration 241 : 0.5927664662172251
Loss in iteration 242 : 0.592541101068602
Loss in iteration 243 : 0.5923164933952151
Loss in iteration 244 : 0.5920926393286841
Loss in iteration 245 : 0.5918695350275871
Loss in iteration 246 : 0.5916471766772122
Loss in iteration 247 : 0.5914255604893255
Loss in iteration 248 : 0.5912046827019348
Loss in iteration 249 : 0.5909845395790525
Loss in iteration 250 : 0.5907651274104735
Loss in iteration 251 : 0.5905464425115405
Loss in iteration 252 : 0.5903284812229257
Loss in iteration 253 : 0.5901112399103992
Loss in iteration 254 : 0.589894714964616
Loss in iteration 255 : 0.5896789028009004
Loss in iteration 256 : 0.5894637998590216
Loss in iteration 257 : 0.5892494026029835
Loss in iteration 258 : 0.5890357075208187
Loss in iteration 259 : 0.5888227111243778
Loss in iteration 260 : 0.5886104099491185
Loss in iteration 261 : 0.588398800553905
Loss in iteration 262 : 0.5881878795208063
Loss in iteration 263 : 0.5879776434548957
Loss in iteration 264 : 0.5877680889840471
Loss in iteration 265 : 0.5875592127587527
Loss in iteration 266 : 0.5873510114519156
Loss in iteration 267 : 0.5871434817586674
Loss in iteration 268 : 0.5869366203961699
Loss in iteration 269 : 0.586730424103434
Loss in iteration 270 : 0.5865248896411362
Loss in iteration 271 : 0.5863200137914241
Loss in iteration 272 : 0.5861157933577429
Loss in iteration 273 : 0.5859122251646613
Loss in iteration 274 : 0.5857093060576695
Loss in iteration 275 : 0.5855070329030316
Loss in iteration 276 : 0.5853054025875953
Loss in iteration 277 : 0.5851044120186161
Loss in iteration 278 : 0.5849040581236019
Loss in iteration 279 : 0.5847043378501244
Loss in iteration 280 : 0.5845052481656687
Loss in iteration 281 : 0.5843067860574552
Loss in iteration 282 : 0.5841089485322807
Loss in iteration 283 : 0.583911732616357
Loss in iteration 284 : 0.5837151353551463
Loss in iteration 285 : 0.5835191538132072
Loss in iteration 286 : 0.5833237850740304
Loss in iteration 287 : 0.583129026239889
Loss in iteration 288 : 0.5829348744316838
Loss in iteration 289 : 0.5827413267887769
Loss in iteration 290 : 0.5825483804688656
Loss in iteration 291 : 0.5823560326478069
Loss in iteration 292 : 0.5821642805194897
Loss in iteration 293 : 0.5819731212956701
Loss in iteration 294 : 0.5817825522058417
Loss in iteration 295 : 0.5815925704970791
Loss in iteration 296 : 0.5814031734339052
Loss in iteration 297 : 0.5812143582981433
Loss in iteration 298 : 0.5810261223887776
Loss in iteration 299 : 0.5808384630218225
Loss in iteration 300 : 0.5806513775301739
Loss in iteration 301 : 0.5804648632634821
Loss in iteration 302 : 0.5802789175880174
Loss in iteration 303 : 0.5800935378865294
Loss in iteration 304 : 0.5799087215581264
Loss in iteration 305 : 0.5797244660181302
Loss in iteration 306 : 0.5795407686979644
Loss in iteration 307 : 0.5793576270450148
Loss in iteration 308 : 0.5791750385225026
Loss in iteration 309 : 0.5789930006093668
Loss in iteration 310 : 0.5788115108001307
Loss in iteration 311 : 0.5786305666047878
Loss in iteration 312 : 0.5784501655486722
Loss in iteration 313 : 0.5782703051723397
Loss in iteration 314 : 0.5780909830314527
Loss in iteration 315 : 0.5779121966966588
Loss in iteration 316 : 0.5777339437534696
Loss in iteration 317 : 0.5775562218021569
Loss in iteration 318 : 0.5773790284576185
Loss in iteration 319 : 0.5772023613492808
Loss in iteration 320 : 0.5770262181209782
Loss in iteration 321 : 0.5768505964308479
Loss in iteration 322 : 0.5766754939512083
Loss in iteration 323 : 0.576500908368458
Loss in iteration 324 : 0.5763268373829653
Loss in iteration 325 : 0.5761532787089568
Loss in iteration 326 : 0.5759802300744188
Loss in iteration 327 : 0.5758076892209839
Loss in iteration 328 : 0.5756356539038291
Loss in iteration 329 : 0.5754641218915726
Loss in iteration 330 : 0.5752930909661687
Loss in iteration 331 : 0.5751225589228148
Loss in iteration 332 : 0.574952523569835
Loss in iteration 333 : 0.5747829827285982
Loss in iteration 334 : 0.5746139342334018
Loss in iteration 335 : 0.5744453759313864
Loss in iteration 336 : 0.5742773056824374
Loss in iteration 337 : 0.5741097213590801
Loss in iteration 338 : 0.5739426208463949
Loss in iteration 339 : 0.5737760020419148
Loss in iteration 340 : 0.5736098628555356
Loss in iteration 341 : 0.5734442012094227
Loss in iteration 342 : 0.5732790150379228
Loss in iteration 343 : 0.5731143022874646
Loss in iteration 344 : 0.5729500609164775
Loss in iteration 345 : 0.5727862888952954
Loss in iteration 346 : 0.5726229842060714
Loss in iteration 347 : 0.5724601448426873
Loss in iteration 348 : 0.5722977688106762
Loss in iteration 349 : 0.5721358541271201
Loss in iteration 350 : 0.5719743988205787
Loss in iteration 351 : 0.571813400931
Loss in iteration 352 : 0.5716528585096322
Loss in iteration 353 : 0.5714927696189469
Loss in iteration 354 : 0.5713331323325558
Loss in iteration 355 : 0.5711739447351215
Loss in iteration 356 : 0.5710152049222893
Loss in iteration 357 : 0.5708569110005962
Loss in iteration 358 : 0.5706990610874
Loss in iteration 359 : 0.5705416533107898
Loss in iteration 360 : 0.5703846858095201
Loss in iteration 361 : 0.5702281567329235
Loss in iteration 362 : 0.5700720642408403
Loss in iteration 363 : 0.5699164065035427
Loss in iteration 364 : 0.56976118170165
Loss in iteration 365 : 0.569606388026071
Loss in iteration 366 : 0.5694520236779108
Loss in iteration 367 : 0.5692980868684131
Loss in iteration 368 : 0.5691445758188816
Loss in iteration 369 : 0.5689914887606004
Loss in iteration 370 : 0.5688388239347776
Loss in iteration 371 : 0.5686865795924645
Loss in iteration 372 : 0.5685347539944848
Loss in iteration 373 : 0.5683833454113707
Loss in iteration 374 : 0.5682323521232884
Loss in iteration 375 : 0.5680817724199767
Loss in iteration 376 : 0.5679316046006719
Loss in iteration 377 : 0.5677818469740434
Loss in iteration 378 : 0.5676324978581276
Loss in iteration 379 : 0.5674835555802619
Loss in iteration 380 : 0.5673350184770221
Loss in iteration 381 : 0.5671868848941451
Loss in iteration 382 : 0.5670391531864839
Loss in iteration 383 : 0.5668918217179275
Loss in iteration 384 : 0.5667448888613466
Loss in iteration 385 : 0.566598352998528
Loss in iteration 386 : 0.5664522125201099
Loss in iteration 387 : 0.5663064658255248
Loss in iteration 388 : 0.5661611113229373
Loss in iteration 389 : 0.5660161474291814
Loss in iteration 390 : 0.5658715725697034
Loss in iteration 391 : 0.5657273851784945
Loss in iteration 392 : 0.5655835836980472
Loss in iteration 393 : 0.5654401665792809
Loss in iteration 394 : 0.5652971322814888
Loss in iteration 395 : 0.5651544792722891
Loss in iteration 396 : 0.5650122060275532
Loss in iteration 397 : 0.5648703110313624
Loss in iteration 398 : 0.5647287927759408
Loss in iteration 399 : 0.5645876497616042
Loss in iteration 400 : 0.5644468804967121
Testing accuracy  of updater 3 on alg 0 with rate 0.09999999999999998 = 0.77075, training accuracy 0.77075, time elapsed: 4446 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6929235752666227
Loss in iteration 3 : 0.6926960665968462
Loss in iteration 4 : 0.6924666730027986
Loss in iteration 5 : 0.6922363372674499
Loss in iteration 6 : 0.6920055789324592
Loss in iteration 7 : 0.691774712053391
Loss in iteration 8 : 0.6915439381601184
Loss in iteration 9 : 0.6913133922107217
Loss in iteration 10 : 0.6910831675971454
Loss in iteration 11 : 0.6908533306884829
Loss in iteration 12 : 0.6906239297064439
Loss in iteration 13 : 0.6903950003411546
Loss in iteration 14 : 0.6901665694061512
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231979
Loss in iteration 17 : 0.6894844480963943
Loss in iteration 18 : 0.689258172092525
Loss in iteration 19 : 0.6890324583520105
Loss in iteration 20 : 0.688807311878355
Loss in iteration 21 : 0.6885827361540362
Loss in iteration 22 : 0.6883587333817476
Loss in iteration 23 : 0.6881353046728773
Loss in iteration 24 : 0.6879124501982635
Loss in iteration 25 : 0.687690169311446
Loss in iteration 26 : 0.6874684606515445
Loss in iteration 27 : 0.6872473222307222
Loss in iteration 28 : 0.6870267515096296
Loss in iteration 29 : 0.6868067454633641
Loss in iteration 30 : 0.6865873006395918
Loss in iteration 31 : 0.686368413210221
Loss in iteration 32 : 0.6861500790174885
Loss in iteration 33 : 0.6859322936152177
Loss in iteration 34 : 0.6857150523058203
Loss in iteration 35 : 0.6854983501734859
Loss in iteration 36 : 0.6852821821139223
Loss in iteration 37 : 0.6850665428610205
Loss in iteration 38 : 0.6848514270106477
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359671
Loss in iteration 41 : 0.6842091641928554
Loss in iteration 42 : 0.6839960858461501
Loss in iteration 43 : 0.683783502475952
Loss in iteration 44 : 0.6835714082201878
Loss in iteration 45 : 0.6833597971844081
Loss in iteration 46 : 0.6831486634501975
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348099
Loss in iteration 49 : 0.6825180666557323
Loss in iteration 50 : 0.6823087826913214
Loss in iteration 51 : 0.6820999462889619
Loss in iteration 52 : 0.6818915514994243
Loss in iteration 53 : 0.6816835923783221
Loss in iteration 54 : 0.6814760629868173
Loss in iteration 55 : 0.6812689573916448
Loss in iteration 56 : 0.6810622696644758
Loss in iteration 57 : 0.6808559938806238
Loss in iteration 58 : 0.6806501241171412
Loss in iteration 59 : 0.6804446544502766
Loss in iteration 60 : 0.6802395789522896
Loss in iteration 61 : 0.6800348916876395
Loss in iteration 62 : 0.6798305867084532
Loss in iteration 63 : 0.6796266580492965
Loss in iteration 64 : 0.6794230997211287
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.6790170699411933
Loss in iteration 67 : 0.6788145863262021
Loss in iteration 68 : 0.678612448696558
Loss in iteration 69 : 0.6784106508201553
Loss in iteration 70 : 0.6782091863823759
Loss in iteration 71 : 0.6780080489709492
Loss in iteration 72 : 0.67780723205871
Loss in iteration 73 : 0.6776067289840471
Loss in iteration 74 : 0.6774065329288731
Loss in iteration 75 : 0.6772066368941004
Loss in iteration 76 : 0.6770070336728068
Loss in iteration 77 : 0.6768077158217213
Loss in iteration 78 : 0.6766086756322008
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.676211395916268
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569093
Loss in iteration 83 : 0.675617348325563
Loss in iteration 84 : 0.6754197948878227
Loss in iteration 85 : 0.6752224566127076
Loss in iteration 86 : 0.675025323682299
Loss in iteration 87 : 0.6748283863086912
Loss in iteration 88 : 0.6746316348550659
Loss in iteration 89 : 0.6744350599463953
Loss in iteration 90 : 0.6742386525578963
Loss in iteration 91 : 0.6740424040750547
Loss in iteration 92 : 0.67384630632539
Loss in iteration 93 : 0.6736503515866574
Loss in iteration 94 : 0.6734545325782229
Loss in iteration 95 : 0.673258842442173
Loss in iteration 96 : 0.6730632747194034
Loss in iteration 97 : 0.6728678233241567
Loss in iteration 98 : 0.6726724825192095
Loss in iteration 99 : 0.672477246892672
Loss in iteration 100 : 0.672282111336883
Loss in iteration 101 : 0.6720870710293809
Loss in iteration 102 : 0.6718921214158095
Loss in iteration 103 : 0.6716972581945452
Loss in iteration 104 : 0.6715024773027857
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759103
Loss in iteration 107 : 0.6709185913007195
Loss in iteration 108 : 0.6707241034543506
Loss in iteration 109 : 0.6705296807976894
Loss in iteration 110 : 0.6703353204679108
Loss in iteration 111 : 0.6701410197704022
Loss in iteration 112 : 0.6699467761711548
Loss in iteration 113 : 0.6697525872896085
Loss in iteration 114 : 0.6695584508918566
Loss in iteration 115 : 0.6693643648842078
Loss in iteration 116 : 0.6691703273070827
Loss in iteration 117 : 0.6689763363292015
Loss in iteration 118 : 0.6687823902420608
Loss in iteration 119 : 0.6685884874546636
Loss in iteration 120 : 0.6683946264884936
Loss in iteration 121 : 0.6682008059727261
Loss in iteration 122 : 0.6680070246396377
Loss in iteration 123 : 0.6678132813202299
Loss in iteration 124 : 0.6676195749400379
Loss in iteration 125 : 0.6674259045151237
Loss in iteration 126 : 0.667232269148228
Loss in iteration 127 : 0.667038668025086
Loss in iteration 128 : 0.6668451004109114
Loss in iteration 129 : 0.6666515656469992
Loss in iteration 130 : 0.6664580631474696
Loss in iteration 131 : 0.6662645923961665
Loss in iteration 132 : 0.6660711529436428
Loss in iteration 133 : 0.665877744404295
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254216
Loss in iteration 136 : 0.6652977013095177
Loss in iteration 137 : 0.6651044137490177
Loss in iteration 138 : 0.6649111560380927
Loss in iteration 139 : 0.6647179281196696
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627471
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155855
Loss in iteration 144 : 0.6637522365612187
Loss in iteration 145 : 0.6635591886637094
Loss in iteration 146 : 0.6633661713503325
Loss in iteration 147 : 0.6631731848817296
Loss in iteration 148 : 0.6629802295502835
Loss in iteration 149 : 0.6627873056785707
Loss in iteration 150 : 0.6625944136178621
Loss in iteration 151 : 0.6624015537466766
Loss in iteration 152 : 0.6622087264694014
Loss in iteration 153 : 0.6620159322149516
Loss in iteration 154 : 0.6618231714354788
Loss in iteration 155 : 0.6616304446051515
Loss in iteration 156 : 0.6614377522189504
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561483
Loss in iteration 159 : 0.660859886963547
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913331
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939863
Loss in iteration 164 : 0.6598975185214344
Loss in iteration 165 : 0.6597051613103267
Loss in iteration 166 : 0.6595128444079461
Loss in iteration 167 : 0.6593205684721011
Loss in iteration 168 : 0.6591283341703846
Loss in iteration 169 : 0.6589361421795201
Loss in iteration 170 : 0.6587439931847026
Loss in iteration 171 : 0.6585518878789912
Loss in iteration 172 : 0.6583598269627259
Loss in iteration 173 : 0.6581678111429787
Loss in iteration 174 : 0.6579758411330449
Loss in iteration 175 : 0.6577839176519344
Loss in iteration 176 : 0.6575920414239275
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481179
Loss in iteration 179 : 0.65701670357145
Loss in iteration 180 : 0.6568250236894342
Loss in iteration 181 : 0.6566333947467301
Loss in iteration 182 : 0.6564418174910612
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452749
Loss in iteration 185 : 0.6558674033633619
Loss in iteration 186 : 0.6556760403843819
Loss in iteration 187 : 0.6554847328672831
Loss in iteration 188 : 0.6552934815725109
Loss in iteration 189 : 0.6551022872617626
Loss in iteration 190 : 0.6549111506977277
Loss in iteration 191 : 0.6547200726438117
Loss in iteration 192 : 0.6545290538638485
Loss in iteration 193 : 0.6543380951217601
Loss in iteration 194 : 0.6541471971811902
Loss in iteration 195 : 0.6539563608050981
Loss in iteration 196 : 0.6537655867552814
Loss in iteration 197 : 0.6535748757918566
Loss in iteration 198 : 0.6533842286726632
Loss in iteration 199 : 0.6531936461525875
Loss in iteration 200 : 0.653003128982836
Loss in iteration 201 : 0.6528126779100925
Loss in iteration 202 : 0.6526222936756358
Loss in iteration 203 : 0.6524319770143664
Loss in iteration 204 : 0.6522417286537278
Loss in iteration 205 : 0.6520515493126077
Loss in iteration 206 : 0.6518614397001324
Loss in iteration 207 : 0.651671400514421
Loss in iteration 208 : 0.6514814324412715
Loss in iteration 209 : 0.6512915361528184
Loss in iteration 210 : 0.6511017123061177
Loss in iteration 211 : 0.6509119615417064
Loss in iteration 212 : 0.6507222844821396
Loss in iteration 213 : 0.6505326817304534
Loss in iteration 214 : 0.6503431538686223
Loss in iteration 215 : 0.6501537014559656
Loss in iteration 216 : 0.6499643250275178
Loss in iteration 217 : 0.6497750250923703
Loss in iteration 218 : 0.6495858021319837
Loss in iteration 219 : 0.6493966565984831
Loss in iteration 220 : 0.6492075889129876
Loss in iteration 221 : 0.6490185994639678
Loss in iteration 222 : 0.6488296886057647
Loss in iteration 223 : 0.6486408566572915
Loss in iteration 224 : 0.6484521039010835
Loss in iteration 225 : 0.6482634305828623
Loss in iteration 226 : 0.6480748369117985
Loss in iteration 227 : 0.647886323061708
Loss in iteration 228 : 0.64769788917346
Loss in iteration 229 : 0.6475095353587869
Loss in iteration 230 : 0.6473212617056988
Loss in iteration 231 : 0.6471330682854867
Loss in iteration 232 : 0.6469449551612092
Loss in iteration 233 : 0.646756922397225
Loss in iteration 234 : 0.6465689700691694
Loss in iteration 235 : 0.6463810982736484
Loss in iteration 236 : 0.6461933071368225
Loss in iteration 237 : 0.6460055968212823
Loss in iteration 238 : 0.6458179675308101
Loss in iteration 239 : 0.6456304195129219
Loss in iteration 240 : 0.6454429530593564
Loss in iteration 241 : 0.6452555685049133
Loss in iteration 242 : 0.645068266225024
Loss in iteration 243 : 0.6448810466325241
Loss in iteration 244 : 0.6446939101740161
Loss in iteration 245 : 0.6445068573260437
Loss in iteration 246 : 0.6443198885913674
Loss in iteration 247 : 0.6441330044953829
Loss in iteration 248 : 0.6439462055828149
Loss in iteration 249 : 0.6437594924146947
Loss in iteration 250 : 0.6435728655656119
Loss in iteration 251 : 0.6433863256212781
Loss in iteration 252 : 0.6431998731763218
Loss in iteration 253 : 0.6430135088323655
Loss in iteration 254 : 0.6428272331963023
Loss in iteration 255 : 0.6426410468787648
Loss in iteration 256 : 0.6424549504928054
Loss in iteration 257 : 0.6422689446526941
Loss in iteration 258 : 0.6420830299728993
Loss in iteration 259 : 0.6418972070671641
Loss in iteration 260 : 0.641711476547701
Loss in iteration 261 : 0.6415258390244867
Loss in iteration 262 : 0.6413402951046446
Loss in iteration 263 : 0.6411548453919075
Loss in iteration 264 : 0.6409694904861272
Loss in iteration 265 : 0.6407842309828747
Loss in iteration 266 : 0.6405990674730616
Loss in iteration 267 : 0.6404140005426404
Loss in iteration 268 : 0.640229030772317
Loss in iteration 269 : 0.6400441587373172
Loss in iteration 270 : 0.6398593850071848
Loss in iteration 271 : 0.639674710145599
Loss in iteration 272 : 0.6394901347102335
Loss in iteration 273 : 0.639305659252621
Loss in iteration 274 : 0.6391212843180487
Loss in iteration 275 : 0.638937010445479
Loss in iteration 276 : 0.638752838167464
Loss in iteration 277 : 0.6385687680101043
Loss in iteration 278 : 0.6383848004929946
Loss in iteration 279 : 0.6382009361292014
Loss in iteration 280 : 0.6380171754252336
Loss in iteration 281 : 0.6378335188810368
Loss in iteration 282 : 0.637649966989992
Loss in iteration 283 : 0.6374665202389228
Loss in iteration 284 : 0.6372831791080968
Loss in iteration 285 : 0.6370999440712594
Loss in iteration 286 : 0.6369168155956506
Loss in iteration 287 : 0.6367337941420306
Loss in iteration 288 : 0.6365508801647254
Loss in iteration 289 : 0.6363680741116532
Loss in iteration 290 : 0.6361853764243693
Loss in iteration 291 : 0.6360027875381179
Loss in iteration 292 : 0.6358203078818729
Loss in iteration 293 : 0.6356379378783885
Loss in iteration 294 : 0.6354556779442525
Loss in iteration 295 : 0.6352735284899457
Loss in iteration 296 : 0.6350914899198921
Loss in iteration 297 : 0.6349095626325213
Loss in iteration 298 : 0.6347277470203182
Loss in iteration 299 : 0.6345460434699073
Loss in iteration 300 : 0.6343644523620822
Loss in iteration 301 : 0.6341829740718926
Loss in iteration 302 : 0.6340016089686892
Loss in iteration 303 : 0.6338203574162106
Loss in iteration 304 : 0.633639219772621
Loss in iteration 305 : 0.6334581963905884
Loss in iteration 306 : 0.6332772876173535
Loss in iteration 307 : 0.6330964937947804
Loss in iteration 308 : 0.6329158152594332
Loss in iteration 309 : 0.6327352523426334
Loss in iteration 310 : 0.6325548053705294
Loss in iteration 311 : 0.6323744746641569
Loss in iteration 312 : 0.6321942605395111
Loss in iteration 313 : 0.6320141633075939
Loss in iteration 314 : 0.6318341832744998
Loss in iteration 315 : 0.6316543207414572
Loss in iteration 316 : 0.6314745760049126
Loss in iteration 317 : 0.6312949493565745
Loss in iteration 318 : 0.6311154410834854
Loss in iteration 319 : 0.6309360514680845
Loss in iteration 320 : 0.6307567807882652
Loss in iteration 321 : 0.6305776293174354
Loss in iteration 322 : 0.630398597324578
Loss in iteration 323 : 0.6302196850743191
Loss in iteration 324 : 0.6300408928269712
Loss in iteration 325 : 0.6298622208386057
Loss in iteration 326 : 0.6296836693611023
Loss in iteration 327 : 0.629505238642216
Loss in iteration 328 : 0.6293269289256227
Loss in iteration 329 : 0.6291487404509839
Loss in iteration 330 : 0.6289706734539993
Loss in iteration 331 : 0.628792728166462
Loss in iteration 332 : 0.6286149048163151
Loss in iteration 333 : 0.6284372036277082
Loss in iteration 334 : 0.6282596248210414
Loss in iteration 335 : 0.628082168613027
Loss in iteration 336 : 0.6279048352167442
Loss in iteration 337 : 0.6277276248416838
Loss in iteration 338 : 0.627550537693804
Loss in iteration 339 : 0.6273735739755779
Loss in iteration 340 : 0.6271967338860511
Loss in iteration 341 : 0.627020017620885
Loss in iteration 342 : 0.6268434253724154
Loss in iteration 343 : 0.6266669573296862
Loss in iteration 344 : 0.6264906136785104
Loss in iteration 345 : 0.6263143946015229
Loss in iteration 346 : 0.6261383002782166
Loss in iteration 347 : 0.6259623308849916
Loss in iteration 348 : 0.625786486595212
Loss in iteration 349 : 0.625610767579242
Loss in iteration 350 : 0.6254351740045059
Loss in iteration 351 : 0.6252597060355184
Loss in iteration 352 : 0.6250843638339423
Loss in iteration 353 : 0.6249091475586299
Loss in iteration 354 : 0.6247340573656706
Loss in iteration 355 : 0.6245590934084377
Loss in iteration 356 : 0.6243842558376298
Loss in iteration 357 : 0.6242095448013192
Loss in iteration 358 : 0.6240349604449996
Loss in iteration 359 : 0.6238605029116274
Loss in iteration 360 : 0.623686172341664
Loss in iteration 361 : 0.6235119688731322
Loss in iteration 362 : 0.6233378926416571
Loss in iteration 363 : 0.6231639437805041
Loss in iteration 364 : 0.6229901224206351
Loss in iteration 365 : 0.6228164286907494
Loss in iteration 366 : 0.6226428627173387
Loss in iteration 367 : 0.6224694246247156
Loss in iteration 368 : 0.6222961145350872
Loss in iteration 369 : 0.6221229325685811
Loss in iteration 370 : 0.6219498788433027
Loss in iteration 371 : 0.621776953475392
Loss in iteration 372 : 0.6216041565790597
Loss in iteration 373 : 0.6214314882666478
Loss in iteration 374 : 0.6212589486486783
Loss in iteration 375 : 0.6210865378339118
Loss in iteration 376 : 0.6209142559293943
Loss in iteration 377 : 0.6207421030405187
Loss in iteration 378 : 0.6205700792710779
Loss in iteration 379 : 0.6203981847233268
Loss in iteration 380 : 0.6202264194980369
Loss in iteration 381 : 0.6200547836945696
Loss in iteration 382 : 0.6198832774109301
Loss in iteration 383 : 0.6197119007438298
Loss in iteration 384 : 0.6195406537887674
Loss in iteration 385 : 0.6193695366400889
Loss in iteration 386 : 0.6191985493910619
Loss in iteration 387 : 0.6190276921339456
Loss in iteration 388 : 0.6188569649600788
Loss in iteration 389 : 0.6186863679599471
Loss in iteration 390 : 0.6185159012232659
Loss in iteration 391 : 0.6183455648390703
Loss in iteration 392 : 0.6181753588957913
Loss in iteration 393 : 0.6180052834813475
Loss in iteration 394 : 0.6178353386832349
Loss in iteration 395 : 0.6176655245886088
Loss in iteration 396 : 0.6174958412843844
Loss in iteration 397 : 0.617326288857317
Loss in iteration 398 : 0.6171568673940976
Loss in iteration 399 : 0.6169875769814386
Loss in iteration 400 : 0.616818417706152
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.726, training accuracy 0.726, time elapsed: 4270 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6929235752666227
Loss in iteration 3 : 0.6926960665968462
Loss in iteration 4 : 0.6924666730027986
Loss in iteration 5 : 0.6922363372674499
Loss in iteration 6 : 0.6920055789324592
Loss in iteration 7 : 0.691774712053391
Loss in iteration 8 : 0.6915439381601184
Loss in iteration 9 : 0.6913133922107217
Loss in iteration 10 : 0.6910831675971454
Loss in iteration 11 : 0.6908533306884829
Loss in iteration 12 : 0.6906239297064439
Loss in iteration 13 : 0.6903950003411546
Loss in iteration 14 : 0.6901665694061512
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231979
Loss in iteration 17 : 0.6894844480963943
Loss in iteration 18 : 0.689258172092525
Loss in iteration 19 : 0.6890324583520105
Loss in iteration 20 : 0.688807311878355
Loss in iteration 21 : 0.6885827361540362
Loss in iteration 22 : 0.6883587333817476
Loss in iteration 23 : 0.6881353046728773
Loss in iteration 24 : 0.6879124501982635
Loss in iteration 25 : 0.687690169311446
Loss in iteration 26 : 0.6874684606515445
Loss in iteration 27 : 0.6872473222307222
Loss in iteration 28 : 0.6870267515096296
Loss in iteration 29 : 0.6868067454633641
Loss in iteration 30 : 0.6865873006395918
Loss in iteration 31 : 0.686368413210221
Loss in iteration 32 : 0.6861500790174885
Loss in iteration 33 : 0.6859322936152177
Loss in iteration 34 : 0.6857150523058203
Loss in iteration 35 : 0.6854983501734859
Loss in iteration 36 : 0.6852821821139223
Loss in iteration 37 : 0.6850665428610205
Loss in iteration 38 : 0.6848514270106477
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359671
Loss in iteration 41 : 0.6842091641928554
Loss in iteration 42 : 0.6839960858461501
Loss in iteration 43 : 0.683783502475952
Loss in iteration 44 : 0.6835714082201878
Loss in iteration 45 : 0.6833597971844081
Loss in iteration 46 : 0.6831486634501975
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348099
Loss in iteration 49 : 0.6825180666557323
Loss in iteration 50 : 0.6823087826913214
Loss in iteration 51 : 0.6820999462889619
Loss in iteration 52 : 0.6818915514994243
Loss in iteration 53 : 0.6816835923783221
Loss in iteration 54 : 0.6814760629868173
Loss in iteration 55 : 0.6812689573916448
Loss in iteration 56 : 0.6810622696644758
Loss in iteration 57 : 0.6808559938806238
Loss in iteration 58 : 0.6806501241171412
Loss in iteration 59 : 0.6804446544502766
Loss in iteration 60 : 0.6802395789522896
Loss in iteration 61 : 0.6800348916876395
Loss in iteration 62 : 0.6798305867084532
Loss in iteration 63 : 0.6796266580492965
Loss in iteration 64 : 0.6794230997211287
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.6790170699411933
Loss in iteration 67 : 0.6788145863262021
Loss in iteration 68 : 0.678612448696558
Loss in iteration 69 : 0.6784106508201553
Loss in iteration 70 : 0.6782091863823759
Loss in iteration 71 : 0.6780080489709492
Loss in iteration 72 : 0.67780723205871
Loss in iteration 73 : 0.6776067289840471
Loss in iteration 74 : 0.6774065329288731
Loss in iteration 75 : 0.6772066368941004
Loss in iteration 76 : 0.6770070336728068
Loss in iteration 77 : 0.6768077158217213
Loss in iteration 78 : 0.6766086756322008
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.676211395916268
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569093
Loss in iteration 83 : 0.675617348325563
Loss in iteration 84 : 0.6754197948878227
Loss in iteration 85 : 0.6752224566127076
Loss in iteration 86 : 0.675025323682299
Loss in iteration 87 : 0.6748283863086912
Loss in iteration 88 : 0.6746316348550659
Loss in iteration 89 : 0.6744350599463953
Loss in iteration 90 : 0.6742386525578963
Loss in iteration 91 : 0.6740424040750547
Loss in iteration 92 : 0.67384630632539
Loss in iteration 93 : 0.6736503515866574
Loss in iteration 94 : 0.6734545325782229
Loss in iteration 95 : 0.673258842442173
Loss in iteration 96 : 0.6730632747194034
Loss in iteration 97 : 0.6728678233241567
Loss in iteration 98 : 0.6726724825192095
Loss in iteration 99 : 0.672477246892672
Loss in iteration 100 : 0.672282111336883
Loss in iteration 101 : 0.6720870710293809
Loss in iteration 102 : 0.6718921214158095
Loss in iteration 103 : 0.6716972581945452
Loss in iteration 104 : 0.6715024773027857
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759103
Loss in iteration 107 : 0.6709185913007195
Loss in iteration 108 : 0.6707241034543506
Loss in iteration 109 : 0.6705296807976894
Loss in iteration 110 : 0.6703353204679108
Loss in iteration 111 : 0.6701410197704022
Loss in iteration 112 : 0.6699467761711548
Loss in iteration 113 : 0.6697525872896085
Loss in iteration 114 : 0.6695584508918566
Loss in iteration 115 : 0.6693643648842078
Loss in iteration 116 : 0.6691703273070827
Loss in iteration 117 : 0.6689763363292015
Loss in iteration 118 : 0.6687823902420608
Loss in iteration 119 : 0.6685884874546636
Loss in iteration 120 : 0.6683946264884936
Loss in iteration 121 : 0.6682008059727261
Loss in iteration 122 : 0.6680070246396377
Loss in iteration 123 : 0.6678132813202299
Loss in iteration 124 : 0.6676195749400379
Loss in iteration 125 : 0.6674259045151237
Loss in iteration 126 : 0.667232269148228
Loss in iteration 127 : 0.667038668025086
Loss in iteration 128 : 0.6668451004109114
Loss in iteration 129 : 0.6666515656469992
Loss in iteration 130 : 0.6664580631474696
Loss in iteration 131 : 0.6662645923961665
Loss in iteration 132 : 0.6660711529436428
Loss in iteration 133 : 0.665877744404295
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254216
Loss in iteration 136 : 0.6652977013095177
Loss in iteration 137 : 0.6651044137490177
Loss in iteration 138 : 0.6649111560380927
Loss in iteration 139 : 0.6647179281196696
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627471
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155855
Loss in iteration 144 : 0.6637522365612187
Loss in iteration 145 : 0.6635591886637094
Loss in iteration 146 : 0.6633661713503325
Loss in iteration 147 : 0.6631731848817296
Loss in iteration 148 : 0.6629802295502835
Loss in iteration 149 : 0.6627873056785707
Loss in iteration 150 : 0.6625944136178621
Loss in iteration 151 : 0.6624015537466766
Loss in iteration 152 : 0.6622087264694014
Loss in iteration 153 : 0.6620159322149516
Loss in iteration 154 : 0.6618231714354788
Loss in iteration 155 : 0.6616304446051515
Loss in iteration 156 : 0.6614377522189504
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561483
Loss in iteration 159 : 0.660859886963547
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913331
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939863
Loss in iteration 164 : 0.6598975185214344
Loss in iteration 165 : 0.6597051613103267
Loss in iteration 166 : 0.6595128444079461
Loss in iteration 167 : 0.6593205684721011
Loss in iteration 168 : 0.6591283341703846
Loss in iteration 169 : 0.6589361421795201
Loss in iteration 170 : 0.6587439931847026
Loss in iteration 171 : 0.6585518878789912
Loss in iteration 172 : 0.6583598269627259
Loss in iteration 173 : 0.6581678111429787
Loss in iteration 174 : 0.6579758411330449
Loss in iteration 175 : 0.6577839176519344
Loss in iteration 176 : 0.6575920414239275
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481179
Loss in iteration 179 : 0.65701670357145
Loss in iteration 180 : 0.6568250236894342
Loss in iteration 181 : 0.6566333947467301
Loss in iteration 182 : 0.6564418174910612
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452749
Loss in iteration 185 : 0.6558674033633619
Loss in iteration 186 : 0.6556760403843819
Loss in iteration 187 : 0.6554847328672831
Loss in iteration 188 : 0.6552934815725109
Loss in iteration 189 : 0.6551022872617626
Loss in iteration 190 : 0.6549111506977277
Loss in iteration 191 : 0.6547200726438117
Loss in iteration 192 : 0.6545290538638485
Loss in iteration 193 : 0.6543380951217601
Loss in iteration 194 : 0.6541471971811902
Loss in iteration 195 : 0.6539563608050981
Loss in iteration 196 : 0.6537655867552814
Loss in iteration 197 : 0.6535748757918566
Loss in iteration 198 : 0.6533842286726632
Loss in iteration 199 : 0.6531936461525875
Loss in iteration 200 : 0.653003128982836
Loss in iteration 201 : 0.6528126779100925
Loss in iteration 202 : 0.6526222936756358
Loss in iteration 203 : 0.6524319770143664
Loss in iteration 204 : 0.6522417286537278
Loss in iteration 205 : 0.6520515493126077
Loss in iteration 206 : 0.6518614397001324
Loss in iteration 207 : 0.651671400514421
Loss in iteration 208 : 0.6514814324412715
Loss in iteration 209 : 0.6512915361528184
Loss in iteration 210 : 0.6511017123061177
Loss in iteration 211 : 0.6509119615417064
Loss in iteration 212 : 0.6507222844821396
Loss in iteration 213 : 0.6505326817304534
Loss in iteration 214 : 0.6503431538686223
Loss in iteration 215 : 0.6501537014559656
Loss in iteration 216 : 0.6499643250275178
Loss in iteration 217 : 0.6497750250923703
Loss in iteration 218 : 0.6495858021319837
Loss in iteration 219 : 0.6493966565984831
Loss in iteration 220 : 0.6492075889129876
Loss in iteration 221 : 0.6490185994639678
Loss in iteration 222 : 0.6488296886057647
Loss in iteration 223 : 0.6486408566572915
Loss in iteration 224 : 0.6484521039010835
Loss in iteration 225 : 0.6482634305828623
Loss in iteration 226 : 0.6480748369117985
Loss in iteration 227 : 0.647886323061708
Loss in iteration 228 : 0.64769788917346
Loss in iteration 229 : 0.6475095353587869
Loss in iteration 230 : 0.6473212617056988
Loss in iteration 231 : 0.6471330682854867
Loss in iteration 232 : 0.6469449551612092
Loss in iteration 233 : 0.646756922397225
Loss in iteration 234 : 0.6465689700691694
Loss in iteration 235 : 0.6463810982736484
Loss in iteration 236 : 0.6461933071368225
Loss in iteration 237 : 0.6460055968212823
Loss in iteration 238 : 0.6458179675308101
Loss in iteration 239 : 0.6456304195129219
Loss in iteration 240 : 0.6454429530593564
Loss in iteration 241 : 0.6452555685049133
Loss in iteration 242 : 0.645068266225024
Loss in iteration 243 : 0.6448810466325241
Loss in iteration 244 : 0.6446939101740161
Loss in iteration 245 : 0.6445068573260437
Loss in iteration 246 : 0.6443198885913674
Loss in iteration 247 : 0.6441330044953829
Loss in iteration 248 : 0.6439462055828149
Loss in iteration 249 : 0.6437594924146947
Loss in iteration 250 : 0.6435728655656119
Loss in iteration 251 : 0.6433863256212781
Loss in iteration 252 : 0.6431998731763218
Loss in iteration 253 : 0.6430135088323655
Loss in iteration 254 : 0.6428272331963023
Loss in iteration 255 : 0.6426410468787648
Loss in iteration 256 : 0.6424549504928054
Loss in iteration 257 : 0.6422689446526941
Loss in iteration 258 : 0.6420830299728993
Loss in iteration 259 : 0.6418972070671641
Loss in iteration 260 : 0.641711476547701
Loss in iteration 261 : 0.6415258390244867
Loss in iteration 262 : 0.6413402951046446
Loss in iteration 263 : 0.6411548453919075
Loss in iteration 264 : 0.6409694904861272
Loss in iteration 265 : 0.6407842309828747
Loss in iteration 266 : 0.6405990674730616
Loss in iteration 267 : 0.6404140005426404
Loss in iteration 268 : 0.640229030772317
Loss in iteration 269 : 0.6400441587373172
Loss in iteration 270 : 0.6398593850071848
Loss in iteration 271 : 0.639674710145599
Loss in iteration 272 : 0.6394901347102335
Loss in iteration 273 : 0.639305659252621
Loss in iteration 274 : 0.6391212843180487
Loss in iteration 275 : 0.638937010445479
Loss in iteration 276 : 0.638752838167464
Loss in iteration 277 : 0.6385687680101043
Loss in iteration 278 : 0.6383848004929946
Loss in iteration 279 : 0.6382009361292014
Loss in iteration 280 : 0.6380171754252336
Loss in iteration 281 : 0.6378335188810368
Loss in iteration 282 : 0.637649966989992
Loss in iteration 283 : 0.6374665202389228
Loss in iteration 284 : 0.6372831791080968
Loss in iteration 285 : 0.6370999440712594
Loss in iteration 286 : 0.6369168155956506
Loss in iteration 287 : 0.6367337941420306
Loss in iteration 288 : 0.6365508801647254
Loss in iteration 289 : 0.6363680741116532
Loss in iteration 290 : 0.6361853764243693
Loss in iteration 291 : 0.6360027875381179
Loss in iteration 292 : 0.6358203078818729
Loss in iteration 293 : 0.6356379378783885
Loss in iteration 294 : 0.6354556779442525
Loss in iteration 295 : 0.6352735284899457
Loss in iteration 296 : 0.6350914899198921
Loss in iteration 297 : 0.6349095626325213
Loss in iteration 298 : 0.6347277470203182
Loss in iteration 299 : 0.6345460434699073
Loss in iteration 300 : 0.6343644523620822
Loss in iteration 301 : 0.6341829740718926
Loss in iteration 302 : 0.6340016089686892
Loss in iteration 303 : 0.6338203574162106
Loss in iteration 304 : 0.633639219772621
Loss in iteration 305 : 0.6334581963905884
Loss in iteration 306 : 0.6332772876173535
Loss in iteration 307 : 0.6330964937947804
Loss in iteration 308 : 0.6329158152594332
Loss in iteration 309 : 0.6327352523426334
Loss in iteration 310 : 0.6325548053705294
Loss in iteration 311 : 0.6323744746641569
Loss in iteration 312 : 0.6321942605395111
Loss in iteration 313 : 0.6320141633075939
Loss in iteration 314 : 0.6318341832744998
Loss in iteration 315 : 0.6316543207414572
Loss in iteration 316 : 0.6314745760049126
Loss in iteration 317 : 0.6312949493565745
Loss in iteration 318 : 0.6311154410834854
Loss in iteration 319 : 0.6309360514680845
Loss in iteration 320 : 0.6307567807882652
Loss in iteration 321 : 0.6305776293174354
Loss in iteration 322 : 0.630398597324578
Loss in iteration 323 : 0.6302196850743191
Loss in iteration 324 : 0.6300408928269712
Loss in iteration 325 : 0.6298622208386057
Loss in iteration 326 : 0.6296836693611023
Loss in iteration 327 : 0.629505238642216
Loss in iteration 328 : 0.6293269289256227
Loss in iteration 329 : 0.6291487404509839
Loss in iteration 330 : 0.6289706734539993
Loss in iteration 331 : 0.628792728166462
Loss in iteration 332 : 0.6286149048163151
Loss in iteration 333 : 0.6284372036277082
Loss in iteration 334 : 0.6282596248210414
Loss in iteration 335 : 0.628082168613027
Loss in iteration 336 : 0.6279048352167442
Loss in iteration 337 : 0.6277276248416838
Loss in iteration 338 : 0.627550537693804
Loss in iteration 339 : 0.6273735739755779
Loss in iteration 340 : 0.6271967338860511
Loss in iteration 341 : 0.627020017620885
Loss in iteration 342 : 0.6268434253724154
Loss in iteration 343 : 0.6266669573296862
Loss in iteration 344 : 0.6264906136785104
Loss in iteration 345 : 0.6263143946015229
Loss in iteration 346 : 0.6261383002782166
Loss in iteration 347 : 0.6259623308849916
Loss in iteration 348 : 0.625786486595212
Loss in iteration 349 : 0.625610767579242
Loss in iteration 350 : 0.6254351740045059
Loss in iteration 351 : 0.6252597060355184
Loss in iteration 352 : 0.6250843638339423
Loss in iteration 353 : 0.6249091475586299
Loss in iteration 354 : 0.6247340573656706
Loss in iteration 355 : 0.6245590934084377
Loss in iteration 356 : 0.6243842558376298
Loss in iteration 357 : 0.6242095448013192
Loss in iteration 358 : 0.6240349604449996
Loss in iteration 359 : 0.6238605029116274
Loss in iteration 360 : 0.623686172341664
Loss in iteration 361 : 0.6235119688731322
Loss in iteration 362 : 0.6233378926416571
Loss in iteration 363 : 0.6231639437805041
Loss in iteration 364 : 0.6229901224206351
Loss in iteration 365 : 0.6228164286907494
Loss in iteration 366 : 0.6226428627173387
Loss in iteration 367 : 0.6224694246247156
Loss in iteration 368 : 0.6222961145350872
Loss in iteration 369 : 0.6221229325685811
Loss in iteration 370 : 0.6219498788433027
Loss in iteration 371 : 0.621776953475392
Loss in iteration 372 : 0.6216041565790597
Loss in iteration 373 : 0.6214314882666478
Loss in iteration 374 : 0.6212589486486783
Loss in iteration 375 : 0.6210865378339118
Loss in iteration 376 : 0.6209142559293943
Loss in iteration 377 : 0.6207421030405187
Loss in iteration 378 : 0.6205700792710779
Loss in iteration 379 : 0.6203981847233268
Loss in iteration 380 : 0.6202264194980369
Loss in iteration 381 : 0.6200547836945696
Loss in iteration 382 : 0.6198832774109301
Loss in iteration 383 : 0.6197119007438298
Loss in iteration 384 : 0.6195406537887674
Loss in iteration 385 : 0.6193695366400889
Loss in iteration 386 : 0.6191985493910619
Loss in iteration 387 : 0.6190276921339456
Loss in iteration 388 : 0.6188569649600788
Loss in iteration 389 : 0.6186863679599471
Loss in iteration 390 : 0.6185159012232659
Loss in iteration 391 : 0.6183455648390703
Loss in iteration 392 : 0.6181753588957913
Loss in iteration 393 : 0.6180052834813475
Loss in iteration 394 : 0.6178353386832349
Loss in iteration 395 : 0.6176655245886088
Loss in iteration 396 : 0.6174958412843844
Loss in iteration 397 : 0.617326288857317
Loss in iteration 398 : 0.6171568673940976
Loss in iteration 399 : 0.6169875769814386
Loss in iteration 400 : 0.616818417706152
Testing accuracy  of updater 4 on alg 0 with rate 1.0 = 0.726, training accuracy 0.726, time elapsed: 4008 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6929235752666227
Loss in iteration 3 : 0.6926960665968462
Loss in iteration 4 : 0.6924666730027986
Loss in iteration 5 : 0.6922363372674499
Loss in iteration 6 : 0.6920055789324592
Loss in iteration 7 : 0.691774712053391
Loss in iteration 8 : 0.6915439381601184
Loss in iteration 9 : 0.6913133922107217
Loss in iteration 10 : 0.6910831675971454
Loss in iteration 11 : 0.6908533306884829
Loss in iteration 12 : 0.6906239297064439
Loss in iteration 13 : 0.6903950003411546
Loss in iteration 14 : 0.6901665694061512
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231979
Loss in iteration 17 : 0.6894844480963943
Loss in iteration 18 : 0.689258172092525
Loss in iteration 19 : 0.6890324583520105
Loss in iteration 20 : 0.688807311878355
Loss in iteration 21 : 0.6885827361540362
Loss in iteration 22 : 0.6883587333817476
Loss in iteration 23 : 0.6881353046728773
Loss in iteration 24 : 0.6879124501982635
Loss in iteration 25 : 0.687690169311446
Loss in iteration 26 : 0.6874684606515445
Loss in iteration 27 : 0.6872473222307222
Loss in iteration 28 : 0.6870267515096296
Loss in iteration 29 : 0.6868067454633641
Loss in iteration 30 : 0.6865873006395918
Loss in iteration 31 : 0.686368413210221
Loss in iteration 32 : 0.6861500790174885
Loss in iteration 33 : 0.6859322936152177
Loss in iteration 34 : 0.6857150523058203
Loss in iteration 35 : 0.6854983501734859
Loss in iteration 36 : 0.6852821821139223
Loss in iteration 37 : 0.6850665428610205
Loss in iteration 38 : 0.6848514270106477
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359671
Loss in iteration 41 : 0.6842091641928554
Loss in iteration 42 : 0.6839960858461501
Loss in iteration 43 : 0.683783502475952
Loss in iteration 44 : 0.6835714082201878
Loss in iteration 45 : 0.6833597971844081
Loss in iteration 46 : 0.6831486634501975
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348099
Loss in iteration 49 : 0.6825180666557323
Loss in iteration 50 : 0.6823087826913214
Loss in iteration 51 : 0.6820999462889619
Loss in iteration 52 : 0.6818915514994243
Loss in iteration 53 : 0.6816835923783221
Loss in iteration 54 : 0.6814760629868173
Loss in iteration 55 : 0.6812689573916448
Loss in iteration 56 : 0.6810622696644758
Loss in iteration 57 : 0.6808559938806238
Loss in iteration 58 : 0.6806501241171412
Loss in iteration 59 : 0.6804446544502766
Loss in iteration 60 : 0.6802395789522896
Loss in iteration 61 : 0.6800348916876395
Loss in iteration 62 : 0.6798305867084532
Loss in iteration 63 : 0.6796266580492965
Loss in iteration 64 : 0.6794230997211287
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.6790170699411933
Loss in iteration 67 : 0.6788145863262021
Loss in iteration 68 : 0.678612448696558
Loss in iteration 69 : 0.6784106508201553
Loss in iteration 70 : 0.6782091863823759
Loss in iteration 71 : 0.6780080489709492
Loss in iteration 72 : 0.67780723205871
Loss in iteration 73 : 0.6776067289840471
Loss in iteration 74 : 0.6774065329288731
Loss in iteration 75 : 0.6772066368941004
Loss in iteration 76 : 0.6770070336728068
Loss in iteration 77 : 0.6768077158217213
Loss in iteration 78 : 0.6766086756322008
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.676211395916268
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569093
Loss in iteration 83 : 0.675617348325563
Loss in iteration 84 : 0.6754197948878227
Loss in iteration 85 : 0.6752224566127076
Loss in iteration 86 : 0.675025323682299
Loss in iteration 87 : 0.6748283863086912
Loss in iteration 88 : 0.6746316348550659
Loss in iteration 89 : 0.6744350599463953
Loss in iteration 90 : 0.6742386525578963
Loss in iteration 91 : 0.6740424040750547
Loss in iteration 92 : 0.67384630632539
Loss in iteration 93 : 0.6736503515866574
Loss in iteration 94 : 0.6734545325782229
Loss in iteration 95 : 0.673258842442173
Loss in iteration 96 : 0.6730632747194034
Loss in iteration 97 : 0.6728678233241567
Loss in iteration 98 : 0.6726724825192095
Loss in iteration 99 : 0.672477246892672
Loss in iteration 100 : 0.672282111336883
Loss in iteration 101 : 0.6720870710293809
Loss in iteration 102 : 0.6718921214158095
Loss in iteration 103 : 0.6716972581945452
Loss in iteration 104 : 0.6715024773027857
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759103
Loss in iteration 107 : 0.6709185913007195
Loss in iteration 108 : 0.6707241034543506
Loss in iteration 109 : 0.6705296807976894
Loss in iteration 110 : 0.6703353204679108
Loss in iteration 111 : 0.6701410197704022
Loss in iteration 112 : 0.6699467761711548
Loss in iteration 113 : 0.6697525872896085
Loss in iteration 114 : 0.6695584508918566
Loss in iteration 115 : 0.6693643648842078
Loss in iteration 116 : 0.6691703273070827
Loss in iteration 117 : 0.6689763363292015
Loss in iteration 118 : 0.6687823902420608
Loss in iteration 119 : 0.6685884874546636
Loss in iteration 120 : 0.6683946264884936
Loss in iteration 121 : 0.6682008059727261
Loss in iteration 122 : 0.6680070246396377
Loss in iteration 123 : 0.6678132813202299
Loss in iteration 124 : 0.6676195749400379
Loss in iteration 125 : 0.6674259045151237
Loss in iteration 126 : 0.667232269148228
Loss in iteration 127 : 0.667038668025086
Loss in iteration 128 : 0.6668451004109114
Loss in iteration 129 : 0.6666515656469992
Loss in iteration 130 : 0.6664580631474696
Loss in iteration 131 : 0.6662645923961665
Loss in iteration 132 : 0.6660711529436428
Loss in iteration 133 : 0.665877744404295
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254216
Loss in iteration 136 : 0.6652977013095177
Loss in iteration 137 : 0.6651044137490177
Loss in iteration 138 : 0.6649111560380927
Loss in iteration 139 : 0.6647179281196696
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627471
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155855
Loss in iteration 144 : 0.6637522365612187
Loss in iteration 145 : 0.6635591886637094
Loss in iteration 146 : 0.6633661713503325
Loss in iteration 147 : 0.6631731848817296
Loss in iteration 148 : 0.6629802295502835
Loss in iteration 149 : 0.6627873056785707
Loss in iteration 150 : 0.6625944136178621
Loss in iteration 151 : 0.6624015537466766
Loss in iteration 152 : 0.6622087264694014
Loss in iteration 153 : 0.6620159322149516
Loss in iteration 154 : 0.6618231714354788
Loss in iteration 155 : 0.6616304446051515
Loss in iteration 156 : 0.6614377522189504
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561483
Loss in iteration 159 : 0.660859886963547
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913331
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939863
Loss in iteration 164 : 0.6598975185214344
Loss in iteration 165 : 0.6597051613103267
Loss in iteration 166 : 0.6595128444079461
Loss in iteration 167 : 0.6593205684721011
Loss in iteration 168 : 0.6591283341703846
Loss in iteration 169 : 0.6589361421795201
Loss in iteration 170 : 0.6587439931847026
Loss in iteration 171 : 0.6585518878789912
Loss in iteration 172 : 0.6583598269627259
Loss in iteration 173 : 0.6581678111429787
Loss in iteration 174 : 0.6579758411330449
Loss in iteration 175 : 0.6577839176519344
Loss in iteration 176 : 0.6575920414239275
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481179
Loss in iteration 179 : 0.65701670357145
Loss in iteration 180 : 0.6568250236894342
Loss in iteration 181 : 0.6566333947467301
Loss in iteration 182 : 0.6564418174910612
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452749
Loss in iteration 185 : 0.6558674033633619
Loss in iteration 186 : 0.6556760403843819
Loss in iteration 187 : 0.6554847328672831
Loss in iteration 188 : 0.6552934815725109
Loss in iteration 189 : 0.6551022872617626
Loss in iteration 190 : 0.6549111506977277
Loss in iteration 191 : 0.6547200726438117
Loss in iteration 192 : 0.6545290538638485
Loss in iteration 193 : 0.6543380951217601
Loss in iteration 194 : 0.6541471971811902
Loss in iteration 195 : 0.6539563608050981
Loss in iteration 196 : 0.6537655867552814
Loss in iteration 197 : 0.6535748757918566
Loss in iteration 198 : 0.6533842286726632
Loss in iteration 199 : 0.6531936461525875
Loss in iteration 200 : 0.653003128982836
Loss in iteration 201 : 0.6528126779100925
Loss in iteration 202 : 0.6526222936756358
Loss in iteration 203 : 0.6524319770143664
Loss in iteration 204 : 0.6522417286537278
Loss in iteration 205 : 0.6520515493126077
Loss in iteration 206 : 0.6518614397001324
Loss in iteration 207 : 0.651671400514421
Loss in iteration 208 : 0.6514814324412715
Loss in iteration 209 : 0.6512915361528184
Loss in iteration 210 : 0.6511017123061177
Loss in iteration 211 : 0.6509119615417064
Loss in iteration 212 : 0.6507222844821396
Loss in iteration 213 : 0.6505326817304534
Loss in iteration 214 : 0.6503431538686223
Loss in iteration 215 : 0.6501537014559656
Loss in iteration 216 : 0.6499643250275178
Loss in iteration 217 : 0.6497750250923703
Loss in iteration 218 : 0.6495858021319837
Loss in iteration 219 : 0.6493966565984831
Loss in iteration 220 : 0.6492075889129876
Loss in iteration 221 : 0.6490185994639678
Loss in iteration 222 : 0.6488296886057647
Loss in iteration 223 : 0.6486408566572915
Loss in iteration 224 : 0.6484521039010835
Loss in iteration 225 : 0.6482634305828623
Loss in iteration 226 : 0.6480748369117985
Loss in iteration 227 : 0.647886323061708
Loss in iteration 228 : 0.64769788917346
Loss in iteration 229 : 0.6475095353587869
Loss in iteration 230 : 0.6473212617056988
Loss in iteration 231 : 0.6471330682854867
Loss in iteration 232 : 0.6469449551612092
Loss in iteration 233 : 0.646756922397225
Loss in iteration 234 : 0.6465689700691694
Loss in iteration 235 : 0.6463810982736484
Loss in iteration 236 : 0.6461933071368225
Loss in iteration 237 : 0.6460055968212823
Loss in iteration 238 : 0.6458179675308101
Loss in iteration 239 : 0.6456304195129219
Loss in iteration 240 : 0.6454429530593564
Loss in iteration 241 : 0.6452555685049133
Loss in iteration 242 : 0.645068266225024
Loss in iteration 243 : 0.6448810466325241
Loss in iteration 244 : 0.6446939101740161
Loss in iteration 245 : 0.6445068573260437
Loss in iteration 246 : 0.6443198885913674
Loss in iteration 247 : 0.6441330044953829
Loss in iteration 248 : 0.6439462055828149
Loss in iteration 249 : 0.6437594924146947
Loss in iteration 250 : 0.6435728655656119
Loss in iteration 251 : 0.6433863256212781
Loss in iteration 252 : 0.6431998731763218
Loss in iteration 253 : 0.6430135088323655
Loss in iteration 254 : 0.6428272331963023
Loss in iteration 255 : 0.6426410468787648
Loss in iteration 256 : 0.6424549504928054
Loss in iteration 257 : 0.6422689446526941
Loss in iteration 258 : 0.6420830299728993
Loss in iteration 259 : 0.6418972070671641
Loss in iteration 260 : 0.641711476547701
Loss in iteration 261 : 0.6415258390244867
Loss in iteration 262 : 0.6413402951046446
Loss in iteration 263 : 0.6411548453919075
Loss in iteration 264 : 0.6409694904861272
Loss in iteration 265 : 0.6407842309828747
Loss in iteration 266 : 0.6405990674730616
Loss in iteration 267 : 0.6404140005426404
Loss in iteration 268 : 0.640229030772317
Loss in iteration 269 : 0.6400441587373172
Loss in iteration 270 : 0.6398593850071848
Loss in iteration 271 : 0.639674710145599
Loss in iteration 272 : 0.6394901347102335
Loss in iteration 273 : 0.639305659252621
Loss in iteration 274 : 0.6391212843180487
Loss in iteration 275 : 0.638937010445479
Loss in iteration 276 : 0.638752838167464
Loss in iteration 277 : 0.6385687680101043
Loss in iteration 278 : 0.6383848004929946
Loss in iteration 279 : 0.6382009361292014
Loss in iteration 280 : 0.6380171754252336
Loss in iteration 281 : 0.6378335188810368
Loss in iteration 282 : 0.637649966989992
Loss in iteration 283 : 0.6374665202389228
Loss in iteration 284 : 0.6372831791080968
Loss in iteration 285 : 0.6370999440712594
Loss in iteration 286 : 0.6369168155956506
Loss in iteration 287 : 0.6367337941420306
Loss in iteration 288 : 0.6365508801647254
Loss in iteration 289 : 0.6363680741116532
Loss in iteration 290 : 0.6361853764243693
Loss in iteration 291 : 0.6360027875381179
Loss in iteration 292 : 0.6358203078818729
Loss in iteration 293 : 0.6356379378783885
Loss in iteration 294 : 0.6354556779442525
Loss in iteration 295 : 0.6352735284899457
Loss in iteration 296 : 0.6350914899198921
Loss in iteration 297 : 0.6349095626325213
Loss in iteration 298 : 0.6347277470203182
Loss in iteration 299 : 0.6345460434699073
Loss in iteration 300 : 0.6343644523620822
Loss in iteration 301 : 0.6341829740718926
Loss in iteration 302 : 0.6340016089686892
Loss in iteration 303 : 0.6338203574162106
Loss in iteration 304 : 0.633639219772621
Loss in iteration 305 : 0.6334581963905884
Loss in iteration 306 : 0.6332772876173535
Loss in iteration 307 : 0.6330964937947804
Loss in iteration 308 : 0.6329158152594332
Loss in iteration 309 : 0.6327352523426334
Loss in iteration 310 : 0.6325548053705294
Loss in iteration 311 : 0.6323744746641569
Loss in iteration 312 : 0.6321942605395111
Loss in iteration 313 : 0.6320141633075939
Loss in iteration 314 : 0.6318341832744998
Loss in iteration 315 : 0.6316543207414572
Loss in iteration 316 : 0.6314745760049126
Loss in iteration 317 : 0.6312949493565745
Loss in iteration 318 : 0.6311154410834854
Loss in iteration 319 : 0.6309360514680845
Loss in iteration 320 : 0.6307567807882652
Loss in iteration 321 : 0.6305776293174354
Loss in iteration 322 : 0.630398597324578
Loss in iteration 323 : 0.6302196850743191
Loss in iteration 324 : 0.6300408928269712
Loss in iteration 325 : 0.6298622208386057
Loss in iteration 326 : 0.6296836693611023
Loss in iteration 327 : 0.629505238642216
Loss in iteration 328 : 0.6293269289256227
Loss in iteration 329 : 0.6291487404509839
Loss in iteration 330 : 0.6289706734539993
Loss in iteration 331 : 0.628792728166462
Loss in iteration 332 : 0.6286149048163151
Loss in iteration 333 : 0.6284372036277082
Loss in iteration 334 : 0.6282596248210414
Loss in iteration 335 : 0.628082168613027
Loss in iteration 336 : 0.6279048352167442
Loss in iteration 337 : 0.6277276248416838
Loss in iteration 338 : 0.627550537693804
Loss in iteration 339 : 0.6273735739755779
Loss in iteration 340 : 0.6271967338860511
Loss in iteration 341 : 0.627020017620885
Loss in iteration 342 : 0.6268434253724154
Loss in iteration 343 : 0.6266669573296862
Loss in iteration 344 : 0.6264906136785104
Loss in iteration 345 : 0.6263143946015229
Loss in iteration 346 : 0.6261383002782166
Loss in iteration 347 : 0.6259623308849916
Loss in iteration 348 : 0.625786486595212
Loss in iteration 349 : 0.625610767579242
Loss in iteration 350 : 0.6254351740045059
Loss in iteration 351 : 0.6252597060355184
Loss in iteration 352 : 0.6250843638339423
Loss in iteration 353 : 0.6249091475586299
Loss in iteration 354 : 0.6247340573656706
Loss in iteration 355 : 0.6245590934084377
Loss in iteration 356 : 0.6243842558376298
Loss in iteration 357 : 0.6242095448013192
Loss in iteration 358 : 0.6240349604449996
Loss in iteration 359 : 0.6238605029116274
Loss in iteration 360 : 0.623686172341664
Loss in iteration 361 : 0.6235119688731322
Loss in iteration 362 : 0.6233378926416571
Loss in iteration 363 : 0.6231639437805041
Loss in iteration 364 : 0.6229901224206351
Loss in iteration 365 : 0.6228164286907494
Loss in iteration 366 : 0.6226428627173387
Loss in iteration 367 : 0.6224694246247156
Loss in iteration 368 : 0.6222961145350872
Loss in iteration 369 : 0.6221229325685811
Loss in iteration 370 : 0.6219498788433027
Loss in iteration 371 : 0.621776953475392
Loss in iteration 372 : 0.6216041565790597
Loss in iteration 373 : 0.6214314882666478
Loss in iteration 374 : 0.6212589486486783
Loss in iteration 375 : 0.6210865378339118
Loss in iteration 376 : 0.6209142559293943
Loss in iteration 377 : 0.6207421030405187
Loss in iteration 378 : 0.6205700792710779
Loss in iteration 379 : 0.6203981847233268
Loss in iteration 380 : 0.6202264194980369
Loss in iteration 381 : 0.6200547836945696
Loss in iteration 382 : 0.6198832774109301
Loss in iteration 383 : 0.6197119007438298
Loss in iteration 384 : 0.6195406537887674
Loss in iteration 385 : 0.6193695366400889
Loss in iteration 386 : 0.6191985493910619
Loss in iteration 387 : 0.6190276921339456
Loss in iteration 388 : 0.6188569649600788
Loss in iteration 389 : 0.6186863679599471
Loss in iteration 390 : 0.6185159012232659
Loss in iteration 391 : 0.6183455648390703
Loss in iteration 392 : 0.6181753588957913
Loss in iteration 393 : 0.6180052834813475
Loss in iteration 394 : 0.6178353386832349
Loss in iteration 395 : 0.6176655245886088
Loss in iteration 396 : 0.6174958412843844
Loss in iteration 397 : 0.617326288857317
Loss in iteration 398 : 0.6171568673940976
Loss in iteration 399 : 0.6169875769814386
Loss in iteration 400 : 0.616818417706152
Testing accuracy  of updater 4 on alg 0 with rate 0.09999999999999998 = 0.726, training accuracy 0.726, time elapsed: 4331 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 65.93856858740773
Loss in iteration 3 : 84.85542655623208
Loss in iteration 4 : 34.27691872039004
Loss in iteration 5 : 40.092561245509295
Loss in iteration 6 : 28.27806602126496
Loss in iteration 7 : 29.248996869359594
Loss in iteration 8 : 25.40268577693369
Loss in iteration 9 : 23.852585622720596
Loss in iteration 10 : 22.677779140495183
Loss in iteration 11 : 21.498266423517432
Loss in iteration 12 : 20.728604530229074
Loss in iteration 13 : 19.857287706840438
Loss in iteration 14 : 18.69437548795499
Loss in iteration 15 : 18.961392619818948
Loss in iteration 16 : 17.37213854585892
Loss in iteration 17 : 18.251118560721938
Loss in iteration 18 : 17.098513658999757
Loss in iteration 19 : 18.121731624418608
Loss in iteration 20 : 16.493724833817552
Loss in iteration 21 : 18.046254887381615
Loss in iteration 22 : 15.310892049383227
Loss in iteration 23 : 17.811896200384627
Loss in iteration 24 : 14.025791290785092
Loss in iteration 25 : 17.580158458896438
Loss in iteration 26 : 15.50260488970056
Loss in iteration 27 : 18.437089891763755
Loss in iteration 28 : 16.452054720749302
Loss in iteration 29 : 18.076336835244916
Loss in iteration 30 : 15.988827838594904
Loss in iteration 31 : 17.465143213334102
Loss in iteration 32 : 15.214956164111719
Loss in iteration 33 : 17.09167190935195
Loss in iteration 34 : 14.95920823625357
Loss in iteration 35 : 17.041640736742476
Loss in iteration 36 : 14.7910832301524
Loss in iteration 37 : 17.08611909372059
Loss in iteration 38 : 14.462614656350524
Loss in iteration 39 : 17.31138175814769
Loss in iteration 40 : 13.976097676197982
Loss in iteration 41 : 17.83744740869134
Loss in iteration 42 : 12.691749160892893
Loss in iteration 43 : 16.89158098359636
Loss in iteration 44 : 14.391248455156513
Loss in iteration 45 : 18.195093748749176
Loss in iteration 46 : 15.614546507149115
Loss in iteration 47 : 18.155127113721434
Loss in iteration 48 : 15.40383211915219
Loss in iteration 49 : 17.57801854988747
Loss in iteration 50 : 14.627180422834208
Loss in iteration 51 : 17.00007908793063
Loss in iteration 52 : 14.333609098941487
Loss in iteration 53 : 16.902434318003245
Loss in iteration 54 : 13.913479753537999
Loss in iteration 55 : 17.107402064557967
Loss in iteration 56 : 13.589072546711751
Loss in iteration 57 : 17.187703310521872
Loss in iteration 58 : 12.560079185570343
Loss in iteration 59 : 16.664036384081083
Loss in iteration 60 : 14.002883919241691
Loss in iteration 61 : 18.04644648426826
Loss in iteration 62 : 15.307472776847874
Loss in iteration 63 : 18.194745728554174
Loss in iteration 64 : 15.128786932900658
Loss in iteration 65 : 17.562019187240303
Loss in iteration 66 : 14.313437951405726
Loss in iteration 67 : 16.886995545535843
Loss in iteration 68 : 14.05491650165142
Loss in iteration 69 : 16.837067442828303
Loss in iteration 70 : 13.772836813126814
Loss in iteration 71 : 17.27576308599983
Loss in iteration 72 : 13.008544165370251
Loss in iteration 73 : 16.63496481346382
Loss in iteration 74 : 12.55230946903658
Loss in iteration 75 : 16.74000579139852
Loss in iteration 76 : 14.33988571597496
Loss in iteration 77 : 18.253731569555455
Loss in iteration 78 : 15.234819154574936
Loss in iteration 79 : 18.061249040300797
Loss in iteration 80 : 14.761682842274057
Loss in iteration 81 : 17.23633077256928
Loss in iteration 82 : 14.259970649474715
Loss in iteration 83 : 16.822325856859628
Loss in iteration 84 : 13.77491771667208
Loss in iteration 85 : 16.868698029772524
Loss in iteration 86 : 13.48649367526422
Loss in iteration 87 : 17.07035139166983
Loss in iteration 88 : 12.661675973206947
Loss in iteration 89 : 16.361933201666922
Loss in iteration 90 : 13.485655727483634
Loss in iteration 91 : 17.511296279489812
Loss in iteration 92 : 14.797305048854817
Loss in iteration 93 : 17.985240873691954
Loss in iteration 94 : 14.829805195376762
Loss in iteration 95 : 17.426129443204232
Loss in iteration 96 : 14.299104661769674
Loss in iteration 97 : 16.799218614515105
Loss in iteration 98 : 14.031688792403484
Loss in iteration 99 : 16.792994097405398
Loss in iteration 100 : 13.722299569920105
Loss in iteration 101 : 17.05833707707053
Loss in iteration 102 : 12.9245060236455
Loss in iteration 103 : 16.43246198114017
Loss in iteration 104 : 13.091534207918212
Loss in iteration 105 : 17.039428061437278
Loss in iteration 106 : 14.418869251532625
Loss in iteration 107 : 17.70700008919939
Loss in iteration 108 : 14.809126951455344
Loss in iteration 109 : 17.44477774745625
Loss in iteration 110 : 14.412869921377645
Loss in iteration 111 : 16.88720455603074
Loss in iteration 112 : 14.037764312304297
Loss in iteration 113 : 16.657272261767382
Loss in iteration 114 : 13.794906258826954
Loss in iteration 115 : 17.00163782371371
Loss in iteration 116 : 13.21618866071105
Loss in iteration 117 : 16.561929512257365
Loss in iteration 118 : 12.687028867117023
Loss in iteration 119 : 16.425423916749583
Loss in iteration 120 : 13.975252691903545
Loss in iteration 121 : 17.566368093595784
Loss in iteration 122 : 14.923495597387625
Loss in iteration 123 : 17.651291672761587
Loss in iteration 124 : 14.691900350361633
Loss in iteration 125 : 17.086565816289664
Loss in iteration 126 : 14.073108774067867
Loss in iteration 127 : 16.490224058329094
Loss in iteration 128 : 13.926959700344689
Loss in iteration 129 : 16.724097120872795
Loss in iteration 130 : 13.486733880258923
Loss in iteration 131 : 16.74783194007909
Loss in iteration 132 : 12.648918570817733
Loss in iteration 133 : 16.180889616899602
Loss in iteration 134 : 13.198335797158744
Loss in iteration 135 : 17.08755643337657
Loss in iteration 136 : 14.737562004711414
Loss in iteration 137 : 17.758755860420614
Loss in iteration 138 : 14.979989668121762
Loss in iteration 139 : 17.333985772833692
Loss in iteration 140 : 14.259686231087315
Loss in iteration 141 : 16.516933911066985
Loss in iteration 142 : 14.026322316221963
Loss in iteration 143 : 16.503982698454905
Loss in iteration 144 : 13.662699427711148
Loss in iteration 145 : 16.799183699131174
Loss in iteration 146 : 12.971055705512692
Loss in iteration 147 : 16.33494924841564
Loss in iteration 148 : 12.648061583020448
Loss in iteration 149 : 16.386505132749317
Loss in iteration 150 : 14.278495527112018
Loss in iteration 151 : 17.654866267705184
Loss in iteration 152 : 14.918334963205647
Loss in iteration 153 : 17.362389105438584
Loss in iteration 154 : 14.623551072331262
Loss in iteration 155 : 16.833559105807346
Loss in iteration 156 : 14.09239140657933
Loss in iteration 157 : 16.38138461911296
Loss in iteration 158 : 13.810912612138464
Loss in iteration 159 : 16.555134509782416
Loss in iteration 160 : 13.504308934895947
Loss in iteration 161 : 16.7283832751954
Loss in iteration 162 : 12.552808732977267
Loss in iteration 163 : 16.013090655324408
Loss in iteration 164 : 13.136138528182464
Loss in iteration 165 : 16.90401817168505
Loss in iteration 166 : 14.723640594825545
Loss in iteration 167 : 17.669780991514372
Loss in iteration 168 : 15.022993292491632
Loss in iteration 169 : 17.190133069497723
Loss in iteration 170 : 14.350422297711452
Loss in iteration 171 : 16.45275064931419
Loss in iteration 172 : 13.990682560477381
Loss in iteration 173 : 16.320800385832364
Loss in iteration 174 : 13.636752111275767
Loss in iteration 175 : 16.625150896133768
Loss in iteration 176 : 13.071517402510333
Loss in iteration 177 : 16.329117907119933
Loss in iteration 178 : 12.571232888653267
Loss in iteration 179 : 16.207589707218588
Loss in iteration 180 : 14.252745894643509
Loss in iteration 181 : 17.532912818097024
Loss in iteration 182 : 14.896414901520561
Loss in iteration 183 : 17.201234619692965
Loss in iteration 184 : 14.583977929968166
Loss in iteration 185 : 16.715171043402705
Loss in iteration 186 : 14.051287201013881
Loss in iteration 187 : 16.18942005560917
Loss in iteration 188 : 13.897149950786181
Loss in iteration 189 : 16.351628961774626
Loss in iteration 190 : 13.672109088770766
Loss in iteration 191 : 16.809783897846124
Loss in iteration 192 : 12.601304515900955
Loss in iteration 193 : 15.896975577127417
Loss in iteration 194 : 12.860935001641613
Loss in iteration 195 : 16.61074788272237
Loss in iteration 196 : 14.726263458458286
Loss in iteration 197 : 17.629935144905577
Loss in iteration 198 : 15.018864896176416
Loss in iteration 199 : 17.078662359046895
Loss in iteration 200 : 14.3980902604307
Loss in iteration 201 : 16.334017353266937
Loss in iteration 202 : 14.113573316804787
Loss in iteration 203 : 16.195691988146425
Loss in iteration 204 : 13.699267314818513
Loss in iteration 205 : 16.395673940853293
Loss in iteration 206 : 13.33793555781735
Loss in iteration 207 : 16.54613980764669
Loss in iteration 208 : 12.615707893220716
Loss in iteration 209 : 16.014301773315267
Loss in iteration 210 : 13.694019606757804
Loss in iteration 211 : 16.967839018067362
Loss in iteration 212 : 14.841319627639955
Loss in iteration 213 : 17.225798566773936
Loss in iteration 214 : 14.635258205112699
Loss in iteration 215 : 16.730925298869323
Loss in iteration 216 : 14.13874108937148
Loss in iteration 217 : 16.169043751729863
Loss in iteration 218 : 14.02914527804985
Loss in iteration 219 : 16.240476801620694
Loss in iteration 220 : 13.681787026271108
Loss in iteration 221 : 16.665218080303827
Loss in iteration 222 : 13.015969510088187
Loss in iteration 223 : 16.14801683999602
Loss in iteration 224 : 12.5432218913043
Loss in iteration 225 : 16.128172266605112
Loss in iteration 226 : 14.46212853472963
Loss in iteration 227 : 17.45205257195877
Loss in iteration 228 : 14.969451358685028
Loss in iteration 229 : 16.990481428282408
Loss in iteration 230 : 14.538112380060301
Loss in iteration 231 : 16.505719675809516
Loss in iteration 232 : 13.912099439036421
Loss in iteration 233 : 15.929960780099556
Loss in iteration 234 : 13.881170273612101
Loss in iteration 235 : 16.193053602391714
Loss in iteration 236 : 13.751168650557876
Loss in iteration 237 : 16.79949891225003
Loss in iteration 238 : 12.798891882125666
Loss in iteration 239 : 15.957474306477675
Loss in iteration 240 : 12.866176316430279
Loss in iteration 241 : 16.406703091447284
Loss in iteration 242 : 14.64338826538599
Loss in iteration 243 : 17.37128282538919
Loss in iteration 244 : 14.963774477863677
Loss in iteration 245 : 16.871740652073814
Loss in iteration 246 : 14.47640421050452
Loss in iteration 247 : 16.264317905963996
Loss in iteration 248 : 14.024393053704644
Loss in iteration 249 : 15.97233095047869
Loss in iteration 250 : 13.688526288730472
Loss in iteration 251 : 16.220135393534765
Loss in iteration 252 : 13.523650955465678
Loss in iteration 253 : 16.654330715909865
Loss in iteration 254 : 12.582466026156846
Loss in iteration 255 : 15.8590855295894
Loss in iteration 256 : 13.590246759332095
Loss in iteration 257 : 16.80886751204892
Loss in iteration 258 : 14.88870425716997
Loss in iteration 259 : 17.193349389963796
Loss in iteration 260 : 14.73575424490432
Loss in iteration 261 : 16.6181539504029
Loss in iteration 262 : 14.194499177772318
Loss in iteration 263 : 16.001516637202087
Loss in iteration 264 : 14.058807631521491
Loss in iteration 265 : 16.036051589663177
Loss in iteration 266 : 13.637141862407105
Loss in iteration 267 : 16.356488450307307
Loss in iteration 268 : 13.260685268928595
Loss in iteration 269 : 16.383994064136186
Loss in iteration 270 : 12.505381361280394
Loss in iteration 271 : 15.903113614271085
Loss in iteration 272 : 14.152075975998029
Loss in iteration 273 : 17.147313627338914
Loss in iteration 274 : 15.034055203510965
Loss in iteration 275 : 17.03284651247833
Loss in iteration 276 : 14.650101262456708
Loss in iteration 277 : 16.455127469593553
Loss in iteration 278 : 14.033704687793403
Loss in iteration 279 : 15.822705022114022
Loss in iteration 280 : 13.890610430808989
Loss in iteration 281 : 16.011134408973138
Loss in iteration 282 : 13.728703825646518
Loss in iteration 283 : 16.578327366504485
Loss in iteration 284 : 13.109175716729354
Loss in iteration 285 : 16.127641579493677
Loss in iteration 286 : 12.679280203965158
Loss in iteration 287 : 16.08145939764113
Loss in iteration 288 : 14.437328484346839
Loss in iteration 289 : 17.205504490247524
Loss in iteration 290 : 14.92617899660606
Loss in iteration 291 : 16.84603541457946
Loss in iteration 292 : 14.618424228264345
Loss in iteration 293 : 16.342058425382714
Loss in iteration 294 : 13.926557214104422
Loss in iteration 295 : 15.78038368314703
Loss in iteration 296 : 13.717875697239219
Loss in iteration 297 : 16.090592839866197
Loss in iteration 298 : 13.745511818525925
Loss in iteration 299 : 16.7186152863029
Loss in iteration 300 : 12.703960303931963
Loss in iteration 301 : 15.774166557105433
Loss in iteration 302 : 13.419967222449625
Loss in iteration 303 : 16.567496124215875
Loss in iteration 304 : 14.71698160519744
Loss in iteration 305 : 17.0799164465796
Loss in iteration 306 : 14.829191813209924
Loss in iteration 307 : 16.62561492988363
Loss in iteration 308 : 14.351603737366899
Loss in iteration 309 : 15.991283264787715
Loss in iteration 310 : 13.955909630133146
Loss in iteration 311 : 15.89402345317392
Loss in iteration 312 : 13.688531303273855
Loss in iteration 313 : 16.331005480463226
Loss in iteration 314 : 13.294191043146416
Loss in iteration 315 : 16.300676940944992
Loss in iteration 316 : 12.59408524983673
Loss in iteration 317 : 15.887415648481209
Loss in iteration 318 : 14.217450378168245
Loss in iteration 319 : 17.058345375045743
Loss in iteration 320 : 14.901688847768371
Loss in iteration 321 : 16.864106053316174
Loss in iteration 322 : 14.72992267064606
Loss in iteration 323 : 16.40209505295782
Loss in iteration 324 : 13.97619192190079
Loss in iteration 325 : 15.731705236230496
Loss in iteration 326 : 13.816723387675408
Loss in iteration 327 : 16.018218775008215
Loss in iteration 328 : 13.749626930091237
Loss in iteration 329 : 16.52811301216386
Loss in iteration 330 : 13.062548335064038
Loss in iteration 331 : 15.963446594843452
Loss in iteration 332 : 13.077310495602184
Loss in iteration 333 : 16.288558253567526
Loss in iteration 334 : 14.477313892242345
Loss in iteration 335 : 17.046325736722917
Loss in iteration 336 : 14.764167348233215
Loss in iteration 337 : 16.58121764787485
Loss in iteration 338 : 14.576679811328901
Loss in iteration 339 : 16.245111603003213
Loss in iteration 340 : 13.786398163431924
Loss in iteration 341 : 15.755249524193198
Loss in iteration 342 : 13.792255621833974
Loss in iteration 343 : 16.297634466823613
Loss in iteration 344 : 13.52247276860743
Loss in iteration 345 : 16.351877748300822
Loss in iteration 346 : 12.805506878601394
Loss in iteration 347 : 15.919126525776445
Loss in iteration 348 : 13.98195507320064
Loss in iteration 349 : 16.747041300341024
Loss in iteration 350 : 14.611024613834427
Loss in iteration 351 : 16.75626263035884
Loss in iteration 352 : 14.712061986566248
Loss in iteration 353 : 16.412378653867407
Loss in iteration 354 : 14.074255536449368
Loss in iteration 355 : 15.86947833163676
Loss in iteration 356 : 13.877919465292534
Loss in iteration 357 : 16.078681847921782
Loss in iteration 358 : 13.788690459784226
Loss in iteration 359 : 16.548410494409886
Loss in iteration 360 : 12.867124956516419
Loss in iteration 361 : 15.754497185647862
Loss in iteration 362 : 13.27601306305579
Loss in iteration 363 : 16.412421165222593
Loss in iteration 364 : 14.600763158345812
Loss in iteration 365 : 17.00520067365095
Loss in iteration 366 : 14.845816540868768
Loss in iteration 367 : 16.51531905620506
Loss in iteration 368 : 14.45027318361125
Loss in iteration 369 : 16.052631329294265
Loss in iteration 370 : 13.808648192821554
Loss in iteration 371 : 15.827174271544715
Loss in iteration 372 : 13.770860134904614
Loss in iteration 373 : 16.331084751206326
Loss in iteration 374 : 13.327510857461425
Loss in iteration 375 : 16.12305748371617
Loss in iteration 376 : 12.85213831574214
Loss in iteration 377 : 15.993957743852048
Loss in iteration 378 : 14.23081027129572
Loss in iteration 379 : 16.877468283460225
Loss in iteration 380 : 14.691420068088913
Loss in iteration 381 : 16.66461068692061
Loss in iteration 382 : 14.65099800911331
Loss in iteration 383 : 16.295927350528384
Loss in iteration 384 : 13.963575595480549
Loss in iteration 385 : 15.809054319690455
Loss in iteration 386 : 13.837675242232992
Loss in iteration 387 : 16.126723031563316
Loss in iteration 388 : 13.748218246411863
Loss in iteration 389 : 16.502474799355152
Loss in iteration 390 : 12.767847686530573
Loss in iteration 391 : 15.730122422985797
Loss in iteration 392 : 13.609258482962007
Loss in iteration 393 : 16.531598834015945
Loss in iteration 394 : 14.711820803731229
Loss in iteration 395 : 16.891694052393053
Loss in iteration 396 : 14.808578200926652
Loss in iteration 397 : 16.390272197070438
Loss in iteration 398 : 14.310862219589685
Loss in iteration 399 : 15.91144980161417
Loss in iteration 400 : 13.862381099823484
Testing accuracy  of updater 5 on alg 0 with rate 10.0 = 0.720125, training accuracy 0.720125, time elapsed: 4159 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 1.1007513764013168
Loss in iteration 3 : 6.8220070198341825
Loss in iteration 4 : 3.366970711634319
Loss in iteration 5 : 3.929280571227054
Loss in iteration 6 : 3.633498720169855
Loss in iteration 7 : 2.539401645225793
Loss in iteration 8 : 3.566498207116566
Loss in iteration 9 : 1.8837642191030273
Loss in iteration 10 : 3.5858645780993794
Loss in iteration 11 : 1.3979301489333502
Loss in iteration 12 : 3.02847402718473
Loss in iteration 13 : 1.654804141629278
Loss in iteration 14 : 2.830713326440483
Loss in iteration 15 : 1.6354431051587794
Loss in iteration 16 : 2.03638811361481
Loss in iteration 17 : 1.6588879156068277
Loss in iteration 18 : 2.287823996963589
Loss in iteration 19 : 1.9900220213282727
Loss in iteration 20 : 2.467703295680407
Loss in iteration 21 : 1.6702335698154906
Loss in iteration 22 : 2.174828067504609
Loss in iteration 23 : 1.7094027054485803
Loss in iteration 24 : 2.0275566013336945
Loss in iteration 25 : 1.6952316279602575
Loss in iteration 26 : 1.9085563457403327
Loss in iteration 27 : 1.6884560285087833
Loss in iteration 28 : 1.8171411090242136
Loss in iteration 29 : 1.68866500575084
Loss in iteration 30 : 1.7439754073489018
Loss in iteration 31 : 1.6944880738648602
Loss in iteration 32 : 1.687612967106075
Loss in iteration 33 : 1.7013686403722104
Loss in iteration 34 : 1.6595872991942158
Loss in iteration 35 : 1.7155711581230457
Loss in iteration 36 : 1.6529407811908103
Loss in iteration 37 : 1.7851218511862648
Loss in iteration 38 : 1.507731410264839
Loss in iteration 39 : 1.6625984492843773
Loss in iteration 40 : 1.4506158007882337
Loss in iteration 41 : 1.7847132996357273
Loss in iteration 42 : 1.707018021197287
Loss in iteration 43 : 1.829947696939829
Loss in iteration 44 : 1.7596324357075452
Loss in iteration 45 : 1.761443813916479
Loss in iteration 46 : 1.6766698853953907
Loss in iteration 47 : 1.720458869582083
Loss in iteration 48 : 1.6217044285305975
Loss in iteration 49 : 1.7052014515431888
Loss in iteration 50 : 1.5876423446919243
Loss in iteration 51 : 1.7031026513989305
Loss in iteration 52 : 1.5660025291567485
Loss in iteration 53 : 1.7053836615492737
Loss in iteration 54 : 1.5583032543607556
Loss in iteration 55 : 1.7131355250631204
Loss in iteration 56 : 1.5577055929535903
Loss in iteration 57 : 1.758229495006617
Loss in iteration 58 : 1.532144487761061
Loss in iteration 59 : 1.7463827607484346
Loss in iteration 60 : 1.3364075353805114
Loss in iteration 61 : 1.6920170582246223
Loss in iteration 62 : 1.5541937626128206
Loss in iteration 63 : 1.8344774395029582
Loss in iteration 64 : 1.6804056971896129
Loss in iteration 65 : 1.8076784487783817
Loss in iteration 66 : 1.6332786713092113
Loss in iteration 67 : 1.7453449164119208
Loss in iteration 68 : 1.5728417241670853
Loss in iteration 69 : 1.7110018272746488
Loss in iteration 70 : 1.5411369997522442
Loss in iteration 71 : 1.7026364354088381
Loss in iteration 72 : 1.5227772291617687
Loss in iteration 73 : 1.7029275020643266
Loss in iteration 74 : 1.5165918331610895
Loss in iteration 75 : 1.7098484876941755
Loss in iteration 76 : 1.518927747174536
Loss in iteration 77 : 1.7547921155795705
Loss in iteration 78 : 1.5063021912635013
Loss in iteration 79 : 1.7456381780723087
Loss in iteration 80 : 1.3192281774819186
Loss in iteration 81 : 1.6642597771303735
Loss in iteration 82 : 1.527721412043874
Loss in iteration 83 : 1.8295578607057414
Loss in iteration 84 : 1.6536995616182781
Loss in iteration 85 : 1.8162253804924737
Loss in iteration 86 : 1.6071369818074472
Loss in iteration 87 : 1.7452507088473364
Loss in iteration 88 : 1.5457041513502394
Loss in iteration 89 : 1.7025392758356908
Loss in iteration 90 : 1.5148804989526954
Loss in iteration 91 : 1.6920120013868998
Loss in iteration 92 : 1.498759591733176
Loss in iteration 93 : 1.6942950672912376
Loss in iteration 94 : 1.4958374882813064
Loss in iteration 95 : 1.7235714105697248
Loss in iteration 96 : 1.4999327273507523
Loss in iteration 97 : 1.7652837093798903
Loss in iteration 98 : 1.339314853824194
Loss in iteration 99 : 1.6554164854220057
Loss in iteration 100 : 1.4797031592701706
Loss in iteration 101 : 1.7880499456606103
Loss in iteration 102 : 1.6182826121323732
Loss in iteration 103 : 1.811202818393952
Loss in iteration 104 : 1.6024478771568507
Loss in iteration 105 : 1.7482766006150277
Loss in iteration 106 : 1.5421537398947067
Loss in iteration 107 : 1.6989629939952058
Loss in iteration 108 : 1.5081738511178238
Loss in iteration 109 : 1.685527984254862
Loss in iteration 110 : 1.490580524083435
Loss in iteration 111 : 1.691451949647687
Loss in iteration 112 : 1.4886090015592797
Loss in iteration 113 : 1.7448205942632653
Loss in iteration 114 : 1.440705598740886
Loss in iteration 115 : 1.7040263994098976
Loss in iteration 116 : 1.373374736487089
Loss in iteration 117 : 1.687517975989365
Loss in iteration 118 : 1.5445864815346735
Loss in iteration 119 : 1.8025886380758815
Loss in iteration 120 : 1.615818140066318
Loss in iteration 121 : 1.7782007513392561
Loss in iteration 122 : 1.5678968126291075
Loss in iteration 123 : 1.7146993064971379
Loss in iteration 124 : 1.5190906984436916
Loss in iteration 125 : 1.6820816335681852
Loss in iteration 126 : 1.4932480998433229
Loss in iteration 127 : 1.6786956569559832
Loss in iteration 128 : 1.482621374890281
Loss in iteration 129 : 1.7067298153605925
Loss in iteration 130 : 1.4791248321738444
Loss in iteration 131 : 1.7398792938190288
Loss in iteration 132 : 1.3578719704518007
Loss in iteration 133 : 1.651801633660984
Loss in iteration 134 : 1.481971437722243
Loss in iteration 135 : 1.7657173001065818
Loss in iteration 136 : 1.5998904461400012
Loss in iteration 137 : 1.786425046968447
Loss in iteration 138 : 1.5845770246816533
Loss in iteration 139 : 1.7283865633594864
Loss in iteration 140 : 1.530541237854123
Loss in iteration 141 : 1.6825629660072068
Loss in iteration 142 : 1.4979328168315933
Loss in iteration 143 : 1.6717471286389904
Loss in iteration 144 : 1.481509111839363
Loss in iteration 145 : 1.691505782223302
Loss in iteration 146 : 1.4785735225622767
Loss in iteration 147 : 1.7374373925923305
Loss in iteration 148 : 1.3675938449050813
Loss in iteration 149 : 1.6479093292658076
Loss in iteration 150 : 1.4588350518662974
Loss in iteration 151 : 1.7421758558810096
Loss in iteration 152 : 1.5878629370304695
Loss in iteration 153 : 1.782117147992012
Loss in iteration 154 : 1.5886786755378857
Loss in iteration 155 : 1.7295619352396994
Loss in iteration 156 : 1.5350533139469367
Loss in iteration 157 : 1.679713789510343
Loss in iteration 158 : 1.4991462206559696
Loss in iteration 159 : 1.6652537586095872
Loss in iteration 160 : 1.4805306345855238
Loss in iteration 161 : 1.6823357941285835
Loss in iteration 162 : 1.4769286928044452
Loss in iteration 163 : 1.7306780355571523
Loss in iteration 164 : 1.373686189195497
Loss in iteration 165 : 1.6453466670101762
Loss in iteration 166 : 1.4517880172898567
Loss in iteration 167 : 1.7294138562920933
Loss in iteration 168 : 1.5815047617606885
Loss in iteration 169 : 1.7751968912704055
Loss in iteration 170 : 1.5887193293805137
Loss in iteration 171 : 1.7258383432211668
Loss in iteration 172 : 1.5360324512395291
Loss in iteration 173 : 1.6751790520115708
Loss in iteration 174 : 1.4986490920984525
Loss in iteration 175 : 1.6596548591312963
Loss in iteration 176 : 1.479272732702184
Loss in iteration 177 : 1.6777685464282537
Loss in iteration 178 : 1.4741881944937116
Loss in iteration 179 : 1.722255354854959
Loss in iteration 180 : 1.3738707243019386
Loss in iteration 181 : 1.6408826128783625
Loss in iteration 182 : 1.4586337309986697
Loss in iteration 183 : 1.727219916833923
Loss in iteration 184 : 1.581271461639241
Loss in iteration 185 : 1.7666842014706412
Loss in iteration 186 : 1.5849525148705483
Loss in iteration 187 : 1.7173897955301844
Loss in iteration 188 : 1.5329691282925326
Loss in iteration 189 : 1.6688087944754324
Loss in iteration 190 : 1.4961842612507734
Loss in iteration 191 : 1.655332002374175
Loss in iteration 192 : 1.4775772686384105
Loss in iteration 193 : 1.6793064421506338
Loss in iteration 194 : 1.467524991173493
Loss in iteration 195 : 1.7090068148590345
Loss in iteration 196 : 1.3721404521078826
Loss in iteration 197 : 1.637521683337322
Loss in iteration 198 : 1.4803243782751934
Loss in iteration 199 : 1.734607335538789
Loss in iteration 200 : 1.586581495206007
Loss in iteration 201 : 1.7557609575431625
Loss in iteration 202 : 1.5765209490591667
Loss in iteration 203 : 1.7036394809301136
Loss in iteration 204 : 1.5251826647440636
Loss in iteration 205 : 1.6604963277794762
Loss in iteration 206 : 1.4912264102982613
Loss in iteration 207 : 1.6529499145557793
Loss in iteration 208 : 1.4755084828053227
Loss in iteration 209 : 1.6887123102313693
Loss in iteration 210 : 1.4483036791467916
Loss in iteration 211 : 1.6847057405059367
Loss in iteration 212 : 1.383158647775484
Loss in iteration 213 : 1.6471429521306988
Loss in iteration 214 : 1.5130990290108544
Loss in iteration 215 : 1.744726657646116
Loss in iteration 216 : 1.5916033340841131
Loss in iteration 217 : 1.7404631990015318
Loss in iteration 218 : 1.5631679143685702
Loss in iteration 219 : 1.686123869253317
Loss in iteration 220 : 1.51422011007539
Loss in iteration 221 : 1.6519166865045634
Loss in iteration 222 : 1.484701891510532
Loss in iteration 223 : 1.653964475079848
Loss in iteration 224 : 1.4735440443661632
Loss in iteration 225 : 1.699959013322098
Loss in iteration 226 : 1.4143628310430987
Loss in iteration 227 : 1.6543949469187524
Loss in iteration 228 : 1.4167046912831807
Loss in iteration 229 : 1.6749568111532942
Loss in iteration 230 : 1.5453616382297135
Loss in iteration 231 : 1.7477086444487377
Loss in iteration 232 : 1.5881379473045145
Loss in iteration 233 : 1.7208355016621053
Loss in iteration 234 : 1.5468196146363506
Loss in iteration 235 : 1.6693731607350428
Loss in iteration 236 : 1.5033138985897918
Loss in iteration 237 : 1.6464944408857467
Loss in iteration 238 : 1.4790012642424313
Loss in iteration 239 : 1.6625511759952696
Loss in iteration 240 : 1.4673766406744166
Loss in iteration 241 : 1.6963123639891104
Loss in iteration 242 : 1.3872250969313005
Loss in iteration 243 : 1.634352053390863
Loss in iteration 244 : 1.4675032993418171
Loss in iteration 245 : 1.7092564666499634
Loss in iteration 246 : 1.5719283962283812
Loss in iteration 247 : 1.7400571582719235
Loss in iteration 248 : 1.5747218628825113
Loss in iteration 249 : 1.696752087076763
Loss in iteration 250 : 1.5275449021285061
Loss in iteration 251 : 1.65453663697619
Loss in iteration 252 : 1.4917201606203923
Loss in iteration 253 : 1.6470586828465532
Loss in iteration 254 : 1.4743005712108601
Loss in iteration 255 : 1.6833146226939193
Loss in iteration 256 : 1.4327851543694374
Loss in iteration 257 : 1.6612145700047032
Loss in iteration 258 : 1.4074017870359592
Loss in iteration 259 : 1.6553210260703846
Loss in iteration 260 : 1.5261175667353173
Loss in iteration 261 : 1.7323585023018697
Loss in iteration 262 : 1.5834349245947015
Loss in iteration 263 : 1.7189808929523438
Loss in iteration 264 : 1.5518967752149588
Loss in iteration 265 : 1.6701246698221284
Loss in iteration 266 : 1.507349533048728
Loss in iteration 267 : 1.644100204249296
Loss in iteration 268 : 1.4805916017475764
Loss in iteration 269 : 1.6581304589982526
Loss in iteration 270 : 1.4650909268904546
Loss in iteration 271 : 1.6859393408635333
Loss in iteration 272 : 1.3947886625929695
Loss in iteration 273 : 1.6333969989631025
Loss in iteration 274 : 1.471088579744779
Loss in iteration 275 : 1.7019656073444172
Loss in iteration 276 : 1.567616787522064
Loss in iteration 277 : 1.7297027964240905
Loss in iteration 278 : 1.570269741345095
Loss in iteration 279 : 1.6894887358845965
Loss in iteration 280 : 1.5256316590819314
Loss in iteration 281 : 1.6504409228089616
Loss in iteration 282 : 1.4906409112090575
Loss in iteration 283 : 1.646251652253854
Loss in iteration 284 : 1.4729189340711248
Loss in iteration 285 : 1.681061518776737
Loss in iteration 286 : 1.421432542837965
Loss in iteration 287 : 1.6469270945771572
Loss in iteration 288 : 1.4284873453506575
Loss in iteration 289 : 1.6655658218932126
Loss in iteration 290 : 1.5375195703737297
Loss in iteration 291 : 1.7256627342718056
Loss in iteration 292 : 1.577390855147488
Loss in iteration 293 : 1.7048404607606777
Loss in iteration 294 : 1.5428379350176529
Loss in iteration 295 : 1.660392970679209
Loss in iteration 296 : 1.5019105996250959
Loss in iteration 297 : 1.6419434082056754
Loss in iteration 298 : 1.4780655188746672
Loss in iteration 299 : 1.6652278160740397
Loss in iteration 300 : 1.4501855307754226
Loss in iteration 301 : 1.666246639201602
Loss in iteration 302 : 1.405672169191715
Loss in iteration 303 : 1.6396847708191808
Loss in iteration 304 : 1.5030274874793967
Loss in iteration 305 : 1.7110323032559853
Loss in iteration 306 : 1.5735249792548682
Loss in iteration 307 : 1.7147465671348163
Loss in iteration 308 : 1.556919921586798
Loss in iteration 309 : 1.6719741817887157
Loss in iteration 310 : 1.513611157107688
Loss in iteration 311 : 1.6430368002795739
Loss in iteration 312 : 1.483988855114075
Loss in iteration 313 : 1.652879585906814
Loss in iteration 314 : 1.4642603118533575
Loss in iteration 315 : 1.6745735528240313
Loss in iteration 316 : 1.4061510181970112
Loss in iteration 317 : 1.633282553006668
Loss in iteration 318 : 1.4730103921028803
Loss in iteration 319 : 1.69138872096952
Loss in iteration 320 : 1.560747049868523
Loss in iteration 321 : 1.7174583079808394
Loss in iteration 322 : 1.5653870802267764
Loss in iteration 323 : 1.6822856538386608
Loss in iteration 324 : 1.5246092258231865
Loss in iteration 325 : 1.6470963654150668
Loss in iteration 326 : 1.4906442190890357
Loss in iteration 327 : 1.6460214558635065
Loss in iteration 328 : 1.4708329416080734
Loss in iteration 329 : 1.6734350834325158
Loss in iteration 330 : 1.4180757706705236
Loss in iteration 331 : 1.637678901429049
Loss in iteration 332 : 1.450398472776956
Loss in iteration 333 : 1.672317617688788
Loss in iteration 334 : 1.5447176841638492
Loss in iteration 335 : 1.7148984617639507
Loss in iteration 336 : 1.5691218742557962
Loss in iteration 337 : 1.6900688864258164
Loss in iteration 338 : 1.53408298356846
Loss in iteration 339 : 1.6519309160083426
Loss in iteration 340 : 1.4971955854693493
Loss in iteration 341 : 1.6423284490032388
Loss in iteration 342 : 1.4752025358212888
Loss in iteration 343 : 1.6681420926978359
Loss in iteration 344 : 1.432138474817662
Loss in iteration 345 : 1.6450570271865212
Loss in iteration 346 : 1.4346842313738226
Loss in iteration 347 : 1.6566132653100063
Loss in iteration 348 : 1.5286201962862553
Loss in iteration 349 : 1.70942645907304
Loss in iteration 350 : 1.5696062483646884
Loss in iteration 351 : 1.6955940876920073
Loss in iteration 352 : 1.5419050279607522
Loss in iteration 353 : 1.6567646080495737
Loss in iteration 354 : 1.5032911615009799
Loss in iteration 355 : 1.6406535360667263
Loss in iteration 356 : 1.4787397333391272
Loss in iteration 357 : 1.6619206213642184
Loss in iteration 358 : 1.4438944902567015
Loss in iteration 359 : 1.6515303981385268
Loss in iteration 360 : 1.4257257945075634
Loss in iteration 361 : 1.6457665707624438
Loss in iteration 362 : 1.5142301549666577
Loss in iteration 363 : 1.7025070473703081
Loss in iteration 364 : 1.5675774303618828
Loss in iteration 365 : 1.6988738189244366
Loss in iteration 366 : 1.5477598812227047
Loss in iteration 367 : 1.6610931785092775
Loss in iteration 368 : 1.5086426890187534
Loss in iteration 369 : 1.640369134722315
Loss in iteration 370 : 1.4818328973919581
Loss in iteration 371 : 1.656657995232785
Loss in iteration 372 : 1.4520383279693776
Loss in iteration 373 : 1.6555471773117205
Loss in iteration 374 : 1.4224378909832192
Loss in iteration 375 : 1.6396766331655945
Loss in iteration 376 : 1.5027933809271985
Loss in iteration 377 : 1.6954478044552548
Loss in iteration 378 : 1.5639975386751246
Loss in iteration 379 : 1.700055579118395
Loss in iteration 380 : 1.5515218066772487
Loss in iteration 381 : 1.6644420953681118
Loss in iteration 382 : 1.512969794888912
Loss in iteration 383 : 1.6408376819592192
Loss in iteration 384 : 1.4845057371485852
Loss in iteration 385 : 1.6529203983466043
Loss in iteration 386 : 1.4572136337486858
Loss in iteration 387 : 1.6572679078489583
Loss in iteration 388 : 1.4227080079673642
Loss in iteration 389 : 1.6369016809188262
Loss in iteration 390 : 1.4947457023312443
Loss in iteration 391 : 1.689269784819613
Loss in iteration 392 : 1.5600279228455143
Loss in iteration 393 : 1.6996676763148808
Loss in iteration 394 : 1.5534892593755572
Loss in iteration 395 : 1.666592715880896
Loss in iteration 396 : 1.5161331998496643
Loss in iteration 397 : 1.6415078190694143
Loss in iteration 398 : 1.486655849098564
Loss in iteration 399 : 1.650517672378689
Loss in iteration 400 : 1.4603874870389608
Testing accuracy  of updater 5 on alg 0 with rate 1.0 = 0.7255, training accuracy 0.7255, time elapsed: 3689 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6746928009740195
Loss in iteration 3 : 0.6624873440034171
Loss in iteration 4 : 0.6519609140456155
Loss in iteration 5 : 0.6422121639245605
Loss in iteration 6 : 0.6330348360655517
Loss in iteration 7 : 0.6243609101765296
Loss in iteration 8 : 0.6161469319277396
Loss in iteration 9 : 0.6083653833885766
Loss in iteration 10 : 0.600986381726957
Loss in iteration 11 : 0.5939891847083069
Loss in iteration 12 : 0.5873497609758715
Loss in iteration 13 : 0.5810536907202061
Loss in iteration 14 : 0.5750851413304886
Loss in iteration 15 : 0.5694476907495056
Loss in iteration 16 : 0.5641695033207964
Loss in iteration 17 : 0.5593930729139142
Loss in iteration 18 : 0.555638029992803
Loss in iteration 19 : 0.5547078649242485
Loss in iteration 20 : 0.5628357178254815
Loss in iteration 21 : 0.5914342983587689
Loss in iteration 22 : 0.652789305370862
Loss in iteration 23 : 0.6429946780628885
Loss in iteration 24 : 0.6275230550599487
Loss in iteration 25 : 0.5720700571239671
Loss in iteration 26 : 0.5586282738674109
Loss in iteration 27 : 0.5444278192704799
Loss in iteration 28 : 0.5396216829126422
Loss in iteration 29 : 0.5348264750128785
Loss in iteration 30 : 0.5336048143887209
Loss in iteration 31 : 0.5323867447342533
Loss in iteration 32 : 0.5343463873824854
Loss in iteration 33 : 0.5362509646786006
Loss in iteration 34 : 0.5419192116002498
Loss in iteration 35 : 0.5447568469199414
Loss in iteration 36 : 0.5520208414595984
Loss in iteration 37 : 0.5504410414591269
Loss in iteration 38 : 0.55323641495984
Loss in iteration 39 : 0.5443841453580747
Loss in iteration 40 : 0.5422948117062643
Loss in iteration 41 : 0.5337621116496889
Loss in iteration 42 : 0.5317168944157742
Loss in iteration 43 : 0.5265983647849146
Loss in iteration 44 : 0.5262794143637932
Loss in iteration 45 : 0.5239076195191369
Loss in iteration 46 : 0.5254790912030196
Loss in iteration 47 : 0.5248213309800724
Loss in iteration 48 : 0.5275298677008875
Loss in iteration 49 : 0.5266615245651656
Loss in iteration 50 : 0.5290626978484784
Loss in iteration 51 : 0.5269767079051055
Loss in iteration 52 : 0.5285641369753152
Loss in iteration 53 : 0.525133043371842
Loss in iteration 54 : 0.5256036978429364
Loss in iteration 55 : 0.5217020145124214
Loss in iteration 56 : 0.5217747212211962
Loss in iteration 57 : 0.5185033127953157
Loss in iteration 58 : 0.5189690158976261
Loss in iteration 59 : 0.5167193583611561
Loss in iteration 60 : 0.5178943282075767
Loss in iteration 61 : 0.5164432215040119
Loss in iteration 62 : 0.5179868494363721
Loss in iteration 63 : 0.5165045276220042
Loss in iteration 64 : 0.5177708835156748
Loss in iteration 65 : 0.5159161637617935
Loss in iteration 66 : 0.5170210397983115
Loss in iteration 67 : 0.514905432392832
Loss in iteration 68 : 0.5157891091077798
Loss in iteration 69 : 0.5134846802299904
Loss in iteration 70 : 0.5141730502285541
Loss in iteration 71 : 0.5119515300123438
Loss in iteration 72 : 0.512674654715325
Loss in iteration 73 : 0.5107959832934423
Loss in iteration 74 : 0.5117462531054793
Loss in iteration 75 : 0.5102542279449505
Loss in iteration 76 : 0.5113307612289114
Loss in iteration 77 : 0.5098977472635755
Loss in iteration 78 : 0.5107978702655291
Loss in iteration 79 : 0.5092455016692438
Loss in iteration 80 : 0.5101332440100126
Loss in iteration 81 : 0.5085682072174698
Loss in iteration 82 : 0.5094740153543015
Loss in iteration 83 : 0.5078392638447307
Loss in iteration 84 : 0.5086533365531671
Loss in iteration 85 : 0.506975839008868
Loss in iteration 86 : 0.5077301219558428
Loss in iteration 87 : 0.5061595800574764
Loss in iteration 88 : 0.5069717699345412
Loss in iteration 89 : 0.5056206130083615
Loss in iteration 90 : 0.5064844309242715
Loss in iteration 91 : 0.5052223878420673
Loss in iteration 92 : 0.5059698080538881
Loss in iteration 93 : 0.5046686575612709
Loss in iteration 94 : 0.5054334438269928
Loss in iteration 95 : 0.5041733631622598
Loss in iteration 96 : 0.5050222532987096
Loss in iteration 97 : 0.5037339344651762
Loss in iteration 98 : 0.5045384736417791
Loss in iteration 99 : 0.5031748735455165
Loss in iteration 100 : 0.503897449884334
Loss in iteration 101 : 0.50255349765348
Loss in iteration 102 : 0.503274870677264
Loss in iteration 103 : 0.5020778448832868
Loss in iteration 104 : 0.50282836112601
Loss in iteration 105 : 0.5017301596757947
Loss in iteration 106 : 0.5023960848571376
Loss in iteration 107 : 0.5012848895142445
Loss in iteration 108 : 0.5019666072414904
Loss in iteration 109 : 0.5009019525022811
Loss in iteration 110 : 0.5016860011177687
Loss in iteration 111 : 0.5006086434316137
Loss in iteration 112 : 0.5013742508059882
Loss in iteration 113 : 0.5002131491053907
Loss in iteration 114 : 0.5008949385712331
Loss in iteration 115 : 0.49971596478251185
Loss in iteration 116 : 0.5003743000056611
Loss in iteration 117 : 0.4993062359563958
Loss in iteration 118 : 0.4999874989428024
Loss in iteration 119 : 0.499022484936905
Loss in iteration 120 : 0.4996361957330209
Loss in iteration 121 : 0.498666536390881
Loss in iteration 122 : 0.49928596877236364
Loss in iteration 123 : 0.49835736336525804
Loss in iteration 124 : 0.4990852029163118
Loss in iteration 125 : 0.49815169612320176
Loss in iteration 126 : 0.498878949927985
Loss in iteration 127 : 0.49785951722973154
Loss in iteration 128 : 0.49850575372825556
Loss in iteration 129 : 0.4974478804631291
Loss in iteration 130 : 0.49806026924817376
Loss in iteration 131 : 0.49709230683449857
Loss in iteration 132 : 0.49772692582245703
Loss in iteration 133 : 0.4968663224118577
Loss in iteration 134 : 0.49744335100433
Loss in iteration 135 : 0.4965806154845139
Loss in iteration 136 : 0.4971502166174414
Loss in iteration 137 : 0.4963213279309496
Loss in iteration 138 : 0.4970035463234634
Loss in iteration 139 : 0.49617588608185464
Loss in iteration 140 : 0.4968726002588998
Loss in iteration 141 : 0.49595801681587603
Loss in iteration 142 : 0.49657643216338665
Loss in iteration 143 : 0.4956082291962409
Loss in iteration 144 : 0.49618554648430635
Loss in iteration 145 : 0.49529282147168474
Loss in iteration 146 : 0.49589398556234565
Loss in iteration 147 : 0.4951153098292006
Loss in iteration 148 : 0.4956661544280794
Loss in iteration 149 : 0.4948863562057686
Loss in iteration 150 : 0.4954138373333664
Loss in iteration 151 : 0.4946604572413448
Loss in iteration 152 : 0.49530563669760147
Loss in iteration 153 : 0.4945602050082456
Loss in iteration 154 : 0.49523488865112436
Loss in iteration 155 : 0.49440142070175225
Loss in iteration 156 : 0.49499926888522644
Loss in iteration 157 : 0.4940978677213346
Loss in iteration 158 : 0.4946471626726867
Loss in iteration 159 : 0.4938093184922803
Loss in iteration 160 : 0.49438527924507447
Loss in iteration 161 : 0.4936716214114081
Loss in iteration 162 : 0.4942049449199257
Loss in iteration 163 : 0.49349185130642115
Loss in iteration 164 : 0.4939816883831546
Loss in iteration 165 : 0.49328649060351404
Loss in iteration 166 : 0.4939002365267582
Loss in iteration 167 : 0.49322149362870754
Loss in iteration 168 : 0.493881748895111
Loss in iteration 169 : 0.4931138227771557
Loss in iteration 170 : 0.493697542500205
Loss in iteration 171 : 0.4928461182163096
Loss in iteration 172 : 0.4933719691362552
Loss in iteration 173 : 0.49257199691177783
Loss in iteration 174 : 0.4931277319565544
Loss in iteration 175 : 0.49246597035683953
Loss in iteration 176 : 0.49299025457211837
Loss in iteration 177 : 0.49233365485688724
Loss in iteration 178 : 0.4927880802908721
Loss in iteration 179 : 0.4921377107786927
Loss in iteration 180 : 0.4927222826565107
Loss in iteration 181 : 0.4920996161241667
Loss in iteration 182 : 0.4927519393074412
Loss in iteration 183 : 0.49203977091594414
Loss in iteration 184 : 0.4926154313603871
Loss in iteration 185 : 0.4918017117749569
Loss in iteration 186 : 0.49230711493339563
Loss in iteration 187 : 0.49153002433674986
Loss in iteration 188 : 0.4920676186931749
Loss in iteration 189 : 0.49144630303974407
Loss in iteration 190 : 0.49197102955536764
Loss in iteration 191 : 0.4913662110016911
Loss in iteration 192 : 0.4917864434120282
Loss in iteration 193 : 0.49116969567304547
Loss in iteration 194 : 0.49172330318689333
Loss in iteration 195 : 0.49114906180627715
Loss in iteration 196 : 0.4917988491136866
Loss in iteration 197 : 0.4911373599244336
Loss in iteration 198 : 0.4917114101278909
Loss in iteration 199 : 0.4909270475231314
Loss in iteration 200 : 0.4914142214336224
Loss in iteration 201 : 0.490646004838954
Loss in iteration 202 : 0.4911642472963198
Loss in iteration 203 : 0.4905708139872362
Loss in iteration 204 : 0.4911070276121437
Loss in iteration 205 : 0.49055546569778585
Loss in iteration 206 : 0.4909442775343756
Loss in iteration 207 : 0.4903510065114433
Loss in iteration 208 : 0.49086613325985945
Loss in iteration 209 : 0.4903338193174726
Loss in iteration 210 : 0.49098495084785704
Loss in iteration 211 : 0.4903734579967218
Loss in iteration 212 : 0.4909537323862783
Loss in iteration 213 : 0.49019459304009605
Loss in iteration 214 : 0.49066608873412365
Loss in iteration 215 : 0.48989334611197227
Loss in iteration 216 : 0.4903868882404305
Loss in iteration 217 : 0.48980350234553155
Loss in iteration 218 : 0.4903621103697262
Loss in iteration 219 : 0.48987291093398605
Loss in iteration 220 : 0.49024121078978156
Loss in iteration 221 : 0.48966271482359575
Loss in iteration 222 : 0.4901226942488214
Loss in iteration 223 : 0.4896242351290943
Loss in iteration 224 : 0.4902773054011716
Loss in iteration 225 : 0.48971964129904355
Loss in iteration 226 : 0.4903166778581734
Loss in iteration 227 : 0.4895843906836664
Loss in iteration 228 : 0.49004509911172023
Loss in iteration 229 : 0.4892560399575777
Loss in iteration 230 : 0.48971508757811366
Loss in iteration 231 : 0.48911307449696256
Loss in iteration 232 : 0.4896962703091553
Loss in iteration 233 : 0.48928432376094316
Loss in iteration 234 : 0.48966793445950724
Loss in iteration 235 : 0.48910299125297607
Loss in iteration 236 : 0.48947631416673265
Loss in iteration 237 : 0.4889963189416048
Loss in iteration 238 : 0.4896429587366628
Loss in iteration 239 : 0.4891472117602413
Loss in iteration 240 : 0.4897757004748296
Loss in iteration 241 : 0.48908126419734915
Loss in iteration 242 : 0.48954238897542984
Loss in iteration 243 : 0.48873063335295525
Loss in iteration 244 : 0.4891437436684563
Loss in iteration 245 : 0.4884805768286402
Loss in iteration 246 : 0.4890600952359638
Loss in iteration 247 : 0.4887216768697261
Loss in iteration 248 : 0.48921338072602044
Loss in iteration 249 : 0.48869775361235773
Loss in iteration 250 : 0.4889369753999591
Loss in iteration 251 : 0.4884395985062671
Loss in iteration 252 : 0.48904445126654367
Loss in iteration 253 : 0.4886188834332773
Loss in iteration 254 : 0.4892972943332288
Loss in iteration 255 : 0.4886682239906197
Loss in iteration 256 : 0.48915529593486134
Loss in iteration 257 : 0.48832926733720267
Loss in iteration 258 : 0.48869376581928264
Loss in iteration 259 : 0.4879253502222906
Loss in iteration 260 : 0.48842287343318297
Loss in iteration 261 : 0.4880481508797295
Loss in iteration 262 : 0.4887664644805941
Loss in iteration 263 : 0.4884720089805011
Loss in iteration 264 : 0.48860266997142704
Loss in iteration 265 : 0.48801435659995285
Loss in iteration 266 : 0.4884515768402329
Loss in iteration 267 : 0.4880781381048693
Loss in iteration 268 : 0.4888119515605205
Loss in iteration 269 : 0.48830305189538925
Loss in iteration 270 : 0.4888713269248286
Loss in iteration 271 : 0.48807684076839714
Loss in iteration 272 : 0.48842282236178813
Loss in iteration 273 : 0.48754201865884567
Loss in iteration 274 : 0.4878881781467633
Loss in iteration 275 : 0.48724581464585026
Loss in iteration 276 : 0.48791343081533217
Loss in iteration 277 : 0.48789420012227286
Loss in iteration 278 : 0.4886629805080594
Loss in iteration 279 : 0.48818513301093275
Loss in iteration 280 : 0.48805819573078685
Loss in iteration 281 : 0.4875198431265769
Loss in iteration 282 : 0.4881404489238248
Loss in iteration 283 : 0.48781007179628433
Loss in iteration 284 : 0.48855551544156445
Loss in iteration 285 : 0.4879430215910391
Loss in iteration 286 : 0.48839910776546946
Loss in iteration 287 : 0.48749788278698053
Loss in iteration 288 : 0.4877550684445898
Loss in iteration 289 : 0.48685703493438987
Loss in iteration 290 : 0.4871748628426474
Loss in iteration 291 : 0.4865313689152515
Loss in iteration 292 : 0.48706798231832943
Loss in iteration 293 : 0.48677504240889
Loss in iteration 294 : 0.48752157035002824
Loss in iteration 295 : 0.48758754297335044
Loss in iteration 296 : 0.4881911755145306
Loss in iteration 297 : 0.4879954571045571
Loss in iteration 298 : 0.487933152079111
Loss in iteration 299 : 0.4874237026237491
Loss in iteration 300 : 0.48768889444058866
Loss in iteration 301 : 0.48714079303581426
Loss in iteration 302 : 0.48752310592777404
Loss in iteration 303 : 0.4869419649663822
Loss in iteration 304 : 0.4873779961335264
Loss in iteration 305 : 0.48703905021044874
Loss in iteration 306 : 0.4878569713448662
Loss in iteration 307 : 0.4877125852505991
Loss in iteration 308 : 0.48779423030939834
Loss in iteration 309 : 0.4874729570236483
Loss in iteration 310 : 0.48739718153679634
Loss in iteration 311 : 0.4867978447688305
Loss in iteration 312 : 0.48737707082070797
Loss in iteration 313 : 0.4870746871695408
Loss in iteration 314 : 0.4878503136499068
Loss in iteration 315 : 0.4872892606887023
Loss in iteration 316 : 0.4877726940818485
Loss in iteration 317 : 0.4869033341511008
Loss in iteration 318 : 0.48716347666441073
Loss in iteration 319 : 0.4862760462153709
Loss in iteration 320 : 0.4865820774459919
Loss in iteration 321 : 0.485981102808075
Loss in iteration 322 : 0.4865898045698742
Loss in iteration 323 : 0.48670948534789765
Loss in iteration 324 : 0.48780293577130657
Loss in iteration 325 : 0.4884540035825536
Loss in iteration 326 : 0.4882508229207294
Loss in iteration 327 : 0.4878927885589922
Loss in iteration 328 : 0.4880072403973165
Loss in iteration 329 : 0.487445095929357
Loss in iteration 330 : 0.48731074054298934
Loss in iteration 331 : 0.48659462879617754
Loss in iteration 332 : 0.4868027538466357
Loss in iteration 333 : 0.4863131046662996
Loss in iteration 334 : 0.48677974671215885
Loss in iteration 335 : 0.4864220099440646
Loss in iteration 336 : 0.48696094735388673
Loss in iteration 337 : 0.4866512160123258
Loss in iteration 338 : 0.48716117819279164
Loss in iteration 339 : 0.4868631486561554
Loss in iteration 340 : 0.48728217698817966
Loss in iteration 341 : 0.4869532177160473
Loss in iteration 342 : 0.4872135838012069
Loss in iteration 343 : 0.48678248879262426
Loss in iteration 344 : 0.48697357649581896
Loss in iteration 345 : 0.4865354748465776
Loss in iteration 346 : 0.4868470054828185
Loss in iteration 347 : 0.48646577241428995
Loss in iteration 348 : 0.4868741285541531
Loss in iteration 349 : 0.4865086933057062
Loss in iteration 350 : 0.4869289226933138
Loss in iteration 351 : 0.48655183502954785
Loss in iteration 352 : 0.48694236783589806
Loss in iteration 353 : 0.4865636147610593
Loss in iteration 354 : 0.48691249847481366
Loss in iteration 355 : 0.4865349034704718
Loss in iteration 356 : 0.4868403082217949
Loss in iteration 357 : 0.4864565045903982
Loss in iteration 358 : 0.48676480473944705
Loss in iteration 359 : 0.4863907059101069
Loss in iteration 360 : 0.4867449336670718
Loss in iteration 361 : 0.48637911954522345
Loss in iteration 362 : 0.4867606623363512
Loss in iteration 363 : 0.48638477627659416
Loss in iteration 364 : 0.4867598485683202
Loss in iteration 365 : 0.4863720813431256
Loss in iteration 366 : 0.4867274309720112
Loss in iteration 367 : 0.48634116918050735
Loss in iteration 368 : 0.4866786650512057
Loss in iteration 369 : 0.48630126426238635
Loss in iteration 370 : 0.48663150317672216
Loss in iteration 371 : 0.4862624992041731
Loss in iteration 372 : 0.4866041696353518
Loss in iteration 373 : 0.4862405928227284
Loss in iteration 374 : 0.48660024539851154
Loss in iteration 375 : 0.48623321184707713
Loss in iteration 376 : 0.4865983403802982
Loss in iteration 377 : 0.486221033896624
Loss in iteration 378 : 0.4865783120775837
Loss in iteration 379 : 0.48619478852212955
Loss in iteration 380 : 0.4865413274093282
Loss in iteration 381 : 0.4861609755277177
Loss in iteration 382 : 0.4865014590614808
Loss in iteration 383 : 0.48612921487443966
Loss in iteration 384 : 0.4864713765856934
Loss in iteration 385 : 0.4861059280047569
Loss in iteration 386 : 0.48645590305598524
Loss in iteration 387 : 0.48609197793127656
Loss in iteration 388 : 0.4864484750620595
Loss in iteration 389 : 0.48608002542817397
Loss in iteration 390 : 0.4864361316439373
Loss in iteration 391 : 0.48606134999652284
Loss in iteration 392 : 0.4864120758956473
Loss in iteration 393 : 0.4860349178040738
Loss in iteration 394 : 0.48638058169449094
Loss in iteration 395 : 0.4860065084822669
Loss in iteration 396 : 0.48635080053473917
Loss in iteration 397 : 0.48598213769867016
Loss in iteration 398 : 0.48632902150828916
Loss in iteration 399 : 0.4859640912939231
Loss in iteration 400 : 0.4863150658350881
Testing accuracy  of updater 5 on alg 0 with rate 0.09999999999999998 = 0.772625, training accuracy 0.772625, time elapsed: 4374 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 3.01661754226205
Loss in iteration 3 : 2.468606915006185
Loss in iteration 4 : 1.839756115281385
Loss in iteration 5 : 1.2652937429736604
Loss in iteration 6 : 1.4080173861407126
Loss in iteration 7 : 0.7223093056718833
Loss in iteration 8 : 1.3929399098024342
Loss in iteration 9 : 1.120983698449653
Loss in iteration 10 : 0.7978223405699186
Loss in iteration 11 : 1.171313618063247
Loss in iteration 12 : 0.9406699714211916
Loss in iteration 13 : 0.6524642584034589
Loss in iteration 14 : 0.9278276752219883
Loss in iteration 15 : 0.827841962283879
Loss in iteration 16 : 0.6659761107210641
Loss in iteration 17 : 0.8685010221846614
Loss in iteration 18 : 0.8062859743013452
Loss in iteration 19 : 0.6641875453761626
Loss in iteration 20 : 0.7921104715473573
Loss in iteration 21 : 0.7407351539948345
Loss in iteration 22 : 0.5953801747093573
Loss in iteration 23 : 0.7035187050469127
Loss in iteration 24 : 0.6831287096403862
Loss in iteration 25 : 0.5912355107702933
Loss in iteration 26 : 0.6793436453779825
Loss in iteration 27 : 0.6404537141497422
Loss in iteration 28 : 0.571838536277377
Loss in iteration 29 : 0.6250141272450315
Loss in iteration 30 : 0.5630339634797176
Loss in iteration 31 : 0.5515256697411476
Loss in iteration 32 : 0.5870752339356232
Loss in iteration 33 : 0.5273216252009586
Loss in iteration 34 : 0.5626451201095944
Loss in iteration 35 : 0.5415109423159097
Loss in iteration 36 : 0.5104608887367251
Loss in iteration 37 : 0.5390041482402657
Loss in iteration 38 : 0.49291919814323654
Loss in iteration 39 : 0.5210282347977018
Loss in iteration 40 : 0.5008740127146766
Loss in iteration 41 : 0.5083286232754236
Loss in iteration 42 : 0.4991041376309612
Loss in iteration 43 : 0.49206719747269473
Loss in iteration 44 : 0.4969867125083717
Loss in iteration 45 : 0.48489349573082463
Loss in iteration 46 : 0.4976511648924832
Loss in iteration 47 : 0.48539072671670225
Loss in iteration 48 : 0.4910590904879071
Loss in iteration 49 : 0.4812820602703366
Loss in iteration 50 : 0.48439956813546275
Loss in iteration 51 : 0.48116003925794787
Loss in iteration 52 : 0.48130028141416237
Loss in iteration 53 : 0.4805344083768848
Loss in iteration 54 : 0.4750310099644994
Loss in iteration 55 : 0.4772587205666613
Loss in iteration 56 : 0.47050241474504334
Loss in iteration 57 : 0.47655302783800313
Loss in iteration 58 : 0.46777804215092167
Loss in iteration 59 : 0.4727788908814945
Loss in iteration 60 : 0.465337313554962
Loss in iteration 61 : 0.46828192480023495
Loss in iteration 62 : 0.466331166041497
Loss in iteration 63 : 0.4649982405803996
Loss in iteration 64 : 0.4665815559619489
Loss in iteration 65 : 0.46219945144201585
Loss in iteration 66 : 0.46535809981445175
Loss in iteration 67 : 0.4624500215370399
Loss in iteration 68 : 0.4638256876111543
Loss in iteration 69 : 0.463340884533388
Loss in iteration 70 : 0.4616627397741662
Loss in iteration 71 : 0.46342621407127466
Loss in iteration 72 : 0.46151023479347575
Loss in iteration 73 : 0.4625998356837416
Loss in iteration 74 : 0.46212953181929917
Loss in iteration 75 : 0.4610291153481562
Loss in iteration 76 : 0.46200893427873607
Loss in iteration 77 : 0.46102242334947535
Loss in iteration 78 : 0.46105627216832057
Loss in iteration 79 : 0.461399070740229
Loss in iteration 80 : 0.46036216268981167
Loss in iteration 81 : 0.460797222180377
Loss in iteration 82 : 0.4609835017467767
Loss in iteration 83 : 0.46025712937402147
Loss in iteration 84 : 0.4607578243730212
Loss in iteration 85 : 0.46079382481769243
Loss in iteration 86 : 0.46066463038783334
Loss in iteration 87 : 0.4616452948697636
Loss in iteration 88 : 0.462730519405309
Loss in iteration 89 : 0.4640477921728783
Loss in iteration 90 : 0.46549687031678705
Loss in iteration 91 : 0.4639621941231182
Loss in iteration 92 : 0.4609024394694305
Loss in iteration 93 : 0.45990291120497634
Loss in iteration 94 : 0.4616906682829
Loss in iteration 95 : 0.4629030789087284
Loss in iteration 96 : 0.4613612109135107
Loss in iteration 97 : 0.45976763516185964
Loss in iteration 98 : 0.4603099479556595
Loss in iteration 99 : 0.46142099078139565
Loss in iteration 100 : 0.4609419044883683
Loss in iteration 101 : 0.45965926712230815
Loss in iteration 102 : 0.4597995159495588
Loss in iteration 103 : 0.4607035510396865
Loss in iteration 104 : 0.4604447774640006
Loss in iteration 105 : 0.45953715531551775
Loss in iteration 106 : 0.45959628946965825
Loss in iteration 107 : 0.4602100333331116
Loss in iteration 108 : 0.46007436325620055
Loss in iteration 109 : 0.45946778564801327
Loss in iteration 110 : 0.45945955339135286
Loss in iteration 111 : 0.4598315877886604
Loss in iteration 112 : 0.45980725063621786
Loss in iteration 113 : 0.4594573630512853
Loss in iteration 114 : 0.459350818757684
Loss in iteration 115 : 0.4595278987463103
Loss in iteration 116 : 0.45960392960398044
Loss in iteration 117 : 0.45945923364266655
Loss in iteration 118 : 0.4593258281763949
Loss in iteration 119 : 0.4593230547676652
Loss in iteration 120 : 0.45938238720521746
Loss in iteration 121 : 0.4593960504234318
Loss in iteration 122 : 0.4593513912950223
Loss in iteration 123 : 0.4592871679104849
Loss in iteration 124 : 0.4592410597742404
Loss in iteration 125 : 0.45923731187216354
Loss in iteration 126 : 0.4592681218034687
Loss in iteration 127 : 0.4592893717843186
Loss in iteration 128 : 0.45926313915093414
Loss in iteration 129 : 0.4591999749119787
Loss in iteration 130 : 0.45914874194119176
Loss in iteration 131 : 0.4591472943534886
Loss in iteration 132 : 0.4591832627812391
Loss in iteration 133 : 0.4592135723954274
Loss in iteration 134 : 0.4592051656263949
Loss in iteration 135 : 0.4591635622046851
Loss in iteration 136 : 0.45911824057805095
Loss in iteration 137 : 0.4590955244716306
Loss in iteration 138 : 0.4590983337433054
Loss in iteration 139 : 0.45911352619637225
Loss in iteration 140 : 0.4591263833970859
Loss in iteration 141 : 0.4591305381697988
Loss in iteration 142 : 0.45912815006327873
Loss in iteration 143 : 0.45912282893890766
Loss in iteration 144 : 0.45911899642262827
Loss in iteration 145 : 0.459115675518534
Loss in iteration 146 : 0.45911315634191013
Loss in iteration 147 : 0.45910897497347164
Loss in iteration 148 : 0.4591044888249814
Loss in iteration 149 : 0.45910079873648424
Loss in iteration 150 : 0.45910130372915436
Loss in iteration 151 : 0.4591082818741625
Loss in iteration 152 : 0.45912780780259743
Loss in iteration 153 : 0.4591662329465386
Loss in iteration 154 : 0.4592421493127061
Loss in iteration 155 : 0.45938076743485157
Loss in iteration 156 : 0.45965590171566656
Loss in iteration 157 : 0.46016991959970893
Loss in iteration 158 : 0.46124407579386867
Loss in iteration 159 : 0.4632178352174895
Loss in iteration 160 : 0.46741538150359735
Loss in iteration 161 : 0.47398823062898654
Loss in iteration 162 : 0.4864744658574106
Loss in iteration 163 : 0.5008789196254039
Loss in iteration 164 : 0.5206212092361646
Loss in iteration 165 : 0.5453751273178354
Loss in iteration 166 : 0.5440897927691389
Loss in iteration 167 : 0.5251961761899522
Loss in iteration 168 : 0.4873046674600959
Loss in iteration 169 : 0.4652563544989713
Loss in iteration 170 : 0.46498487963791124
Loss in iteration 171 : 0.4786338860658517
Loss in iteration 172 : 0.49475284110433954
Loss in iteration 173 : 0.49947571054992024
Loss in iteration 174 : 0.48647849827571576
Loss in iteration 175 : 0.469840575608805
Loss in iteration 176 : 0.46124429696087854
Loss in iteration 177 : 0.4629945409062517
Loss in iteration 178 : 0.47063074409158895
Loss in iteration 179 : 0.4786916201275777
Loss in iteration 180 : 0.4842127120067985
Loss in iteration 181 : 0.48620213332175044
Loss in iteration 182 : 0.4805466644752031
Loss in iteration 183 : 0.47377378571527184
Loss in iteration 184 : 0.46752909746006294
Loss in iteration 185 : 0.46314339469121035
Loss in iteration 186 : 0.4604315994457442
Loss in iteration 187 : 0.45930864720297804
Loss in iteration 188 : 0.4599912115375613
Loss in iteration 189 : 0.4620484445276405
Loss in iteration 190 : 0.46522218817314637
Loss in iteration 191 : 0.47112433403989107
Loss in iteration 192 : 0.4833080236496881
Loss in iteration 193 : 0.5040118515375279
Loss in iteration 194 : 0.5410596500639713
Loss in iteration 195 : 0.566649806337231
Loss in iteration 196 : 0.5783450280685501
Loss in iteration 197 : 0.5299913711457174
Loss in iteration 198 : 0.48177827612332647
Loss in iteration 199 : 0.4616669274722567
Loss in iteration 200 : 0.47671009491637356
Loss in iteration 201 : 0.5060080646741494
Loss in iteration 202 : 0.5143019118659347
Loss in iteration 203 : 0.4973873888811253
Loss in iteration 204 : 0.47181873600897967
Loss in iteration 205 : 0.4612919237230922
Loss in iteration 206 : 0.4686113397798907
Loss in iteration 207 : 0.48413751378335124
Loss in iteration 208 : 0.4955595538398613
Loss in iteration 209 : 0.4917064367638378
Loss in iteration 210 : 0.4793197271531846
Loss in iteration 211 : 0.46604973203756456
Loss in iteration 212 : 0.46029635839713745
Loss in iteration 213 : 0.46267594178236754
Loss in iteration 214 : 0.4701424121720697
Loss in iteration 215 : 0.4798059815044827
Loss in iteration 216 : 0.48799041709748453
Loss in iteration 217 : 0.4962936497047119
Loss in iteration 218 : 0.5000669557829908
Loss in iteration 219 : 0.5044465789051743
Loss in iteration 220 : 0.5016005159935911
Loss in iteration 221 : 0.49838126442783665
Loss in iteration 222 : 0.48833670053659484
Loss in iteration 223 : 0.4786149844224286
Loss in iteration 224 : 0.46891962965353046
Loss in iteration 225 : 0.4624938433349242
Loss in iteration 226 : 0.45975010177843006
Loss in iteration 227 : 0.46028950199845
Loss in iteration 228 : 0.463335605759327
Loss in iteration 229 : 0.4681173515710071
Loss in iteration 230 : 0.47449339071490815
Loss in iteration 231 : 0.4818165455249783
Loss in iteration 232 : 0.4910339923199702
Loss in iteration 233 : 0.4986364084801723
Loss in iteration 234 : 0.5067353837727656
Loss in iteration 235 : 0.5049998344041412
Loss in iteration 236 : 0.49974504253638136
Loss in iteration 237 : 0.48584770539966415
Loss in iteration 238 : 0.47312096253890273
Loss in iteration 239 : 0.46381103104650273
Loss in iteration 240 : 0.4599334461730781
Loss in iteration 241 : 0.4609251064480146
Loss in iteration 242 : 0.4656495505133083
Loss in iteration 243 : 0.47368885306855146
Loss in iteration 244 : 0.48448509940922635
Loss in iteration 245 : 0.4991063849796584
Loss in iteration 246 : 0.5132245735615079
Loss in iteration 247 : 0.5276429098402052
Loss in iteration 248 : 0.5241482098833933
Loss in iteration 249 : 0.5127657010403851
Loss in iteration 250 : 0.4876383583841483
Loss in iteration 251 : 0.468632496227624
Loss in iteration 252 : 0.4610135130691806
Loss in iteration 253 : 0.46486031677889067
Loss in iteration 254 : 0.47584788973747716
Loss in iteration 255 : 0.48813207683634013
Loss in iteration 256 : 0.49936814288638165
Loss in iteration 257 : 0.502989999150234
Loss in iteration 258 : 0.5000335076379241
Loss in iteration 259 : 0.48995754497078897
Loss in iteration 260 : 0.47976344382468844
Loss in iteration 261 : 0.469447835103983
Loss in iteration 262 : 0.462688550480306
Loss in iteration 263 : 0.45998684757597824
Loss in iteration 264 : 0.4610718001033059
Loss in iteration 265 : 0.46465697233680237
Loss in iteration 266 : 0.4694887663958432
Loss in iteration 267 : 0.4760691810694932
Loss in iteration 268 : 0.4837798834510619
Loss in iteration 269 : 0.4950769197395959
Loss in iteration 270 : 0.5068204597208945
Loss in iteration 271 : 0.5209986387675003
Loss in iteration 272 : 0.5247329147089054
Loss in iteration 273 : 0.5240700692133801
Loss in iteration 274 : 0.5040925974762228
Loss in iteration 275 : 0.4834836024626867
Loss in iteration 276 : 0.4666096411571669
Loss in iteration 277 : 0.4607602052557303
Loss in iteration 278 : 0.4646991144787829
Loss in iteration 279 : 0.4741743818572591
Loss in iteration 280 : 0.48583030766052937
Loss in iteration 281 : 0.49420550997703877
Loss in iteration 282 : 0.4976417937388633
Loss in iteration 283 : 0.4936998252961712
Loss in iteration 284 : 0.486274552063048
Loss in iteration 285 : 0.476414444856023
Loss in iteration 286 : 0.4685156525377836
Loss in iteration 287 : 0.4626638403268547
Loss in iteration 288 : 0.46000609847592033
Loss in iteration 289 : 0.46028311451991455
Loss in iteration 290 : 0.4626973264183687
Loss in iteration 291 : 0.46675770178353465
Loss in iteration 292 : 0.47244191827733995
Loss in iteration 293 : 0.48203236147013717
Loss in iteration 294 : 0.4956682149719783
Loss in iteration 295 : 0.5175817988853355
Loss in iteration 296 : 0.5378938570233572
Loss in iteration 297 : 0.5570001818844538
Loss in iteration 298 : 0.5414492214786851
Loss in iteration 299 : 0.5151377085653289
Loss in iteration 300 : 0.4798297930637815
Loss in iteration 301 : 0.46237391903536423
Loss in iteration 302 : 0.46579798525684407
Loss in iteration 303 : 0.4815835252395999
Loss in iteration 304 : 0.49817654366463004
Loss in iteration 305 : 0.5020689331768565
Loss in iteration 306 : 0.4940828474955682
Loss in iteration 307 : 0.47929931371763396
Loss in iteration 308 : 0.4663873734705141
Loss in iteration 309 : 0.46051868081015934
Loss in iteration 310 : 0.46174336432585056
Loss in iteration 311 : 0.46757240882713075
Loss in iteration 312 : 0.47503411192799105
Loss in iteration 313 : 0.4813110154950035
Loss in iteration 314 : 0.48407015616586
Loss in iteration 315 : 0.4851808489860294
Loss in iteration 316 : 0.48219590466398693
Loss in iteration 317 : 0.4796626402011684
Loss in iteration 318 : 0.4762505936721371
Loss in iteration 319 : 0.4745993787771997
Loss in iteration 320 : 0.47378069092504316
Loss in iteration 321 : 0.4744722571066361
Loss in iteration 322 : 0.4764409855149879
Loss in iteration 323 : 0.4810579635340848
Loss in iteration 324 : 0.4871134421202929
Loss in iteration 325 : 0.497416741722942
Loss in iteration 326 : 0.505478964012447
Loss in iteration 327 : 0.5146338673633625
Loss in iteration 328 : 0.5115658456258263
Loss in iteration 329 : 0.5034940030920049
Loss in iteration 330 : 0.4867078394661729
Loss in iteration 331 : 0.4720128895616093
Loss in iteration 332 : 0.4625107087084182
Loss in iteration 333 : 0.459958637387885
Loss in iteration 334 : 0.46317195095703834
Loss in iteration 335 : 0.4701644551699995
Loss in iteration 336 : 0.4795833482062815
Loss in iteration 337 : 0.4890293163710454
Loss in iteration 338 : 0.49850519203800864
Loss in iteration 339 : 0.5025711564106926
Loss in iteration 340 : 0.5039091058501831
Loss in iteration 341 : 0.49584432282030755
Loss in iteration 342 : 0.48613686473008055
Loss in iteration 343 : 0.4741205094912491
Loss in iteration 344 : 0.4654307343210311
Loss in iteration 345 : 0.4607374399514146
Loss in iteration 346 : 0.45982794122398646
Loss in iteration 347 : 0.46191633624166134
Loss in iteration 348 : 0.46663735893568303
Loss in iteration 349 : 0.4749127220552954
Loss in iteration 350 : 0.4876591039306478
Loss in iteration 351 : 0.5079804636665319
Loss in iteration 352 : 0.5306672089980925
Loss in iteration 353 : 0.5546359221751472
Loss in iteration 354 : 0.5469682664115264
Loss in iteration 355 : 0.5255664207790135
Loss in iteration 356 : 0.48731500968278213
Loss in iteration 357 : 0.4647881363367346
Loss in iteration 358 : 0.464044369701107
Loss in iteration 359 : 0.47870237443013447
Loss in iteration 360 : 0.49736832744416026
Loss in iteration 361 : 0.5055548213081512
Loss in iteration 362 : 0.5015724733364729
Loss in iteration 363 : 0.487529595415068
Loss in iteration 364 : 0.47229752670373865
Loss in iteration 365 : 0.46274384485700876
Loss in iteration 366 : 0.4604157280055243
Loss in iteration 367 : 0.46377908704556986
Loss in iteration 368 : 0.4703010666022824
Loss in iteration 369 : 0.47709180271059054
Loss in iteration 370 : 0.481663841385169
Loss in iteration 371 : 0.48427626505022864
Loss in iteration 372 : 0.4829779692704329
Loss in iteration 373 : 0.481591144404177
Loss in iteration 374 : 0.47903751549510015
Loss in iteration 375 : 0.47826842783228346
Loss in iteration 376 : 0.47814866714758975
Loss in iteration 377 : 0.47992859933870136
Loss in iteration 378 : 0.4828433365811336
Loss in iteration 379 : 0.48857842994322825
Loss in iteration 380 : 0.4943711281344034
Loss in iteration 381 : 0.5029644430546725
Loss in iteration 382 : 0.5058377749782559
Loss in iteration 383 : 0.5070690235258273
Loss in iteration 384 : 0.49825484714961327
Loss in iteration 385 : 0.487078238003292
Loss in iteration 386 : 0.4743339085230591
Loss in iteration 387 : 0.46506036380682203
Loss in iteration 388 : 0.46042501175984535
Loss in iteration 389 : 0.4601307323445252
Loss in iteration 390 : 0.4631606684836258
Loss in iteration 391 : 0.4685821125508462
Loss in iteration 392 : 0.4761591475302305
Loss in iteration 393 : 0.48528872200843237
Loss in iteration 394 : 0.4966369453839187
Loss in iteration 395 : 0.5058366703169597
Loss in iteration 396 : 0.5139395544225279
Loss in iteration 397 : 0.5102873111400795
Loss in iteration 398 : 0.50226293660369
Loss in iteration 399 : 0.48650940737394516
Loss in iteration 400 : 0.4731307835477286
Testing accuracy  of updater 6 on alg 0 with rate 2.0 = 0.789125, training accuracy 0.789125, time elapsed: 4179 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6807039267482182
Loss in iteration 3 : 0.6670263425453151
Loss in iteration 4 : 0.6539477330797808
Loss in iteration 5 : 0.6359719974969402
Loss in iteration 6 : 0.6178474059421816
Loss in iteration 7 : 0.6026102905732572
Loss in iteration 8 : 0.5861374787279243
Loss in iteration 9 : 0.5691813558523761
Loss in iteration 10 : 0.5556987145024093
Loss in iteration 11 : 0.5448229609460693
Loss in iteration 12 : 0.534483026894369
Loss in iteration 13 : 0.5255785350520521
Loss in iteration 14 : 0.5189677772675684
Loss in iteration 15 : 0.5133507620403778
Loss in iteration 16 : 0.5077730474309896
Loss in iteration 17 : 0.5029468683019238
Loss in iteration 18 : 0.49938661113557953
Loss in iteration 19 : 0.4964605902005354
Loss in iteration 20 : 0.4936255033307402
Loss in iteration 21 : 0.49116544677620905
Loss in iteration 22 : 0.48938904235589953
Loss in iteration 23 : 0.487988985672868
Loss in iteration 24 : 0.4865521378422369
Loss in iteration 25 : 0.4851361257128673
Loss in iteration 26 : 0.4839945646411786
Loss in iteration 27 : 0.4830854281764044
Loss in iteration 28 : 0.4821689997158196
Loss in iteration 29 : 0.481200220384266
Loss in iteration 30 : 0.48034603784503727
Loss in iteration 31 : 0.47968059690972276
Loss in iteration 32 : 0.4790830692799862
Loss in iteration 33 : 0.47844566415355994
Loss in iteration 34 : 0.47781442723104933
Loss in iteration 35 : 0.47726935803604875
Loss in iteration 36 : 0.4767813872143499
Loss in iteration 37 : 0.4762662801261939
Loss in iteration 38 : 0.4757148237324999
Loss in iteration 39 : 0.47518976170241145
Loss in iteration 40 : 0.4747195723508378
Loss in iteration 41 : 0.4742654838886329
Loss in iteration 42 : 0.4737947317987292
Loss in iteration 43 : 0.4733294710793324
Loss in iteration 44 : 0.47290326608388217
Loss in iteration 45 : 0.47250827398316697
Loss in iteration 46 : 0.4721137578976585
Loss in iteration 47 : 0.4717146449144122
Loss in iteration 48 : 0.47133244283194337
Loss in iteration 49 : 0.4709769443430161
Loss in iteration 50 : 0.4706337082395412
Loss in iteration 51 : 0.4702909783750141
Loss in iteration 52 : 0.46995783960609677
Loss in iteration 53 : 0.46964766327768254
Loss in iteration 54 : 0.46935796243804984
Loss in iteration 55 : 0.469077294633173
Loss in iteration 56 : 0.4688035173753963
Loss in iteration 57 : 0.46854391913180393
Loss in iteration 58 : 0.4683008752343625
Loss in iteration 59 : 0.46806780643056994
Loss in iteration 60 : 0.46783978607660653
Loss in iteration 61 : 0.4676199503102302
Loss in iteration 62 : 0.4674127015063678
Loss in iteration 63 : 0.46721646507667713
Loss in iteration 64 : 0.46702701403432617
Loss in iteration 65 : 0.46684416364762904
Loss in iteration 66 : 0.4666708860252705
Loss in iteration 67 : 0.4665076173275226
Loss in iteration 68 : 0.4663514230929413
Loss in iteration 69 : 0.4662004298433914
Loss in iteration 70 : 0.46605574815578066
Loss in iteration 71 : 0.46591844128405363
Loss in iteration 72 : 0.46578721800895445
Loss in iteration 73 : 0.4656603309987978
Loss in iteration 74 : 0.4655379458944765
Loss in iteration 75 : 0.46542116739120193
Loss in iteration 76 : 0.4653098013181067
Loss in iteration 77 : 0.4652025329440185
Loss in iteration 78 : 0.4650987831178567
Loss in iteration 79 : 0.46499903872842835
Loss in iteration 80 : 0.46490343993060596
Loss in iteration 81 : 0.4648111861125677
Loss in iteration 82 : 0.4647215710963396
Loss in iteration 83 : 0.46463473969570457
Loss in iteration 84 : 0.4645510238987145
Loss in iteration 85 : 0.46447014654824487
Loss in iteration 86 : 0.4643915572471262
Loss in iteration 87 : 0.4643151589550934
Loss in iteration 88 : 0.4642412015123047
Loss in iteration 89 : 0.46416965012062733
Loss in iteration 90 : 0.46410011316669164
Loss in iteration 91 : 0.46403234361379864
Loss in iteration 92 : 0.46396642515990044
Loss in iteration 93 : 0.46390241212468786
Loss in iteration 94 : 0.463840094890464
Loss in iteration 95 : 0.46377924272970694
Loss in iteration 96 : 0.4637198562517118
Loss in iteration 97 : 0.46366202612600965
Loss in iteration 98 : 0.46360568558873366
Loss in iteration 99 : 0.46355066020889346
Loss in iteration 100 : 0.46349687894483216
Loss in iteration 101 : 0.4634443835419089
Loss in iteration 102 : 0.4633931594646086
Loss in iteration 103 : 0.4633430897858694
Loss in iteration 104 : 0.46329408424029533
Loss in iteration 105 : 0.46324614886033
Loss in iteration 106 : 0.4631992974323406
Loss in iteration 107 : 0.46315347299267956
Loss in iteration 108 : 0.4631086001345174
Loss in iteration 109 : 0.4630646598676539
Loss in iteration 110 : 0.4630216634918676
Loss in iteration 111 : 0.462979584781209
Loss in iteration 112 : 0.46293836475759736
Loss in iteration 113 : 0.46289796745127765
Loss in iteration 114 : 0.4628583890497372
Loss in iteration 115 : 0.46281961555028267
Loss in iteration 116 : 0.4627816065623325
Loss in iteration 117 : 0.46274432638346163
Loss in iteration 118 : 0.46270776408915654
Loss in iteration 119 : 0.46267191262276797
Loss in iteration 120 : 0.4626367474735669
Loss in iteration 121 : 0.46260223826804936
Loss in iteration 122 : 0.46256836797141865
Loss in iteration 123 : 0.46253512750209713
Loss in iteration 124 : 0.4625024992563518
Loss in iteration 125 : 0.4624704581966738
Loss in iteration 126 : 0.46243898549625373
Loss in iteration 127 : 0.4624080705688683
Loss in iteration 128 : 0.4623777005573264
Loss in iteration 129 : 0.46234785657164196
Loss in iteration 130 : 0.4623185213143024
Loss in iteration 131 : 0.4622896834908114
Loss in iteration 132 : 0.4622613324145412
Loss in iteration 133 : 0.4622334533111627
Loss in iteration 134 : 0.46220603068123567
Loss in iteration 135 : 0.4621790526678721
Loss in iteration 136 : 0.4621525091700464
Loss in iteration 137 : 0.46212638803803147
Loss in iteration 138 : 0.46210067599232524
Loss in iteration 139 : 0.4620753619275927
Loss in iteration 140 : 0.46205043676458957
Loss in iteration 141 : 0.4620258906988564
Loss in iteration 142 : 0.46200171276446167
Loss in iteration 143 : 0.46197789292088887
Loss in iteration 144 : 0.46195442271253245
Loss in iteration 145 : 0.46193129364905716
Loss in iteration 146 : 0.4619084963829109
Loss in iteration 147 : 0.46188602190105416
Loss in iteration 148 : 0.4618638624077655
Loss in iteration 149 : 0.46184201049570295
Loss in iteration 150 : 0.4618204582732464
Loss in iteration 151 : 0.4617991978870865
Loss in iteration 152 : 0.4617782223121192
Loss in iteration 153 : 0.4617575250321431
Loss in iteration 154 : 0.4617370993098382
Loss in iteration 155 : 0.46171693830253846
Loss in iteration 156 : 0.46169703566278913
Loss in iteration 157 : 0.4616773855283876
Loss in iteration 158 : 0.46165798201711916
Loss in iteration 159 : 0.46163881914857574
Loss in iteration 160 : 0.4616198912478719
Loss in iteration 161 : 0.4616011930737234
Loss in iteration 162 : 0.4615827194973905
Loss in iteration 163 : 0.46156446533036893
Loss in iteration 164 : 0.46154642554568603
Loss in iteration 165 : 0.4615285954419881
Loss in iteration 166 : 0.4615109704757586
Loss in iteration 167 : 0.4614935460892425
Loss in iteration 168 : 0.4614763178135442
Loss in iteration 169 : 0.46145928142115583
Loss in iteration 170 : 0.46144243286034964
Loss in iteration 171 : 0.46142576811391195
Loss in iteration 172 : 0.461409283225149
Loss in iteration 173 : 0.46139297441230315
Loss in iteration 174 : 0.4613768380589558
Loss in iteration 175 : 0.46136087061193193
Loss in iteration 176 : 0.46134506856616014
Loss in iteration 177 : 0.46132942853892495
Loss in iteration 178 : 0.46131394728719677
Loss in iteration 179 : 0.46129862164431457
Loss in iteration 180 : 0.46128344849216185
Loss in iteration 181 : 0.4612684248051822
Loss in iteration 182 : 0.4612535476759679
Loss in iteration 183 : 0.46123881427913915
Loss in iteration 184 : 0.4612242218409401
Loss in iteration 185 : 0.4612097676596329
Loss in iteration 186 : 0.4611954491283814
Loss in iteration 187 : 0.4611812637173731
Loss in iteration 188 : 0.46116720894876484
Loss in iteration 189 : 0.46115328240423287
Loss in iteration 190 : 0.46113948174298386
Loss in iteration 191 : 0.4611258046947573
Loss in iteration 192 : 0.461112249041207
Loss in iteration 193 : 0.46109881261619234
Loss in iteration 194 : 0.46108549331797716
Loss in iteration 195 : 0.4610722891075615
Loss in iteration 196 : 0.4610591979956251
Loss in iteration 197 : 0.46104621803939344
Loss in iteration 198 : 0.4610333473499835
Loss in iteration 199 : 0.4610205840930208
Loss in iteration 200 : 0.461007926480121
Loss in iteration 201 : 0.4609953727649704
Loss in iteration 202 : 0.4609829212475165
Loss in iteration 203 : 0.4609705702753836
Loss in iteration 204 : 0.46095831823843775
Loss in iteration 205 : 0.4609461635648834
Loss in iteration 206 : 0.460934104723128
Loss in iteration 207 : 0.4609221402230396
Loss in iteration 208 : 0.4609102686124896
Loss in iteration 209 : 0.46089848847402903
Loss in iteration 210 : 0.4608867984254377
Loss in iteration 211 : 0.46087519712067876
Loss in iteration 212 : 0.4608636832477665
Loss in iteration 213 : 0.46085225552609205
Loss in iteration 214 : 0.46084091270628497
Loss in iteration 215 : 0.46082965357079103
Loss in iteration 216 : 0.4608184769324892
Loss in iteration 217 : 0.4608073816325739
Loss in iteration 218 : 0.46079636653999967
Loss in iteration 219 : 0.4607854305516893
Loss in iteration 220 : 0.4607745725915938
Loss in iteration 221 : 0.4607637916090771
Loss in iteration 222 : 0.46075308657823794
Loss in iteration 223 : 0.4607424564979327
Loss in iteration 224 : 0.4607319003910919
Loss in iteration 225 : 0.460721417303472
Loss in iteration 226 : 0.4607110063029561
Loss in iteration 227 : 0.46070066647936714
Loss in iteration 228 : 0.46069039694393915
Loss in iteration 229 : 0.46068019682833883
Loss in iteration 230 : 0.460670065284008
Loss in iteration 231 : 0.46066000148190145
Loss in iteration 232 : 0.46065000461203703
Loss in iteration 233 : 0.460640073882736
Loss in iteration 234 : 0.4606302085200109
Loss in iteration 235 : 0.4606204077672769
Loss in iteration 236 : 0.46061067088493246
Loss in iteration 237 : 0.46060099714976516
Loss in iteration 238 : 0.46059138585440873
Loss in iteration 239 : 0.46058183630702215
Loss in iteration 240 : 0.46057234783093004
Loss in iteration 241 : 0.4605629197641119
Loss in iteration 242 : 0.46055355145874466
Loss in iteration 243 : 0.46054424228087004
Loss in iteration 244 : 0.4605349916100797
Loss in iteration 245 : 0.46052579883908684
Loss in iteration 246 : 0.4605166633733224
Loss in iteration 247 : 0.46050758463063246
Loss in iteration 248 : 0.46049856204098094
Loss in iteration 249 : 0.46048959504608405
Loss in iteration 250 : 0.4604806830990624
Loss in iteration 251 : 0.46047182566414535
Loss in iteration 252 : 0.460463022216415
Loss in iteration 253 : 0.46045427224148067
Loss in iteration 254 : 0.46044557523517765
Loss in iteration 255 : 0.46043693070330244
Testing accuracy  of updater 6 on alg 0 with rate 0.2 = 0.78825, training accuracy 0.78825, time elapsed: 2610 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6909619287669748
Loss in iteration 3 : 0.6873083960659883
Loss in iteration 4 : 0.6830109544497363
Loss in iteration 5 : 0.6787077010604758
Loss in iteration 6 : 0.6746812285863939
Loss in iteration 7 : 0.6708733111756111
Loss in iteration 8 : 0.6670343109273111
Loss in iteration 9 : 0.662907604872203
Loss in iteration 10 : 0.6583627362658376
Loss in iteration 11 : 0.6534391221978813
Loss in iteration 12 : 0.6483083115570605
Loss in iteration 13 : 0.6431900155366559
Loss in iteration 14 : 0.6382645414146811
Loss in iteration 15 : 0.6336162162831985
Loss in iteration 16 : 0.6292242142603757
Loss in iteration 17 : 0.6249958761878377
Loss in iteration 18 : 0.6208214468341895
Loss in iteration 19 : 0.6166242985408071
Loss in iteration 20 : 0.6123875933069505
Loss in iteration 21 : 0.6081517966939464
Loss in iteration 22 : 0.6039903260666445
Loss in iteration 23 : 0.5999776128652727
Loss in iteration 24 : 0.5961634930713167
Loss in iteration 25 : 0.5925620707169035
Loss in iteration 26 : 0.589155713592355
Loss in iteration 27 : 0.5859090441678866
Loss in iteration 28 : 0.5827854549331511
Loss in iteration 29 : 0.5797596983412663
Loss in iteration 30 : 0.5768231356868255
Loss in iteration 31 : 0.5739816341402573
Loss in iteration 32 : 0.5712486253145626
Loss in iteration 33 : 0.5686368937333557
Loss in iteration 34 : 0.5661522802820173
Loss in iteration 35 : 0.563791124252381
Loss in iteration 36 : 0.5615415855058684
Loss in iteration 37 : 0.5593876114501118
Loss in iteration 38 : 0.5573136473941472
Loss in iteration 39 : 0.5553083198915931
Loss in iteration 40 : 0.5533660423036013
Loss in iteration 41 : 0.5514864250013475
Loss in iteration 42 : 0.5496721469651615
Loss in iteration 43 : 0.5479263259370692
Loss in iteration 44 : 0.5462503671640238
Loss in iteration 45 : 0.5446428951422244
Loss in iteration 46 : 0.5430998794530838
Loss in iteration 47 : 0.5416156476830097
Loss in iteration 48 : 0.5401842566099535
Loss in iteration 49 : 0.5388006940020978
Loss in iteration 50 : 0.5374615550057292
Loss in iteration 51 : 0.5361650846249927
Loss in iteration 52 : 0.5349107051475073
Loss in iteration 53 : 0.533698287268679
Loss in iteration 54 : 0.5325274513798903
Loss in iteration 55 : 0.5313971165285928
Loss in iteration 56 : 0.5303053910593437
Loss in iteration 57 : 0.5292497711132398
Loss in iteration 58 : 0.5282275221805185
Loss in iteration 59 : 0.5272360859618209
Loss in iteration 60 : 0.5262733784724443
Loss in iteration 61 : 0.5253379065658228
Loss in iteration 62 : 0.5244287017901172
Loss in iteration 63 : 0.5235451279042915
Loss in iteration 64 : 0.5226866463375158
Loss in iteration 65 : 0.521852619718413
Loss in iteration 66 : 0.5210422053393393
Loss in iteration 67 : 0.5202543519224684
Loss in iteration 68 : 0.519487878426521
Loss in iteration 69 : 0.518741592768492
Loss in iteration 70 : 0.5180144047618935
Loss in iteration 71 : 0.5173053987551386
Loss in iteration 72 : 0.5166138508829774
Loss in iteration 73 : 0.515939195882335
Loss in iteration 74 : 0.5152809628569787
Loss in iteration 75 : 0.5146387049833949
Loss in iteration 76 : 0.5140119450439984
Loss in iteration 77 : 0.5134001495672216
Loss in iteration 78 : 0.5128027332126779
Loss in iteration 79 : 0.5122190856348746
Loss in iteration 80 : 0.5116486078757501
Loss in iteration 81 : 0.5110907450862205
Loss in iteration 82 : 0.5105450061432
Loss in iteration 83 : 0.5100109665700743
Loss in iteration 84 : 0.509488256912266
Loss in iteration 85 : 0.5089765426611664
Loss in iteration 86 : 0.5084755031417836
Loss in iteration 87 : 0.507984815616072
Loss in iteration 88 : 0.5075041480558925
Loss in iteration 89 : 0.5070331607733098
Loss in iteration 90 : 0.5065715144297565
Loss in iteration 91 : 0.5061188805515896
Loss in iteration 92 : 0.5056749507149736
Loss in iteration 93 : 0.5052394417293724
Loss in iteration 94 : 0.5048120958732004
Loss in iteration 95 : 0.5043926768957631
Loss in iteration 96 : 0.503980963618736
Loss in iteration 97 : 0.5035767433264702
Loss in iteration 98 : 0.5031798067744643
Loss in iteration 99 : 0.5027899458180061
Loss in iteration 100 : 0.5024069537043373
Loss in iteration 101 : 0.5020306272896048
Loss in iteration 102 : 0.5016607700308413
Loss in iteration 103 : 0.5012971946103516
Loss in iteration 104 : 0.5009397243901621
Loss in iteration 105 : 0.5005881934016935
Loss in iteration 106 : 0.5002424450689096
Loss in iteration 107 : 0.4999023302001201
Loss in iteration 108 : 0.4995677048969933
Loss in iteration 109 : 0.49923842893027137
Loss in iteration 110 : 0.4989143648913408
Loss in iteration 111 : 0.4985953781454614
Loss in iteration 112 : 0.4982813373773047
Loss in iteration 113 : 0.4979721153915979
Loss in iteration 114 : 0.4976675898271438
Loss in iteration 115 : 0.49736764353822377
Loss in iteration 116 : 0.4970721645456407
Loss in iteration 117 : 0.49678104560709924
Loss in iteration 118 : 0.4964941835602242
Loss in iteration 119 : 0.49621147863016174
Loss in iteration 120 : 0.49593283386868725
Loss in iteration 121 : 0.49565815482281894
Loss in iteration 122 : 0.49538734944709495
Loss in iteration 123 : 0.4951203282016564
Loss in iteration 124 : 0.4948570042372213
Loss in iteration 125 : 0.4945972935633986
Loss in iteration 126 : 0.4943411151231181
Loss in iteration 127 : 0.49408839073940386
Loss in iteration 128 : 0.4938390449454826
Loss in iteration 129 : 0.4935930047420502
Loss in iteration 130 : 0.49335019933927876
Loss in iteration 131 : 0.4931105599355553
Loss in iteration 132 : 0.4928740195653902
Loss in iteration 133 : 0.49264051302381096
Loss in iteration 134 : 0.4924099768521883
Loss in iteration 135 : 0.49218234935667343
Loss in iteration 136 : 0.4919575706277473
Loss in iteration 137 : 0.4917355825361383
Loss in iteration 138 : 0.4915163286929719
Loss in iteration 139 : 0.49129975437563383
Loss in iteration 140 : 0.4910858064312347
Loss in iteration 141 : 0.4908744331745585
Loss in iteration 142 : 0.49066558429638896
Loss in iteration 143 : 0.4904592107927011
Loss in iteration 144 : 0.490255264917711
Loss in iteration 145 : 0.49005370015693095
Loss in iteration 146 : 0.48985447121169723
Loss in iteration 147 : 0.48965753398554146
Loss in iteration 148 : 0.48946284556443764
Loss in iteration 149 : 0.48927036418666137
Loss in iteration 150 : 0.48908004920220094
Loss in iteration 151 : 0.48889186102505344
Loss in iteration 152 : 0.4887057610835186
Loss in iteration 153 : 0.48852171177353787
Loss in iteration 154 : 0.48833967641869935
Loss in iteration 155 : 0.4881596192381977
Loss in iteration 156 : 0.48798150532194107
Loss in iteration 157 : 0.4878053006103288
Loss in iteration 158 : 0.48763097187578375
Loss in iteration 159 : 0.4874584867034184
Loss in iteration 160 : 0.4872878134693193
Loss in iteration 161 : 0.4871189213161576
Loss in iteration 162 : 0.4869517801269879
Loss in iteration 163 : 0.48678636049864216
Loss in iteration 164 : 0.4866226337163121
Loss in iteration 165 : 0.4864605717304195
Loss in iteration 166 : 0.48630014713628894
Loss in iteration 167 : 0.48614133315637714
Loss in iteration 168 : 0.48598410362439837
Loss in iteration 169 : 0.4858284329703573
Loss in iteration 170 : 0.48567429620567326
Loss in iteration 171 : 0.4855216689078411
Loss in iteration 172 : 0.4853705272044483
Loss in iteration 173 : 0.4852208477567965
Loss in iteration 174 : 0.4850726077435166
Loss in iteration 175 : 0.484925784844712
Loss in iteration 176 : 0.4847803572269653
Loss in iteration 177 : 0.4846363035294563
Loss in iteration 178 : 0.48449360285113025
Loss in iteration 179 : 0.4843522347387326
Loss in iteration 180 : 0.4842121791754256
Loss in iteration 181 : 0.4840734165697099
Loss in iteration 182 : 0.4839359277444348
Loss in iteration 183 : 0.483799693925845
Loss in iteration 184 : 0.48366469673268525
Loss in iteration 185 : 0.48353091816547306
Loss in iteration 186 : 0.48339834059608083
Loss in iteration 187 : 0.4832669467577641
Loss in iteration 188 : 0.483136719735661
Loss in iteration 189 : 0.4830076429577914
Loss in iteration 190 : 0.4828797001864772
Loss in iteration 191 : 0.48275287551009055
Loss in iteration 192 : 0.48262715333502537
Loss in iteration 193 : 0.482502518377844
Loss in iteration 194 : 0.4823789556575409
Loss in iteration 195 : 0.4822564504879261
Loss in iteration 196 : 0.48213498847017183
Loss in iteration 197 : 0.48201455548555633
Loss in iteration 198 : 0.4818951376884305
Loss in iteration 199 : 0.48177672149946044
Loss in iteration 200 : 0.48165929359911597
Loss in iteration 201 : 0.48154284092140276
Loss in iteration 202 : 0.48142735064780257
Loss in iteration 203 : 0.4813128102013991
Loss in iteration 204 : 0.48119920724114734
Loss in iteration 205 : 0.4810865296562808
Loss in iteration 206 : 0.48097476556084645
Loss in iteration 207 : 0.48086390328838013
Loss in iteration 208 : 0.48075393138671463
Loss in iteration 209 : 0.4806448386129531
Loss in iteration 210 : 0.4805366139285827
Loss in iteration 211 : 0.48042924649475865
Loss in iteration 212 : 0.4803227256677237
Loss in iteration 213 : 0.48021704099436524
Loss in iteration 214 : 0.4801121822078974
Loss in iteration 215 : 0.48000813922364866
Loss in iteration 216 : 0.47990490213495474
Loss in iteration 217 : 0.47980246120915115
Loss in iteration 218 : 0.47970080688366223
Loss in iteration 219 : 0.47959992976219096
Loss in iteration 220 : 0.4794998206110129
Loss in iteration 221 : 0.4794004703553603
Loss in iteration 222 : 0.47930187007593006
Loss in iteration 223 : 0.4792040110054608
Loss in iteration 224 : 0.47910688452542743
Loss in iteration 225 : 0.47901048216281245
Loss in iteration 226 : 0.4789147955869584
Loss in iteration 227 : 0.4788198166065071
Loss in iteration 228 : 0.4787255371664048
Loss in iteration 229 : 0.4786319493449936
Loss in iteration 230 : 0.4785390453511709
Loss in iteration 231 : 0.4784468175216177
Loss in iteration 232 : 0.47835525831810505
Loss in iteration 233 : 0.4782643603248635
Loss in iteration 234 : 0.47817411624602735
Loss in iteration 235 : 0.4780845189031404
Loss in iteration 236 : 0.4779955612327209
Loss in iteration 237 : 0.4779072362838911
Loss in iteration 238 : 0.4778195372160596
Loss in iteration 239 : 0.47773245729666425
Loss in iteration 240 : 0.4776459898989691
Loss in iteration 241 : 0.47756012849989665
Loss in iteration 242 : 0.4774748666779481
Loss in iteration 243 : 0.4773901981111402
Loss in iteration 244 : 0.4773061165750049
Loss in iteration 245 : 0.47722261594064436
Loss in iteration 246 : 0.4771396901728191
Loss in iteration 247 : 0.4770573333280965
Loss in iteration 248 : 0.4769755395530309
Loss in iteration 249 : 0.47689430308239444
Loss in iteration 250 : 0.47681361823744406
Loss in iteration 251 : 0.47673347942423183
Loss in iteration 252 : 0.47665388113195856
Loss in iteration 253 : 0.47657481793135764
Loss in iteration 254 : 0.4764962844731211
Loss in iteration 255 : 0.4764182754863616
Loss in iteration 256 : 0.47634078577711336
Loss in iteration 257 : 0.47626381022685793
Loss in iteration 258 : 0.47618734379109706
Loss in iteration 259 : 0.4761113814979418
Loss in iteration 260 : 0.4760359184467551
Loss in iteration 261 : 0.4759609498067996
Loss in iteration 262 : 0.47588647081593577
Loss in iteration 263 : 0.47581247677933936
Loss in iteration 264 : 0.47573896306824837
Loss in iteration 265 : 0.4756659251187446
Loss in iteration 266 : 0.4755933584305494
Loss in iteration 267 : 0.47552125856585836
Loss in iteration 268 : 0.4754496211481987
Loss in iteration 269 : 0.4753784418613031
Loss in iteration 270 : 0.4753077164480259
Loss in iteration 271 : 0.4752374407092596
Loss in iteration 272 : 0.47516761050289824
Loss in iteration 273 : 0.47509822174280664
Loss in iteration 274 : 0.4750292703978199
Loss in iteration 275 : 0.47496075249076497
Loss in iteration 276 : 0.4748926640974936
Loss in iteration 277 : 0.47482500134595146
Loss in iteration 278 : 0.47475776041525103
Loss in iteration 279 : 0.4746909375347805
Loss in iteration 280 : 0.47462452898331176
Loss in iteration 281 : 0.47455853108814955
Loss in iteration 282 : 0.47449294022428
Loss in iteration 283 : 0.4744277528135481
Loss in iteration 284 : 0.4743629653238471
Loss in iteration 285 : 0.47429857426832034
Loss in iteration 286 : 0.47423457620459875
Loss in iteration 287 : 0.47417096773402745
Loss in iteration 288 : 0.47410774550093143
Loss in iteration 289 : 0.47404490619187656
Loss in iteration 290 : 0.4739824465349664
Loss in iteration 291 : 0.4739203632991349
Loss in iteration 292 : 0.4738586532934627
Loss in iteration 293 : 0.4737973133665129
Loss in iteration 294 : 0.47373634040565826
Loss in iteration 295 : 0.4736757313364558
Loss in iteration 296 : 0.4736154831219991
Loss in iteration 297 : 0.47355559276230674
Loss in iteration 298 : 0.47349605729371536
Loss in iteration 299 : 0.4734368737882894
Loss in iteration 300 : 0.47337803935322825
Loss in iteration 301 : 0.4733195511303037
Loss in iteration 302 : 0.47326140629529817
Loss in iteration 303 : 0.473203602057453
Loss in iteration 304 : 0.47314613565893765
Loss in iteration 305 : 0.47308900437431284
Loss in iteration 306 : 0.4730322055100194
Loss in iteration 307 : 0.472975736403872
Loss in iteration 308 : 0.47291959442455817
Loss in iteration 309 : 0.47286377697115073
Loss in iteration 310 : 0.472808281472632
Loss in iteration 311 : 0.47275310538742504
Loss in iteration 312 : 0.47269824620292944
Loss in iteration 313 : 0.47264370143507334
Loss in iteration 314 : 0.4725894686278717
Loss in iteration 315 : 0.47253554535298503
Loss in iteration 316 : 0.4724819292093055
Loss in iteration 317 : 0.4724286178225261
Loss in iteration 318 : 0.47237560884473667
Loss in iteration 319 : 0.47232289995402377
Loss in iteration 320 : 0.47227048885407114
Loss in iteration 321 : 0.47221837327377186
Loss in iteration 322 : 0.472166550966852
Loss in iteration 323 : 0.472115019711493
Loss in iteration 324 : 0.47206377730996735
Loss in iteration 325 : 0.47201282158828245
Loss in iteration 326 : 0.4719621503958174
Loss in iteration 327 : 0.4719117616049881
Loss in iteration 328 : 0.47186165311089784
Loss in iteration 329 : 0.4718118228310118
Loss in iteration 330 : 0.4717622687048196
Loss in iteration 331 : 0.4717129886935188
Loss in iteration 332 : 0.4716639807797003
Loss in iteration 333 : 0.4716152429670333
Loss in iteration 334 : 0.4715667732799646
Loss in iteration 335 : 0.47151856976341106
Loss in iteration 336 : 0.47147063048247906
Loss in iteration 337 : 0.4714229535221624
Loss in iteration 338 : 0.4713755369870654
Loss in iteration 339 : 0.47132837900112706
Loss in iteration 340 : 0.4712814777073351
Loss in iteration 341 : 0.47123483126746996
Loss in iteration 342 : 0.47118843786183595
Loss in iteration 343 : 0.4711422956889977
Loss in iteration 344 : 0.4710964029655339
Loss in iteration 345 : 0.4710507579257762
Loss in iteration 346 : 0.47100535882156824
Loss in iteration 347 : 0.4709602039220318
Loss in iteration 348 : 0.4709152915133069
Loss in iteration 349 : 0.4708706198983459
Loss in iteration 350 : 0.47082618739665927
Loss in iteration 351 : 0.47078199234410756
Loss in iteration 352 : 0.4707380330926698
Loss in iteration 353 : 0.47069430801022544
Loss in iteration 354 : 0.47065081548034565
Loss in iteration 355 : 0.4706075539020802
Loss in iteration 356 : 0.47056452168974694
Loss in iteration 357 : 0.470521717272731
Loss in iteration 358 : 0.47047913909528644
Loss in iteration 359 : 0.4704367856163329
Loss in iteration 360 : 0.47039465530927294
Loss in iteration 361 : 0.4703527466617872
Loss in iteration 362 : 0.4703110581756598
Loss in iteration 363 : 0.4702695883665856
Loss in iteration 364 : 0.4702283357639994
Loss in iteration 365 : 0.47018729891088534
Loss in iteration 366 : 0.4701464763636086
Loss in iteration 367 : 0.4701058666917427
Loss in iteration 368 : 0.47006546847790287
Loss in iteration 369 : 0.4700252803175706
Loss in iteration 370 : 0.4699853008189393
Loss in iteration 371 : 0.4699455286027467
Loss in iteration 372 : 0.4699059623021222
Loss in iteration 373 : 0.46986660056242047
Loss in iteration 374 : 0.4698274420410807
Loss in iteration 375 : 0.4697884854074647
Loss in iteration 376 : 0.4697497293427148
Loss in iteration 377 : 0.46971117253960487
Loss in iteration 378 : 0.46967281370239633
Loss in iteration 379 : 0.4696346515466987
Loss in iteration 380 : 0.4695966847993242
Loss in iteration 381 : 0.46955891219815704
Loss in iteration 382 : 0.46952133249201666
Loss in iteration 383 : 0.46948394444052427
Loss in iteration 384 : 0.4694467468139667
Loss in iteration 385 : 0.4694097383931803
Loss in iteration 386 : 0.4693729179694073
Loss in iteration 387 : 0.46933628434418784
Loss in iteration 388 : 0.4692998363292272
Loss in iteration 389 : 0.4692635727462712
Loss in iteration 390 : 0.46922749242699835
Loss in iteration 391 : 0.4691915942128966
Loss in iteration 392 : 0.4691558769551449
Loss in iteration 393 : 0.46912033951450377
Loss in iteration 394 : 0.46908498076120314
Loss in iteration 395 : 0.46904979957482684
Loss in iteration 396 : 0.4690147948442147
Loss in iteration 397 : 0.4689799654673422
Testing accuracy  of updater 6 on alg 0 with rate 0.01999999999999999 = 0.785625, training accuracy 0.785625, time elapsed: 4978 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 20.90243994357049
Loss in iteration 3 : 12.517217681051934
Loss in iteration 4 : 12.194135457128455
Loss in iteration 5 : 6.689634168295119
Loss in iteration 6 : 7.672148955668924
Loss in iteration 7 : 2.987619417986647
Loss in iteration 8 : 6.708088140274809
Loss in iteration 9 : 5.70103792037505
Loss in iteration 10 : 5.415723511937633
Loss in iteration 11 : 5.344570548341493
Loss in iteration 12 : 3.591161506583002
Loss in iteration 13 : 3.7273300847818045
Loss in iteration 14 : 4.35727417140801
Loss in iteration 15 : 3.757872133469872
Loss in iteration 16 : 3.4722586201249435
Loss in iteration 17 : 3.4423614117700443
Loss in iteration 18 : 3.2277178660685313
Loss in iteration 19 : 3.4742633290523526
Loss in iteration 20 : 3.602575932295087
Loss in iteration 21 : 3.0316464418226485
Loss in iteration 22 : 2.4875756558645046
Loss in iteration 23 : 2.5832205040610834
Loss in iteration 24 : 2.5967153893571377
Loss in iteration 25 : 2.5996784612260293
Loss in iteration 26 : 2.652912795718911
Loss in iteration 27 : 2.2691396241660224
Loss in iteration 28 : 2.1724366603116367
Loss in iteration 29 : 2.1653741279886525
Loss in iteration 30 : 1.9281047838589485
Loss in iteration 31 : 1.9303509436778028
Loss in iteration 32 : 1.7322635755721634
Loss in iteration 33 : 1.7607365968527404
Loss in iteration 34 : 1.732396943023086
Loss in iteration 35 : 1.6080456779959196
Loss in iteration 36 : 1.5851269284250156
Loss in iteration 37 : 1.4452720524539586
Loss in iteration 38 : 1.4200118073928651
Loss in iteration 39 : 1.316743122856346
Loss in iteration 40 : 1.3867726880473403
Loss in iteration 41 : 1.3099323016930726
Loss in iteration 42 : 1.2495815452936414
Loss in iteration 43 : 1.1697296380618032
Loss in iteration 44 : 1.1675372948839136
Loss in iteration 45 : 1.1683680037844697
Loss in iteration 46 : 1.047534529102782
Loss in iteration 47 : 1.0894958662398515
Loss in iteration 48 : 1.0388372150022684
Loss in iteration 49 : 0.9638454012953593
Loss in iteration 50 : 0.9479003879314478
Loss in iteration 51 : 0.9554563399452397
Loss in iteration 52 : 0.9242441820329391
Loss in iteration 53 : 0.8573120597244169
Loss in iteration 54 : 0.824740950680018
Loss in iteration 55 : 0.8005714799016787
Loss in iteration 56 : 0.7376934159102673
Loss in iteration 57 : 0.7221141220825767
Loss in iteration 58 : 0.691544882123159
Loss in iteration 59 : 0.6253458107613833
Loss in iteration 60 : 0.6421145455262065
Loss in iteration 61 : 0.6291511466708095
Loss in iteration 62 : 0.7496253075573628
Loss in iteration 63 : 1.6704611635906828
Loss in iteration 64 : 3.358499936856498
Loss in iteration 65 : 0.9142584208748763
Loss in iteration 66 : 1.276803236540182
Loss in iteration 67 : 2.446395668525969
Loss in iteration 68 : 0.7937994938656753
Loss in iteration 69 : 1.462499285866225
Loss in iteration 70 : 1.6372053860313127
Loss in iteration 71 : 0.886356607350463
Loss in iteration 72 : 1.6925698511940268
Loss in iteration 73 : 1.0252796077673108
Loss in iteration 74 : 1.151973197073933
Loss in iteration 75 : 1.302334693231502
Loss in iteration 76 : 0.8285261031511089
Loss in iteration 77 : 1.167515707226244
Loss in iteration 78 : 0.8983001881212371
Loss in iteration 79 : 0.8008382757569359
Loss in iteration 80 : 1.0540645360437293
Loss in iteration 81 : 0.819370888211828
Loss in iteration 82 : 0.6880921888213568
Loss in iteration 83 : 0.858245388472199
Loss in iteration 84 : 0.8599372072467595
Loss in iteration 85 : 0.7527240631084251
Loss in iteration 86 : 0.6583496299666521
Loss in iteration 87 : 0.5819799404488205
Loss in iteration 88 : 0.5919833472640796
Loss in iteration 89 : 0.5737233406817152
Loss in iteration 90 : 0.6973170482413045
Loss in iteration 91 : 1.2641989614089788
Loss in iteration 92 : 2.5793622659077196
Loss in iteration 93 : 1.8346680942321463
Loss in iteration 94 : 0.5761139012964273
Loss in iteration 95 : 1.1031263508944171
Loss in iteration 96 : 1.958989041736537
Loss in iteration 97 : 0.995328856379911
Loss in iteration 98 : 0.7916960000102652
Loss in iteration 99 : 0.9267630072410523
Loss in iteration 100 : 1.1073695743602283
Loss in iteration 101 : 0.9107166556817072
Loss in iteration 102 : 0.6621463279183476
Loss in iteration 103 : 0.8507593237954002
Loss in iteration 104 : 0.906696930441055
Loss in iteration 105 : 0.7243616719523978
Loss in iteration 106 : 0.8461176810583378
Loss in iteration 107 : 0.623440632418968
Loss in iteration 108 : 0.7494277717659045
Loss in iteration 109 : 0.6718157682520652
Loss in iteration 110 : 0.6603502844572438
Loss in iteration 111 : 0.7187100586774814
Loss in iteration 112 : 0.6751762691878376
Loss in iteration 113 : 0.8405246277630141
Loss in iteration 114 : 0.942411207185429
Loss in iteration 115 : 1.1495912946806381
Loss in iteration 116 : 1.6147991671781696
Loss in iteration 117 : 0.8745425604290916
Loss in iteration 118 : 1.0994300728711683
Loss in iteration 119 : 0.8050017673170308
Loss in iteration 120 : 0.9686046297017763
Loss in iteration 121 : 1.0362656464261621
Loss in iteration 122 : 1.2271784651417839
Loss in iteration 123 : 1.2839028185043657
Loss in iteration 124 : 1.2088924252064095
Loss in iteration 125 : 0.7070888538924108
Loss in iteration 126 : 0.7536528604709505
Loss in iteration 127 : 0.6172881346306924
Loss in iteration 128 : 0.6398888150195312
Loss in iteration 129 : 0.6596425911554759
Loss in iteration 130 : 0.5896858604958891
Loss in iteration 131 : 0.743075401277088
Loss in iteration 132 : 0.8672114957884637
Loss in iteration 133 : 1.5820561057012976
Loss in iteration 134 : 1.6016789871202741
Loss in iteration 135 : 1.0119765444265283
Loss in iteration 136 : 0.7145223138650374
Loss in iteration 137 : 0.6269770724449482
Loss in iteration 138 : 0.5895569439363227
Loss in iteration 139 : 0.6411606129593618
Loss in iteration 140 : 0.6750734218425136
Loss in iteration 141 : 0.9491248645298156
Loss in iteration 142 : 1.5857057524802116
Loss in iteration 143 : 1.7623550382412558
Loss in iteration 144 : 1.1988012474293737
Loss in iteration 145 : 0.7384402907328453
Loss in iteration 146 : 0.6672153079499076
Loss in iteration 147 : 0.6172623096777693
Loss in iteration 148 : 0.6267630959734043
Loss in iteration 149 : 0.6342501436308805
Loss in iteration 150 : 0.6688436483943676
Loss in iteration 151 : 0.8459464730796519
Loss in iteration 152 : 1.306581602856685
Loss in iteration 153 : 1.818628871594437
Loss in iteration 154 : 1.2844174096971213
Loss in iteration 155 : 0.7944803850762377
Loss in iteration 156 : 0.6674895647807
Loss in iteration 157 : 0.6030752639319262
Loss in iteration 158 : 0.5974906980960091
Loss in iteration 159 : 0.5743226323520759
Loss in iteration 160 : 0.5609179160809219
Loss in iteration 161 : 0.5598790377411291
Loss in iteration 162 : 0.6038655651095786
Loss in iteration 163 : 0.971977443766513
Loss in iteration 164 : 2.342373219397302
Loss in iteration 165 : 1.8161623669973377
Loss in iteration 166 : 0.8274206343531748
Loss in iteration 167 : 0.5848070134489669
Loss in iteration 168 : 0.569915760276797
Loss in iteration 169 : 0.6325615058721247
Loss in iteration 170 : 0.7854159409833896
Loss in iteration 171 : 1.0280746640597536
Loss in iteration 172 : 1.2325927782067947
Loss in iteration 173 : 1.1453213428033846
Loss in iteration 174 : 0.9337051060959668
Loss in iteration 175 : 0.7920650146846522
Loss in iteration 176 : 0.7291586304120132
Loss in iteration 177 : 0.7459906461849466
Loss in iteration 178 : 0.8199389863911211
Loss in iteration 179 : 0.950799656120367
Loss in iteration 180 : 1.0775791050356573
Loss in iteration 181 : 1.0587464161101656
Loss in iteration 182 : 0.9296029071025971
Loss in iteration 183 : 0.800016884641788
Loss in iteration 184 : 0.6995507648123109
Loss in iteration 185 : 0.6612382189475168
Loss in iteration 186 : 0.6461910466055404
Loss in iteration 187 : 0.7007434646219127
Loss in iteration 188 : 0.8973313945795344
Loss in iteration 189 : 1.4371586378130365
Loss in iteration 190 : 1.7612738360425393
Loss in iteration 191 : 1.2128849433745972
Loss in iteration 192 : 0.7863663231279362
Loss in iteration 193 : 0.6529771951616501
Loss in iteration 194 : 0.6026826239869758
Loss in iteration 195 : 0.5903578098296858
Loss in iteration 196 : 0.571585321721527
Loss in iteration 197 : 0.5663863155328985
Loss in iteration 198 : 0.5790598284846065
Loss in iteration 199 : 0.7170870232170955
Loss in iteration 200 : 1.3777479657409395
Loss in iteration 201 : 2.399553763315075
Loss in iteration 202 : 1.1581788086414926
Loss in iteration 203 : 0.6568473587959366
Loss in iteration 204 : 0.5645571584862805
Loss in iteration 205 : 0.5840585389392104
Loss in iteration 206 : 0.6746329134731754
Loss in iteration 207 : 0.9037255400129373
Loss in iteration 208 : 1.2813937028712556
Loss in iteration 209 : 1.4328722922366952
Loss in iteration 210 : 1.113820662857777
Loss in iteration 211 : 0.8288020370144015
Loss in iteration 212 : 0.7097079636146141
Loss in iteration 213 : 0.6612854977328334
Loss in iteration 214 : 0.6795662361639806
Loss in iteration 215 : 0.7412022883442624
Loss in iteration 216 : 0.9189247597710297
Loss in iteration 217 : 1.2121310836033554
Loss in iteration 218 : 1.3441329209684318
Loss in iteration 219 : 1.1315813299275883
Loss in iteration 220 : 0.871836590691033
Loss in iteration 221 : 0.7010800408962401
Loss in iteration 222 : 0.6381931995395941
Loss in iteration 223 : 0.5978680441791183
Loss in iteration 224 : 0.5850169556443334
Loss in iteration 225 : 0.5746564661220711
Loss in iteration 226 : 0.6347863208508918
Loss in iteration 227 : 0.9673607709076041
Loss in iteration 228 : 2.063650796539943
Loss in iteration 229 : 1.9087681310431905
Loss in iteration 230 : 1.0485559812707315
Loss in iteration 231 : 0.7315760409395939
Loss in iteration 232 : 0.6831925674749448
Loss in iteration 233 : 0.6794477505221775
Loss in iteration 234 : 0.7175183740602167
Loss in iteration 235 : 0.7459514691180007
Loss in iteration 236 : 0.8699228402196092
Loss in iteration 237 : 1.0621651002686154
Loss in iteration 238 : 1.3198099693803564
Loss in iteration 239 : 1.2383764960562944
Loss in iteration 240 : 1.067417977140629
Loss in iteration 241 : 0.921306547092415
Loss in iteration 242 : 0.8809451170866123
Loss in iteration 243 : 0.832537101324803
Loss in iteration 244 : 0.8308863033454433
Loss in iteration 245 : 0.7981045950258969
Loss in iteration 246 : 0.8108220577341462
Loss in iteration 247 : 0.7916804708597388
Loss in iteration 248 : 0.8708226509239764
Loss in iteration 249 : 0.9862298710713474
Loss in iteration 250 : 1.2469768206068808
Loss in iteration 251 : 1.3173364269668082
Loss in iteration 252 : 1.2233039933285415
Loss in iteration 253 : 0.9907422798423636
Loss in iteration 254 : 0.8787437750762787
Loss in iteration 255 : 0.7752545060256414
Loss in iteration 256 : 0.7639851321230542
Loss in iteration 257 : 0.7491814018873313
Loss in iteration 258 : 0.8308671018968214
Loss in iteration 259 : 0.9663438878287058
Loss in iteration 260 : 1.2563709644176357
Loss in iteration 261 : 1.4200700382757714
Loss in iteration 262 : 1.2467149670155868
Loss in iteration 263 : 0.9312486834852887
Loss in iteration 264 : 0.7887114057966572
Loss in iteration 265 : 0.7014701910518361
Loss in iteration 266 : 0.6934771260201964
Loss in iteration 267 : 0.6815622357025295
Loss in iteration 268 : 0.7515161706307456
Loss in iteration 269 : 0.9314235770348662
Loss in iteration 270 : 1.3799944791422372
Loss in iteration 271 : 1.6614959736397554
Loss in iteration 272 : 1.312240624285386
Loss in iteration 273 : 0.9304679831802576
Loss in iteration 274 : 0.7651551520780935
Loss in iteration 275 : 0.6938515882416941
Loss in iteration 276 : 0.6743856328158929
Loss in iteration 277 : 0.6697007737627438
Loss in iteration 278 : 0.7189594035946015
Loss in iteration 279 : 0.8984270773921043
Loss in iteration 280 : 1.38359095729214
Loss in iteration 281 : 1.867490317354308
Loss in iteration 282 : 1.3548701751776395
Loss in iteration 283 : 0.8876043223596687
Loss in iteration 284 : 0.7268399104447921
Loss in iteration 285 : 0.6709432272125423
Loss in iteration 286 : 0.6661636611254039
Loss in iteration 287 : 0.6669247364421245
Loss in iteration 288 : 0.7271927209380842
Loss in iteration 289 : 0.927370286831374
Loss in iteration 290 : 1.4388089991327275
Loss in iteration 291 : 1.8464907506739852
Loss in iteration 292 : 1.331954079626084
Loss in iteration 293 : 0.9174322088079334
Loss in iteration 294 : 0.7629788444003409
Loss in iteration 295 : 0.7183168443966818
Loss in iteration 296 : 0.7197278021161969
Loss in iteration 297 : 0.7538955110354428
Loss in iteration 298 : 0.8592025048274602
Loss in iteration 299 : 1.1125830447692115
Loss in iteration 300 : 1.4369228823927274
Loss in iteration 301 : 1.5180449833028655
Loss in iteration 302 : 1.160873389305374
Loss in iteration 303 : 0.900141363919845
Loss in iteration 304 : 0.7897758038754653
Loss in iteration 305 : 0.754266335783096
Loss in iteration 306 : 0.7702752943853657
Loss in iteration 307 : 0.8260936516875079
Loss in iteration 308 : 0.9835133581583941
Loss in iteration 309 : 1.2633430392118952
Loss in iteration 310 : 1.442796202922982
Loss in iteration 311 : 1.2845686493047346
Loss in iteration 312 : 1.0390510315610524
Loss in iteration 313 : 0.8839247053834847
Loss in iteration 314 : 0.8349427301079921
Loss in iteration 315 : 0.8167560249415496
Loss in iteration 316 : 0.8694657222834914
Loss in iteration 317 : 0.9729761995632975
Loss in iteration 318 : 1.1674652539890047
Loss in iteration 319 : 1.3072559215323527
Loss in iteration 320 : 1.282502900803757
Loss in iteration 321 : 1.108181022282841
Loss in iteration 322 : 0.977002212857735
Loss in iteration 323 : 0.8648565205624962
Loss in iteration 324 : 0.8554366459501895
Loss in iteration 325 : 0.8536581215647946
Loss in iteration 326 : 0.9541565779939211
Loss in iteration 327 : 1.0561084935180425
Loss in iteration 328 : 1.25749402768157
Loss in iteration 329 : 1.293417477286026
Loss in iteration 330 : 1.2359930247210413
Loss in iteration 331 : 0.9937553921602957
Loss in iteration 332 : 0.9269690002554483
Loss in iteration 333 : 0.8407506776036336
Loss in iteration 334 : 0.9007713394059975
Loss in iteration 335 : 0.9116257715066384
Loss in iteration 336 : 1.1017336727067881
Loss in iteration 337 : 1.2310474640660463
Loss in iteration 338 : 1.3741803516681483
Loss in iteration 339 : 1.1435286977879986
Loss in iteration 340 : 1.0394949069250137
Loss in iteration 341 : 0.9025681851574499
Loss in iteration 342 : 0.9302763189313269
Loss in iteration 343 : 0.903717042605927
Loss in iteration 344 : 1.0288792415935681
Loss in iteration 345 : 1.1140014164149825
Loss in iteration 346 : 1.2752284426449116
Loss in iteration 347 : 1.1838589216749844
Loss in iteration 348 : 1.1262841120344114
Loss in iteration 349 : 0.9898292983338561
Loss in iteration 350 : 0.9841990088704913
Loss in iteration 351 : 0.9245872942224548
Loss in iteration 352 : 1.0031332336398209
Loss in iteration 353 : 1.0489282661385593
Loss in iteration 354 : 1.204181344632244
Loss in iteration 355 : 1.1760247867124485
Loss in iteration 356 : 1.1862977230904468
Loss in iteration 357 : 1.0607543213443968
Loss in iteration 358 : 1.0465424920735584
Loss in iteration 359 : 0.9441918196302119
Loss in iteration 360 : 0.9931531657994762
Loss in iteration 361 : 0.9948340556226064
Loss in iteration 362 : 1.1454181148261642
Loss in iteration 363 : 1.148805754698289
Loss in iteration 364 : 1.2418478936120172
Loss in iteration 365 : 1.1429657941079163
Loss in iteration 366 : 1.1346586002445456
Loss in iteration 367 : 0.9788046313646799
Loss in iteration 368 : 0.9932843661509999
Loss in iteration 369 : 0.9456663875513452
Loss in iteration 370 : 1.0659148083962335
Loss in iteration 371 : 1.0775159963374084
Loss in iteration 372 : 1.242435125801576
Loss in iteration 373 : 1.222245711179653
Loss in iteration 374 : 1.2513035371536958
Loss in iteration 375 : 1.0449433388546714
Loss in iteration 376 : 1.0152764871727251
Loss in iteration 377 : 0.9224973828083501
Loss in iteration 378 : 1.0030681504079253
Loss in iteration 379 : 0.9991560072165716
Loss in iteration 380 : 1.1833484584920353
Loss in iteration 381 : 1.2497253293350021
Loss in iteration 382 : 1.353743299405638
Loss in iteration 383 : 1.1330498409235037
Loss in iteration 384 : 1.0624825555202597
Loss in iteration 385 : 0.9353592153922925
Loss in iteration 386 : 0.981986147323824
Loss in iteration 387 : 0.9589252454146894
Loss in iteration 388 : 1.1195797908281622
Loss in iteration 389 : 1.2163354240993103
Loss in iteration 390 : 1.3808533207239875
Loss in iteration 391 : 1.2083516567953838
Loss in iteration 392 : 1.1295183661186707
Loss in iteration 393 : 0.9800977779416986
Loss in iteration 394 : 0.997473871615703
Loss in iteration 395 : 0.94842384441944
Loss in iteration 396 : 1.0701867867154724
Loss in iteration 397 : 1.1486843698634608
Loss in iteration 398 : 1.3430683622463389
Loss in iteration 399 : 1.2525390420554319
Loss in iteration 400 : 1.2149971353762639
Testing accuracy  of updater 7 on alg 0 with rate 20.0 = 0.736375, training accuracy 0.736375, time elapsed: 4148 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.7003222129999269
Loss in iteration 3 : 0.6470640040555308
Loss in iteration 4 : 0.6386561642357131
Loss in iteration 5 : 0.5873439748281942
Loss in iteration 6 : 0.5827305710779901
Loss in iteration 7 : 0.5462716550051104
Loss in iteration 8 : 0.5344833942702998
Loss in iteration 9 : 0.5204666416573482
Loss in iteration 10 : 0.5045796443401225
Loss in iteration 11 : 0.506968608223446
Loss in iteration 12 : 0.4951424054975247
Loss in iteration 13 : 0.49083466182369245
Loss in iteration 14 : 0.49213379017313946
Loss in iteration 15 : 0.4856088399314951
Loss in iteration 16 : 0.48590991127789157
Loss in iteration 17 : 0.48681974472810247
Loss in iteration 18 : 0.4823067431797739
Loss in iteration 19 : 0.4829359219040011
Loss in iteration 20 : 0.48387978713483293
Loss in iteration 21 : 0.48055917501254625
Loss in iteration 22 : 0.4801096025961081
Loss in iteration 23 : 0.4804151232836109
Loss in iteration 24 : 0.47799122724357607
Loss in iteration 25 : 0.47749914846710917
Loss in iteration 26 : 0.47773323321559724
Loss in iteration 27 : 0.4756531499917482
Loss in iteration 28 : 0.4746713770491493
Loss in iteration 29 : 0.4747337490874681
Loss in iteration 30 : 0.47313389103365067
Loss in iteration 31 : 0.47192840968697214
Loss in iteration 32 : 0.4716396073928242
Loss in iteration 33 : 0.470328060260623
Loss in iteration 34 : 0.4692942047978006
Loss in iteration 35 : 0.46906878395917473
Loss in iteration 36 : 0.4680345531018197
Loss in iteration 37 : 0.46704651533406627
Loss in iteration 38 : 0.4667947855053084
Loss in iteration 39 : 0.4660892316901217
Loss in iteration 40 : 0.46536741054268566
Loss in iteration 41 : 0.46517854613511933
Loss in iteration 42 : 0.4646852096967719
Loss in iteration 43 : 0.46420956948069547
Loss in iteration 44 : 0.4641662091343062
Loss in iteration 45 : 0.46386702031100674
Loss in iteration 46 : 0.4635190548393123
Loss in iteration 47 : 0.46348861896585036
Loss in iteration 48 : 0.46330164590690903
Loss in iteration 49 : 0.4630964501590854
Loss in iteration 50 : 0.4631022151827317
Loss in iteration 51 : 0.4629664094003223
Loss in iteration 52 : 0.4628310171265014
Loss in iteration 53 : 0.4628594578456408
Loss in iteration 54 : 0.4627681563679614
Loss in iteration 55 : 0.4626661595092085
Loss in iteration 56 : 0.46267117823718945
Loss in iteration 57 : 0.4625932659277038
Loss in iteration 58 : 0.4625274834558985
Loss in iteration 59 : 0.46252784771560934
Loss in iteration 60 : 0.46245013040744504
Loss in iteration 61 : 0.4623871270910299
Loss in iteration 62 : 0.46237118431227336
Loss in iteration 63 : 0.4622960889146352
Loss in iteration 64 : 0.46223338375530093
Loss in iteration 65 : 0.4621971058107005
Loss in iteration 66 : 0.46212151714537403
Loss in iteration 67 : 0.46206658891874997
Loss in iteration 68 : 0.4620268398164024
Loss in iteration 69 : 0.46195526710274726
Loss in iteration 70 : 0.4619020005867811
Loss in iteration 71 : 0.46185952264958074
Loss in iteration 72 : 0.461797045985955
Loss in iteration 73 : 0.46174995730389345
Loss in iteration 74 : 0.461707266723658
Loss in iteration 75 : 0.46165251004402874
Loss in iteration 76 : 0.4616127856777547
Loss in iteration 77 : 0.46157445968989463
Loss in iteration 78 : 0.461526902901062
Loss in iteration 79 : 0.46149067481214034
Loss in iteration 80 : 0.46145508140938796
Loss in iteration 81 : 0.46141490754658027
Loss in iteration 82 : 0.4613834827852907
Loss in iteration 83 : 0.4613509297437315
Loss in iteration 84 : 0.46131609465844825
Loss in iteration 85 : 0.4612884998130663
Loss in iteration 86 : 0.46125931430910827
Loss in iteration 87 : 0.4612285129364744
Loss in iteration 88 : 0.4612025685618881
Loss in iteration 89 : 0.4611751583741471
Loss in iteration 90 : 0.461147536740645
Loss in iteration 91 : 0.46112325177596014
Loss in iteration 92 : 0.4610971448520106
Loss in iteration 93 : 0.4610712464762026
Loss in iteration 94 : 0.4610478082240593
Loss in iteration 95 : 0.46102289234677524
Loss in iteration 96 : 0.46099833761078773
Loss in iteration 97 : 0.4609753008691403
Loss in iteration 98 : 0.460951256842189
Loss in iteration 99 : 0.4609280249938839
Loss in iteration 100 : 0.46090579606394555
Loss in iteration 101 : 0.4608827223170594
Loss in iteration 102 : 0.4608604587091064
Loss in iteration 103 : 0.46083891301742314
Loss in iteration 104 : 0.46081689421999444
Loss in iteration 105 : 0.4607956289595277
Loss in iteration 106 : 0.4607747665641103
Loss in iteration 107 : 0.46075371551942623
Loss in iteration 108 : 0.4607334478041008
Loss in iteration 109 : 0.46071343592157116
Loss in iteration 110 : 0.46069335559826347
Loss in iteration 111 : 0.4606739305682818
Loss in iteration 112 : 0.46065470356287896
Loss in iteration 113 : 0.4606355817292948
Loss in iteration 114 : 0.46061699901649555
Loss in iteration 115 : 0.4605985326246174
Loss in iteration 116 : 0.46058025955984977
Loss in iteration 117 : 0.46056244532709045
Loss in iteration 118 : 0.4605447206091613
Loss in iteration 119 : 0.4605272026417628
Loss in iteration 120 : 0.46051003173397953
Loss in iteration 121 : 0.4604929590208045
Loss in iteration 122 : 0.46047612751820016
Loss in iteration 123 : 0.4604595632328818
Loss in iteration 124 : 0.4604430937659187
Loss in iteration 125 : 0.4604268678769379
Loss in iteration 126 : 0.46041086319873287
Loss in iteration 127 : 0.4603949690296813
Loss in iteration 128 : 0.46037930026552804
Loss in iteration 129 : 0.4603638112241265
Loss in iteration 130 : 0.46034845588793916
Loss in iteration 131 : 0.46033331782820097
Loss in iteration 132 : 0.4603183333031429
Loss in iteration 133 : 0.46030348995214454
Loss in iteration 134 : 0.46028884637362066
Loss in iteration 135 : 0.4602743463737796
Loss in iteration 136 : 0.4602599964272553
Loss in iteration 137 : 0.46024582607605763
Loss in iteration 138 : 0.4602317924392991
Loss in iteration 139 : 0.4602179162908351
Loss in iteration 140 : 0.4602042067529556
Loss in iteration 141 : 0.4601906301581788
Loss in iteration 142 : 0.4601772086225019
Loss in iteration 143 : 0.46016394095630836
Loss in iteration 144 : 0.46015080714085993
Loss in iteration 145 : 0.4601378248890214
Loss in iteration 146 : 0.4601249847297239
Loss in iteration 147 : 0.46011227763580564
Loss in iteration 148 : 0.46009971742942607
Loss in iteration 149 : 0.46008729162636597
Loss in iteration 150 : 0.4600749968341087
Loss in iteration 151 : 0.46006284127714564
Loss in iteration 152 : 0.4600508142980945
Loss in iteration 153 : 0.4600389171167899
Loss in iteration 154 : 0.4600271523969667
Loss in iteration 155 : 0.46001551123247203
Loss in iteration 156 : 0.46000399702827327
Loss in iteration 157 : 0.45999260934266667
Loss in iteration 158 : 0.4599813416901752
Loss in iteration 159 : 0.4599701970544261
Loss in iteration 160 : 0.45995917301802564
Loss in iteration 161 : 0.4599482661068717
Loss in iteration 162 : 0.4599374785132667
Loss in iteration 163 : 0.4599268066171834
Loss in iteration 164 : 0.4599162486969689
Loss in iteration 165 : 0.4599058059288665
Loss in iteration 166 : 0.4598954748804392
Loss in iteration 167 : 0.4598852548763117
Loss in iteration 168 : 0.459875145783755
Loss in iteration 169 : 0.45986514480023655
Loss in iteration 170 : 0.4598552519361856
Loss in iteration 171 : 0.4598454661625857
Loss in iteration 172 : 0.45983578523452584
Loss in iteration 173 : 0.4598262091622322
Loss in iteration 174 : 0.45981673650816535
Loss in iteration 175 : 0.4598073657284913
Loss in iteration 176 : 0.4597980965402395
Loss in iteration 177 : 0.45978892730351595
Loss in iteration 178 : 0.4597798570115123
Loss in iteration 179 : 0.45977088510621117
Loss in iteration 180 : 0.45976200998576167
Loss in iteration 181 : 0.45975323086992514
Loss in iteration 182 : 0.4597445469271542
Loss in iteration 183 : 0.45973595679686685
Loss in iteration 184 : 0.45972745980453317
Loss in iteration 185 : 0.4597190549056181
Loss in iteration 186 : 0.45971074095572906
Loss in iteration 187 : 0.4597025172836217
Loss in iteration 188 : 0.4596943827831947
Loss in iteration 189 : 0.4596863364793402
Loss in iteration 190 : 0.4596783776147803
Loss in iteration 191 : 0.45967050511735413
Loss in iteration 192 : 0.45966271815393484
Loss in iteration 193 : 0.4596550158892318
Loss in iteration 194 : 0.4596473973137777
Loss in iteration 195 : 0.45963986165788595
Loss in iteration 196 : 0.4596324080485235
Loss in iteration 197 : 0.45962503556600753
Loss in iteration 198 : 0.45961774344832124
Loss in iteration 199 : 0.4596105308119703
Loss in iteration 200 : 0.4596033968251928
Loss in iteration 201 : 0.45959634072206623
Loss in iteration 202 : 0.4595893616354937
Loss in iteration 203 : 0.45958245878696924
Loss in iteration 204 : 0.45957563140307944
Loss in iteration 205 : 0.4595688786605988
Loss in iteration 206 : 0.45956219981382707
Loss in iteration 207 : 0.4595555940855642
Loss in iteration 208 : 0.45954906069915125
Loss in iteration 209 : 0.4595425989297754
Loss in iteration 210 : 0.4595362080124104
Loss in iteration 211 : 0.4595298872083859
Loss in iteration 212 : 0.4595236358019797
Loss in iteration 213 : 0.45951745305269187
Loss in iteration 214 : 0.4595113382543144
Loss in iteration 215 : 0.45950529069990464
Loss in iteration 216 : 0.45949930967681824
Loss in iteration 217 : 0.45949339450259197
Loss in iteration 218 : 0.4594875444845463
Loss in iteration 219 : 0.4594817589381443
Loss in iteration 220 : 0.4594760371973714
Loss in iteration 221 : 0.4594703785883648
Loss in iteration 222 : 0.45946478245329564
Loss in iteration 223 : 0.45945924814120526
Loss in iteration 224 : 0.4594537749991873
Loss in iteration 225 : 0.4594483623912049
Loss in iteration 226 : 0.4594430096823377
Loss in iteration 227 : 0.4594377162421462
Loss in iteration 228 : 0.45943248145262067
Loss in iteration 229 : 0.4594273046958028
Loss in iteration 230 : 0.4594221853629582
Loss in iteration 231 : 0.45941712285291453
Loss in iteration 232 : 0.4594121165660488
Loss in iteration 233 : 0.4594071659129462
Loss in iteration 234 : 0.459402270308599
Testing accuracy  of updater 7 on alg 0 with rate 2.0 = 0.789125, training accuracy 0.789125, time elapsed: 2488 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6888675253949501
Loss in iteration 3 : 0.6826216196038367
Loss in iteration 4 : 0.6764947570304451
Loss in iteration 5 : 0.6708969462189998
Loss in iteration 6 : 0.6648993747895641
Loss in iteration 7 : 0.65760536300972
Loss in iteration 8 : 0.6490909091130942
Loss in iteration 9 : 0.6402809723807068
Loss in iteration 10 : 0.6321315358539475
Loss in iteration 11 : 0.6249147611431133
Loss in iteration 12 : 0.6181750513204192
Loss in iteration 13 : 0.6112779973192021
Loss in iteration 14 : 0.6039860128178209
Loss in iteration 15 : 0.5965865942229791
Loss in iteration 16 : 0.5895755282055624
Loss in iteration 17 : 0.5832539776805953
Loss in iteration 18 : 0.5775706532178995
Loss in iteration 19 : 0.5722596524079381
Loss in iteration 20 : 0.5670907441074494
Loss in iteration 21 : 0.5620239737293079
Loss in iteration 22 : 0.557188277600359
Loss in iteration 23 : 0.5527449017026169
Loss in iteration 24 : 0.548757619827448
Loss in iteration 25 : 0.545157825234356
Loss in iteration 26 : 0.5418078251420981
Loss in iteration 27 : 0.538597394665435
Loss in iteration 28 : 0.5355004114945195
Loss in iteration 29 : 0.5325626091652312
Loss in iteration 30 : 0.5298449412376864
Loss in iteration 31 : 0.5273706696979359
Loss in iteration 32 : 0.5251098499819192
Loss in iteration 33 : 0.5230021710296346
Loss in iteration 34 : 0.5209944116443754
Loss in iteration 35 : 0.5190655363991032
Loss in iteration 36 : 0.5172267091354977
Loss in iteration 37 : 0.5155018863154859
Loss in iteration 38 : 0.5139051667194949
Loss in iteration 39 : 0.5124294211450408
Loss in iteration 40 : 0.5110506785451463
Loss in iteration 41 : 0.5097421737325408
Loss in iteration 42 : 0.5084873898631588
Loss in iteration 43 : 0.5072843584488981
Loss in iteration 44 : 0.5061404835015666
Loss in iteration 45 : 0.5050630550888898
Loss in iteration 46 : 0.50405207748608
Loss in iteration 47 : 0.5030991911337417
Loss in iteration 48 : 0.5021920104606129
Loss in iteration 49 : 0.5013201470346657
Loss in iteration 50 : 0.5004790079807875
Loss in iteration 51 : 0.4996695989547078
Loss in iteration 52 : 0.4988952597439932
Loss in iteration 53 : 0.4981578442780736
Loss in iteration 54 : 0.49745565845971973
Loss in iteration 55 : 0.49678397902327265
Loss in iteration 56 : 0.4961373187418651
Loss in iteration 57 : 0.4955117663627694
Loss in iteration 58 : 0.4949060278457082
Loss in iteration 59 : 0.4943208239118934
Loss in iteration 60 : 0.493757314513522
Loss in iteration 61 : 0.4932156422277454
Loss in iteration 62 : 0.4926943959074962
Loss in iteration 63 : 0.49219110827231427
Loss in iteration 64 : 0.4917032912509546
Loss in iteration 65 : 0.49122931036048273
Loss in iteration 66 : 0.49076863349119343
Loss in iteration 67 : 0.49032143753353796
Loss in iteration 68 : 0.48988792236038686
Loss in iteration 69 : 0.48946777698080407
Loss in iteration 70 : 0.4890600684547327
Loss in iteration 71 : 0.48866353222908193
Loss in iteration 72 : 0.48827702036495896
Loss in iteration 73 : 0.4878998227715515
Loss in iteration 74 : 0.4875317035688951
Loss in iteration 75 : 0.4871726854448624
Loss in iteration 76 : 0.48682274966870476
Loss in iteration 77 : 0.48648163380621473
Loss in iteration 78 : 0.48614881902628665
Loss in iteration 79 : 0.48582367534645887
Loss in iteration 80 : 0.4855056517309814
Loss in iteration 81 : 0.4851943955230163
Loss in iteration 82 : 0.4848897478780755
Loss in iteration 83 : 0.48459164134020505
Loss in iteration 84 : 0.48429997499818245
Loss in iteration 85 : 0.48401454041949477
Loss in iteration 86 : 0.4837350291510571
Loss in iteration 87 : 0.48346110135828824
Loss in iteration 88 : 0.48319246525164167
Loss in iteration 89 : 0.482928920758747
Loss in iteration 90 : 0.4826703497670899
Loss in iteration 91 : 0.4824166683173688
Loss in iteration 92 : 0.48216777424682833
Loss in iteration 93 : 0.4819235198386227
Loss in iteration 94 : 0.4816837195003281
Loss in iteration 95 : 0.48144818120113897
Loss in iteration 96 : 0.4812167394474478
Loss in iteration 97 : 0.4809892710080673
Loss in iteration 98 : 0.48076568773637945
Loss in iteration 99 : 0.4805459145629463
Loss in iteration 100 : 0.48032986737504957
Loss in iteration 101 : 0.4801174427661048
Loss in iteration 102 : 0.479908522834943
Loss in iteration 103 : 0.4797029893511365
Loss in iteration 104 : 0.4795007375647131
Loss in iteration 105 : 0.4793016820199474
Loss in iteration 106 : 0.4791057526125402
Loss in iteration 107 : 0.47891288484333305
Loss in iteration 108 : 0.4787230106594599
Loss in iteration 109 : 0.4785360547254132
Loss in iteration 110 : 0.47835193705081097
Loss in iteration 111 : 0.4781705792279717
Loss in iteration 112 : 0.4779919100745657
Loss in iteration 113 : 0.47781586761178974
Loss in iteration 114 : 0.47764239691588845
Loss in iteration 115 : 0.47747144574511025
Loss in iteration 116 : 0.4773029607117233
Loss in iteration 117 : 0.4771368859473761
Loss in iteration 118 : 0.47697316447242405
Loss in iteration 119 : 0.47681174095516127
Loss in iteration 120 : 0.476652564032989
Loss in iteration 121 : 0.4764955869554095
Loss in iteration 122 : 0.4763407664666105
Loss in iteration 123 : 0.47618806082925674
Loss in iteration 124 : 0.47603742819354605
Loss in iteration 125 : 0.47588882609990607
Loss in iteration 126 : 0.47574221213440937
Loss in iteration 127 : 0.47559754512281033
Loss in iteration 128 : 0.4754547860736574
Loss in iteration 129 : 0.4753138983723833
Loss in iteration 130 : 0.47517484723772596
Loss in iteration 131 : 0.4750375988574835
Loss in iteration 132 : 0.4749021197194057
Loss in iteration 133 : 0.4747683764485413
Loss in iteration 134 : 0.4746363361250026
Loss in iteration 135 : 0.4745059667965887
Loss in iteration 136 : 0.4743772378475594
Loss in iteration 137 : 0.4742501200283269
Loss in iteration 138 : 0.4741245851750189
Loss in iteration 139 : 0.4740006058139637
Loss in iteration 140 : 0.47387815487362545
Loss in iteration 141 : 0.47375720562631335
Loss in iteration 142 : 0.47363773183323304
Loss in iteration 143 : 0.4735197079603228
Loss in iteration 144 : 0.47340310931913393
Loss in iteration 145 : 0.473287912056572
Loss in iteration 146 : 0.4731740930157674
Loss in iteration 147 : 0.473061629557744
Loss in iteration 148 : 0.472950499438934
Loss in iteration 149 : 0.4728406807912785
Loss in iteration 150 : 0.4727321521867575
Loss in iteration 151 : 0.47262489272539643
Loss in iteration 152 : 0.4725188820847727
Loss in iteration 153 : 0.4724141005023679
Loss in iteration 154 : 0.47231052870494244
Loss in iteration 155 : 0.47220814782607806
Loss in iteration 156 : 0.47210693935204245
Loss in iteration 157 : 0.47200688511318417
Loss in iteration 158 : 0.4719079673098145
Loss in iteration 159 : 0.471810168544804
Loss in iteration 160 : 0.47171347183686
Loss in iteration 161 : 0.47161786060432964
Loss in iteration 162 : 0.4715233186279201
Loss in iteration 163 : 0.47142983001111405
Loss in iteration 164 : 0.4713373791550895
Loss in iteration 165 : 0.4712459507540075
Loss in iteration 166 : 0.471155529804336
Loss in iteration 167 : 0.4710661016154752
Loss in iteration 168 : 0.47097765181078194
Loss in iteration 169 : 0.4708901663156287
Loss in iteration 170 : 0.47080363133707587
Loss in iteration 171 : 0.470718033343716
Loss in iteration 172 : 0.4706333590526344
Loss in iteration 173 : 0.4705495954253331
Loss in iteration 174 : 0.47046672966926806
Loss in iteration 175 : 0.47038474923929097
Loss in iteration 176 : 0.4703036418345986
Loss in iteration 177 : 0.4702233953902205
Loss in iteration 178 : 0.47014399806549734
Loss in iteration 179 : 0.4700654382333156
Loss in iteration 180 : 0.4699877044728748
Loss in iteration 181 : 0.4699107855663556
Loss in iteration 182 : 0.46983467049771405
Loss in iteration 183 : 0.4697593484510544
Loss in iteration 184 : 0.46968480880679947
Loss in iteration 185 : 0.46961104113556623
Loss in iteration 186 : 0.4695380351909657
Loss in iteration 187 : 0.46946578090306545
Loss in iteration 188 : 0.4693942683735617
Loss in iteration 189 : 0.46932348787271677
Loss in iteration 190 : 0.4692534298370984
Loss in iteration 191 : 0.4691840848670639
Loss in iteration 192 : 0.46911544372325614
Loss in iteration 193 : 0.46904749732223866
Loss in iteration 194 : 0.4689802367318308
Loss in iteration 195 : 0.4689136531668957
Loss in iteration 196 : 0.4688477379859866
Loss in iteration 197 : 0.46878248268872075
Loss in iteration 198 : 0.4687178789134614
Loss in iteration 199 : 0.46865391843480697
Loss in iteration 200 : 0.4685905931606641
Loss in iteration 201 : 0.46852789512895865
Loss in iteration 202 : 0.46846581650435004
Loss in iteration 203 : 0.46840434957519356
Loss in iteration 204 : 0.46834348675094417
Loss in iteration 205 : 0.4682832205598687
Loss in iteration 206 : 0.46822354364691265
Loss in iteration 207 : 0.46816444877145935
Loss in iteration 208 : 0.46810592880493634
Loss in iteration 209 : 0.4680479767283422
Loss in iteration 210 : 0.46799058562980317
Loss in iteration 211 : 0.46793374870232973
Loss in iteration 212 : 0.46787745924175084
Loss in iteration 213 : 0.4678217106448303
Loss in iteration 214 : 0.46776649640741064
Loss in iteration 215 : 0.46771181012254226
Loss in iteration 216 : 0.46765764547853794
Loss in iteration 217 : 0.4676039962570367
Loss in iteration 218 : 0.46755085633110083
Loss in iteration 219 : 0.46749821966343175
Loss in iteration 220 : 0.4674460803046867
Loss in iteration 221 : 0.467394432391881
Loss in iteration 222 : 0.46734327014680777
Loss in iteration 223 : 0.467292587874482
Loss in iteration 224 : 0.4672423799615591
Loss in iteration 225 : 0.4671926408747879
Loss in iteration 226 : 0.4671433651595074
Loss in iteration 227 : 0.467094547438209
Loss in iteration 228 : 0.4670461824091622
Loss in iteration 229 : 0.4669982648450716
Loss in iteration 230 : 0.4669507895917771
Loss in iteration 231 : 0.46690375156693786
Loss in iteration 232 : 0.466857145758749
Loss in iteration 233 : 0.46681096722467014
Loss in iteration 234 : 0.4667652110901979
Loss in iteration 235 : 0.4667198725476661
Loss in iteration 236 : 0.4666749468551069
Loss in iteration 237 : 0.46663042933512044
Loss in iteration 238 : 0.46658631537376954
Loss in iteration 239 : 0.4665426004195031
Loss in iteration 240 : 0.46649927998207136
Loss in iteration 241 : 0.466456349631496
Loss in iteration 242 : 0.4664138049970491
Loss in iteration 243 : 0.4663716417662541
Loss in iteration 244 : 0.4663298556839337
Loss in iteration 245 : 0.46628844255125923
Loss in iteration 246 : 0.4662473982248258
Loss in iteration 247 : 0.4662067186157343
Loss in iteration 248 : 0.466166399688702
Loss in iteration 249 : 0.46612643746118054
Loss in iteration 250 : 0.46608682800250295
Loss in iteration 251 : 0.4660475674330471
Loss in iteration 252 : 0.46600865192341495
Loss in iteration 253 : 0.4659700776936392
Loss in iteration 254 : 0.4659318410123892
Loss in iteration 255 : 0.4658939381962113
Loss in iteration 256 : 0.4658563656087557
Loss in iteration 257 : 0.4658191196600538
Loss in iteration 258 : 0.465782196805785
Loss in iteration 259 : 0.4657455935465648
Loss in iteration 260 : 0.4657093064272618
Loss in iteration 261 : 0.4656733320363041
Loss in iteration 262 : 0.46563766700501913
Loss in iteration 263 : 0.46560230800696784
Loss in iteration 264 : 0.46556725175730923
Loss in iteration 265 : 0.4655324950121606
Loss in iteration 266 : 0.46549803456798794
Loss in iteration 267 : 0.4654638672609844
Loss in iteration 268 : 0.46542998996649304
Loss in iteration 269 : 0.46539639959840406
Loss in iteration 270 : 0.4653630931085982
Loss in iteration 271 : 0.46533006748637074
Loss in iteration 272 : 0.46529731975788574
Loss in iteration 273 : 0.4652648469856341
Loss in iteration 274 : 0.4652326462678983
Loss in iteration 275 : 0.46520071473822794
Loss in iteration 276 : 0.4651690495649414
Loss in iteration 277 : 0.4651376479506023
Loss in iteration 278 : 0.4651065071315429
Loss in iteration 279 : 0.4650756243773631
Loss in iteration 280 : 0.46504499699046487
Loss in iteration 281 : 0.4650146223055755
Loss in iteration 282 : 0.46498449768929284
Loss in iteration 283 : 0.4649546205396324
Loss in iteration 284 : 0.46492498828557816
Loss in iteration 285 : 0.46489559838665334
Loss in iteration 286 : 0.46486644833248736
Loss in iteration 287 : 0.4648375356423956
Loss in iteration 288 : 0.4648088578649637
Loss in iteration 289 : 0.4647804125776429
Loss in iteration 290 : 0.4647521973863503
Loss in iteration 291 : 0.46472420992507335
Loss in iteration 292 : 0.4646964478554836
Loss in iteration 293 : 0.4646689088665596
Loss in iteration 294 : 0.46464159067421185
Loss in iteration 295 : 0.4646144910209098
Loss in iteration 296 : 0.4645876076753303
Loss in iteration 297 : 0.4645609384319923
Loss in iteration 298 : 0.4645344811109171
Loss in iteration 299 : 0.46450823355727594
Loss in iteration 300 : 0.46448219364105614
Loss in iteration 301 : 0.46445635925673096
Loss in iteration 302 : 0.46443072832292354
Loss in iteration 303 : 0.4644052987820971
Loss in iteration 304 : 0.46438006860022985
Loss in iteration 305 : 0.4643550357665036
Loss in iteration 306 : 0.46433019829300304
Loss in iteration 307 : 0.46430555421440894
Loss in iteration 308 : 0.46428110158770186
Loss in iteration 309 : 0.4642568384918726
Loss in iteration 310 : 0.46423276302762934
Loss in iteration 311 : 0.4642088733171222
Loss in iteration 312 : 0.4641851675036579
Loss in iteration 313 : 0.4641616437514274
Loss in iteration 314 : 0.46413830024523833
Loss in iteration 315 : 0.46411513519024333
Loss in iteration 316 : 0.46409214681168737
Loss in iteration 317 : 0.46406933335463907
Loss in iteration 318 : 0.4640466930837485
Loss in iteration 319 : 0.46402422428298334
Loss in iteration 320 : 0.4640019252553986
Loss in iteration 321 : 0.463979794322884
Loss in iteration 322 : 0.4639578298259248
Loss in iteration 323 : 0.46393603012337337
Loss in iteration 324 : 0.46391439359221054
Loss in iteration 325 : 0.4638929186273245
Loss in iteration 326 : 0.46387160364127805
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999996 = 0.79025, training accuracy 0.79025, time elapsed: 2969 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 4.8558452841449675
Loss in iteration 3 : 4.932588061608756
Loss in iteration 4 : 1.480491367804904
Loss in iteration 5 : 1.1069842418577869
Loss in iteration 6 : 0.7976234227949963
Loss in iteration 7 : 0.8172371760950683
Loss in iteration 8 : 0.857296416147334
Loss in iteration 9 : 0.8436630400673937
Loss in iteration 10 : 0.7858796440778205
Loss in iteration 11 : 0.7165514312599892
Loss in iteration 12 : 0.6618541843399676
Loss in iteration 13 : 0.6310898224293143
Loss in iteration 14 : 0.6285544225302222
Loss in iteration 15 : 0.6348439335914198
Loss in iteration 16 : 0.6474860388341624
Loss in iteration 17 : 0.6511573874959251
Loss in iteration 18 : 0.6712594616115521
Loss in iteration 19 : 0.6857601463553247
Loss in iteration 20 : 0.7383678023389129
Loss in iteration 21 : 0.7535748885309259
Loss in iteration 22 : 0.7830334340617182
Loss in iteration 23 : 0.7548960761957554
Loss in iteration 24 : 0.7320765210040606
Loss in iteration 25 : 0.7010767868717379
Loss in iteration 26 : 0.6792776755744433
Loss in iteration 27 : 0.660331509731629
Loss in iteration 28 : 0.6520374746877986
Loss in iteration 29 : 0.643914150193179
Loss in iteration 30 : 0.6479278066149691
Loss in iteration 31 : 0.6478249747712957
Loss in iteration 32 : 0.6634967057905011
Loss in iteration 33 : 0.6670704311162214
Loss in iteration 34 : 0.6884518265210604
Loss in iteration 35 : 0.6898856540710244
Loss in iteration 36 : 0.7055531271635905
Loss in iteration 37 : 0.7100767846666572
Loss in iteration 38 : 0.7126944963821154
Loss in iteration 39 : 0.7130377966463004
Loss in iteration 40 : 0.7015705036840033
Loss in iteration 41 : 0.6928539963884183
Loss in iteration 42 : 0.6771844703092685
Loss in iteration 43 : 0.6643259479473856
Loss in iteration 44 : 0.6520061811353598
Loss in iteration 45 : 0.6394317341600723
Loss in iteration 46 : 0.632891454104299
Loss in iteration 47 : 0.6226387153692876
Loss in iteration 48 : 0.6220785841060007
Loss in iteration 49 : 0.6145926396442055
Loss in iteration 50 : 0.6196880569845046
Loss in iteration 51 : 0.6142298851889626
Loss in iteration 52 : 0.624119460889303
Loss in iteration 53 : 0.6183227563074875
Loss in iteration 54 : 0.6300137818740991
Loss in iteration 55 : 0.6233230049133981
Loss in iteration 56 : 0.634213774640444
Loss in iteration 57 : 0.6324133022363346
Loss in iteration 58 : 0.6423619264715992
Loss in iteration 59 : 0.6459004613603208
Loss in iteration 60 : 0.6523580805195236
Loss in iteration 61 : 0.6560860986057684
Loss in iteration 62 : 0.6567290025295741
Loss in iteration 63 : 0.6574902137175498
Loss in iteration 64 : 0.6536021909510346
Loss in iteration 65 : 0.651657139986778
Loss in iteration 66 : 0.646390135139025
Loss in iteration 67 : 0.643417385347639
Loss in iteration 68 : 0.6392772161779535
Loss in iteration 69 : 0.6367298122455968
Loss in iteration 70 : 0.6349207685196033
Loss in iteration 71 : 0.6335252363846827
Loss in iteration 72 : 0.634227375692676
Loss in iteration 73 : 0.634011750136511
Loss in iteration 74 : 0.636716701861987
Loss in iteration 75 : 0.6371365502577735
Loss in iteration 76 : 0.6409366241580791
Loss in iteration 77 : 0.6410725429181954
Loss in iteration 78 : 0.6450336586095912
Loss in iteration 79 : 0.643937479388926
Loss in iteration 80 : 0.6474999910564988
Loss in iteration 81 : 0.6446116285453499
Loss in iteration 82 : 0.6477484469574933
Loss in iteration 83 : 0.6431825061302645
Loss in iteration 84 : 0.6462077210658479
Loss in iteration 85 : 0.6407516927852789
Loss in iteration 86 : 0.6440141434694305
Loss in iteration 87 : 0.6387907595593013
Loss in iteration 88 : 0.6424749030139537
Loss in iteration 89 : 0.6384251030017997
Loss in iteration 90 : 0.642449013088294
Loss in iteration 91 : 0.6399662739026339
Loss in iteration 92 : 0.6439495942021961
Loss in iteration 93 : 0.6428375242820923
Loss in iteration 94 : 0.64620140639125
Loss in iteration 95 : 0.6458742066076844
Loss in iteration 96 : 0.6480904081119172
Loss in iteration 97 : 0.64787719348557
Loss in iteration 98 : 0.6487410329557595
Loss in iteration 99 : 0.6481584977938769
Loss in iteration 100 : 0.6478966312561812
Loss in iteration 101 : 0.6467731402247036
Loss in iteration 102 : 0.6459232727507515
Loss in iteration 103 : 0.644350340044846
Loss in iteration 104 : 0.6435287898042925
Loss in iteration 105 : 0.6417174216546639
Loss in iteration 106 : 0.6414248258945049
Loss in iteration 107 : 0.6395637675481446
Loss in iteration 108 : 0.640091398192252
Loss in iteration 109 : 0.6382606257349246
Loss in iteration 110 : 0.6396822514952172
Loss in iteration 111 : 0.6378327670930328
Loss in iteration 112 : 0.6400473019194801
Loss in iteration 113 : 0.638040309256073
Loss in iteration 114 : 0.6408392513831507
Loss in iteration 115 : 0.6385289154586635
Loss in iteration 116 : 0.6416695168016238
Loss in iteration 117 : 0.6389968302884342
Loss in iteration 118 : 0.6422621972685183
Loss in iteration 119 : 0.639310163945446
Loss in iteration 120 : 0.642540367583616
Loss in iteration 121 : 0.6395123013945615
Loss in iteration 122 : 0.6426023233007142
Loss in iteration 123 : 0.6397351967453384
Loss in iteration 124 : 0.6426118620477554
Loss in iteration 125 : 0.6400820084722466
Loss in iteration 126 : 0.6426813674268183
Loss in iteration 127 : 0.6405542903202923
Loss in iteration 128 : 0.6428157649196983
Loss in iteration 129 : 0.641050952355305
Loss in iteration 130 : 0.6429301643785248
Loss in iteration 131 : 0.6414207458854028
Loss in iteration 132 : 0.6429104985939138
Loss in iteration 133 : 0.6415314106502358
Loss in iteration 134 : 0.6426764595385225
Loss in iteration 135 : 0.6413222013425361
Loss in iteration 136 : 0.642218287347637
Loss in iteration 137 : 0.6408215817050413
Loss in iteration 138 : 0.6415983544365123
Loss in iteration 139 : 0.6401300656201433
Loss in iteration 140 : 0.6409250748275207
Loss in iteration 141 : 0.6393817997633794
Loss in iteration 142 : 0.6403150326412149
Loss in iteration 143 : 0.6387028831760136
Loss in iteration 144 : 0.6398589075701211
Loss in iteration 145 : 0.6381806569190883
Loss in iteration 146 : 0.6396012108236514
Loss in iteration 147 : 0.6378507962783033
Loss in iteration 148 : 0.6395370619837103
Loss in iteration 149 : 0.6377019377808524
Loss in iteration 150 : 0.6396235811357283
Loss in iteration 151 : 0.6376925037390135
Loss in iteration 152 : 0.6397997067168242
Loss in iteration 153 : 0.6377717586892714
Loss in iteration 154 : 0.6400066733442503
Loss in iteration 155 : 0.6378971588069299
Loss in iteration 156 : 0.6402023007667794
Loss in iteration 157 : 0.6380427280197145
Loss in iteration 158 : 0.6403654875165663
Loss in iteration 159 : 0.6381975831429447
Loss in iteration 160 : 0.6404916558654664
Loss in iteration 161 : 0.638357918413761
Loss in iteration 162 : 0.6405834039668591
Loss in iteration 163 : 0.6385178690136618
Loss in iteration 164 : 0.6406417427768804
Loss in iteration 165 : 0.6386640437322303
Loss in iteration 166 : 0.6406618526146294
Loss in iteration 167 : 0.6387759346604396
Loss in iteration 168 : 0.6406344127944531
Loss in iteration 169 : 0.6388313812357514
Loss in iteration 170 : 0.6405508086096329
Loss in iteration 171 : 0.6388141320176817
Loss in iteration 172 : 0.6404090598081607
Loss in iteration 173 : 0.6387200053335941
Loss in iteration 174 : 0.6402174908969676
Loss in iteration 175 : 0.6385591388240887
Loss in iteration 176 : 0.6399945672912127
Loss in iteration 177 : 0.6383536738467108
Loss in iteration 178 : 0.6397651209178449
Loss in iteration 179 : 0.6381320347418296
Loss in iteration 180 : 0.639554570174897
Loss in iteration 181 : 0.6379220177462347
Loss in iteration 182 : 0.6393832494599843
Loss in iteration 183 : 0.6377449534641932
Loss in iteration 184 : 0.6392626080139368
Loss in iteration 185 : 0.6376124614985409
Loss in iteration 186 : 0.6391941434820124
Loss in iteration 187 : 0.6375262235744436
Loss in iteration 188 : 0.6391709291759368
Loss in iteration 189 : 0.637480205047674
Loss in iteration 190 : 0.6391808289430112
Loss in iteration 191 : 0.637464148849616
Loss in iteration 192 : 0.6392101651990604
Loss in iteration 193 : 0.6374670548775653
Loss in iteration 194 : 0.6392467334716718
Loss in iteration 195 : 0.6374796725573677
Loss in iteration 196 : 0.639281510645179
Loss in iteration 197 : 0.6374955854518071
Loss in iteration 198 : 0.639308968398222
Loss in iteration 199 : 0.6375110238340242
Loss in iteration 200 : 0.6393263643162044
Loss in iteration 201 : 0.6375239172885916
Loss in iteration 202 : 0.6393326064060059
Loss in iteration 203 : 0.6375328094664305
Loss in iteration 204 : 0.6393272512474351
Loss in iteration 205 : 0.637536126514162
Loss in iteration 206 : 0.63930997655733
Loss in iteration 207 : 0.6375320209018221
Loss in iteration 208 : 0.639280586080293
Loss in iteration 209 : 0.6375187279110274
Loss in iteration 210 : 0.6392393716681816
Loss in iteration 211 : 0.6374951739271097
Loss in iteration 212 : 0.6391875440088605
Loss in iteration 213 : 0.637461514406926
Loss in iteration 214 : 0.6391274635992269
Loss in iteration 215 : 0.6374193495755532
Loss in iteration 216 : 0.6390625229385652
Loss in iteration 217 : 0.6373715187145707
Loss in iteration 218 : 0.6389966881033476
Loss in iteration 219 : 0.6373215409018668
Loss in iteration 220 : 0.6389338398909343
Loss in iteration 221 : 0.6372728897516342
Loss in iteration 222 : 0.6388771186954778
Loss in iteration 223 : 0.6372283276592662
Loss in iteration 224 : 0.6388284614660138
Loss in iteration 225 : 0.6371894812794507
Loss in iteration 226 : 0.638788441371835
Loss in iteration 227 : 0.6371567427100556
Loss in iteration 228 : 0.6387564172186517
Loss in iteration 229 : 0.6371294715999385
Loss in iteration 230 : 0.638730908877982
Loss in iteration 231 : 0.6371063901369525
Loss in iteration 232 : 0.6387100646716543
Loss in iteration 233 : 0.6370860278997428
Loss in iteration 234 : 0.6386920860711912
Loss in iteration 235 : 0.6370670890083094
Loss in iteration 236 : 0.6386755153195729
Loss in iteration 237 : 0.6370486650490005
Loss in iteration 238 : 0.6386593519316862
Loss in iteration 239 : 0.6370302803468517
Loss in iteration 240 : 0.6386430211770653
Loss in iteration 241 : 0.6370118086698323
Loss in iteration 242 : 0.6386262540920943
Loss in iteration 243 : 0.6369933279973474
Loss in iteration 244 : 0.6386089473281102
Loss in iteration 245 : 0.6369749795172566
Loss in iteration 246 : 0.638591055890826
Loss in iteration 247 : 0.6369568753792298
Loss in iteration 248 : 0.638572543359531
Loss in iteration 249 : 0.6369390692088427
Loss in iteration 250 : 0.6385533852175513
Loss in iteration 251 : 0.6369215763770777
Loss in iteration 252 : 0.6385336014355669
Loss in iteration 253 : 0.6369044158682868
Loss in iteration 254 : 0.6385132888797291
Loss in iteration 255 : 0.6368876446938146
Loss in iteration 256 : 0.638492631133716
Loss in iteration 257 : 0.6368713661232726
Loss in iteration 258 : 0.6384718775357014
Loss in iteration 259 : 0.6368557083046797
Loss in iteration 260 : 0.6384512979086598
Loss in iteration 261 : 0.6368407834831303
Loss in iteration 262 : 0.6384311291329311
Loss in iteration 263 : 0.6368266453333692
Loss in iteration 264 : 0.6384115317738055
Loss in iteration 265 : 0.6368132613882735
Loss in iteration 266 : 0.6383925700149782
Loss in iteration 267 : 0.6368005107126189
Loss in iteration 268 : 0.638374219090176
Loss in iteration 269 : 0.6367882073039007
Loss in iteration 270 : 0.6383563950708303
Loss in iteration 271 : 0.6367761410333683
Loss in iteration 272 : 0.6383389954976391
Loss in iteration 273 : 0.6367641230977042
Loss in iteration 274 : 0.6383219376096485
Loss in iteration 275 : 0.6367520229793748
Loss in iteration 276 : 0.638305183650756
Loss in iteration 277 : 0.6367397880431032
Loss in iteration 278 : 0.6382887482952919
Loss in iteration 279 : 0.6367274431311459
Loss in iteration 280 : 0.6382726893630059
Loss in iteration 281 : 0.6367150735251714
Loss in iteration 282 : 0.6382570876473533
Loss in iteration 283 : 0.6367027985772182
Loss in iteration 284 : 0.6382420236748318
Loss in iteration 285 : 0.6366907443451885
Loss in iteration 286 : 0.6382275584452558
Loss in iteration 287 : 0.6366790219563571
Loss in iteration 288 : 0.638213722476965
Loss in iteration 289 : 0.6366677152138197
Loss in iteration 290 : 0.6382005140639349
Loss in iteration 291 : 0.6366568774948677
Loss in iteration 292 : 0.6381879047674061
Loss in iteration 293 : 0.6366465354015238
Loss in iteration 294 : 0.6381758486016217
Loss in iteration 295 : 0.636636695514173
Loss in iteration 296 : 0.6381642913193166
Loss in iteration 297 : 0.636627350956209
Loss in iteration 298 : 0.6381531773098982
Loss in iteration 299 : 0.6366184858100193
Loss in iteration 300 : 0.6381424532397642
Loss in iteration 301 : 0.6366100770314805
Loss in iteration 302 : 0.6381320690167825
Loss in iteration 303 : 0.6366020947619458
Loss in iteration 304 : 0.6381219774869585
Loss in iteration 305 : 0.6365945024721322
Loss in iteration 306 : 0.6381121343290017
Loss in iteration 307 : 0.6365872581636043
Loss in iteration 308 : 0.6381024990607695
Loss in iteration 309 : 0.6365803171439013
Loss in iteration 310 : 0.6380930372513366
Loss in iteration 311 : 0.6365736360539982
Loss in iteration 312 : 0.6380837233094158
Loss in iteration 313 : 0.6365671772073768
Loss in iteration 314 : 0.6380745428526534
Loss in iteration 315 : 0.6365609121015289
Loss in iteration 316 : 0.6380654937349413
Loss in iteration 317 : 0.6365548232013811
Loss in iteration 318 : 0.6380565852311928
Loss in iteration 319 : 0.6365489036285106
Loss in iteration 320 : 0.6380478354535414
Loss in iteration 321 : 0.6365431549986231
Loss in iteration 322 : 0.6380392675805602
Loss in iteration 323 : 0.6365375841205787
Loss in iteration 324 : 0.6380309057614807
Loss in iteration 325 : 0.6365321994714379
Loss in iteration 326 : 0.6380227715511214
Loss in iteration 327 : 0.6365270082716092
Loss in iteration 328 : 0.638014881482881
Loss in iteration 329 : 0.6365220146774295
Loss in iteration 330 : 0.6380072460112683
Loss in iteration 331 : 0.6365172192162386
Loss in iteration 332 : 0.637999869686605
Loss in iteration 333 : 0.6365126192441243
Loss in iteration 334 : 0.6379927521698252
Loss in iteration 335 : 0.6365082100016123
Loss in iteration 336 : 0.6379858896057898
Loss in iteration 337 : 0.636503985807123
Loss in iteration 338 : 0.6379792759376628
Loss in iteration 339 : 0.6364999410334485
Loss in iteration 340 : 0.6379729039073307
Loss in iteration 341 : 0.6364960706915553
Loss in iteration 342 : 0.63796676567256
Loss in iteration 343 : 0.6364923706244251
Loss in iteration 344 : 0.637960853117295
Loss in iteration 345 : 0.6364888374349411
Loss in iteration 346 : 0.6379551580011603
Loss in iteration 347 : 0.6364854683116393
Loss in iteration 348 : 0.6379496720862965
Loss in iteration 349 : 0.6364822608838511
Loss in iteration 350 : 0.6379443873191346
Loss in iteration 351 : 0.636479213164672
Loss in iteration 352 : 0.63793929606856
Loss in iteration 353 : 0.6364763235647064
Loss in iteration 354 : 0.6379343913640696
Loss in iteration 355 : 0.6364735909115623
Loss in iteration 356 : 0.6379296670573517
Loss in iteration 357 : 0.6364710144030987
Loss in iteration 358 : 0.6379251178493791
Loss in iteration 359 : 0.6364685934515025
Loss in iteration 360 : 0.6379207391692504
Loss in iteration 361 : 0.6364663274231044
Loss in iteration 362 : 0.637916526939687
Loss in iteration 363 : 0.6364642153238426
Loss in iteration 364 : 0.6379124772985099
Loss in iteration 365 : 0.6364622555055901
Loss in iteration 366 : 0.637908586354456
Loss in iteration 367 : 0.6364604454666796
Loss in iteration 368 : 0.6379048500390829
Loss in iteration 369 : 0.6364587817943554
Loss in iteration 370 : 0.637901264082651
Loss in iteration 371 : 0.6364572602581238
Loss in iteration 372 : 0.6378978241037733
Loss in iteration 373 : 0.6364558760253425
Loss in iteration 374 : 0.6378945257728756
Loss in iteration 375 : 0.6364546239447202
Loss in iteration 376 : 0.6378913649955393
Loss in iteration 377 : 0.6364534988358648
Loss in iteration 378 : 0.637888338065449
Loss in iteration 379 : 0.6364524957326164
Loss in iteration 380 : 0.6378854417535411
Loss in iteration 381 : 0.6364516100488911
Loss in iteration 382 : 0.6378826733224092
Loss in iteration 383 : 0.636450837659876
Loss in iteration 384 : 0.6378800304757668
Loss in iteration 385 : 0.6364501749113376
Loss in iteration 386 : 0.6378775112657301
Loss in iteration 387 : 0.6364496185809091
Loss in iteration 388 : 0.6378751139843604
Loss in iteration 389 : 0.6364491658166938
Loss in iteration 390 : 0.6378728370612562
Loss in iteration 391 : 0.6364488140726926
Loss in iteration 392 : 0.6378706789800124
Loss in iteration 393 : 0.6364485610514621
Loss in iteration 394 : 0.637868638216666
Loss in iteration 395 : 0.6364484046558742
Loss in iteration 396 : 0.6378667131965658
Loss in iteration 397 : 0.6364483429467683
Loss in iteration 398 : 0.6378649022634224
Loss in iteration 399 : 0.6364483741022419
Loss in iteration 400 : 0.6378632036554027
Testing accuracy  of updater 8 on alg 0 with rate 2.0 = 0.717875, training accuracy 0.717875, time elapsed: 3585 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6815338148924077
Loss in iteration 3 : 0.67535109899594
Loss in iteration 4 : 0.6656071826582556
Loss in iteration 5 : 0.6533916294664676
Loss in iteration 6 : 0.6416021809667314
Loss in iteration 7 : 0.6307429138288498
Loss in iteration 8 : 0.6200880040841714
Loss in iteration 9 : 0.6093925817165118
Loss in iteration 10 : 0.5989588102747639
Loss in iteration 11 : 0.5890962752320925
Loss in iteration 12 : 0.5799222579938146
Loss in iteration 13 : 0.5714444678739735
Loss in iteration 14 : 0.5636484337170752
Loss in iteration 15 : 0.5565143596091525
Loss in iteration 16 : 0.5500083671956566
Loss in iteration 17 : 0.5440831008602156
Loss in iteration 18 : 0.5386873247904348
Loss in iteration 19 : 0.533773694014185
Loss in iteration 20 : 0.529300698893497
Loss in iteration 21 : 0.5252312319545497
Loss in iteration 22 : 0.5215309027304333
Loss in iteration 23 : 0.5181672385932918
Loss in iteration 24 : 0.5151095082024404
Loss in iteration 25 : 0.5123287175311828
Loss in iteration 26 : 0.509797624124187
Loss in iteration 27 : 0.5074907972533926
Loss in iteration 28 : 0.5053847364942271
Loss in iteration 29 : 0.5034579964599792
Loss in iteration 30 : 0.5016912521598804
Loss in iteration 31 : 0.5000672742137207
Loss in iteration 32 : 0.49857082239384404
Loss in iteration 33 : 0.497188484158125
Loss in iteration 34 : 0.49590848411524285
Loss in iteration 35 : 0.4947204832843846
Loss in iteration 36 : 0.49361538165485275
Loss in iteration 37 : 0.49258513474795007
Loss in iteration 38 : 0.49162259242223716
Loss in iteration 39 : 0.49072136470015926
Loss in iteration 40 : 0.4898757153399779
Loss in iteration 41 : 0.4890804803669214
Loss in iteration 42 : 0.4883310065906395
Loss in iteration 43 : 0.4876231043809114
Loss in iteration 44 : 0.4869530093302011
Loss in iteration 45 : 0.48631734847643443
Loss in iteration 46 : 0.48571310812541857
Loss in iteration 47 : 0.4851376017090996
Loss in iteration 48 : 0.4845884373165379
Loss in iteration 49 : 0.48406348538528465
Loss in iteration 50 : 0.4835608474826589
Loss in iteration 51 : 0.4830788271755001
Loss in iteration 52 : 0.48261590378757113
Loss in iteration 53 : 0.4821707095050856
Loss in iteration 54 : 0.48174200992818517
Loss in iteration 55 : 0.48132868785974037
Loss in iteration 56 : 0.4809297299138432
Loss in iteration 57 : 0.4805442154229553
Loss in iteration 58 : 0.48017130711135075
Loss in iteration 59 : 0.47981024305840164
Loss in iteration 60 : 0.47946032957059637
Loss in iteration 61 : 0.4791209346904747
Loss in iteration 62 : 0.47879148217499384
Loss in iteration 63 : 0.4784714458623312
Loss in iteration 64 : 0.4781603444088507
Loss in iteration 65 : 0.47785773641569274
Loss in iteration 66 : 0.47756321597971113
Loss in iteration 67 : 0.4772764087008674
Loss in iteration 68 : 0.4769969681633945
Loss in iteration 69 : 0.47672457288705616
Loss in iteration 70 : 0.4764589237227917
Loss in iteration 71 : 0.47619974164838913
Loss in iteration 72 : 0.4759467659070245
Loss in iteration 73 : 0.4756997524260274
Loss in iteration 74 : 0.47545847245446754
Loss in iteration 75 : 0.47522271136513766
Loss in iteration 76 : 0.47499226757696866
Loss in iteration 77 : 0.4747669515658618
Loss in iteration 78 : 0.47454658494348056
Loss in iteration 79 : 0.47433099959320646
Loss in iteration 80 : 0.4741200368595653
Loss in iteration 81 : 0.47391354679164455
Loss in iteration 82 : 0.4737113874427954
Loss in iteration 83 : 0.4735134242286719
Loss in iteration 84 : 0.4733195293442209
Loss in iteration 85 : 0.47312958123830073
Loss in iteration 86 : 0.47294346414263166
Loss in iteration 87 : 0.47276106765028314
Loss in iteration 88 : 0.4725822863379875
Loss in iteration 89 : 0.47240701942630403
Loss in iteration 90 : 0.47223517047196456
Loss in iteration 91 : 0.47206664708739643
Loss in iteration 92 : 0.47190136068335264
Loss in iteration 93 : 0.4717392262315633
Loss in iteration 94 : 0.47158016204514086
Loss in iteration 95 : 0.4714240895752516
Loss in iteration 96 : 0.4712709332229954
Loss in iteration 97 : 0.4711206201657474
Loss in iteration 98 : 0.4709730801972954
Loss in iteration 99 : 0.4708282455810521
Loss in iteration 100 : 0.47068605091557547
Loss in iteration 101 : 0.47054643301139526
Loss in iteration 102 : 0.4704093307781523
Loss in iteration 103 : 0.4702746851209194
Loss in iteration 104 : 0.4701424388446005
Loss in iteration 105 : 0.47001253656538833
Loss in iteration 106 : 0.4698849246283461
Loss in iteration 107 : 0.46975955103032174
Loss in iteration 108 : 0.46963636534757314
Loss in iteration 109 : 0.46951531866759205
Loss in iteration 110 : 0.46939636352476904
Loss in iteration 111 : 0.46927945383961867
Loss in iteration 112 : 0.46916454486135556
Loss in iteration 113 : 0.4690515931136537
Loss in iteration 114 : 0.4689405563434332
Loss in iteration 115 : 0.4688313934725344
Loss in iteration 116 : 0.46872406455211413
Loss in iteration 117 : 0.4686185307195947
Loss in iteration 118 : 0.4685147541580258
Loss in iteration 119 : 0.4684126980576503
Loss in iteration 120 : 0.4683123265795258
Loss in iteration 121 : 0.4682136048210305
Loss in iteration 122 : 0.46811649878312506
Loss in iteration 123 : 0.4680209753392047
Loss in iteration 124 : 0.4679270022054561
Loss in iteration 125 : 0.4678345479125946
Loss in iteration 126 : 0.4677435817789053
Loss in iteration 127 : 0.46765407388449226
Loss in iteration 128 : 0.46756599504670227
Loss in iteration 129 : 0.46747931679659177
Loss in iteration 130 : 0.4673940113564536
Loss in iteration 131 : 0.46731005161829037
Loss in iteration 132 : 0.4672274111231991
Loss in iteration 133 : 0.46714606404162407
Loss in iteration 134 : 0.4670659851544076
Loss in iteration 135 : 0.46698714983460526
Loss in iteration 136 : 0.46690953403001895
Loss in iteration 137 : 0.4668331142464092
Loss in iteration 138 : 0.46675786753133786
Loss in iteration 139 : 0.466683771458631
Loss in iteration 140 : 0.46661080411340256
Loss in iteration 141 : 0.4665389440776339
Loss in iteration 142 : 0.466468170416269
Loss in iteration 143 : 0.46639846266381985
Loss in iteration 144 : 0.46632980081142666
Loss in iteration 145 : 0.4662621652944016
Loss in iteration 146 : 0.4661955369801762
Loss in iteration 147 : 0.46612989715669817
Loss in iteration 148 : 0.46606522752119744
Loss in iteration 149 : 0.46600151016935254
Loss in iteration 150 : 0.46593872758481186
Loss in iteration 151 : 0.46587686262906425
Loss in iteration 152 : 0.4658158985316515
Loss in iteration 153 : 0.4657558188806793
Loss in iteration 154 : 0.46569660761365284
Loss in iteration 155 : 0.4656382490085947
Loss in iteration 156 : 0.4655807276754448
Loss in iteration 157 : 0.4655240285477226
Loss in iteration 158 : 0.46546813687446
Loss in iteration 159 : 0.46541303821236174
Loss in iteration 160 : 0.46535871841822757
Loss in iteration 161 : 0.46530516364157415
Loss in iteration 162 : 0.4652523603175051
Loss in iteration 163 : 0.46520029515977296
Loss in iteration 164 : 0.46514895515404536
Loss in iteration 165 : 0.46509832755138436
Loss in iteration 166 : 0.46504839986188656
Loss in iteration 167 : 0.4649991598485326
Loss in iteration 168 : 0.46495059552119516
Loss in iteration 169 : 0.4649026951308174
Loss in iteration 170 : 0.46485544716376753
Loss in iteration 171 : 0.464808840336331
Loss in iteration 172 : 0.4647628635893731
Loss in iteration 173 : 0.46471750608314244
Loss in iteration 174 : 0.4646727571922089
Loss in iteration 175 : 0.4646286065005547
Loss in iteration 176 : 0.4645850437967867
Loss in iteration 177 : 0.46454205906948337
Loss in iteration 178 : 0.4644996425026602
Loss in iteration 179 : 0.46445778447137126
Loss in iteration 180 : 0.4644164755374048
Loss in iteration 181 : 0.4643757064451116
Loss in iteration 182 : 0.4643354681173342
Loss in iteration 183 : 0.4642957516514493
Loss in iteration 184 : 0.46425654831549784
Loss in iteration 185 : 0.46421784954443157
Loss in iteration 186 : 0.4641796469364558
Loss in iteration 187 : 0.46414193224944694
Loss in iteration 188 : 0.464104697397487
Loss in iteration 189 : 0.4640679344474653
Loss in iteration 190 : 0.4640316356157871
Loss in iteration 191 : 0.46399579326513174
Loss in iteration 192 : 0.46396039990133486
Loss in iteration 193 : 0.4639254481703164
Loss in iteration 194 : 0.46389093085509103
Loss in iteration 195 : 0.4638568408728676
Loss in iteration 196 : 0.4638231712721983
Loss in iteration 197 : 0.46378991523021645
Loss in iteration 198 : 0.4637570660499249
Loss in iteration 199 : 0.46372461715757257
Loss in iteration 200 : 0.46369256210007
Loss in iteration 201 : 0.4636608945424779
Loss in iteration 202 : 0.4636296082655673
Loss in iteration 203 : 0.4635986971634206
Loss in iteration 204 : 0.46356815524109235
Loss in iteration 205 : 0.46353797661233975
Loss in iteration 206 : 0.4635081554973924
Loss in iteration 207 : 0.4634786862207753
Loss in iteration 208 : 0.46344956320919506
Loss in iteration 209 : 0.4634207809894624
Loss in iteration 210 : 0.4633923341864679
Loss in iteration 211 : 0.4633642175212117
Loss in iteration 212 : 0.46333642580885875
Loss in iteration 213 : 0.4633089539568681
Loss in iteration 214 : 0.46328179696314
Loss in iteration 215 : 0.463254949914212
Loss in iteration 216 : 0.463228407983509
Loss in iteration 217 : 0.46320216642961454
Loss in iteration 218 : 0.46317622059459174
Loss in iteration 219 : 0.46315056590234577
Loss in iteration 220 : 0.4631251978570033
Loss in iteration 221 : 0.46310011204135737
Loss in iteration 222 : 0.46307530411532605
Loss in iteration 223 : 0.4630507698144485
Loss in iteration 224 : 0.4630265049484194
Loss in iteration 225 : 0.46300250539965726
Loss in iteration 226 : 0.4629787671218886
Loss in iteration 227 : 0.4629552861387909
Loss in iteration 228 : 0.46293205854263914
Loss in iteration 229 : 0.46290908049299184
Loss in iteration 230 : 0.46288634821541097
Loss in iteration 231 : 0.4628638580002009
Loss in iteration 232 : 0.46284160620117804
Loss in iteration 233 : 0.46281958923447103
Loss in iteration 234 : 0.4627978035773343
Loss in iteration 235 : 0.46277624576700566
Loss in iteration 236 : 0.46275491239956973
Loss in iteration 237 : 0.4627338001288562
Loss in iteration 238 : 0.46271290566536266
Loss in iteration 239 : 0.4626922257751904
Loss in iteration 240 : 0.4626717572790109
Loss in iteration 241 : 0.4626514970510538
Loss in iteration 242 : 0.4626314420181102
Loss in iteration 243 : 0.4626115891585645
Loss in iteration 244 : 0.4625919355014376
Loss in iteration 245 : 0.46257247812546054
Loss in iteration 246 : 0.4625532141581539
Loss in iteration 247 : 0.4625341407749421
Loss in iteration 248 : 0.4625152551982716
Loss in iteration 249 : 0.4624965546967513
Loss in iteration 250 : 0.46247803658431913
Loss in iteration 251 : 0.4624596982194162
Loss in iteration 252 : 0.4624415370041746
Loss in iteration 253 : 0.46242355038363403
Loss in iteration 254 : 0.4624057358449652
Loss in iteration 255 : 0.46238809091671124
Loss in iteration 256 : 0.46237061316804173
Loss in iteration 257 : 0.4623533002080283
Loss in iteration 258 : 0.4623361496849286
Loss in iteration 259 : 0.46231915928548556
Loss in iteration 260 : 0.46230232673424076
Loss in iteration 261 : 0.46228564979286874
Loss in iteration 262 : 0.4622691262595086
Loss in iteration 263 : 0.4622527539681221
Loss in iteration 264 : 0.462236530787861
Loss in iteration 265 : 0.4622204546224492
Loss in iteration 266 : 0.46220452340956364
Loss in iteration 267 : 0.46218873512024594
Loss in iteration 268 : 0.46217308775831767
Loss in iteration 269 : 0.4621575793598022
Loss in iteration 270 : 0.4621422079923614
Loss in iteration 271 : 0.4621269717547463
Loss in iteration 272 : 0.4621118687762548
Loss in iteration 273 : 0.46209689721620123
Loss in iteration 274 : 0.4620820552633934
Loss in iteration 275 : 0.4620673411356233
Loss in iteration 276 : 0.4620527530791644
Loss in iteration 277 : 0.46203828936828584
Testing accuracy  of updater 8 on alg 0 with rate 0.2 = 0.79075, training accuracy 0.79075, time elapsed: 2456 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6910578862042606
Loss in iteration 3 : 0.6883948095293277
Loss in iteration 4 : 0.6854443727261849
Loss in iteration 5 : 0.6824248193880713
Loss in iteration 6 : 0.6794701033890008
Loss in iteration 7 : 0.6766348661931947
Loss in iteration 8 : 0.6739135610960965
Loss in iteration 9 : 0.6712653941091336
Loss in iteration 10 : 0.6686379295178027
Loss in iteration 11 : 0.6659848635805274
Loss in iteration 12 : 0.663276285777203
Loss in iteration 13 : 0.6605018982006365
Loss in iteration 14 : 0.6576689091259643
Loss in iteration 15 : 0.654796767725578
Loss in iteration 16 : 0.6519107963115552
Loss in iteration 17 : 0.6490363341388814
Loss in iteration 18 : 0.6461944183249952
Loss in iteration 19 : 0.6433994322104918
Loss in iteration 20 : 0.6406586484603651
Loss in iteration 21 : 0.6379732442873094
Loss in iteration 22 : 0.6353401921328673
Loss in iteration 23 : 0.6327544181136159
Loss in iteration 24 : 0.6302107323064344
Loss in iteration 25 : 0.6277052150298801
Loss in iteration 26 : 0.6252359372302125
Loss in iteration 27 : 0.6228030579247207
Loss in iteration 28 : 0.6204084520645025
Loss in iteration 29 : 0.6180550711048166
Loss in iteration 30 : 0.6157462336492479
Loss in iteration 31 : 0.6134850012772943
Loss in iteration 32 : 0.6112737343959579
Loss in iteration 33 : 0.6091138619177111
Loss in iteration 34 : 0.607005848959737
Loss in iteration 35 : 0.6049493147754996
Loss in iteration 36 : 0.6029432395743722
Loss in iteration 37 : 0.6009862007251018
Loss in iteration 38 : 0.5990765910348286
Loss in iteration 39 : 0.5972127889275962
Loss in iteration 40 : 0.595393267771453
Loss in iteration 41 : 0.5936166460724077
Loss in iteration 42 : 0.5918816900609059
Loss in iteration 43 : 0.5901872849983274
Loss in iteration 44 : 0.5885323919787231
Loss in iteration 45 : 0.5869160043189626
Loss in iteration 46 : 0.5853371132281623
Loss in iteration 47 : 0.5837946876178645
Loss in iteration 48 : 0.5822876686460754
Loss in iteration 49 : 0.5808149764808038
Loss in iteration 50 : 0.579375525042607
Loss in iteration 51 : 0.5779682400518573
Loss in iteration 52 : 0.5765920762718327
Loss in iteration 53 : 0.5752460310148217
Loss in iteration 54 : 0.5739291523785464
Loss in iteration 55 : 0.5726405419871372
Loss in iteration 56 : 0.5713793530140339
Loss in iteration 57 : 0.5701447848665842
Loss in iteration 58 : 0.5689360761162063
Loss in iteration 59 : 0.5677524971358815
Loss in iteration 60 : 0.5665933435688896
Loss in iteration 61 : 0.5654579313162581
Loss in iteration 62 : 0.5643455932976309
Loss in iteration 63 : 0.5632556778838735
Loss in iteration 64 : 0.5621875486592288
Loss in iteration 65 : 0.5611405850547021
Loss in iteration 66 : 0.560114183387175
Loss in iteration 67 : 0.5591077579112179
Loss in iteration 68 : 0.5581207416077959
Loss in iteration 69 : 0.5571525865635628
Loss in iteration 70 : 0.5562027639108478
Loss in iteration 71 : 0.555270763385758
Loss in iteration 72 : 0.5543560926135063
Loss in iteration 73 : 0.5534582762476764
Loss in iteration 74 : 0.5525768550802873
Loss in iteration 75 : 0.5517113852117845
Loss in iteration 76 : 0.5508614373346307
Loss in iteration 77 : 0.5500265961489307
Loss in iteration 78 : 0.5492064599000942
Loss in iteration 79 : 0.5484006400095757
Loss in iteration 80 : 0.5476087607612634
Loss in iteration 81 : 0.5468304590065488
Loss in iteration 82 : 0.5460653838581515
Loss in iteration 83 : 0.5453131963532447
Loss in iteration 84 : 0.5445735690778664
Loss in iteration 85 : 0.5438461857545721
Loss in iteration 86 : 0.5431307408025229
Loss in iteration 87 : 0.5424269388833419
Loss in iteration 88 : 0.5417344944469115
Loss in iteration 89 : 0.5410531312898509
Loss in iteration 90 : 0.5403825821362405
Loss in iteration 91 : 0.5397225882463772
Loss in iteration 92 : 0.5390728990555924
Loss in iteration 93 : 0.5384332718421151
Loss in iteration 94 : 0.5378034714208126
Loss in iteration 95 : 0.5371832698585829
Loss in iteration 96 : 0.5365724462069176
Loss in iteration 97 : 0.5359707862477676
Loss in iteration 98 : 0.5353780822496491
Loss in iteration 99 : 0.5347941327320661
Loss in iteration 100 : 0.5342187422372713
Loss in iteration 101 : 0.5336517211092067
Loss in iteration 102 : 0.5330928852799266
Loss in iteration 103 : 0.5325420560640409
Loss in iteration 104 : 0.53199905996168
Loss in iteration 105 : 0.5314637284703317
Loss in iteration 106 : 0.5309358979055719
Loss in iteration 107 : 0.5304154092304784
Loss in iteration 108 : 0.5299021078932485
Loss in iteration 109 : 0.5293958436723983
Loss in iteration 110 : 0.5288964705288378
Loss in iteration 111 : 0.5284038464641362
Loss in iteration 112 : 0.5279178333843354
Loss in iteration 113 : 0.5274382969688347
Loss in iteration 114 : 0.5269651065439496
Loss in iteration 115 : 0.5264981349608673
Loss in iteration 116 : 0.5260372584778622
Loss in iteration 117 : 0.5255823566466633
Loss in iteration 118 : 0.5251333122028994
Loss in iteration 119 : 0.5246900109606117
Loss in iteration 120 : 0.5242523417107287
Loss in iteration 121 : 0.5238201961234756
Loss in iteration 122 : 0.5233934686545749
Loss in iteration 123 : 0.5229720564551185
Loss in iteration 124 : 0.5225558592849768
Loss in iteration 125 : 0.5221447794295766
Loss in iteration 126 : 0.5217387216198603
Loss in iteration 127 : 0.5213375929553279
Loss in iteration 128 : 0.5209413028299564
Loss in iteration 129 : 0.5205497628608796
Loss in iteration 130 : 0.5201628868197272
Loss in iteration 131 : 0.519780590566485
Loss in iteration 132 : 0.5194027919857801
Loss in iteration 133 : 0.5190294109255038
Loss in iteration 134 : 0.5186603691376911
Loss in iteration 135 : 0.5182955902215434
Loss in iteration 136 : 0.5179349995685347
Loss in iteration 137 : 0.5175785243095103
Loss in iteration 138 : 0.5172260932636855
Loss in iteration 139 : 0.5168776368894802
Loss in iteration 140 : 0.5165330872370921
Loss in iteration 141 : 0.5161923779027439
Loss in iteration 142 : 0.5158554439845328
Loss in iteration 143 : 0.5155222220397944
Loss in iteration 144 : 0.5151926500439395
Loss in iteration 145 : 0.5148666673506979
Loss in iteration 146 : 0.5145442146536845
Loss in iteration 147 : 0.5142252339492777
Loss in iteration 148 : 0.5139096685007336
Loss in iteration 149 : 0.5135974628034828
Loss in iteration 150 : 0.5132885625515801
Loss in iteration 151 : 0.5129829146052675
Loss in iteration 152 : 0.5126804669595791
Loss in iteration 153 : 0.5123811687139833
Loss in iteration 154 : 0.5120849700430062
Loss in iteration 155 : 0.5117918221677936
Loss in iteration 156 : 0.5115016773285892
Loss in iteration 157 : 0.5112144887580893
Loss in iteration 158 : 0.5109302106556404
Loss in iteration 159 : 0.5106487981622414
Loss in iteration 160 : 0.5103702073363409
Loss in iteration 161 : 0.5100943951303754
Loss in iteration 162 : 0.5098213193680387
Loss in iteration 163 : 0.5095509387222437
Loss in iteration 164 : 0.5092832126937693
Loss in iteration 165 : 0.5090181015905424
Loss in iteration 166 : 0.5087555665075602
Loss in iteration 167 : 0.5084955693074035
Loss in iteration 168 : 0.5082380726013385
Loss in iteration 169 : 0.5079830397309797
Loss in iteration 170 : 0.5077304347504964
Loss in iteration 171 : 0.5074802224093385
Loss in iteration 172 : 0.5072323681354676
Loss in iteration 173 : 0.5069868380190766
Loss in iteration 174 : 0.5067435987967854
Loss in iteration 175 : 0.5065026178362796
Loss in iteration 176 : 0.506263863121394
Loss in iteration 177 : 0.5060273032376269
Loss in iteration 178 : 0.5057929073580552
Loss in iteration 179 : 0.5055606452296487
Loss in iteration 180 : 0.5053304871599671
Loss in iteration 181 : 0.5051024040042378
Loss in iteration 182 : 0.5048763671527712
Loss in iteration 183 : 0.5046523485187483
Loss in iteration 184 : 0.5044303205263263
Loss in iteration 185 : 0.5042102560990765
Loss in iteration 186 : 0.5039921286487274
Loss in iteration 187 : 0.5037759120642377
Loss in iteration 188 : 0.5035615807011274
Loss in iteration 189 : 0.5033491093711304
Loss in iteration 190 : 0.5031384733320929
Loss in iteration 191 : 0.5029296482781714
Loss in iteration 192 : 0.5027226103302537
Loss in iteration 193 : 0.5025173360266654
Loss in iteration 194 : 0.502313802314104
Loss in iteration 195 : 0.5021119865388052
Loss in iteration 196 : 0.5019118664379493
Loss in iteration 197 : 0.5017134201312852
Loss in iteration 198 : 0.5015166261129649
Loss in iteration 199 : 0.5013214632435977
Loss in iteration 200 : 0.5011279107424959
Loss in iteration 201 : 0.5009359481801258
Loss in iteration 202 : 0.5007455554707442
Loss in iteration 203 : 0.500556712865217
Loss in iteration 204 : 0.5003694009440319
Loss in iteration 205 : 0.5001836006104697
Loss in iteration 206 : 0.499999293083948
Loss in iteration 207 : 0.49981645989353546
Loss in iteration 208 : 0.49963508287163005
Loss in iteration 209 : 0.4994551441477676
Loss in iteration 210 : 0.4992766261426159
Loss in iteration 211 : 0.49909951156208276
Loss in iteration 212 : 0.4989237833915937
Loss in iteration 213 : 0.4987494248904826
Loss in iteration 214 : 0.4985764195865423
Loss in iteration 215 : 0.4984047512706768
Loss in iteration 216 : 0.4982344039917142
Loss in iteration 217 : 0.4980653620513097
Loss in iteration 218 : 0.49789760999898613
Loss in iteration 219 : 0.4977311326272966
Loss in iteration 220 : 0.4975659149670848
Loss in iteration 221 : 0.4974019422828658
Loss in iteration 222 : 0.4972392000683138
Loss in iteration 223 : 0.4970776740418468
Loss in iteration 224 : 0.49691735014232463
Loss in iteration 225 : 0.49675821452483054
Loss in iteration 226 : 0.49660025355657234
Loss in iteration 227 : 0.49644345381284694
Loss in iteration 228 : 0.49628780207312073
Loss in iteration 229 : 0.4961332853171897
Loss in iteration 230 : 0.49597989072142135
Loss in iteration 231 : 0.495827605655092
Loss in iteration 232 : 0.49567641767679027
Loss in iteration 233 : 0.49552631453091905
Loss in iteration 234 : 0.49537728414425924
Loss in iteration 235 : 0.4952293146226184
Loss in iteration 236 : 0.49508239424754913
Loss in iteration 237 : 0.4949365114731384
Loss in iteration 238 : 0.4947916549228765
Loss in iteration 239 : 0.4946478133865763
Loss in iteration 240 : 0.49450497581737535
Loss in iteration 241 : 0.49436313132879733
Loss in iteration 242 : 0.4942222691918744
Loss in iteration 243 : 0.4940823788323325
Loss in iteration 244 : 0.49394344982783694
Loss in iteration 245 : 0.49380547190529916
Loss in iteration 246 : 0.49366843493823764
Loss in iteration 247 : 0.4935323289441898
Loss in iteration 248 : 0.4933971440821893
Loss in iteration 249 : 0.49326287065029306
Loss in iteration 250 : 0.4931294990831499
Loss in iteration 251 : 0.49299701994962564
Loss in iteration 252 : 0.4928654239504937
Loss in iteration 253 : 0.4927347019161427
Loss in iteration 254 : 0.4926048448043548
Loss in iteration 255 : 0.4924758436981193
Loss in iteration 256 : 0.4923476898035002
Loss in iteration 257 : 0.49222037444753053
Loss in iteration 258 : 0.49209388907617
Loss in iteration 259 : 0.49196822525228423
Loss in iteration 260 : 0.4918433746536819
Loss in iteration 261 : 0.4917193290711806
Loss in iteration 262 : 0.4915960804067156
Loss in iteration 263 : 0.4914736206714789
Loss in iteration 264 : 0.49135194198411225
Loss in iteration 265 : 0.49123103656891837
Loss in iteration 266 : 0.49111089675411856
Loss in iteration 267 : 0.4909915149701327
Loss in iteration 268 : 0.4908728837479118
Loss in iteration 269 : 0.49075499571728654
Loss in iteration 270 : 0.49063784360535145
Loss in iteration 271 : 0.49052142023488654
Loss in iteration 272 : 0.4904057185228096
Loss in iteration 273 : 0.49029073147864294
Loss in iteration 274 : 0.49017645220303585
Loss in iteration 275 : 0.4900628738862845
Loss in iteration 276 : 0.4899499898069162
Loss in iteration 277 : 0.4898377933302585
Loss in iteration 278 : 0.4897262779070817
Loss in iteration 279 : 0.48961543707222016
Loss in iteration 280 : 0.4895052644432623
Loss in iteration 281 : 0.48939575371923
Loss in iteration 282 : 0.48928689867931263
Loss in iteration 283 : 0.4891786931816017
Loss in iteration 284 : 0.4890711311618552
Loss in iteration 285 : 0.48896420663229934
Loss in iteration 286 : 0.4888579136804306
Loss in iteration 287 : 0.48875224646785415
Loss in iteration 288 : 0.4886471992291431
Loss in iteration 289 : 0.48854276627070603
Loss in iteration 290 : 0.4884389419696949
Loss in iteration 291 : 0.48833572077291465
Loss in iteration 292 : 0.4882330971957646
Loss in iteration 293 : 0.4881310658211962
Loss in iteration 294 : 0.4880296212986796
Loss in iteration 295 : 0.48792875834321436
Loss in iteration 296 : 0.48782847173432126
Loss in iteration 297 : 0.4877287563150863
Loss in iteration 298 : 0.48762960699120267
Loss in iteration 299 : 0.48753101873003457
Loss in iteration 300 : 0.4874329865597001
Loss in iteration 301 : 0.48733550556816374
Loss in iteration 302 : 0.48723857090235134
Loss in iteration 303 : 0.4871421777672844
Loss in iteration 304 : 0.4870463214252102
Loss in iteration 305 : 0.48695099719477364
Loss in iteration 306 : 0.48685620045018424
Loss in iteration 307 : 0.4867619266204049
Loss in iteration 308 : 0.4866681711883549
Loss in iteration 309 : 0.4865749296901229
Loss in iteration 310 : 0.48648219771419654
Loss in iteration 311 : 0.48638997090071123
Loss in iteration 312 : 0.4862982449406946
Loss in iteration 313 : 0.4862070155753429
Loss in iteration 314 : 0.4861162785953008
Loss in iteration 315 : 0.48602602983994925
Loss in iteration 316 : 0.4859362651967136
Loss in iteration 317 : 0.4858469806003856
Loss in iteration 318 : 0.48575817203243854
Loss in iteration 319 : 0.48566983552038034
Loss in iteration 320 : 0.48558196713709373
Loss in iteration 321 : 0.48549456300020205
Loss in iteration 322 : 0.4854076192714467
Loss in iteration 323 : 0.48532113215605693
Loss in iteration 324 : 0.4852350979021504
Loss in iteration 325 : 0.48514951280014046
Loss in iteration 326 : 0.485064373182138
Loss in iteration 327 : 0.4849796754213794
Loss in iteration 328 : 0.48489541593165847
Loss in iteration 329 : 0.48481159116676886
Loss in iteration 330 : 0.4847281976199494
Loss in iteration 331 : 0.4846452318233473
Loss in iteration 332 : 0.4845626903474809
Loss in iteration 333 : 0.4844805698007266
Loss in iteration 334 : 0.4843988668287914
Loss in iteration 335 : 0.4843175781142154
Loss in iteration 336 : 0.48423670037586386
Loss in iteration 337 : 0.48415623036845096
Loss in iteration 338 : 0.4840761648820379
Loss in iteration 339 : 0.48399650074157835
Loss in iteration 340 : 0.48391723480642895
Loss in iteration 341 : 0.4838383639699142
Loss in iteration 342 : 0.48375988515884666
Loss in iteration 343 : 0.4836817953331043
Loss in iteration 344 : 0.4836040914851804
Loss in iteration 345 : 0.4835267706397473
Loss in iteration 346 : 0.4834498298532446
Loss in iteration 347 : 0.483373266213447
Loss in iteration 348 : 0.48329707683906653
Loss in iteration 349 : 0.483221258879332
Loss in iteration 350 : 0.48314580951359815
Loss in iteration 351 : 0.48307072595095496
Loss in iteration 352 : 0.48299600542983595
Loss in iteration 353 : 0.4829216452176346
Loss in iteration 354 : 0.4828476426103336
Loss in iteration 355 : 0.4827739949321384
Loss in iteration 356 : 0.48270069953510153
Loss in iteration 357 : 0.4826277537987813
Loss in iteration 358 : 0.48255515512987057
Loss in iteration 359 : 0.48248290096186236
Loss in iteration 360 : 0.4824109887547052
Loss in iteration 361 : 0.4823394159944577
Loss in iteration 362 : 0.48226818019297046
Loss in iteration 363 : 0.4821972788875432
Loss in iteration 364 : 0.48212670964061977
Loss in iteration 365 : 0.4820564700394505
Loss in iteration 366 : 0.481986557695801
Loss in iteration 367 : 0.48191697024562813
Loss in iteration 368 : 0.4818477053487808
Loss in iteration 369 : 0.48177876068870773
Loss in iteration 370 : 0.48171013397215323
Loss in iteration 371 : 0.481641822928871
Loss in iteration 372 : 0.481573825311341
Loss in iteration 373 : 0.48150613889448335
Loss in iteration 374 : 0.4814387614753803
Loss in iteration 375 : 0.481371690873008
Loss in iteration 376 : 0.48130492492796106
Loss in iteration 377 : 0.48123846150219085
Loss in iteration 378 : 0.481172298478738
Loss in iteration 379 : 0.4811064337614858
Loss in iteration 380 : 0.4810408652748899
Loss in iteration 381 : 0.48097559096374015
Loss in iteration 382 : 0.4809106087929058
Loss in iteration 383 : 0.4808459167471003
Loss in iteration 384 : 0.4807815128306322
Loss in iteration 385 : 0.4807173950671737
Loss in iteration 386 : 0.4806535614995244
Loss in iteration 387 : 0.4805900101893842
Loss in iteration 388 : 0.4805267392171251
Loss in iteration 389 : 0.48046374668156755
Loss in iteration 390 : 0.48040103069976
Loss in iteration 391 : 0.48033858940675844
Loss in iteration 392 : 0.48027642095542045
Loss in iteration 393 : 0.4802145235161861
Loss in iteration 394 : 0.4801528952768723
Loss in iteration 395 : 0.4800915344424646
Loss in iteration 396 : 0.4800304392349243
Loss in iteration 397 : 0.479969607892975
Loss in iteration 398 : 0.4799090386719155
Loss in iteration 399 : 0.4798487298434197
Loss in iteration 400 : 0.4797886796953496
Testing accuracy  of updater 8 on alg 0 with rate 0.01999999999999999 = 0.7815, training accuracy 0.7815, time elapsed: 3605 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 12.569829085734174
Loss in iteration 3 : 13.880316658153774
Loss in iteration 4 : 8.01656966282436
Loss in iteration 5 : 9.181551262563625
Loss in iteration 6 : 6.868710291161589
Loss in iteration 7 : 4.3840141478995776
Loss in iteration 8 : 7.925858092046513
Loss in iteration 9 : 3.089631588936259
Loss in iteration 10 : 5.819007554891209
Loss in iteration 11 : 5.6138049390914855
Loss in iteration 12 : 2.4261894468436984
Loss in iteration 13 : 4.8616263305679945
Loss in iteration 14 : 4.518143093545742
Loss in iteration 15 : 2.530655831765028
Loss in iteration 16 : 4.344273712842106
Loss in iteration 17 : 3.8584510908143375
Loss in iteration 18 : 2.616707257922222
Loss in iteration 19 : 3.851840714637694
Loss in iteration 20 : 3.3488412445782996
Loss in iteration 21 : 2.429400420533823
Loss in iteration 22 : 3.3385344421293794
Loss in iteration 23 : 2.816023945490412
Loss in iteration 24 : 2.4040462982645256
Loss in iteration 25 : 3.0186591741522104
Loss in iteration 26 : 2.4779136073416486
Loss in iteration 27 : 2.1415486861230493
Loss in iteration 28 : 2.6069465804773437
Loss in iteration 29 : 1.9681903844424584
Loss in iteration 30 : 2.1172058877882454
Loss in iteration 31 : 2.2136074627713747
Loss in iteration 32 : 1.7436470477457082
Loss in iteration 33 : 2.0219463487106997
Loss in iteration 34 : 1.7109765525889933
Loss in iteration 35 : 1.6676263810083547
Loss in iteration 36 : 1.7224346619592712
Loss in iteration 37 : 1.4093322169279616
Loss in iteration 38 : 1.6439530718874293
Loss in iteration 39 : 1.2994242826258364
Loss in iteration 40 : 1.4896755472158256
Loss in iteration 41 : 1.2055869881545636
Loss in iteration 42 : 1.3928948830010752
Loss in iteration 43 : 1.1267215890734519
Loss in iteration 44 : 1.3301920292898457
Loss in iteration 45 : 1.070667684952798
Loss in iteration 46 : 1.2027638800284743
Loss in iteration 47 : 1.1661931785755055
Loss in iteration 48 : 1.0510193765837408
Loss in iteration 49 : 1.1349543349958373
Loss in iteration 50 : 1.056114689630855
Loss in iteration 51 : 1.0784635428287428
Loss in iteration 52 : 1.0027546075804286
Loss in iteration 53 : 1.0766585179779786
Loss in iteration 54 : 0.9775281764454544
Loss in iteration 55 : 1.2013451625655194
Loss in iteration 56 : 1.073893054787787
Loss in iteration 57 : 0.9049619013899659
Loss in iteration 58 : 1.1069739350641497
Loss in iteration 59 : 1.0093260713228756
Loss in iteration 60 : 0.8321899692331449
Loss in iteration 61 : 0.688698730799028
Loss in iteration 62 : 0.7178221783769257
Loss in iteration 63 : 0.8599585151092745
Loss in iteration 64 : 0.9624942176140819
Loss in iteration 65 : 0.9658661590926615
Loss in iteration 66 : 0.772020865921136
Loss in iteration 67 : 0.7922655685713961
Loss in iteration 68 : 0.6127731112218542
Loss in iteration 69 : 0.7737537636287668
Loss in iteration 70 : 0.5727806140121247
Loss in iteration 71 : 0.829448065407425
Loss in iteration 72 : 0.9900884412848165
Loss in iteration 73 : 1.7564896249791664
Loss in iteration 74 : 1.1535386638635763
Loss in iteration 75 : 0.7409687962864488
Loss in iteration 76 : 0.7528990111518833
Loss in iteration 77 : 0.7420647328913063
Loss in iteration 78 : 1.51159778335686
Loss in iteration 79 : 1.142518826644304
Loss in iteration 80 : 0.9147490706417388
Loss in iteration 81 : 0.5847423571280872
Loss in iteration 82 : 0.7775405556707414
Loss in iteration 83 : 0.8099058347404716
Loss in iteration 84 : 0.9406797010383554
Loss in iteration 85 : 0.7422198198198859
Loss in iteration 86 : 0.6574848663257894
Loss in iteration 87 : 0.6727888643486526
Loss in iteration 88 : 0.7056092819340104
Loss in iteration 89 : 0.9207348067127736
Loss in iteration 90 : 1.0119561403815127
Loss in iteration 91 : 0.9089110331661819
Loss in iteration 92 : 0.7852233493854162
Loss in iteration 93 : 0.5884784322281261
Loss in iteration 94 : 0.6476251310147614
Loss in iteration 95 : 0.5323446707865979
Loss in iteration 96 : 0.6474023276674918
Loss in iteration 97 : 0.5394867755678429
Loss in iteration 98 : 0.6437833480382317
Loss in iteration 99 : 0.6288882715047361
Loss in iteration 100 : 0.8093950511842449
Loss in iteration 101 : 1.2428605732047444
Loss in iteration 102 : 1.1025556771544878
Loss in iteration 103 : 1.1549567605160815
Loss in iteration 104 : 0.5921826544209273
Loss in iteration 105 : 0.7059504634354008
Loss in iteration 106 : 0.9463952329885608
Loss in iteration 107 : 0.7660340628905278
Loss in iteration 108 : 0.7610583397753089
Loss in iteration 109 : 0.5542308872337567
Loss in iteration 110 : 0.674445261711573
Loss in iteration 111 : 0.7015001176624374
Loss in iteration 112 : 0.6470619260814698
Loss in iteration 113 : 0.7231736413913167
Loss in iteration 114 : 0.5975057046382674
Loss in iteration 115 : 0.6329194034791336
Loss in iteration 116 : 0.585131431575199
Loss in iteration 117 : 0.5900328466722475
Loss in iteration 118 : 0.5724837278095878
Loss in iteration 119 : 0.6012667846557178
Loss in iteration 120 : 0.5632178390764466
Loss in iteration 121 : 0.6828822829636513
Loss in iteration 122 : 0.701979378526651
Loss in iteration 123 : 1.1003313539790884
Loss in iteration 124 : 1.0016651121286622
Loss in iteration 125 : 0.8993082709167912
Loss in iteration 126 : 0.692675142858317
Loss in iteration 127 : 0.561911233887068
Loss in iteration 128 : 0.5114388490008671
Loss in iteration 129 : 0.5165340427529416
Loss in iteration 130 : 0.5374810065026329
Loss in iteration 131 : 0.620177187828781
Loss in iteration 132 : 0.7656726000775703
Loss in iteration 133 : 0.871144685605936
Loss in iteration 134 : 0.8939114139799108
Loss in iteration 135 : 0.6314428472067912
Loss in iteration 136 : 0.5323374638425682
Loss in iteration 137 : 0.5117328159189289
Loss in iteration 138 : 0.5715011373507056
Loss in iteration 139 : 0.6637445664445608
Loss in iteration 140 : 0.6693549805995304
Loss in iteration 141 : 0.6518336090051716
Loss in iteration 142 : 0.5752942755914318
Loss in iteration 143 : 0.551031111886389
Loss in iteration 144 : 0.5232499632006823
Loss in iteration 145 : 0.5340755706168909
Loss in iteration 146 : 0.5560131828637728
Loss in iteration 147 : 0.6697185370420022
Loss in iteration 148 : 0.8509661760247906
Loss in iteration 149 : 1.1108728635752991
Loss in iteration 150 : 0.7151517199787788
Loss in iteration 151 : 0.5731052208273081
Loss in iteration 152 : 0.4992995434389748
Loss in iteration 153 : 0.5947832386503765
Loss in iteration 154 : 0.6490852481202379
Loss in iteration 155 : 0.6605821074681265
Loss in iteration 156 : 0.6452147135048472
Loss in iteration 157 : 0.5585487116175748
Loss in iteration 158 : 0.5768607640754309
Loss in iteration 159 : 0.5027442050562286
Loss in iteration 160 : 0.5563079264205887
Loss in iteration 161 : 0.49317872012639175
Loss in iteration 162 : 0.5740317193068432
Loss in iteration 163 : 0.5590852433364212
Loss in iteration 164 : 0.7259818620557358
Loss in iteration 165 : 0.7360649108265183
Loss in iteration 166 : 0.8081568676938341
Loss in iteration 167 : 0.6266229860526801
Loss in iteration 168 : 0.5446812900850833
Loss in iteration 169 : 0.4890762272365138
Loss in iteration 170 : 0.4930115817565545
Loss in iteration 171 : 0.5230001738714409
Loss in iteration 172 : 0.5985232271906036
Loss in iteration 173 : 0.7664417783195354
Loss in iteration 174 : 0.8267558347304648
Loss in iteration 175 : 0.8417938348751464
Loss in iteration 176 : 0.597510274342154
Loss in iteration 177 : 0.5270093275133398
Loss in iteration 178 : 0.4898361150223028
Loss in iteration 179 : 0.550980319109751
Loss in iteration 180 : 0.5719583907977233
Loss in iteration 181 : 0.5658245802660897
Loss in iteration 182 : 0.5464318415701872
Loss in iteration 183 : 0.4944816675426123
Loss in iteration 184 : 0.5170675211361391
Loss in iteration 185 : 0.47499343075109174
Loss in iteration 186 : 0.5045753573150864
Loss in iteration 187 : 0.47202950922539816
Loss in iteration 188 : 0.49874508812951274
Loss in iteration 189 : 0.47753473709990385
Loss in iteration 190 : 0.521441958729242
Loss in iteration 191 : 0.5953011623370246
Loss in iteration 192 : 0.8813778684691251
Loss in iteration 193 : 0.9928676504613648
Loss in iteration 194 : 0.8967495872193137
Loss in iteration 195 : 0.5696293363169093
Loss in iteration 196 : 0.4914231674898941
Loss in iteration 197 : 0.6191431630233505
Loss in iteration 198 : 0.7384794399127381
Loss in iteration 199 : 0.6676817062564933
Loss in iteration 200 : 0.5139397642786844
Loss in iteration 201 : 0.4883265752194193
Loss in iteration 202 : 0.5799924135808548
Loss in iteration 203 : 0.656218646412425
Loss in iteration 204 : 0.6057487792387815
Loss in iteration 205 : 0.5073846790057944
Loss in iteration 206 : 0.4839345892200384
Loss in iteration 207 : 0.534196640562741
Loss in iteration 208 : 0.577623025818009
Loss in iteration 209 : 0.5589588883334511
Loss in iteration 210 : 0.5015145675754183
Loss in iteration 211 : 0.47402684843103865
Loss in iteration 212 : 0.48543793141690844
Loss in iteration 213 : 0.5265390861296473
Loss in iteration 214 : 0.5988128223607491
Loss in iteration 215 : 0.6735955473750564
Loss in iteration 216 : 0.7622965038508512
Loss in iteration 217 : 0.6963719528780846
Loss in iteration 218 : 0.6193330607610728
Loss in iteration 219 : 0.5226581410324966
Loss in iteration 220 : 0.4785171195719666
Loss in iteration 221 : 0.4783964352140432
Loss in iteration 222 : 0.5100848758289719
Loss in iteration 223 : 0.5555132505757724
Loss in iteration 224 : 0.5832555460886526
Loss in iteration 225 : 0.589833260269879
Loss in iteration 226 : 0.5594140636814771
Loss in iteration 227 : 0.5304151673859412
Loss in iteration 228 : 0.4988254870965509
Loss in iteration 229 : 0.48150803966642436
Loss in iteration 230 : 0.47121270652335534
Loss in iteration 231 : 0.46789066183226635
Loss in iteration 232 : 0.46675164798503327
Loss in iteration 233 : 0.47052806684191617
Loss in iteration 234 : 0.47820677212492446
Loss in iteration 235 : 0.49617453681142115
Loss in iteration 236 : 0.5375379835739559
Loss in iteration 237 : 0.6201770552605677
Loss in iteration 238 : 0.783942377575567
Loss in iteration 239 : 0.7863514488513642
Loss in iteration 240 : 0.707033615668335
Loss in iteration 241 : 0.5330034397811251
Loss in iteration 242 : 0.476349239364362
Loss in iteration 243 : 0.5198392712549678
Loss in iteration 244 : 0.5953022622249898
Loss in iteration 245 : 0.6097981152196547
Loss in iteration 246 : 0.5366692018950325
Loss in iteration 247 : 0.4813586699238698
Loss in iteration 248 : 0.47570657448718917
Loss in iteration 249 : 0.5137539985380375
Loss in iteration 250 : 0.555580705545703
Loss in iteration 251 : 0.5662342925892582
Loss in iteration 252 : 0.5440196170577501
Loss in iteration 253 : 0.5012161686933243
Loss in iteration 254 : 0.4739286502558868
Loss in iteration 255 : 0.46933350632009324
Loss in iteration 256 : 0.4859431941498584
Loss in iteration 257 : 0.5097458533403789
Loss in iteration 258 : 0.5343344541601607
Loss in iteration 259 : 0.5621426750042585
Loss in iteration 260 : 0.579752485025599
Loss in iteration 261 : 0.6094083463624381
Loss in iteration 262 : 0.6031220201580167
Loss in iteration 263 : 0.5990519863475425
Loss in iteration 264 : 0.5482789746041781
Loss in iteration 265 : 0.5129155580611973
Loss in iteration 266 : 0.4805596383740647
Loss in iteration 267 : 0.4695403510881538
Loss in iteration 268 : 0.46740631614524636
Loss in iteration 269 : 0.4812419293012855
Loss in iteration 270 : 0.49885836242506876
Loss in iteration 271 : 0.5234202109667537
Loss in iteration 272 : 0.5458249906095471
Loss in iteration 273 : 0.5581845292112353
Loss in iteration 274 : 0.5716024181825062
Loss in iteration 275 : 0.5523669955381438
Loss in iteration 276 : 0.5303425761613358
Loss in iteration 277 : 0.493654481256556
Loss in iteration 278 : 0.4763138002140285
Loss in iteration 279 : 0.4654208305856599
Loss in iteration 280 : 0.47296320855195734
Loss in iteration 281 : 0.48185248231727323
Loss in iteration 282 : 0.5049967318911696
Loss in iteration 283 : 0.535314307897369
Loss in iteration 284 : 0.5758859142604612
Loss in iteration 285 : 0.62385786113422
Loss in iteration 286 : 0.6168029172599345
Loss in iteration 287 : 0.598115496554885
Loss in iteration 288 : 0.5276677533488292
Loss in iteration 289 : 0.4868166552712007
Loss in iteration 290 : 0.46894037128825355
Loss in iteration 291 : 0.49014132132020516
Loss in iteration 292 : 0.5127876646393595
Loss in iteration 293 : 0.5331248876590332
Loss in iteration 294 : 0.5258616665278462
Loss in iteration 295 : 0.5082042804446407
Loss in iteration 296 : 0.49272467351221394
Loss in iteration 297 : 0.47987909911331433
Loss in iteration 298 : 0.4733433913224739
Loss in iteration 299 : 0.4660358223166603
Loss in iteration 300 : 0.46640790040359503
Loss in iteration 301 : 0.4627345299458494
Loss in iteration 302 : 0.4658151625360206
Loss in iteration 303 : 0.46237339486416734
Loss in iteration 304 : 0.46595597303957925
Loss in iteration 305 : 0.46485561071409703
Loss in iteration 306 : 0.47308987186534784
Loss in iteration 307 : 0.48427534393018923
Loss in iteration 308 : 0.521870931574181
Loss in iteration 309 : 0.6138709363135613
Loss in iteration 310 : 0.7289399843674603
Loss in iteration 311 : 0.8211591269837768
Loss in iteration 312 : 0.6127414027577518
Loss in iteration 313 : 0.49026555052965987
Loss in iteration 314 : 0.4839756256399467
Loss in iteration 315 : 0.559760644218606
Loss in iteration 316 : 0.5956176552637212
Loss in iteration 317 : 0.535373323318834
Loss in iteration 318 : 0.47601189066277655
Loss in iteration 319 : 0.48562795586876495
Loss in iteration 320 : 0.5356751108831931
Loss in iteration 321 : 0.5509878875347708
Loss in iteration 322 : 0.5141731054961942
Loss in iteration 323 : 0.47527757850345254
Loss in iteration 324 : 0.47083422927947005
Loss in iteration 325 : 0.492044967648283
Loss in iteration 326 : 0.5128777900551769
Loss in iteration 327 : 0.5076714047697147
Loss in iteration 328 : 0.48852622556450614
Loss in iteration 329 : 0.4688665049325146
Loss in iteration 330 : 0.46245005212506063
Loss in iteration 331 : 0.4689377950238888
Loss in iteration 332 : 0.4847889567949463
Loss in iteration 333 : 0.5149922692249898
Loss in iteration 334 : 0.5571899936169901
Loss in iteration 335 : 0.6299172930186189
Loss in iteration 336 : 0.6455485682276465
Loss in iteration 337 : 0.6339001054104478
Loss in iteration 338 : 0.5400485125204575
Loss in iteration 339 : 0.4801451272512463
Loss in iteration 340 : 0.46832489245779707
Loss in iteration 341 : 0.4981811003787851
Loss in iteration 342 : 0.5327240974484732
Loss in iteration 343 : 0.5304536562145572
Loss in iteration 344 : 0.5048058587031388
Loss in iteration 345 : 0.47606659404902507
Loss in iteration 346 : 0.46451558800922577
Loss in iteration 347 : 0.47256431660651854
Loss in iteration 348 : 0.49149103503472236
Loss in iteration 349 : 0.5090830347834271
Loss in iteration 350 : 0.5107618962700643
Loss in iteration 351 : 0.5005141797187647
Loss in iteration 352 : 0.48154289152589186
Loss in iteration 353 : 0.467377008794082
Loss in iteration 354 : 0.46207809259275223
Loss in iteration 355 : 0.46550369471192155
Loss in iteration 356 : 0.475778881476157
Loss in iteration 357 : 0.4908234787606443
Loss in iteration 358 : 0.5174234079404555
Loss in iteration 359 : 0.5486219407639749
Loss in iteration 360 : 0.5981753438108287
Loss in iteration 361 : 0.6056996581842784
Loss in iteration 362 : 0.5984862759403726
Loss in iteration 363 : 0.533756580938282
Loss in iteration 364 : 0.4846859242577869
Loss in iteration 365 : 0.46448758753479236
Loss in iteration 366 : 0.47692842447603506
Loss in iteration 367 : 0.5046867660662676
Loss in iteration 368 : 0.5194305852511331
Loss in iteration 369 : 0.5166668121620726
Loss in iteration 370 : 0.4948378253875226
Loss in iteration 371 : 0.47434509773561545
Loss in iteration 372 : 0.46333747523425983
Loss in iteration 373 : 0.46326820921140843
Loss in iteration 374 : 0.4710841236867392
Loss in iteration 375 : 0.4822449101627299
Loss in iteration 376 : 0.4931805962785911
Loss in iteration 377 : 0.4971888214165644
Loss in iteration 378 : 0.49804702019954655
Loss in iteration 379 : 0.49063436699562424
Loss in iteration 380 : 0.48408013198152033
Loss in iteration 381 : 0.4768668587478606
Loss in iteration 382 : 0.4728052399396745
Loss in iteration 383 : 0.4705366087197883
Loss in iteration 384 : 0.47085537456023896
Loss in iteration 385 : 0.47356339300248446
Loss in iteration 386 : 0.48150006077562096
Loss in iteration 387 : 0.49563872432171935
Loss in iteration 388 : 0.5252954534076119
Loss in iteration 389 : 0.5575521107146655
Loss in iteration 390 : 0.5986891520122022
Loss in iteration 391 : 0.5791289880595505
Loss in iteration 392 : 0.5409042706193027
Loss in iteration 393 : 0.4878272487432206
Loss in iteration 394 : 0.4638003834773408
Loss in iteration 395 : 0.471742217642049
Loss in iteration 396 : 0.4977327133353814
Loss in iteration 397 : 0.5224616442343126
Loss in iteration 398 : 0.521114077159293
Loss in iteration 399 : 0.5047498738604924
Loss in iteration 400 : 0.4805974982550871
Testing accuracy  of updater 9 on alg 0 with rate 2.0 = 0.785125, training accuracy 0.785125, time elapsed: 3619 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6804958788234788
Loss in iteration 3 : 0.6722574943361402
Loss in iteration 4 : 0.6561849731961461
Loss in iteration 5 : 0.6356363823582502
Loss in iteration 6 : 0.6211744584402858
Loss in iteration 7 : 0.6053349469640767
Loss in iteration 8 : 0.585073025402609
Loss in iteration 9 : 0.5691128116884765
Loss in iteration 10 : 0.5574777544155297
Loss in iteration 11 : 0.5451769073870931
Loss in iteration 12 : 0.5338203711248306
Loss in iteration 13 : 0.5262117309800517
Loss in iteration 14 : 0.5200441557432837
Loss in iteration 15 : 0.5130442433053845
Loss in iteration 16 : 0.5068051283071379
Loss in iteration 17 : 0.5026995012095711
Loss in iteration 18 : 0.4994343607684758
Loss in iteration 19 : 0.49585301890729716
Loss in iteration 20 : 0.4926923095169198
Loss in iteration 21 : 0.4907320616226168
Loss in iteration 22 : 0.48932779271871846
Loss in iteration 23 : 0.4876203997018799
Loss in iteration 24 : 0.48582769746847815
Loss in iteration 25 : 0.4845543463902814
Loss in iteration 26 : 0.48369659417212074
Loss in iteration 27 : 0.4827290897021095
Loss in iteration 28 : 0.4815832597658058
Loss in iteration 29 : 0.48063580598893657
Loss in iteration 30 : 0.48002696994091926
Loss in iteration 31 : 0.47948030779685424
Loss in iteration 32 : 0.4787839925281299
Loss in iteration 33 : 0.47807160560231854
Loss in iteration 34 : 0.47752937323815947
Loss in iteration 35 : 0.477090194685529
Loss in iteration 36 : 0.4765719039933033
Loss in iteration 37 : 0.47596334831209064
Loss in iteration 38 : 0.47540675862710075
Loss in iteration 39 : 0.47496173815993165
Loss in iteration 40 : 0.4745376795322818
Loss in iteration 41 : 0.47405846484185654
Loss in iteration 42 : 0.47357136909838654
Loss in iteration 43 : 0.47315167298924427
Loss in iteration 44 : 0.47278369120619923
Loss in iteration 45 : 0.4723979889635283
Loss in iteration 46 : 0.4719807551507706
Loss in iteration 47 : 0.47158210070912665
Loss in iteration 48 : 0.4712307159007785
Loss in iteration 49 : 0.47089716790647207
Loss in iteration 50 : 0.4705497388891407
Loss in iteration 51 : 0.47020293348420705
Loss in iteration 52 : 0.46988851157295014
Loss in iteration 53 : 0.4696058893584526
Loss in iteration 54 : 0.4693284422423809
Loss in iteration 55 : 0.4690461094664848
Loss in iteration 56 : 0.468775449569877
Loss in iteration 57 : 0.46852893335319074
Loss in iteration 58 : 0.4682963619061841
Loss in iteration 59 : 0.46806348066242515
Loss in iteration 60 : 0.4678334430492516
Loss in iteration 61 : 0.4676188830329341
Loss in iteration 62 : 0.4674215615134809
Loss in iteration 63 : 0.4672316906267571
Loss in iteration 64 : 0.4670441388704852
Loss in iteration 65 : 0.4668644842635265
Loss in iteration 66 : 0.4666979661585575
Loss in iteration 67 : 0.466540934691811
Loss in iteration 68 : 0.4663870731149643
Loss in iteration 69 : 0.466236529562038
Loss in iteration 70 : 0.4660939402981318
Loss in iteration 71 : 0.46596032082823646
Loss in iteration 72 : 0.46583191499684373
Loss in iteration 73 : 0.46570633609378614
Loss in iteration 74 : 0.46558549352232675
Loss in iteration 75 : 0.465471553925428
Loss in iteration 76 : 0.4653632311014966
Loss in iteration 77 : 0.4652578649376553
Loss in iteration 78 : 0.4651551499107111
Loss in iteration 79 : 0.4650566992951957
Loss in iteration 80 : 0.46496289525724616
Loss in iteration 81 : 0.46487221930890643
Loss in iteration 82 : 0.46478358477290505
Loss in iteration 83 : 0.46469763031071076
Loss in iteration 84 : 0.4646152347699163
Loss in iteration 85 : 0.4645359677975714
Loss in iteration 86 : 0.4644587781984365
Loss in iteration 87 : 0.4643834690746254
Loss in iteration 88 : 0.46431061956460906
Loss in iteration 89 : 0.46424036982212546
Loss in iteration 90 : 0.46417209975062507
Loss in iteration 91 : 0.4641053177049472
Loss in iteration 92 : 0.46404021281779717
Loss in iteration 93 : 0.463977112730014
Loss in iteration 94 : 0.4639158555661049
Loss in iteration 95 : 0.46385602007567367
Loss in iteration 96 : 0.4637975037981212
Loss in iteration 97 : 0.4637405200241749
Loss in iteration 98 : 0.46368512687737656
Loss in iteration 99 : 0.46363107780357476
Loss in iteration 100 : 0.4635781560701464
Loss in iteration 101 : 0.4635264058813019
Loss in iteration 102 : 0.46347593794159697
Loss in iteration 103 : 0.46342668059400705
Loss in iteration 104 : 0.46337845716267045
Loss in iteration 105 : 0.46333121166617897
Loss in iteration 106 : 0.4632850172575286
Loss in iteration 107 : 0.46323989405673943
Loss in iteration 108 : 0.46319574190463686
Loss in iteration 109 : 0.46315246538310356
Loss in iteration 110 : 0.46311006851395164
Loss in iteration 111 : 0.46306858436782017
Loss in iteration 112 : 0.4630279761652507
Loss in iteration 113 : 0.46298816300685525
Loss in iteration 114 : 0.4629491088364338
Loss in iteration 115 : 0.4629108305627951
Loss in iteration 116 : 0.4628733285056383
Loss in iteration 117 : 0.4628365569274167
Loss in iteration 118 : 0.4628004701908681
Loss in iteration 119 : 0.4627650608310722
Loss in iteration 120 : 0.4627303339948478
Loss in iteration 121 : 0.46269626827417154
Loss in iteration 122 : 0.46266282400591674
Loss in iteration 123 : 0.4626299780009789
Loss in iteration 124 : 0.46259772825723167
Loss in iteration 125 : 0.46256606744535167
Loss in iteration 126 : 0.4625349705336493
Loss in iteration 127 : 0.4625044121849281
Loss in iteration 128 : 0.4624743822352047
Loss in iteration 129 : 0.462444876414525
Loss in iteration 130 : 0.46241588066499084
Loss in iteration 131 : 0.46238737349350734
Loss in iteration 132 : 0.4623593393648975
Loss in iteration 133 : 0.4623317710990814
Loss in iteration 134 : 0.4623046598584123
Loss in iteration 135 : 0.46227799009327925
Loss in iteration 136 : 0.4622517461103178
Loss in iteration 137 : 0.46222591828932735
Loss in iteration 138 : 0.4622004997134722
Loss in iteration 139 : 0.46217547998772623
Loss in iteration 140 : 0.46215084592668165
Loss in iteration 141 : 0.46212658672889756
Loss in iteration 142 : 0.4621026950310747
Loss in iteration 143 : 0.46207916302664226
Loss in iteration 144 : 0.46205598036733225
Loss in iteration 145 : 0.4620331366488442
Loss in iteration 146 : 0.46201062391901304
Loss in iteration 147 : 0.4619884354751174
Loss in iteration 148 : 0.46196656345170783
Loss in iteration 149 : 0.4619449990160832
Loss in iteration 150 : 0.46192373435746004
Loss in iteration 151 : 0.4619027631250068
Loss in iteration 152 : 0.4618820789251893
Loss in iteration 153 : 0.4618616744733368
Loss in iteration 154 : 0.461841542534958
Loss in iteration 155 : 0.4618216769060274
Loss in iteration 156 : 0.4618020719546424
Loss in iteration 157 : 0.4617827216772729
Loss in iteration 158 : 0.4617636197689634
Loss in iteration 159 : 0.4617447604057286
Loss in iteration 160 : 0.461726138426394
Loss in iteration 161 : 0.4617077487451378
Loss in iteration 162 : 0.46168958600715143
Loss in iteration 163 : 0.46167164494340773
Loss in iteration 164 : 0.4616539207482947
Loss in iteration 165 : 0.46163640890033264
Loss in iteration 166 : 0.46161910479394364
Loss in iteration 167 : 0.46160200376623794
Loss in iteration 168 : 0.46158510140045156
Loss in iteration 169 : 0.4615683935940089
Loss in iteration 170 : 0.46155187632793243
Loss in iteration 171 : 0.46153554553395926
Loss in iteration 172 : 0.4615193972340636
Loss in iteration 173 : 0.4615034276849117
Loss in iteration 174 : 0.4614876333027634
Loss in iteration 175 : 0.4614720105168626
Loss in iteration 176 : 0.46145655577963796
Loss in iteration 177 : 0.46144126568412924
Loss in iteration 178 : 0.4614261369878916
Loss in iteration 179 : 0.46141116652154734
Loss in iteration 180 : 0.4613963511378567
Loss in iteration 181 : 0.4613816877665574
Loss in iteration 182 : 0.4613671734684767
Loss in iteration 183 : 0.4613528054034333
Loss in iteration 184 : 0.46133858077257706
Loss in iteration 185 : 0.4613244968230878
Loss in iteration 186 : 0.46131055089273765
Loss in iteration 187 : 0.4612967404161066
Loss in iteration 188 : 0.46128306288693316
Loss in iteration 189 : 0.4612695158385885
Loss in iteration 190 : 0.4612560968658759
Loss in iteration 191 : 0.46124280364486947
Loss in iteration 192 : 0.4612296339186137
Loss in iteration 193 : 0.46121658547417677
Loss in iteration 194 : 0.46120365614469677
Loss in iteration 195 : 0.4611908438259787
Loss in iteration 196 : 0.46117814647718924
Loss in iteration 197 : 0.46116556210509113
Loss in iteration 198 : 0.46115308875641725
Loss in iteration 199 : 0.46114072452614163
Loss in iteration 200 : 0.46112846756401293
Loss in iteration 201 : 0.46111631606774267
Loss in iteration 202 : 0.46110426827374384
Loss in iteration 203 : 0.46109232245801923
Loss in iteration 204 : 0.4610804769421087
Loss in iteration 205 : 0.46106873009228844
Loss in iteration 206 : 0.4610570803126967
Loss in iteration 207 : 0.46104552604217064
Loss in iteration 208 : 0.4610340657571685
Loss in iteration 209 : 0.4610226979735327
Loss in iteration 210 : 0.46101142124304023
Loss in iteration 211 : 0.46100023414947816
Loss in iteration 212 : 0.4609891353088268
Loss in iteration 213 : 0.4609781233711089
Loss in iteration 214 : 0.46096719701940925
Loss in iteration 215 : 0.46095635496674126
Loss in iteration 216 : 0.4609455959546331
Loss in iteration 217 : 0.4609349187539581
Loss in iteration 218 : 0.4609243221650812
Loss in iteration 219 : 0.46091380501599677
Loss in iteration 220 : 0.4609033661605095
Loss in iteration 221 : 0.460893004478092
Loss in iteration 222 : 0.46088271887423166
Loss in iteration 223 : 0.46087250827959153
Loss in iteration 224 : 0.4608623716484752
Loss in iteration 225 : 0.4608523079580483
Loss in iteration 226 : 0.46084231620841265
Loss in iteration 227 : 0.46083239542228666
Loss in iteration 228 : 0.4608225446439377
Loss in iteration 229 : 0.4608127629382484
Loss in iteration 230 : 0.4608030493904409
Loss in iteration 231 : 0.46079340310591954
Testing accuracy  of updater 9 on alg 0 with rate 0.2 = 0.78825, training accuracy 0.78825, time elapsed: 2099 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6912820143991008
Loss in iteration 3 : 0.688098178417253
Loss in iteration 4 : 0.6842364119669273
Loss in iteration 5 : 0.6802426596764914
Loss in iteration 6 : 0.6764334157095875
Loss in iteration 7 : 0.6728696365487704
Loss in iteration 8 : 0.6694256185561146
Loss in iteration 9 : 0.6659034536250269
Loss in iteration 10 : 0.6621391519181261
Loss in iteration 11 : 0.6580659534148982
Loss in iteration 12 : 0.6537257877520861
Loss in iteration 13 : 0.6492392316933822
Loss in iteration 14 : 0.6447542256373407
Loss in iteration 15 : 0.640395258967206
Loss in iteration 16 : 0.636229827110757
Loss in iteration 17 : 0.6322601098600219
Loss in iteration 18 : 0.6284379911305144
Loss in iteration 19 : 0.6246939551018971
Loss in iteration 20 : 0.6209673735138925
Loss in iteration 21 : 0.6172276186433696
Loss in iteration 22 : 0.61348070605531
Loss in iteration 23 : 0.6097622161328504
Loss in iteration 24 : 0.6061217382446523
Loss in iteration 25 : 0.6026057786236704
Loss in iteration 26 : 0.5992450156317285
Loss in iteration 27 : 0.5960489946025671
Loss in iteration 28 : 0.5930082181447316
Loss in iteration 29 : 0.5901012572982807
Loss in iteration 30 : 0.5873035392188471
Loss in iteration 31 : 0.5845947935402994
Loss in iteration 32 : 0.5819632957739143
Loss in iteration 33 : 0.5794064510043188
Loss in iteration 34 : 0.576928438224031
Loss in iteration 35 : 0.5745363141035175
Loss in iteration 36 : 0.5722360942747787
Loss in iteration 37 : 0.5700299920762075
Loss in iteration 38 : 0.5679153962366225
Loss in iteration 39 : 0.5658855333891346
Loss in iteration 40 : 0.563931276358888
Loss in iteration 41 : 0.5620433332837594
Loss in iteration 42 : 0.5602140983443447
Loss in iteration 43 : 0.558438692520009
Loss in iteration 44 : 0.5567150568989282
Loss in iteration 45 : 0.5550432645542352
Loss in iteration 46 : 0.5534244070708462
Loss in iteration 47 : 0.5518594576359586
Loss in iteration 48 : 0.5503484329156111
Loss in iteration 49 : 0.5488900219689479
Loss in iteration 50 : 0.5474816831252545
Loss in iteration 51 : 0.5461200795359691
Loss in iteration 52 : 0.5448016590161793
Loss in iteration 53 : 0.5435231869087069
Loss in iteration 54 : 0.5422820955333999
Loss in iteration 55 : 0.5410765935953531
Loss in iteration 56 : 0.539905556500722
Loss in iteration 57 : 0.5387682735272208
Loss in iteration 58 : 0.537664150480224
Loss in iteration 59 : 0.5365924577979858
Loss in iteration 60 : 0.5355521830705398
Loss in iteration 61 : 0.5345420065865212
Loss in iteration 62 : 0.5335603815355178
Loss in iteration 63 : 0.5326056760436226
Loss in iteration 64 : 0.5316763262480754
Loss in iteration 65 : 0.5307709568588767
Loss in iteration 66 : 0.5298884430456929
Loss in iteration 67 : 0.5290279082964444
Loss in iteration 68 : 0.5281886708729107
Loss in iteration 69 : 0.5273701624482592
Loss in iteration 70 : 0.5265718450079504
Loss in iteration 71 : 0.5257931472347086
Loss in iteration 72 : 0.5250334321974413
Loss in iteration 73 : 0.5242919976377258
Loss in iteration 74 : 0.5235681014780003
Loss in iteration 75 : 0.522861000224382
Loss in iteration 76 : 0.5221699872023123
Loss in iteration 77 : 0.5214944203464614
Loss in iteration 78 : 0.5208337341123702
Loss in iteration 79 : 0.5201874353105403
Loss in iteration 80 : 0.51955508686445
Loss in iteration 81 : 0.5189362858041925
Loss in iteration 82 : 0.5183306420296316
Loss in iteration 83 : 0.5177377628847949
Loss in iteration 84 : 0.5171572461094491
Loss in iteration 85 : 0.51658868110382
Loss in iteration 86 : 0.5160316563564674
Loss in iteration 87 : 0.5154857697613233
Loss in iteration 88 : 0.5149506384776094
Loss in iteration 89 : 0.5144259057717044
Loss in iteration 90 : 0.5139112435521215
Loss in iteration 91 : 0.513406350649489
Loss in iteration 92 : 0.5129109479522621
Loss in iteration 93 : 0.5124247720774988
Loss in iteration 94 : 0.5119475692912417
Loss in iteration 95 : 0.5114790909940752
Loss in iteration 96 : 0.5110190914414722
Loss in iteration 97 : 0.5105673276865119
Loss in iteration 98 : 0.510123561190862
Loss in iteration 99 : 0.5096875602545995
Loss in iteration 100 : 0.5092591023894837
Loss in iteration 101 : 0.5088379759556234
Loss in iteration 102 : 0.5084239807052255
Loss in iteration 103 : 0.5080169272234173
Loss in iteration 104 : 0.5076166355353623
Loss in iteration 105 : 0.5072229333058962
Loss in iteration 106 : 0.5068356540786003
Loss in iteration 107 : 0.5064546359080017
Loss in iteration 108 : 0.5060797205769283
Loss in iteration 109 : 0.5057107534141416
Loss in iteration 110 : 0.5053475835816509
Loss in iteration 111 : 0.5049900646155436
Loss in iteration 112 : 0.5046380549880789
Loss in iteration 113 : 0.5042914185018688
Loss in iteration 114 : 0.5039500244073433
Loss in iteration 115 : 0.503613747225213
Loss in iteration 116 : 0.5032824663325828
Loss in iteration 117 : 0.502956065418671
Loss in iteration 118 : 0.5026344319283722
Loss in iteration 119 : 0.5023174565931142
Loss in iteration 120 : 0.5020050331093455
Loss in iteration 121 : 0.5016970579790979
Loss in iteration 122 : 0.5013934304867003
Loss in iteration 123 : 0.5010940527596043
Loss in iteration 124 : 0.5007988298528021
Loss in iteration 125 : 0.5005076698039593
Loss in iteration 126 : 0.5002204836252102
Loss in iteration 127 : 0.4999371852207946
Loss in iteration 128 : 0.4996576912407622
Loss in iteration 129 : 0.49938192089554945
Loss in iteration 130 : 0.49910979576175407
Loss in iteration 131 : 0.4988412396066549
Loss in iteration 132 : 0.49857617825014255
Loss in iteration 133 : 0.49831453947096144
Loss in iteration 134 : 0.49805625295318795
Loss in iteration 135 : 0.4978012502607415
Loss in iteration 136 : 0.4975494648242227
Loss in iteration 137 : 0.4973008319251146
Loss in iteration 138 : 0.49705528866656346
Loss in iteration 139 : 0.4968127739258131
Loss in iteration 140 : 0.49657322828922634
Loss in iteration 141 : 0.49633659397528646
Loss in iteration 142 : 0.49610281475326756
Loss in iteration 143 : 0.4958718358653145
Loss in iteration 144 : 0.49564360395778634
Loss in iteration 145 : 0.4954180670248902
Loss in iteration 146 : 0.49519517436461696
Loss in iteration 147 : 0.49497487654447386
Loss in iteration 148 : 0.49475712537318384
Loss in iteration 149 : 0.4945418738742325
Loss in iteration 150 : 0.4943290762579161
Loss in iteration 151 : 0.49411868788994284
Loss in iteration 152 : 0.4939106652561608
Loss in iteration 153 : 0.49370496592433644
Loss in iteration 154 : 0.49350154850474576
Loss in iteration 155 : 0.4933003726115462
Loss in iteration 156 : 0.49310139882666065
Loss in iteration 157 : 0.49290458866721865
Loss in iteration 158 : 0.49270990455680624
Loss in iteration 159 : 0.4925173098001161
Loss in iteration 160 : 0.4923267685600402
Loss in iteration 161 : 0.4921382458361295
Loss in iteration 162 : 0.4919517074433943
Loss in iteration 163 : 0.49176711999076267
Loss in iteration 164 : 0.4915844508588691
Loss in iteration 165 : 0.4914036681773292
Loss in iteration 166 : 0.49122474080179473
Loss in iteration 167 : 0.4910476382913663
Loss in iteration 168 : 0.4908723308868296
Loss in iteration 169 : 0.4906987894900966
Loss in iteration 170 : 0.4905269856449992
Loss in iteration 171 : 0.49035689151943723
Loss in iteration 172 : 0.4901884798886583
Loss in iteration 173 : 0.49002172411940226
Loss in iteration 174 : 0.4898565981546296
Loss in iteration 175 : 0.4896930764985591
Loss in iteration 176 : 0.48953113420191313
Loss in iteration 177 : 0.48937074684730825
Loss in iteration 178 : 0.48921189053483316
Loss in iteration 179 : 0.4890545418679445
Loss in iteration 180 : 0.48889867793975944
Loss in iteration 181 : 0.488744276319883
Loss in iteration 182 : 0.4885913150418037
Loss in iteration 183 : 0.48843977259086924
Loss in iteration 184 : 0.4882896278927823
Loss in iteration 185 : 0.4881408603025764
Loss in iteration 186 : 0.48799344959395075
Loss in iteration 187 : 0.4878473759488926
Loss in iteration 188 : 0.4877026199475442
Loss in iteration 189 : 0.48755916255826626
Loss in iteration 190 : 0.4874169851279056
Loss in iteration 191 : 0.4872760693722803
Loss in iteration 192 : 0.4871363973669072
Loss in iteration 193 : 0.48699795153802233
Loss in iteration 194 : 0.4868607146538601
Loss in iteration 195 : 0.48672466981627105
Loss in iteration 196 : 0.48658980045260763
Loss in iteration 197 : 0.4864560903078966
Loss in iteration 198 : 0.48632352343726304
Loss in iteration 199 : 0.48619208419856536
Loss in iteration 200 : 0.48606175724525996
Loss in iteration 201 : 0.48593252751942306
Loss in iteration 202 : 0.4858043802449645
Loss in iteration 203 : 0.4856773009210167
Loss in iteration 204 : 0.48555127531549347
Loss in iteration 205 : 0.48542628945882843
Loss in iteration 206 : 0.4853023296378984
Loss in iteration 207 : 0.4851793823901122
Loss in iteration 208 : 0.48505743449769223
Loss in iteration 209 : 0.48493647298211057
Loss in iteration 210 : 0.484816485098691
Loss in iteration 211 : 0.48469745833135264
Loss in iteration 212 : 0.48457938038749954
Loss in iteration 213 : 0.4844622391930361
Loss in iteration 214 : 0.48434602288751344
Loss in iteration 215 : 0.48423071981938326
Loss in iteration 216 : 0.48411631854139137
Loss in iteration 217 : 0.48400280780606963
Loss in iteration 218 : 0.48389017656135935
Loss in iteration 219 : 0.483778413946342
Loss in iteration 220 : 0.48366750928708585
Loss in iteration 221 : 0.48355745209260553
Loss in iteration 222 : 0.48344823205092735
Loss in iteration 223 : 0.48333983902525934
Loss in iteration 224 : 0.48323226305025446
Loss in iteration 225 : 0.48312549432837887
Loss in iteration 226 : 0.4830195232263651
Loss in iteration 227 : 0.48291434027175184
Loss in iteration 228 : 0.4828099361495162
Loss in iteration 229 : 0.4827063016987887
Loss in iteration 230 : 0.48260342790964195
Loss in iteration 231 : 0.4825013059199721
Loss in iteration 232 : 0.4823999270124441
Loss in iteration 233 : 0.4822992826115291
Loss in iteration 234 : 0.4821993642806034
Loss in iteration 235 : 0.4821001637191164
Loss in iteration 236 : 0.482001672759846
Loss in iteration 237 : 0.4819038833661923
Loss in iteration 238 : 0.48180678762956414
Loss in iteration 239 : 0.4817103777668055
Loss in iteration 240 : 0.4816146461176947
Loss in iteration 241 : 0.48151958514249743
Loss in iteration 242 : 0.4814251874195845
Loss in iteration 243 : 0.4813314456430961
Loss in iteration 244 : 0.48123835262066256
Loss in iteration 245 : 0.4811459012711885
Loss in iteration 246 : 0.48105408462267596
Loss in iteration 247 : 0.4809628958101106
Loss in iteration 248 : 0.48087232807338354
Loss in iteration 249 : 0.4807823747552718
Loss in iteration 250 : 0.48069302929946356
Loss in iteration 251 : 0.48060428524862575
Loss in iteration 252 : 0.48051613624251693
Loss in iteration 253 : 0.48042857601614425
Loss in iteration 254 : 0.4803415983979614
Loss in iteration 255 : 0.48025519730810584
Loss in iteration 256 : 0.48016936675667865
Loss in iteration 257 : 0.48008410084205944
Loss in iteration 258 : 0.4799993937492628
Loss in iteration 259 : 0.4799152397483321
Loss in iteration 260 : 0.4798316331927549
Loss in iteration 261 : 0.4797485685179381
Loss in iteration 262 : 0.4796660402396988
Loss in iteration 263 : 0.47958404295278556
Loss in iteration 264 : 0.47950257132944646
Loss in iteration 265 : 0.4794216201180216
Loss in iteration 266 : 0.47934118414155386
Loss in iteration 267 : 0.4792612582964532
Loss in iteration 268 : 0.4791818375511699
Loss in iteration 269 : 0.4791029169449058
Loss in iteration 270 : 0.47902449158634614
Loss in iteration 271 : 0.4789465566524336
Loss in iteration 272 : 0.4788691073871443
Loss in iteration 273 : 0.47879213910030843
Loss in iteration 274 : 0.47871564716645854
Loss in iteration 275 : 0.47863962702367635
Loss in iteration 276 : 0.4785640741724992
Loss in iteration 277 : 0.47848898417481717
Loss in iteration 278 : 0.4784143526528211
Loss in iteration 279 : 0.4783401752879412
Loss in iteration 280 : 0.47826644781983974
Loss in iteration 281 : 0.47819316604540396
Loss in iteration 282 : 0.4781203258177674
Loss in iteration 283 : 0.47804792304534444
Loss in iteration 284 : 0.47797595369089646
Loss in iteration 285 : 0.47790441377060205
Loss in iteration 286 : 0.4778332993531591
Loss in iteration 287 : 0.47776260655889885
Loss in iteration 288 : 0.47769233155891766
Loss in iteration 289 : 0.47762247057422813
Loss in iteration 290 : 0.47755301987492194
Loss in iteration 291 : 0.4774839757793603
Loss in iteration 292 : 0.4774153346533701
Loss in iteration 293 : 0.4773470929094576
Loss in iteration 294 : 0.47727924700604124
Loss in iteration 295 : 0.47721179344669984
Loss in iteration 296 : 0.47714472877943087
Loss in iteration 297 : 0.4770780495959278
Loss in iteration 298 : 0.47701175253086525
Loss in iteration 299 : 0.4769458342612091
Loss in iteration 300 : 0.47688029150552796
Loss in iteration 301 : 0.4768151210233245
Loss in iteration 302 : 0.47675031961437825
Loss in iteration 303 : 0.4766858841181024
Loss in iteration 304 : 0.4766218114129108
Loss in iteration 305 : 0.4765580984155962
Loss in iteration 306 : 0.4764947420807248
Loss in iteration 307 : 0.47643173940003536
Loss in iteration 308 : 0.47636908740185807
Loss in iteration 309 : 0.4763067831505363
Loss in iteration 310 : 0.4762448237458673
Loss in iteration 311 : 0.47618320632254213
Loss in iteration 312 : 0.4761219280496098
Loss in iteration 313 : 0.4760609861299396
Loss in iteration 314 : 0.47600037779970034
Loss in iteration 315 : 0.475940100327848
Loss in iteration 316 : 0.4758801510156202
Loss in iteration 317 : 0.4758205271960394
Loss in iteration 318 : 0.4757612262334329
Loss in iteration 319 : 0.4757022455229524
Loss in iteration 320 : 0.4756435824901057
Loss in iteration 321 : 0.4755852345903021
Loss in iteration 322 : 0.47552719930839554
Loss in iteration 323 : 0.4754694741582432
Loss in iteration 324 : 0.4754120566822734
Loss in iteration 325 : 0.47535494445105403
Loss in iteration 326 : 0.47529813506287827
Loss in iteration 327 : 0.475241626143346
Loss in iteration 328 : 0.47518541534496594
Loss in iteration 329 : 0.4751295003467549
Loss in iteration 330 : 0.47507387885384617
Loss in iteration 331 : 0.47501854859710363
Loss in iteration 332 : 0.4749635073327578
Loss in iteration 333 : 0.4749087528420172
Loss in iteration 334 : 0.4748542829307241
Loss in iteration 335 : 0.47480009542897483
Loss in iteration 336 : 0.4747461881907895
Loss in iteration 337 : 0.47469255909375857
Loss in iteration 338 : 0.4746392060386962
Loss in iteration 339 : 0.4745861269493238
Loss in iteration 340 : 0.4745333197719282
Loss in iteration 341 : 0.47448078247504477
Loss in iteration 342 : 0.4744285130491451
Loss in iteration 343 : 0.47437650950631693
Loss in iteration 344 : 0.4743247698799709
Loss in iteration 345 : 0.47427329222452624
Loss in iteration 346 : 0.4742220746151277
Loss in iteration 347 : 0.47417111514734367
Loss in iteration 348 : 0.47412041193689836
Loss in iteration 349 : 0.4740699631193683
Loss in iteration 350 : 0.4740197668499254
Loss in iteration 351 : 0.4739698213030555
Loss in iteration 352 : 0.4739201246722958
Loss in iteration 353 : 0.47387067516996934
Loss in iteration 354 : 0.47382147102693123
Loss in iteration 355 : 0.4737725104923119
Loss in iteration 356 : 0.47372379183326585
Loss in iteration 357 : 0.47367531333473517
Loss in iteration 358 : 0.47362707329919385
Loss in iteration 359 : 0.47357907004642424
Loss in iteration 360 : 0.47353130191327636
Loss in iteration 361 : 0.47348376725343777
Loss in iteration 362 : 0.473436464437209
Loss in iteration 363 : 0.4733893918512848
Loss in iteration 364 : 0.4733425478985283
Loss in iteration 365 : 0.47329593099776274
Loss in iteration 366 : 0.4732495395835543
Loss in iteration 367 : 0.47320337210600594
Loss in iteration 368 : 0.47315742703055536
Loss in iteration 369 : 0.4731117028377674
Loss in iteration 370 : 0.4730661980231392
Loss in iteration 371 : 0.47302091109690575
Loss in iteration 372 : 0.47297584058384573
Loss in iteration 373 : 0.4729309850230925
Loss in iteration 374 : 0.4728863429679459
Loss in iteration 375 : 0.4728419129856926
Loss in iteration 376 : 0.47279769365742663
Loss in iteration 377 : 0.4727536835778667
Loss in iteration 378 : 0.472709881355184
Loss in iteration 379 : 0.4726662856108305
Loss in iteration 380 : 0.47262289497937
Loss in iteration 381 : 0.4725797081083091
Loss in iteration 382 : 0.4725367236579356
Loss in iteration 383 : 0.47249394030115277
Loss in iteration 384 : 0.4724513567233283
Loss in iteration 385 : 0.4724089716221285
Loss in iteration 386 : 0.4723667837073665
Loss in iteration 387 : 0.47232479170085706
Loss in iteration 388 : 0.47228299433625237
Loss in iteration 389 : 0.47224139035891083
Loss in iteration 390 : 0.47219997852574347
Loss in iteration 391 : 0.4721587576050704
Loss in iteration 392 : 0.4721177263764824
Loss in iteration 393 : 0.4720768836306997
Loss in iteration 394 : 0.4720362281694409
Testing accuracy  of updater 9 on alg 0 with rate 0.01999999999999999 = 0.783875, training accuracy 0.783875, time elapsed: 3560 millisecond.
