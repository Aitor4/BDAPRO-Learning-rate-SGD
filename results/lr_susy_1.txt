objc[2860]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x105d8d4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x1075ba4e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/03/02 13:03:45 INFO SparkContext: Running Spark version 2.0.0
18/03/02 13:03:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/03/02 13:03:46 INFO SecurityManager: Changing view acls to: Aitor
18/03/02 13:03:46 INFO SecurityManager: Changing modify acls to: Aitor
18/03/02 13:03:46 INFO SecurityManager: Changing view acls groups to: 
18/03/02 13:03:46 INFO SecurityManager: Changing modify acls groups to: 
18/03/02 13:03:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/03/02 13:03:47 INFO Utils: Successfully started service 'sparkDriver' on port 52544.
18/03/02 13:03:47 INFO SparkEnv: Registering MapOutputTracker
18/03/02 13:03:47 INFO SparkEnv: Registering BlockManagerMaster
18/03/02 13:03:47 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-92301c1c-b48f-473b-bffb-80b63721f4b5
18/03/02 13:03:47 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/03/02 13:03:47 INFO SparkEnv: Registering OutputCommitCoordinator
18/03/02 13:03:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/03/02 13:03:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/03/02 13:03:48 INFO Executor: Starting executor ID driver on host localhost
18/03/02 13:03:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52545.
18/03/02 13:03:48 INFO NettyBlockTransferService: Server created on 192.168.2.140:52545
18/03/02 13:03:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 52545)
18/03/02 13:03:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:52545 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 52545)
18/03/02 13:03:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 52545)
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 2.1415497101064873
Loss in iteration 3 : 22.85361191192403
Loss in iteration 4 : 7.213353416805544
Loss in iteration 5 : 16.311916736010918
Loss in iteration 6 : 11.442165808524466
Loss in iteration 7 : 9.853829878915425
Loss in iteration 8 : 14.822841816914096
Loss in iteration 9 : 5.208611633251856
Loss in iteration 10 : 14.418368881409107
Loss in iteration 11 : 5.027456977489386
Loss in iteration 12 : 13.161213417949353
Loss in iteration 13 : 5.731375784753692
Loss in iteration 14 : 12.185442371765935
Loss in iteration 15 : 6.159044453681338
Loss in iteration 16 : 11.112710431471642
Loss in iteration 17 : 6.636997211211092
Loss in iteration 18 : 10.102649849043805
Loss in iteration 19 : 6.967889281158184
Loss in iteration 20 : 9.149213153490672
Loss in iteration 21 : 7.15718168974725
Loss in iteration 22 : 8.298333441163845
Loss in iteration 23 : 7.163533922187185
Loss in iteration 24 : 7.592512334522436
Loss in iteration 25 : 7.024069640371656
Loss in iteration 26 : 7.001492905922758
Loss in iteration 27 : 6.835819726364148
Loss in iteration 28 : 6.4606389128556385
Loss in iteration 29 : 6.63876531446032
Loss in iteration 30 : 6.033601126197935
Loss in iteration 31 : 6.439824430191861
Loss in iteration 32 : 5.710873207592275
Loss in iteration 33 : 6.245350939543617
Loss in iteration 34 : 5.44309167995375
Loss in iteration 35 : 6.0594075812203005
Loss in iteration 36 : 5.200301852161034
Loss in iteration 37 : 5.885788329523037
Loss in iteration 38 : 5.004616699027924
Loss in iteration 39 : 5.736806682123509
Loss in iteration 40 : 4.851267538800054
Loss in iteration 41 : 5.613576091966802
Loss in iteration 42 : 4.729931568486461
Loss in iteration 43 : 5.5120830716904
Loss in iteration 44 : 4.630276631813023
Loss in iteration 45 : 5.4270139346544495
Loss in iteration 46 : 4.545162113838045
Loss in iteration 47 : 5.353566656837554
Loss in iteration 48 : 4.469785388997431
Loss in iteration 49 : 5.288163857034761
Loss in iteration 50 : 4.401410122187842
Loss in iteration 51 : 5.229022865600883
Loss in iteration 52 : 4.3385883143096
Loss in iteration 53 : 5.174854409074386
Loss in iteration 54 : 4.280386792480607
Loss in iteration 55 : 5.124470984959158
Loss in iteration 56 : 4.226226279031749
Loss in iteration 57 : 5.077069142260422
Loss in iteration 58 : 4.175914895832697
Loss in iteration 59 : 5.032397573236799
Loss in iteration 60 : 4.129519093742632
Loss in iteration 61 : 4.990574392627254
Loss in iteration 62 : 4.08713867194044
Loss in iteration 63 : 4.95206484802872
Loss in iteration 64 : 4.048840182490219
Loss in iteration 65 : 4.916261251653182
Loss in iteration 66 : 4.01440949960704
Loss in iteration 67 : 4.874381138066345
Loss in iteration 68 : 3.981769288192356
Loss in iteration 69 : 4.83448277459639
Loss in iteration 70 : 3.952872989644928
Loss in iteration 71 : 4.8022691426236905
Loss in iteration 72 : 3.9286288774560054
Loss in iteration 73 : 4.773430204168935
Loss in iteration 74 : 3.9075326140819673
Loss in iteration 75 : 4.747192746517277
Loss in iteration 76 : 3.8890180694357106
Loss in iteration 77 : 4.723103048137378
Loss in iteration 78 : 3.8726655838955986
Loss in iteration 79 : 4.700407225364935
Loss in iteration 80 : 3.8580813833123857
Loss in iteration 81 : 4.678727245758649
Loss in iteration 82 : 3.8449907952584828
Loss in iteration 83 : 4.658333625544989
Loss in iteration 84 : 3.8332396266691986
Loss in iteration 85 : 4.639642696867711
Loss in iteration 86 : 3.8227048524843505
Loss in iteration 87 : 4.622707303622451
Loss in iteration 88 : 3.813232619260323
Loss in iteration 89 : 4.607271357472221
Loss in iteration 90 : 3.804653931589657
Loss in iteration 91 : 4.593003643782058
Loss in iteration 92 : 3.7968150376857315
Loss in iteration 93 : 4.57961375047165
Loss in iteration 94 : 3.789588069232212
Loss in iteration 95 : 4.56687118653145
Loss in iteration 96 : 3.782868997575614
Loss in iteration 97 : 4.5545954055774365
Loss in iteration 98 : 3.7765727914623524
Loss in iteration 99 : 4.542647400503683
Loss in iteration 100 : 3.770629649844063
Loss in iteration 101 : 4.530928520818161
Loss in iteration 102 : 3.764982874725794
Loss in iteration 103 : 4.519380758973124
Loss in iteration 104 : 3.7595876083399964
Loss in iteration 105 : 4.507981275665571
Loss in iteration 106 : 3.7544094130874366
Loss in iteration 107 : 4.496731860714981
Loss in iteration 108 : 3.7494225376554593
Loss in iteration 109 : 4.48565133860516
Loss in iteration 110 : 3.74460872698373
Loss in iteration 111 : 4.47477442362361
Loss in iteration 112 : 3.739957026183864
Loss in iteration 113 : 4.464150638117984
Loss in iteration 114 : 3.735463585827826
Loss in iteration 115 : 4.45383623086792
Loss in iteration 116 : 3.7311300825786455
Loss in iteration 117 : 4.443880688490428
Loss in iteration 118 : 3.7269606722615047
Loss in iteration 119 : 4.434315458348575
Loss in iteration 120 : 3.722958792152137
Loss in iteration 121 : 4.425150136896228
Loss in iteration 122 : 3.7191251801056024
Loss in iteration 123 : 4.416375643569702
Loss in iteration 124 : 3.715457514505187
Loss in iteration 125 : 4.407970755457085
Loss in iteration 126 : 3.7119512108707604
Loss in iteration 127 : 4.399908642092711
Loss in iteration 128 : 3.708600644949529
Loss in iteration 129 : 4.392161674376894
Loss in iteration 130 : 3.7054002371300276
Loss in iteration 131 : 4.384704178381811
Loss in iteration 132 : 3.702345115227138
Loss in iteration 133 : 4.377513519395152
Loss in iteration 134 : 3.6994313007040427
Loss in iteration 135 : 4.370570095459718
Loss in iteration 136 : 3.696655501401559
Loss in iteration 137 : 4.363856757756021
Loss in iteration 138 : 3.6940146601231665
Loss in iteration 139 : 4.357358037644814
Loss in iteration 140 : 3.69150542486464
Loss in iteration 141 : 4.3510594278918715
Loss in iteration 142 : 3.689123686267899
Loss in iteration 143 : 4.344946858132719
Loss in iteration 144 : 3.6868642813604513
Loss in iteration 145 : 4.339006419114514
Loss in iteration 146 : 3.684720903657308
Loss in iteration 147 : 4.3332243237615256
Loss in iteration 148 : 3.682686204813089
Loss in iteration 149 : 4.327587047880171
Loss in iteration 150 : 3.680752035595317
Loss in iteration 151 : 4.32208157221889
Loss in iteration 152 : 3.678909759233
Loss in iteration 153 : 4.316695649062869
Loss in iteration 154 : 3.6771505749916793
Loss in iteration 155 : 4.311418033798199
Loss in iteration 156 : 3.6754658062230323
Loss in iteration 157 : 4.306238645869301
Loss in iteration 158 : 3.6738471269411006
Loss in iteration 159 : 4.301148646516101
Loss in iteration 160 : 3.6722867184940706
Loss in iteration 161 : 4.296140438085904
Loss in iteration 162 : 3.6707773605262672
Loss in iteration 163 : 4.29120760025629
Loss in iteration 164 : 3.6693124678706295
Loss in iteration 165 : 4.286344782975209
Loss in iteration 166 : 3.667886088186671
Loss in iteration 167 : 4.281547575994306
Loss in iteration 168 : 3.6664928753025268
Loss in iteration 169 : 4.276812372245753
Loss in iteration 170 : 3.665128051446496
Loss in iteration 171 : 4.272136238343606
Loss in iteration 172 : 3.663787368705065
Loss in iteration 173 : 4.267516801100019
Loss in iteration 174 : 3.662467076706323
Loss in iteration 175 : 4.262952154707883
Loss in iteration 176 : 3.661163900111198
Loss in iteration 177 : 4.25844078949256
Loss in iteration 178 : 3.659875026306523
Loss in iteration 179 : 4.2539815400760235
Loss in iteration 180 : 3.6585981009910813
Loss in iteration 181 : 4.249573548557096
Loss in iteration 182 : 3.657331227354079
Loss in iteration 183 : 4.245216236976744
Loss in iteration 184 : 3.656072963451222
Loss in iteration 185 : 4.240909282953664
Loss in iteration 186 : 3.654822312298674
Loss in iteration 187 : 4.236652592923431
Loss in iteration 188 : 3.6535787001259123
Loss in iteration 189 : 4.232446268784111
Loss in iteration 190 : 3.652341939997887
Loss in iteration 191 : 4.228290565720402
Loss in iteration 192 : 3.6511121803217637
Loss in iteration 193 : 4.224185841217953
Loss in iteration 194 : 3.649889840166032
Loss in iteration 195 : 4.220132497401895
Loss in iteration 196 : 3.648675535383164
Loss in iteration 197 : 4.216130920463316
Loss in iteration 198 : 3.6474700008621768
Loss in iteration 199 : 4.212181421802624
Loss in iteration 200 : 3.64627401464082
Testing accuracy  of updater 0 on alg 0 with rate 10.0 = 0.7265, training accuracy 0.7165, time elapsed: 14107 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6826496441898317
Loss in iteration 3 : 0.7251674502731246
Loss in iteration 4 : 0.9043540796081143
Loss in iteration 5 : 0.9965633061226776
Loss in iteration 6 : 1.2081521260774448
Loss in iteration 7 : 0.8731585215684701
Loss in iteration 8 : 1.073014754531772
Loss in iteration 9 : 0.8494648798849442
Loss in iteration 10 : 1.0034789436340183
Loss in iteration 11 : 0.8103927207765962
Loss in iteration 12 : 0.9262320596095245
Loss in iteration 13 : 0.7768120276425861
Loss in iteration 14 : 0.860720184020779
Loss in iteration 15 : 0.7443777698293702
Loss in iteration 16 : 0.8036105906410159
Loss in iteration 17 : 0.7136515407463083
Loss in iteration 18 : 0.7544269284821327
Loss in iteration 19 : 0.684960693473743
Loss in iteration 20 : 0.712241332371743
Loss in iteration 21 : 0.6585618776820297
Loss in iteration 22 : 0.6761260965331396
Loss in iteration 23 : 0.634572064081359
Loss in iteration 24 : 0.6452250038763818
Loss in iteration 25 : 0.6129825721249872
Loss in iteration 26 : 0.6187857076040143
Loss in iteration 27 : 0.5936977934166692
Loss in iteration 28 : 0.5961615356064665
Loss in iteration 29 : 0.5765719973701612
Loss in iteration 30 : 0.576803147805839
Loss in iteration 31 : 0.5614411483053005
Loss in iteration 32 : 0.5602505572244721
Loss in iteration 33 : 0.5481516058199751
Loss in iteration 34 : 0.5461293625914501
Loss in iteration 35 : 0.5365757609816606
Loss in iteration 36 : 0.5341429191740226
Loss in iteration 37 : 0.52660148708145
Loss in iteration 38 : 0.5240483432437417
Loss in iteration 39 : 0.5181100333716709
Loss in iteration 40 : 0.5156273972185368
Loss in iteration 41 : 0.5109670373527165
Loss in iteration 42 : 0.5086719651262649
Loss in iteration 43 : 0.5050273743713417
Loss in iteration 44 : 0.5029833242899197
Loss in iteration 45 : 0.5001430085720839
Loss in iteration 46 : 0.49837509626748
Loss in iteration 47 : 0.496168632742591
Loss in iteration 48 : 0.4946751998761722
Loss in iteration 49 : 0.49296513038157863
Loss in iteration 50 : 0.49172673976376763
Loss in iteration 51 : 0.4904020457561844
Loss in iteration 52 : 0.48938863858323767
Loss in iteration 53 : 0.4883597570458247
Loss in iteration 54 : 0.4875363403764554
Loss in iteration 55 : 0.4867314363284452
Loss in iteration 56 : 0.48606243330642773
Loss in iteration 57 : 0.4854245479649704
Loss in iteration 58 : 0.48487685970730654
Loss in iteration 59 : 0.48436158836233767
Loss in iteration 60 : 0.4839064482816853
Loss in iteration 61 : 0.4834799069309343
Loss in iteration 62 : 0.48309369286384124
Loss in iteration 63 : 0.4827306520284418
Loss in iteration 64 : 0.48239489858648527
Loss in iteration 65 : 0.4820770578169887
Loss in iteration 66 : 0.481777944023318
Loss in iteration 67 : 0.48149237392771793
Loss in iteration 68 : 0.4812199423273427
Loss in iteration 69 : 0.48095773431227723
Loss in iteration 70 : 0.4807050421351246
Loss in iteration 71 : 0.4804601928845779
Loss in iteration 72 : 0.48022252631115747
Loss in iteration 73 : 0.47999106030261374
Loss in iteration 74 : 0.4797652787843924
Loss in iteration 75 : 0.47954459001548977
Loss in iteration 76 : 0.4793286196210816
Loss in iteration 77 : 0.4791169993370013
Loss in iteration 78 : 0.47890946459118233
Loss in iteration 79 : 0.47870577621154803
Loss in iteration 80 : 0.4785057459565658
Loss in iteration 81 : 0.4783092097067881
Loss in iteration 82 : 0.47811602922303953
Loss in iteration 83 : 0.4779260843467565
Loss in iteration 84 : 0.47773926860758414
Loss in iteration 85 : 0.47755548795291963
Loss in iteration 86 : 0.47737465595514855
Loss in iteration 87 : 0.47719669447855595
Loss in iteration 88 : 0.47702152995001107
Loss in iteration 89 : 0.47684909437836404
Loss in iteration 90 : 0.4766793227711587
Loss in iteration 91 : 0.47651215402444713
Loss in iteration 92 : 0.476347529223618
Loss in iteration 93 : 0.47618539230456486
Loss in iteration 94 : 0.4760256889616755
Loss in iteration 95 : 0.4758683671010295
Loss in iteration 96 : 0.4757133761436958
Loss in iteration 97 : 0.47556066731927527
Loss in iteration 98 : 0.4754101932198838
Loss in iteration 99 : 0.47526190798027457
Loss in iteration 100 : 0.4751157669888521
Loss in iteration 101 : 0.47497172699091883
Loss in iteration 102 : 0.4748297458976417
Loss in iteration 103 : 0.4746897828389947
Loss in iteration 104 : 0.4745517980339202
Loss in iteration 105 : 0.47441575281138515
Loss in iteration 106 : 0.47428160951899306
Loss in iteration 107 : 0.4741493315245061
Loss in iteration 108 : 0.47401888314887397
Loss in iteration 109 : 0.4738902296562816
Loss in iteration 110 : 0.47376333720292696
Loss in iteration 111 : 0.4736381728207309
Loss in iteration 112 : 0.47351470437650217
Loss in iteration 113 : 0.4733929005525054
Loss in iteration 114 : 0.47327273081267207
Loss in iteration 115 : 0.4731541653819755
Loss in iteration 116 : 0.47303717521758765
Loss in iteration 117 : 0.4729217319882045
Loss in iteration 118 : 0.47280780804882544
Loss in iteration 119 : 0.4726953764206714
Loss in iteration 120 : 0.47258441076872265
Loss in iteration 121 : 0.4724748853825776
Loss in iteration 122 : 0.47236677515618014
Loss in iteration 123 : 0.47226005556976863
Loss in iteration 124 : 0.47215470267141607
Loss in iteration 125 : 0.4720506930601141
Loss in iteration 126 : 0.47194800386884933
Loss in iteration 127 : 0.47184661274881046
Loss in iteration 128 : 0.47174649785380723
Loss in iteration 129 : 0.47164763782555064
Loss in iteration 130 : 0.4715500117792559
Loss in iteration 131 : 0.47145359928993924
Loss in iteration 132 : 0.4713583803790812
Loss in iteration 133 : 0.4712643355018713
Loss in iteration 134 : 0.4711714455348327
Loss in iteration 135 : 0.4710796917639378
Loss in iteration 136 : 0.47098905587311557
Loss in iteration 137 : 0.4708995199331736
Loss in iteration 138 : 0.4708110663910997
Loss in iteration 139 : 0.47072367805974186
Loss in iteration 140 : 0.47063733810783154
Loss in iteration 141 : 0.47055203005036117
Loss in iteration 142 : 0.4704677377392743
Loss in iteration 143 : 0.47038444535448115
Loss in iteration 144 : 0.4703021373951693
Loss in iteration 145 : 0.4702207986714011
Loss in iteration 146 : 0.47014041429599723
Loss in iteration 147 : 0.47006096967667893
Loss in iteration 148 : 0.4699824505084694
Loss in iteration 149 : 0.469904842766343
Loss in iteration 150 : 0.4698281326981088
Loss in iteration 151 : 0.4697523068175252
Loss in iteration 152 : 0.46967735189762694
Loss in iteration 153 : 0.46960325496427324
Loss in iteration 154 : 0.46953000328988853
Loss in iteration 155 : 0.469457584387403
Loss in iteration 156 : 0.4693859860043849
Loss in iteration 157 : 0.4693151961173414
Loss in iteration 158 : 0.46924520292621147
Loss in iteration 159 : 0.4691759948490097
Loss in iteration 160 : 0.46910756051663915
Loss in iteration 161 : 0.46903988876786623
Loss in iteration 162 : 0.4689729686444297
Loss in iteration 163 : 0.4689067893863123
Loss in iteration 164 : 0.4688413404271405
Loss in iteration 165 : 0.46877661138972065
Loss in iteration 166 : 0.4687125920817095
Loss in iteration 167 : 0.4686492724914051
Loss in iteration 168 : 0.4685866427836612
Loss in iteration 169 : 0.4685246932959166
Loss in iteration 170 : 0.46846341453434004
Loss in iteration 171 : 0.4684027971700838
Loss in iteration 172 : 0.46834283203563604
Loss in iteration 173 : 0.4682835101212835
Loss in iteration 174 : 0.46822482257166703
Loss in iteration 175 : 0.4681667606824352
Loss in iteration 176 : 0.4681093158969834
Loss in iteration 177 : 0.46805247980328984
Loss in iteration 178 : 0.4679962441308309
Loss in iteration 179 : 0.4679406007475827
Loss in iteration 180 : 0.4678855416571017
Loss in iteration 181 : 0.4678310589956815
Loss in iteration 182 : 0.46777714502958817
Loss in iteration 183 : 0.46772379215236287
Loss in iteration 184 : 0.4676709928822023
Loss in iteration 185 : 0.46761873985939567
Loss in iteration 186 : 0.4675670258438458
Loss in iteration 187 : 0.467515843712632
Loss in iteration 188 : 0.4674651864576492
Loss in iteration 189 : 0.4674150471833077
Loss in iteration 190 : 0.46736541910428137
Loss in iteration 191 : 0.4673162955433197
Loss in iteration 192 : 0.46726766992911245
Loss in iteration 193 : 0.4672195357942063
Loss in iteration 194 : 0.4671718867729725
Loss in iteration 195 : 0.4671247165996278
Loss in iteration 196 : 0.4670780191062973
Loss in iteration 197 : 0.4670317882211299
Loss in iteration 198 : 0.4669860179664578
Loss in iteration 199 : 0.4669407024569972
Loss in iteration 200 : 0.4668958358980927
Testing accuracy  of updater 0 on alg 0 with rate 1.0 = 0.786, training accuracy 0.788875, time elapsed: 7210 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6878658918511905
Loss in iteration 3 : 0.6839999898353137
Loss in iteration 4 : 0.6808139087951742
Loss in iteration 5 : 0.677975901546472
Loss in iteration 6 : 0.6753351733505975
Loss in iteration 7 : 0.6728218611194671
Loss in iteration 8 : 0.6704024793367365
Loss in iteration 9 : 0.6680600344706835
Loss in iteration 10 : 0.6657851055363632
Loss in iteration 11 : 0.6635718203046861
Loss in iteration 12 : 0.6614160283793024
Loss in iteration 13 : 0.6593144648356347
Loss in iteration 14 : 0.6572643624102471
Loss in iteration 15 : 0.6552632676432811
Loss in iteration 16 : 0.6533089502150726
Loss in iteration 17 : 0.6513993551912485
Loss in iteration 18 : 0.6495325752975568
Loss in iteration 19 : 0.6477068327923042
Loss in iteration 20 : 0.6459204661632978
Loss in iteration 21 : 0.644171919450056
Loss in iteration 22 : 0.642459733162238
Loss in iteration 23 : 0.6407825362970553
Loss in iteration 24 : 0.6391390392002939
Loss in iteration 25 : 0.6375280271260978
Loss in iteration 26 : 0.6359483544020398
Loss in iteration 27 : 0.6343989391308696
Loss in iteration 28 : 0.6328787583733338
Loss in iteration 29 : 0.6313868437643312
Loss in iteration 30 : 0.6299222775200342
Loss in iteration 31 : 0.628484188797992
Loss in iteration 32 : 0.6270717503759341
Loss in iteration 33 : 0.6256841756183785
Loss in iteration 34 : 0.6243207157031215
Loss in iteration 35 : 0.6229806570825671
Loss in iteration 36 : 0.6216633191572709
Loss in iteration 37 : 0.6203680521414721
Loss in iteration 38 : 0.6190942351023517
Loss in iteration 39 : 0.6178412741567081
Loss in iteration 40 : 0.6166086008103466
Loss in iteration 41 : 0.6153956704270143
Loss in iteration 42 : 0.6142019608150308
Loss in iteration 43 : 0.6130269709209855
Loss in iteration 44 : 0.6118702196209093
Loss in iteration 45 : 0.6107312446003313
Loss in iteration 46 : 0.6096096013154192
Loss in iteration 47 : 0.6085048620282401
Loss in iteration 48 : 0.6074166149098144
Loss in iteration 49 : 0.6063444632052245
Loss in iteration 50 : 0.6052880244556705
Loss in iteration 51 : 0.6042469297727397
Loss in iteration 52 : 0.6032208231607084
Loss in iteration 53 : 0.6022093608829874
Loss in iteration 54 : 0.6012122108692548
Loss in iteration 55 : 0.6002290521600734
Loss in iteration 56 : 0.5992595743861366
Loss in iteration 57 : 0.5983034772794824
Loss in iteration 58 : 0.5973604702142866
Loss in iteration 59 : 0.596430271775044
Loss in iteration 60 : 0.5955126093501357
Loss in iteration 61 : 0.5946072187489437
Loss in iteration 62 : 0.5937138438408326
Loss in iteration 63 : 0.5928322362144707
Loss in iteration 64 : 0.5919621548560601
Loss in iteration 65 : 0.5911033658451936
Loss in iteration 66 : 0.5902556420671355
Loss in iteration 67 : 0.5894187629404287
Loss in iteration 68 : 0.5885925141588136
Loss in iteration 69 : 0.5877766874465371
Loss in iteration 70 : 0.5869710803261734
Loss in iteration 71 : 0.5861754958981633
Loss in iteration 72 : 0.5853897426313487
Loss in iteration 73 : 0.5846136341638044
Loss in iteration 74 : 0.583846989113345
Loss in iteration 75 : 0.5830896308971145
Loss in iteration 76 : 0.5823413875597198
Loss in iteration 77 : 0.5816020916094072
Loss in iteration 78 : 0.580871579861795
Loss in iteration 79 : 0.5801496932907492
Loss in iteration 80 : 0.5794362768859773
Loss in iteration 81 : 0.5787311795169685
Loss in iteration 82 : 0.5780342538029272
Loss in iteration 83 : 0.5773453559883688
Loss in iteration 84 : 0.5766643458240656
Loss in iteration 85 : 0.5759910864530616
Loss in iteration 86 : 0.5753254443014856
Loss in iteration 87 : 0.5746672889739048
Loss in iteration 88 : 0.5740164931529829
Loss in iteration 89 : 0.5733729325032351
Loss in iteration 90 : 0.5727364855786493
Loss in iteration 91 : 0.5721070337339966
Loss in iteration 92 : 0.5714844610396395
Loss in iteration 93 : 0.5708686541996694
Loss in iteration 94 : 0.5702595024732074
Loss in iteration 95 : 0.5696568975987187
Loss in iteration 96 : 0.5690607337211949
Loss in iteration 97 : 0.568470907322068
Loss in iteration 98 : 0.5678873171517355
Loss in iteration 99 : 0.5673098641645654
Loss in iteration 100 : 0.5667384514562771
Loss in iteration 101 : 0.5661729842035769
Loss in iteration 102 : 0.5656133696059708
Loss in iteration 103 : 0.5650595168296259
Loss in iteration 104 : 0.5645113369532248
Loss in iteration 105 : 0.5639687429156891
Loss in iteration 106 : 0.5634316494657308
Loss in iteration 107 : 0.5628999731131155
Loss in iteration 108 : 0.562373632081596
Loss in iteration 109 : 0.5618525462634238
Loss in iteration 110 : 0.5613366371753887
Loss in iteration 111 : 0.5608258279163048
Loss in iteration 112 : 0.5603200431259148
Loss in iteration 113 : 0.5598192089451131
Loss in iteration 114 : 0.5593232529774744
Loss in iteration 115 : 0.558832104252012
Loss in iteration 116 : 0.5583456931871228
Loss in iteration 117 : 0.5578639515556761
Loss in iteration 118 : 0.5573868124511993
Loss in iteration 119 : 0.5569142102551146
Loss in iteration 120 : 0.556446080604996
Loss in iteration 121 : 0.5559823603637956
Loss in iteration 122 : 0.555522987590016
Loss in iteration 123 : 0.5550679015087794
Loss in iteration 124 : 0.5546170424837759
Loss in iteration 125 : 0.5541703519900388
Loss in iteration 126 : 0.553727772587544
Loss in iteration 127 : 0.55328924789557
Loss in iteration 128 : 0.5528547225678229
Loss in iteration 129 : 0.5524241422682826
Loss in iteration 130 : 0.5519974536477374
Loss in iteration 131 : 0.551574604321011
Loss in iteration 132 : 0.5511555428448192
Loss in iteration 133 : 0.5507402186962718
Loss in iteration 134 : 0.5503285822519707
Loss in iteration 135 : 0.5499205847676969
Loss in iteration 136 : 0.5495161783586634
Loss in iteration 137 : 0.5491153159803114
Loss in iteration 138 : 0.5487179514096435
Loss in iteration 139 : 0.5483240392270539
Loss in iteration 140 : 0.5479335347986659
Loss in iteration 141 : 0.5475463942591391
Loss in iteration 142 : 0.5471625744949421
Loss in iteration 143 : 0.5467820331280704
Loss in iteration 144 : 0.5464047285001973
Loss in iteration 145 : 0.5460306196572443
Loss in iteration 146 : 0.5456596663343599
Loss in iteration 147 : 0.5452918289412828
Loss in iteration 148 : 0.5449270685480951
Loss in iteration 149 : 0.5445653468713367
Loss in iteration 150 : 0.5442066262604827
Loss in iteration 151 : 0.5438508696847598
Loss in iteration 152 : 0.5434980407203076
Loss in iteration 153 : 0.5431481035376543
Loss in iteration 154 : 0.5428010228895168
Loss in iteration 155 : 0.5424567640988994
Loss in iteration 156 : 0.5421152930474964
Loss in iteration 157 : 0.5417765761643764
Loss in iteration 158 : 0.541440580414948
Loss in iteration 159 : 0.5411072732901959
Loss in iteration 160 : 0.5407766227961843
Loss in iteration 161 : 0.5404485974438099
Loss in iteration 162 : 0.5401231662388019
Loss in iteration 163 : 0.5398002986719722
Loss in iteration 164 : 0.5394799647096877
Loss in iteration 165 : 0.5391621347845804
Loss in iteration 166 : 0.5388467797864681
Loss in iteration 167 : 0.5385338710534989
Loss in iteration 168 : 0.5382233803634977
Loss in iteration 169 : 0.5379152799255178
Loss in iteration 170 : 0.5376095423715904
Loss in iteration 171 : 0.537306140748665
Loss in iteration 172 : 0.5370050485107306
Loss in iteration 173 : 0.5367062395111304
Loss in iteration 174 : 0.5364096879950407
Loss in iteration 175 : 0.5361153685921244
Loss in iteration 176 : 0.5358232563093571
Loss in iteration 177 : 0.5355333265240081
Loss in iteration 178 : 0.5352455549767813
Loss in iteration 179 : 0.5349599177651116
Loss in iteration 180 : 0.5346763913366122
Loss in iteration 181 : 0.5343949524826617
Loss in iteration 182 : 0.5341155783321371
Loss in iteration 183 : 0.5338382463452865
Loss in iteration 184 : 0.5335629343077335
Loss in iteration 185 : 0.5332896203246098
Loss in iteration 186 : 0.533018282814826
Loss in iteration 187 : 0.5327489005054529
Loss in iteration 188 : 0.5324814524262331
Loss in iteration 189 : 0.5322159179042101
Loss in iteration 190 : 0.5319522765584663
Loss in iteration 191 : 0.5316905082949829
Loss in iteration 192 : 0.5314305933016004
Loss in iteration 193 : 0.5311725120430871
Loss in iteration 194 : 0.5309162452563185
Loss in iteration 195 : 0.5306617739455485
Loss in iteration 196 : 0.5304090793777873
Loss in iteration 197 : 0.530158143078269
Loss in iteration 198 : 0.5299089468260176
Loss in iteration 199 : 0.5296614726495077
Loss in iteration 200 : 0.529415702822401
Testing accuracy  of updater 0 on alg 0 with rate 0.09999999999999998 = 0.7705, training accuracy 0.77325, time elapsed: 6530 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 2.141549710106487
Loss in iteration 3 : 19.912833318335267
Loss in iteration 4 : 7.263295659762945
Loss in iteration 5 : 24.58427093781558
Loss in iteration 6 : 31.901257938297892
Loss in iteration 7 : 15.57133599274605
Loss in iteration 8 : 13.779093573662783
Loss in iteration 9 : 23.582129583930495
Loss in iteration 10 : 8.618430102979147
Loss in iteration 11 : 10.918571670793309
Loss in iteration 12 : 17.659501645415308
Loss in iteration 13 : 7.16053708086142
Loss in iteration 14 : 8.20029906588534
Loss in iteration 15 : 14.427973212444344
Loss in iteration 16 : 9.454683713401025
Loss in iteration 17 : 5.318414726782044
Loss in iteration 18 : 11.04638904820668
Loss in iteration 19 : 9.955899554541906
Loss in iteration 20 : 5.4665585467424265
Loss in iteration 21 : 8.305500239795176
Loss in iteration 22 : 9.744697860132804
Loss in iteration 23 : 6.645437470716198
Loss in iteration 24 : 6.001239377838234
Loss in iteration 25 : 8.502556853610042
Loss in iteration 26 : 6.876025234315827
Loss in iteration 27 : 5.391798725838795
Loss in iteration 28 : 7.0420421281213645
Loss in iteration 29 : 6.8931363474977045
Loss in iteration 30 : 5.133855167160712
Loss in iteration 31 : 5.7916445347340515
Loss in iteration 32 : 6.294025807286104
Loss in iteration 33 : 4.758703691352913
Loss in iteration 34 : 5.16841415044141
Loss in iteration 35 : 5.52539319110993
Loss in iteration 36 : 4.392151481039284
Loss in iteration 37 : 4.543210763209459
Loss in iteration 38 : 4.784818901469643
Loss in iteration 39 : 3.805035884314101
Loss in iteration 40 : 4.2506347640195825
Loss in iteration 41 : 4.014342802231828
Loss in iteration 42 : 3.3997963330524295
Loss in iteration 43 : 3.8467454039220867
Loss in iteration 44 : 3.160172225413176
Loss in iteration 45 : 3.310189456329309
Loss in iteration 46 : 3.1639960498708937
Loss in iteration 47 : 2.805599765870568
Loss in iteration 48 : 3.0097529552486013
Loss in iteration 49 : 2.521242703766329
Loss in iteration 50 : 2.769267774964827
Loss in iteration 51 : 2.3237423376401605
Loss in iteration 52 : 2.569282828517358
Loss in iteration 53 : 2.1586788591704895
Loss in iteration 54 : 2.377332257454432
Loss in iteration 55 : 2.0380957616833686
Loss in iteration 56 : 2.16267790842266
Loss in iteration 57 : 2.073022376723159
Loss in iteration 58 : 1.8608108547270508
Loss in iteration 59 : 2.0778006146006707
Loss in iteration 60 : 1.9286789847298254
Loss in iteration 61 : 1.6747598926326428
Loss in iteration 62 : 1.8112064366349185
Loss in iteration 63 : 1.964689776962384
Loss in iteration 64 : 1.7589848327164155
Loss in iteration 65 : 1.507486554360387
Loss in iteration 66 : 1.3359119160781407
Loss in iteration 67 : 1.2872074944714755
Loss in iteration 68 : 1.3628429209650237
Loss in iteration 69 : 1.6733350209944373
Loss in iteration 70 : 2.5933275492470864
Loss in iteration 71 : 2.943139864469456
Loss in iteration 72 : 2.7794999319688967
Loss in iteration 73 : 1.6589877320216708
Loss in iteration 74 : 1.2420285142859167
Loss in iteration 75 : 1.0752238167407235
Loss in iteration 76 : 1.015655432474963
Loss in iteration 77 : 0.9769628586303005
Loss in iteration 78 : 0.9350367815832237
Loss in iteration 79 : 0.888247009121558
Loss in iteration 80 : 0.8450223117942588
Loss in iteration 81 : 0.8034707569505233
Loss in iteration 82 : 0.7585552062250626
Loss in iteration 83 : 0.9160433060436876
Loss in iteration 84 : 4.712245406406155
Loss in iteration 85 : 9.429583232799732
Loss in iteration 86 : 0.7199790481710339
Loss in iteration 87 : 5.741364590626521
Loss in iteration 88 : 7.1849913328582415
Loss in iteration 89 : 1.110953000131733
Loss in iteration 90 : 3.6739964108534533
Loss in iteration 91 : 5.545528041089411
Loss in iteration 92 : 1.3896224439489415
Loss in iteration 93 : 5.680948586173454
Loss in iteration 94 : 2.102349818282379
Loss in iteration 95 : 3.1540048385493966
Loss in iteration 96 : 2.91652474215626
Loss in iteration 97 : 1.8816454437444636
Loss in iteration 98 : 3.1470744729158278
Loss in iteration 99 : 1.763401909297429
Loss in iteration 100 : 3.3357931019855633
Loss in iteration 101 : 2.012769627318217
Loss in iteration 102 : 2.435025485189391
Loss in iteration 103 : 2.227377733152316
Loss in iteration 104 : 1.6129411189259504
Loss in iteration 105 : 2.2208312400824113
Loss in iteration 106 : 1.4963518756693674
Loss in iteration 107 : 1.6828863751822178
Loss in iteration 108 : 2.1344425583766635
Loss in iteration 109 : 1.3546712650097945
Loss in iteration 110 : 1.2836152344879173
Loss in iteration 111 : 1.9491771427084414
Loss in iteration 112 : 2.3049869903859816
Loss in iteration 113 : 2.404757256735863
Loss in iteration 114 : 1.899595805471051
Loss in iteration 115 : 1.8571016526696067
Loss in iteration 116 : 2.1154349772989116
Loss in iteration 117 : 2.851614497939137
Loss in iteration 118 : 2.929635161239701
Loss in iteration 119 : 2.2610556018328603
Loss in iteration 120 : 1.647695042590934
Loss in iteration 121 : 1.3394952121859491
Loss in iteration 122 : 1.1689546479361803
Loss in iteration 123 : 1.0780433194403527
Loss in iteration 124 : 1.0238620842052941
Loss in iteration 125 : 1.0033539038235684
Loss in iteration 126 : 1.0582218225299114
Loss in iteration 127 : 1.5848676892393383
Loss in iteration 128 : 3.7758739041244715
Loss in iteration 129 : 5.67079705640119
Loss in iteration 130 : 1.0474936097012029
Loss in iteration 131 : 2.1628611673072013
Loss in iteration 132 : 6.641929762855321
Loss in iteration 133 : 1.1109071262229564
Loss in iteration 134 : 4.330040907524448
Loss in iteration 135 : 4.527218564972136
Loss in iteration 136 : 1.6400906456235518
Loss in iteration 137 : 6.063466341587531
Loss in iteration 138 : 1.5440383709621892
Loss in iteration 139 : 4.311789647190169
Loss in iteration 140 : 1.9015406100889392
Loss in iteration 141 : 2.9534617873183366
Loss in iteration 142 : 2.933121731357539
Loss in iteration 143 : 2.0678903640489525
Loss in iteration 144 : 3.5957232458844075
Loss in iteration 145 : 1.6175706674692252
Loss in iteration 146 : 2.644568096826696
Loss in iteration 147 : 1.874485799797562
Loss in iteration 148 : 1.6310466802803674
Loss in iteration 149 : 2.3565883328750536
Loss in iteration 150 : 1.414273122524704
Loss in iteration 151 : 1.703977908288281
Loss in iteration 152 : 2.5137953197999385
Loss in iteration 153 : 1.4153044949944054
Loss in iteration 154 : 1.1775037580476069
Loss in iteration 155 : 1.9056632963298559
Loss in iteration 156 : 2.648818949426995
Loss in iteration 157 : 2.8623488127397603
Loss in iteration 158 : 1.834983078929835
Loss in iteration 159 : 1.4055128260082983
Loss in iteration 160 : 1.3015110653640587
Loss in iteration 161 : 1.4677686246082116
Loss in iteration 162 : 2.20320164498127
Loss in iteration 163 : 3.709888205314422
Loss in iteration 164 : 3.059382712238133
Loss in iteration 165 : 2.063884369803855
Loss in iteration 166 : 1.5404742797523823
Loss in iteration 167 : 1.4180929273274427
Loss in iteration 168 : 1.3793428100298653
Loss in iteration 169 : 1.4984045348851405
Loss in iteration 170 : 1.6743557017819057
Loss in iteration 171 : 2.113729756077377
Loss in iteration 172 : 2.1257142794290007
Loss in iteration 173 : 2.293735389427652
Loss in iteration 174 : 1.761563910729139
Loss in iteration 175 : 1.6398700132704902
Loss in iteration 176 : 1.4415260174306848
Loss in iteration 177 : 1.5274648261841204
Loss in iteration 178 : 1.6634339634911115
Loss in iteration 179 : 2.261628677397668
Loss in iteration 180 : 2.439721540504872
Loss in iteration 181 : 2.6071164707500896
Loss in iteration 182 : 1.91770750486476
Loss in iteration 183 : 1.6718145179668622
Loss in iteration 184 : 1.4609333590290705
Loss in iteration 185 : 1.5216572250251974
Loss in iteration 186 : 1.6617604609130519
Loss in iteration 187 : 2.3195968921784496
Loss in iteration 188 : 2.6198032978792067
Loss in iteration 189 : 2.8747673724375287
Loss in iteration 190 : 1.8528677524575072
Loss in iteration 191 : 1.5275004826445577
Loss in iteration 192 : 1.339696048723018
Loss in iteration 193 : 1.407403172393579
Loss in iteration 194 : 1.5513176431215323
Loss in iteration 195 : 2.1638038009364955
Loss in iteration 196 : 2.5262199900138373
Loss in iteration 197 : 2.8912106402852515
Loss in iteration 198 : 1.9054585644266548
Loss in iteration 199 : 1.5449677015936223
Loss in iteration 200 : 1.3523389750657298
Testing accuracy  of updater 1 on alg 0 with rate 10.0 = 0.769, training accuracy 0.760625, time elapsed: 6962 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6826496441898317
Loss in iteration 3 : 0.6339254231375362
Loss in iteration 4 : 0.5867704951414214
Loss in iteration 5 : 0.5629247813403957
Loss in iteration 6 : 0.5309264422630863
Loss in iteration 7 : 0.5148220298238637
Loss in iteration 8 : 0.5014300866706937
Loss in iteration 9 : 0.4936134084567477
Loss in iteration 10 : 0.4895398353767425
Loss in iteration 11 : 0.4873414015503837
Loss in iteration 12 : 0.48534746038929394
Loss in iteration 13 : 0.4859659965021575
Loss in iteration 14 : 0.48401731685190746
Loss in iteration 15 : 0.4857146609389198
Loss in iteration 16 : 0.4837876997639102
Loss in iteration 17 : 0.4847076607097183
Loss in iteration 18 : 0.4833683345675886
Loss in iteration 19 : 0.4825484894877986
Loss in iteration 20 : 0.48196585997005537
Loss in iteration 21 : 0.47999165564753626
Loss in iteration 22 : 0.47963191667020355
Loss in iteration 23 : 0.4776483370884678
Loss in iteration 24 : 0.476717130597674
Loss in iteration 25 : 0.47526808608302523
Loss in iteration 26 : 0.47358347020652986
Loss in iteration 27 : 0.47253592833282265
Loss in iteration 28 : 0.4706389269933054
Loss in iteration 29 : 0.4696970128909924
Loss in iteration 30 : 0.46819256438895096
Loss in iteration 31 : 0.4671926091231702
Loss in iteration 32 : 0.46627051259625146
Loss in iteration 33 : 0.46528340419196335
Loss in iteration 34 : 0.46482699453703147
Loss in iteration 35 : 0.46406281057037757
Loss in iteration 36 : 0.46388582935762
Loss in iteration 37 : 0.46348106477789824
Loss in iteration 38 : 0.4634087126422414
Loss in iteration 39 : 0.46329872825637874
Loss in iteration 40 : 0.46321047433555473
Loss in iteration 41 : 0.4632567117253558
Loss in iteration 42 : 0.46314454713927095
Loss in iteration 43 : 0.4632409180417411
Loss in iteration 44 : 0.4631398172202634
Loss in iteration 45 : 0.46320677657766907
Loss in iteration 46 : 0.46312256262486984
Loss in iteration 47 : 0.4631095199857109
Loss in iteration 48 : 0.463032772281511
Loss in iteration 49 : 0.462941256714982
Loss in iteration 50 : 0.4628671986026972
Loss in iteration 51 : 0.46272439643518243
Loss in iteration 52 : 0.46263978907545183
Loss in iteration 53 : 0.46246977902090747
Loss in iteration 54 : 0.4623628086743371
Loss in iteration 55 : 0.46219243478786054
Loss in iteration 56 : 0.46207066732774504
Loss in iteration 57 : 0.4619260445249076
Loss in iteration 58 : 0.46180401697337475
Loss in iteration 59 : 0.4616935354725827
Loss in iteration 60 : 0.4615795874842703
Loss in iteration 61 : 0.46149676893302466
Loss in iteration 62 : 0.46139748605998376
Loss in iteration 63 : 0.4613338746553849
Loss in iteration 64 : 0.4612547734858636
Loss in iteration 65 : 0.46120218185355194
Loss in iteration 66 : 0.46114207350457126
Loss in iteration 67 : 0.46109296267961036
Loss in iteration 68 : 0.46104641306191096
Loss in iteration 69 : 0.46099839924326047
Loss in iteration 70 : 0.4609606847620852
Loss in iteration 71 : 0.4609154926907647
Loss in iteration 72 : 0.46088221312122696
Loss in iteration 73 : 0.4608408301643374
Loss in iteration 74 : 0.46080762791188684
Loss in iteration 75 : 0.46076958080876157
Loss in iteration 76 : 0.46073450571090807
Loss in iteration 77 : 0.4606993442039976
Loss in iteration 78 : 0.46066300766105794
Loss in iteration 79 : 0.46062996710910664
Loss in iteration 80 : 0.460593615459844
Loss in iteration 81 : 0.46056162310107585
Loss in iteration 82 : 0.4605265930050759
Loss in iteration 83 : 0.4604953972268911
Loss in iteration 84 : 0.4604630641970627
Loss in iteration 85 : 0.4604331470013139
Loss in iteration 86 : 0.46040406424013064
Loss in iteration 87 : 0.4603757400010658
Loss in iteration 88 : 0.4603494449734092
Loss in iteration 89 : 0.4603228402708441
Loss in iteration 90 : 0.4602986516056313
Loss in iteration 91 : 0.4602739558033927
Loss in iteration 92 : 0.46025134754557473
Loss in iteration 93 : 0.4602285474075361
Loss in iteration 94 : 0.4602070513402943
Loss in iteration 95 : 0.4601858487701123
Loss in iteration 96 : 0.46016518049895905
Loss in iteration 97 : 0.4601452477436257
Loss in iteration 98 : 0.4601253585649194
Loss in iteration 99 : 0.460106389453425
Loss in iteration 100 : 0.4600872469682232
Loss in iteration 101 : 0.46006891118800597
Loss in iteration 102 : 0.4600504297269225
Loss in iteration 103 : 0.4600325139243527
Loss in iteration 104 : 0.4600146612598618
Loss in iteration 105 : 0.45999714500344263
Loss in iteration 106 : 0.4599799229333456
Loss in iteration 107 : 0.4599628661744211
Loss in iteration 108 : 0.45994624413583185
Loss in iteration 109 : 0.4599297156539008
Loss in iteration 110 : 0.4599136656808428
Loss in iteration 111 : 0.45989774110907583
Loss in iteration 112 : 0.4598822594784108
Loss in iteration 113 : 0.45986697955883843
Loss in iteration 114 : 0.45985205252424216
Loss in iteration 115 : 0.45983739709280935
Loss in iteration 116 : 0.459823003854957
Loss in iteration 117 : 0.45980893050836097
Loss in iteration 118 : 0.4597950665324818
Loss in iteration 119 : 0.4597815360845375
Loss in iteration 120 : 0.4597681967184146
Loss in iteration 121 : 0.4597551656652332
Loss in iteration 122 : 0.45974232941635623
Loss in iteration 123 : 0.4597297573301424
Loss in iteration 124 : 0.45971739659249383
Loss in iteration 125 : 0.45970525815332314
Loss in iteration 126 : 0.4596933453434739
Loss in iteration 127 : 0.4596816229980902
Loss in iteration 128 : 0.4596701272209326
Loss in iteration 129 : 0.45965880429874023
Loss in iteration 130 : 0.4596476983046848
Loss in iteration 131 : 0.459636763006502
Loss in iteration 132 : 0.45962603029527704
Loss in iteration 133 : 0.45961547401971176
Loss in iteration 134 : 0.4596051040551408
Loss in iteration 135 : 0.45959491538139075
Loss in iteration 136 : 0.45958489868767843
Loss in iteration 137 : 0.4595750640510281
Loss in iteration 138 : 0.4595653923717036
Loss in iteration 139 : 0.45955589887160664
Loss in iteration 140 : 0.4595465637686346
Loss in iteration 141 : 0.459537398561092
Loss in iteration 142 : 0.45952838918876876
Loss in iteration 143 : 0.459519539541399
Loss in iteration 144 : 0.459510843819869
Loss in iteration 145 : 0.459502298631699
Loss in iteration 146 : 0.4594939046917484
Loss in iteration 147 : 0.4594856538583363
Loss in iteration 148 : 0.4594775496148299
Loss in iteration 149 : 0.45946958263263626
Loss in iteration 150 : 0.4594617559580026
Loss in iteration 151 : 0.4594540621120512
Loss in iteration 152 : 0.45944650185397157
Loss in iteration 153 : 0.4594390707841749
Loss in iteration 154 : 0.4594317668911936
Loss in iteration 155 : 0.4594245884741663
Loss in iteration 156 : 0.459417531568454
Loss in iteration 157 : 0.4594105960761398
Loss in iteration 158 : 0.4594037774666971
Loss in iteration 159 : 0.4593970759471389
Loss in iteration 160 : 0.459390487503027
Loss in iteration 161 : 0.459384011644847
Loss in iteration 162 : 0.45937764540784537
Loss in iteration 163 : 0.45937138727704047
Loss in iteration 164 : 0.45936523538314533
Loss in iteration 165 : 0.45935918742783977
Loss in iteration 166 : 0.4593532422862556
Loss in iteration 167 : 0.45934739732215923
Loss in iteration 168 : 0.45934165163035506
Loss in iteration 169 : 0.4593360026686851
Loss in iteration 170 : 0.45933044936700074
Loss in iteration 171 : 0.4593249895759478
Loss in iteration 172 : 0.45931962190424264
Loss in iteration 173 : 0.45931434465709264
Loss in iteration 174 : 0.4593091561452591
Loss in iteration 175 : 0.4593040550063474
Loss in iteration 176 : 0.45929903940031785
Loss in iteration 177 : 0.4592941081213666
Loss in iteration 178 : 0.45928925936723713
Loss in iteration 179 : 0.4592844919445271
Loss in iteration 180 : 0.45927980421745085
Loss in iteration 181 : 0.4592751949160043
Loss in iteration 182 : 0.4592706626038703
Loss in iteration 183 : 0.45926620592461337
Loss in iteration 184 : 0.4592618236132378
Loss in iteration 185 : 0.45925751427814354
Loss in iteration 186 : 0.45925327676228006
Loss in iteration 187 : 0.4592491097018753
Loss in iteration 188 : 0.45924501197983103
Loss in iteration 189 : 0.4592409823060672
Loss in iteration 190 : 0.4592370195604802
Loss in iteration 191 : 0.45923312254807697
Loss in iteration 192 : 0.4592292901388858
Loss in iteration 193 : 0.4592255212289514
Loss in iteration 194 : 0.45922181469194173
Testing accuracy  of updater 1 on alg 0 with rate 1.0 = 0.7865, training accuracy 0.7895, time elapsed: 6468 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6878658918511906
Loss in iteration 3 : 0.6807027170966676
Loss in iteration 4 : 0.6739294871347783
Loss in iteration 5 : 0.6668886231145077
Loss in iteration 6 : 0.6579684441529687
Loss in iteration 7 : 0.6470866160584807
Loss in iteration 8 : 0.6358501545886248
Loss in iteration 9 : 0.6258176075743542
Loss in iteration 10 : 0.6170799796352965
Loss in iteration 11 : 0.6086149317782449
Loss in iteration 12 : 0.5996947076182619
Loss in iteration 13 : 0.5906248408492142
Loss in iteration 14 : 0.5822230682148322
Loss in iteration 15 : 0.5749146859145032
Loss in iteration 16 : 0.5684454167556627
Loss in iteration 17 : 0.5623019911408129
Loss in iteration 18 : 0.5562424100141591
Loss in iteration 19 : 0.5504326885724573
Loss in iteration 20 : 0.5451825083412976
Loss in iteration 21 : 0.540615946825232
Loss in iteration 22 : 0.5365812551388363
Loss in iteration 23 : 0.532822110633772
Loss in iteration 24 : 0.5291993831674038
Loss in iteration 25 : 0.5257602506283421
Loss in iteration 26 : 0.5226337961602794
Loss in iteration 27 : 0.5198843942263013
Loss in iteration 28 : 0.5174547948874172
Loss in iteration 29 : 0.5152224074099971
Loss in iteration 30 : 0.5130951623791214
Loss in iteration 31 : 0.5110616419569136
Loss in iteration 32 : 0.5091677590336269
Loss in iteration 33 : 0.5074555801347291
Loss in iteration 34 : 0.5059201648144043
Loss in iteration 35 : 0.5045134235986847
Loss in iteration 36 : 0.5031813971044398
Loss in iteration 37 : 0.5018989110339537
Loss in iteration 38 : 0.5006757386534603
Loss in iteration 39 : 0.4995358671741111
Loss in iteration 40 : 0.49849131296566734
Loss in iteration 41 : 0.4975310688101293
Loss in iteration 42 : 0.49662985596631404
Loss in iteration 43 : 0.49576578659868753
Loss in iteration 44 : 0.4949320733439208
Loss in iteration 45 : 0.4941355934582124
Loss in iteration 46 : 0.49338616832575266
Loss in iteration 47 : 0.49268622853532734
Loss in iteration 48 : 0.4920281782595225
Loss in iteration 49 : 0.49139971808030614
Loss in iteration 50 : 0.490791653573255
Loss in iteration 51 : 0.49020209842286877
Loss in iteration 52 : 0.48963482150678056
Loss in iteration 53 : 0.4890941049426021
Loss in iteration 54 : 0.4885804409233666
Loss in iteration 55 : 0.4880898936479986
Loss in iteration 56 : 0.48761679413389564
Loss in iteration 57 : 0.4871571845372767
Loss in iteration 58 : 0.4867104508352633
Loss in iteration 59 : 0.4862783720903912
Loss in iteration 60 : 0.48586276062300227
Loss in iteration 61 : 0.4854636000986485
Loss in iteration 62 : 0.4850788584947807
Loss in iteration 63 : 0.48470576326723647
Loss in iteration 64 : 0.48434236542817505
Loss in iteration 65 : 0.4839882588628249
Loss in iteration 66 : 0.4836441295503574
Loss in iteration 67 : 0.48331067569814223
Loss in iteration 68 : 0.4829877600924295
Loss in iteration 69 : 0.4826743239164927
Loss in iteration 70 : 0.4823689670102037
Loss in iteration 71 : 0.4820706668370649
Loss in iteration 72 : 0.481779120807974
Loss in iteration 73 : 0.48149455449285355
Loss in iteration 74 : 0.48121722994971655
Loss in iteration 75 : 0.48094704354828127
Loss in iteration 76 : 0.4806834648577373
Loss in iteration 77 : 0.4804257889362363
Loss in iteration 78 : 0.4801734694714032
Loss in iteration 79 : 0.4799262921837498
Loss in iteration 80 : 0.47968430268418694
Loss in iteration 81 : 0.479447584342553
Loss in iteration 82 : 0.4792160630783826
Loss in iteration 83 : 0.47898946306799467
Loss in iteration 84 : 0.47876741239221415
Loss in iteration 85 : 0.47854959824936183
Loss in iteration 86 : 0.47833585842962983
Loss in iteration 87 : 0.4781261605289941
Loss in iteration 88 : 0.47792050441025413
Loss in iteration 89 : 0.4777188272581821
Loss in iteration 90 : 0.4775209732568076
Loss in iteration 91 : 0.47732673503584305
Loss in iteration 92 : 0.4771359252626399
Loss in iteration 93 : 0.47694842504934987
Loss in iteration 94 : 0.4767641815804593
Loss in iteration 95 : 0.4765831661917653
Loss in iteration 96 : 0.47640532792255597
Loss in iteration 97 : 0.47623057366549154
Loss in iteration 98 : 0.4760587824946966
Loss in iteration 99 : 0.47588983788627814
Loss in iteration 100 : 0.4757236530472288
Loss in iteration 101 : 0.47556017395497563
Loss in iteration 102 : 0.47539936231089874
Loss in iteration 103 : 0.4752411733138024
Loss in iteration 104 : 0.4750855436433984
Loss in iteration 105 : 0.47493239539988474
Loss in iteration 106 : 0.47478165024688085
Loss in iteration 107 : 0.4746332424993802
Loss in iteration 108 : 0.4744871227890706
Loss in iteration 109 : 0.4743432516989641
Loss in iteration 110 : 0.47420158939315216
Loss in iteration 111 : 0.4740620886928313
Loss in iteration 112 : 0.4739246953657457
Loss in iteration 113 : 0.4737893539927974
Loss in iteration 114 : 0.47365601448764505
Loss in iteration 115 : 0.4735246348838847
Loss in iteration 116 : 0.47339517927126057
Loss in iteration 117 : 0.473267613113262
Loss in iteration 118 : 0.47314189943502516
Loss in iteration 119 : 0.47301799813244055
Loss in iteration 120 : 0.4728958682064386
Loss in iteration 121 : 0.4727754708825607
Loss in iteration 122 : 0.47265677140380546
Loss in iteration 123 : 0.47253973856857584
Loss in iteration 124 : 0.472424342715938
Loss in iteration 125 : 0.47231055372254255
Loss in iteration 126 : 0.47219834026621615
Loss in iteration 127 : 0.47208767055739864
Loss in iteration 128 : 0.4719785137620181
Loss in iteration 129 : 0.47187084104925253
Loss in iteration 130 : 0.4717646256463809
Loss in iteration 131 : 0.4716598420394222
Loss in iteration 132 : 0.4715564649787484
Loss in iteration 133 : 0.4714544689468312
Loss in iteration 134 : 0.47135382832375944
Loss in iteration 135 : 0.471254517995887
Loss in iteration 136 : 0.4711565139200756
Loss in iteration 137 : 0.47105979327893305
Loss in iteration 138 : 0.47096433419377265
Loss in iteration 139 : 0.4708701152502489
Loss in iteration 140 : 0.4707771151618111
Loss in iteration 141 : 0.4706853127460128
Loss in iteration 142 : 0.47059468715652747
Loss in iteration 143 : 0.47050521816299323
Loss in iteration 144 : 0.47041688628125505
Loss in iteration 145 : 0.4703296726903457
Loss in iteration 146 : 0.47024355902071036
Loss in iteration 147 : 0.4701585271640598
Loss in iteration 148 : 0.4700745592129649
Loss in iteration 149 : 0.46999163753573125
Loss in iteration 150 : 0.46990974490574344
Loss in iteration 151 : 0.46982886458591905
Loss in iteration 152 : 0.46974898031670737
Loss in iteration 153 : 0.4696700762272031
Loss in iteration 154 : 0.4695921367339036
Loss in iteration 155 : 0.4695151464872088
Loss in iteration 156 : 0.46943909038423576
Loss in iteration 157 : 0.46936395362133837
Loss in iteration 158 : 0.46928972174006856
Loss in iteration 159 : 0.46921638063358556
Loss in iteration 160 : 0.46914391651204473
Loss in iteration 161 : 0.469072315851799
Loss in iteration 162 : 0.4690015653588519
Loss in iteration 163 : 0.468931651962332
Loss in iteration 164 : 0.46886256283200906
Loss in iteration 165 : 0.4687942854000297
Loss in iteration 166 : 0.46872680736836875
Loss in iteration 167 : 0.46866011669610486
Loss in iteration 168 : 0.46859420157455406
Loss in iteration 169 : 0.4685290504044649
Loss in iteration 170 : 0.4684646517855676
Loss in iteration 171 : 0.4684009945191623
Loss in iteration 172 : 0.46833806761625035
Loss in iteration 173 : 0.46827586030182755
Loss in iteration 174 : 0.4682143620102694
Loss in iteration 175 : 0.46815356237342015
Loss in iteration 176 : 0.46809345120736484
Loss in iteration 177 : 0.46803401850362436
Loss in iteration 178 : 0.4679752544267381
Loss in iteration 179 : 0.4679171493159253
Loss in iteration 180 : 0.4678596936864538
Loss in iteration 181 : 0.46780287822746314
Loss in iteration 182 : 0.4677466937958726
Loss in iteration 183 : 0.46769113140857704
Loss in iteration 184 : 0.46763618223584236
Loss in iteration 185 : 0.46758183759756106
Loss in iteration 186 : 0.4675280889619965
Loss in iteration 187 : 0.4674749279452183
Loss in iteration 188 : 0.46742234630942553
Loss in iteration 189 : 0.4673703359594491
Loss in iteration 190 : 0.46731888893805423
Loss in iteration 191 : 0.4672679974213539
Loss in iteration 192 : 0.4672176537153765
Loss in iteration 193 : 0.46716785025395835
Loss in iteration 194 : 0.46711857959733183
Loss in iteration 195 : 0.46706983443050215
Loss in iteration 196 : 0.46702160756085537
Loss in iteration 197 : 0.4669738919150707
Loss in iteration 198 : 0.46692668053584363
Loss in iteration 199 : 0.46687996657901165
Loss in iteration 200 : 0.46683374331132077
Testing accuracy  of updater 1 on alg 0 with rate 0.09999999999999998 = 0.786, training accuracy 0.78875, time elapsed: 6905 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 4.016337962504759
Loss in iteration 3 : 42.50079140495107
Loss in iteration 4 : 1.3409879028154394
Loss in iteration 5 : 20.770324756889686
Loss in iteration 6 : 4.66130341189984
Loss in iteration 7 : 19.463252964053837
Loss in iteration 8 : 7.370614807709536
Loss in iteration 9 : 12.116460569775276
Loss in iteration 10 : 11.86053235558203
Loss in iteration 11 : 7.767552345407414
Loss in iteration 12 : 7.85268857673279
Loss in iteration 13 : 6.041902174972178
Loss in iteration 14 : 5.54703072320269
Loss in iteration 15 : 4.615297343999989
Loss in iteration 16 : 4.25866468110403
Loss in iteration 17 : 3.914750015946776
Loss in iteration 18 : 3.7499132410104092
Loss in iteration 19 : 3.571410610357795
Loss in iteration 20 : 3.4669286724386
Loss in iteration 21 : 3.367376312717454
Loss in iteration 22 : 3.272201623119568
Loss in iteration 23 : 3.179272049542842
Loss in iteration 24 : 3.092162923041213
Loss in iteration 25 : 3.010242823125017
Loss in iteration 26 : 2.970034283928976
Loss in iteration 27 : 2.9863717026568803
Loss in iteration 28 : 3.3054091620549606
Loss in iteration 29 : 3.9039537556998303
Loss in iteration 30 : 6.57219829640485
Loss in iteration 31 : 6.70718524486107
Loss in iteration 32 : 9.984871282708285
Loss in iteration 33 : 6.433584676633962
Loss in iteration 34 : 8.86984527920753
Loss in iteration 35 : 7.052267480835793
Loss in iteration 36 : 8.294011505600102
Loss in iteration 37 : 6.558499089793518
Loss in iteration 38 : 7.21584931567216
Loss in iteration 39 : 6.035675146856265
Loss in iteration 40 : 6.29718446743519
Loss in iteration 41 : 5.467627252363823
Loss in iteration 42 : 5.526578377162185
Loss in iteration 43 : 4.954389049467674
Loss in iteration 44 : 5.0677583364593435
Loss in iteration 45 : 4.682253840805406
Loss in iteration 46 : 4.939047091029668
Loss in iteration 47 : 4.646012768924973
Loss in iteration 48 : 5.085264360809141
Loss in iteration 49 : 4.738782098156343
Loss in iteration 50 : 5.407379719812936
Loss in iteration 51 : 5.018141541691366
Loss in iteration 52 : 5.928644770738485
Loss in iteration 53 : 5.338793878847568
Loss in iteration 54 : 6.349737808106549
Loss in iteration 55 : 5.4962269677751125
Loss in iteration 56 : 6.447996614487123
Loss in iteration 57 : 5.48534042620383
Loss in iteration 58 : 6.306572941388077
Loss in iteration 59 : 5.395761267867278
Loss in iteration 60 : 6.062786311165834
Loss in iteration 61 : 5.238397992947777
Loss in iteration 62 : 5.778797292674792
Loss in iteration 63 : 5.07128296876521
Loss in iteration 64 : 5.545409152424116
Loss in iteration 65 : 4.959385816909765
Loss in iteration 66 : 5.4262550872800155
Loss in iteration 67 : 4.912105062255779
Loss in iteration 68 : 5.4165119031885425
Loss in iteration 69 : 4.9522651038874805
Loss in iteration 70 : 5.520521279082596
Loss in iteration 71 : 5.062832642475021
Loss in iteration 72 : 5.692361505089938
Loss in iteration 73 : 5.185590262251543
Loss in iteration 74 : 5.852072140972362
Loss in iteration 75 : 5.288263573172421
Loss in iteration 76 : 5.961474864878854
Loss in iteration 77 : 5.339965595904992
Loss in iteration 78 : 5.9909584303985275
Loss in iteration 79 : 5.338429377458412
Loss in iteration 80 : 5.950818355269696
Loss in iteration 81 : 5.299516396550885
Loss in iteration 82 : 5.872041352041502
Loss in iteration 83 : 5.237653662372388
Loss in iteration 84 : 5.781050137386949
Loss in iteration 85 : 5.172940974660576
Loss in iteration 86 : 5.704260510611099
Loss in iteration 87 : 5.124091582887633
Loss in iteration 88 : 5.660332987327756
Loss in iteration 89 : 5.101689436925232
Loss in iteration 90 : 5.65517087842501
Loss in iteration 91 : 5.107646545500022
Loss in iteration 92 : 5.683206390354495
Loss in iteration 93 : 5.134746721547803
Loss in iteration 94 : 5.72923028727621
Loss in iteration 95 : 5.17037600877729
Loss in iteration 96 : 5.774902296910438
Loss in iteration 97 : 5.2018161832087895
Loss in iteration 98 : 5.805640310368646
Loss in iteration 99 : 5.220306886645047
Loss in iteration 100 : 5.814596198310328
Loss in iteration 101 : 5.223352214987691
Loss in iteration 102 : 5.803627781259623
Loss in iteration 103 : 5.213918236929671
Loss in iteration 104 : 5.780552908465223
Loss in iteration 105 : 5.198001573171765
Loss in iteration 106 : 5.755067021657686
Loss in iteration 107 : 5.182292221210544
Loss in iteration 108 : 5.735534214301134
Loss in iteration 109 : 5.172111216796611
Loss in iteration 110 : 5.72684019025848
Loss in iteration 111 : 5.169921712544934
Loss in iteration 112 : 5.729437629601537
Loss in iteration 113 : 5.17495236245641
Loss in iteration 114 : 5.73989115740483
Loss in iteration 115 : 5.184046703963547
Loss in iteration 116 : 5.752720994293109
Loss in iteration 117 : 5.193230886740732
Loss in iteration 118 : 5.7626796411674475
Loss in iteration 119 : 5.19925026843191
Loss in iteration 120 : 5.766498701615394
Loss in iteration 121 : 5.200536026720306
Loss in iteration 122 : 5.76359497298615
Loss in iteration 123 : 5.197394036559055
Loss in iteration 124 : 5.755720952436029
Loss in iteration 125 : 5.191537706508959
Loss in iteration 126 : 5.745924054704949
Loss in iteration 127 : 5.185275156239422
Loss in iteration 128 : 5.73731894604229
Loss in iteration 129 : 5.180663516218631
Loss in iteration 130 : 5.732079772616398
Loss in iteration 131 : 5.178879851929238
Loss in iteration 132 : 5.73090097980704
Loss in iteration 133 : 5.17997533013841
Loss in iteration 134 : 5.73302280575707
Loss in iteration 135 : 5.183049742345262
Loss in iteration 136 : 5.7367413531943585
Loss in iteration 137 : 5.186724925468085
Loss in iteration 138 : 5.740154888714747
Loss in iteration 139 : 5.1896939959889075
Loss in iteration 140 : 5.741837736230935
Loss in iteration 141 : 5.191137001460577
Loss in iteration 142 : 5.741217917027787
Loss in iteration 143 : 5.190889838405376
Loss in iteration 144 : 5.7385920098381
Loss in iteration 145 : 5.189364229716556
Loss in iteration 146 : 5.734850848351233
Loss in iteration 147 : 5.187297503621612
Loss in iteration 148 : 5.731067435221904
Loss in iteration 149 : 5.185446323718665
Loss in iteration 150 : 5.728107930244578
Loss in iteration 151 : 5.184332927323395
Loss in iteration 152 : 5.72638598814597
Loss in iteration 153 : 5.1841174293745516
Loss in iteration 154 : 5.725814129548099
Loss in iteration 155 : 5.184616928080295
Loss in iteration 156 : 5.72593322555253
Loss in iteration 157 : 5.185438091675514
Loss in iteration 158 : 5.726143568078367
Loss in iteration 159 : 5.186155374718533
Loss in iteration 160 : 5.725938451771893
Loss in iteration 161 : 5.186464023507948
Loss in iteration 162 : 5.725059110130962
Loss in iteration 163 : 5.186260724308005
Loss in iteration 164 : 5.72353395321867
Loss in iteration 165 : 5.185639554540411
Loss in iteration 166 : 5.721612522629406
Loss in iteration 167 : 5.184821738896665
Loss in iteration 168 : 5.71963771627554
Loss in iteration 169 : 5.184055766498351
Loss in iteration 170 : 5.717911336891457
Loss in iteration 171 : 5.183527153362016
Loss in iteration 172 : 5.716599359584451
Loss in iteration 173 : 5.183306650815453
Loss in iteration 174 : 5.715701316469725
Loss in iteration 175 : 5.1833474468360095
Loss in iteration 176 : 5.71508212048701
Loss in iteration 177 : 5.183523160071854
Loss in iteration 178 : 5.714543692176245
Loss in iteration 179 : 5.183685849536636
Loss in iteration 180 : 5.713904301285664
Loss in iteration 181 : 5.1837203116402195
Loss in iteration 182 : 5.713056992119222
Loss in iteration 183 : 5.1835768323527756
Loss in iteration 184 : 5.71199121413033
Loss in iteration 185 : 5.1832754186122045
Loss in iteration 186 : 5.71077748082891
Loss in iteration 187 : 5.182885577876502
Loss in iteration 188 : 5.709527485943961
Loss in iteration 189 : 5.182493200156761
Loss in iteration 190 : 5.708347985452494
Loss in iteration 191 : 5.18216825081592
Loss in iteration 192 : 5.707305313307717
Loss in iteration 193 : 5.181944109194633
Loss in iteration 194 : 5.706410558024253
Loss in iteration 195 : 5.1818133802887285
Loss in iteration 196 : 5.705626496079909
Loss in iteration 197 : 5.181738470652628
Loss in iteration 198 : 5.7048897486661545
Loss in iteration 199 : 5.181670485234117
Loss in iteration 200 : 5.704137653270405
Testing accuracy  of updater 2 on alg 0 with rate 10.0 = 0.76, training accuracy 0.76575, time elapsed: 8340 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.7390243244034621
Loss in iteration 3 : 1.5461235979835177
Loss in iteration 4 : 2.4961351716319102
Loss in iteration 5 : 0.5491945676139456
Loss in iteration 6 : 0.5116117840937398
Loss in iteration 7 : 0.5594905327553571
Loss in iteration 8 : 0.6994699929099826
Loss in iteration 9 : 0.9559049119020601
Loss in iteration 10 : 1.0668534021602682
Loss in iteration 11 : 0.8088249835379879
Loss in iteration 12 : 0.7371409817547595
Loss in iteration 13 : 0.675282572682014
Loss in iteration 14 : 0.610996712234939
Loss in iteration 15 : 0.5707663607584039
Loss in iteration 16 : 0.5430751210357778
Loss in iteration 17 : 0.5282697556481832
Loss in iteration 18 : 0.5200434020403024
Loss in iteration 19 : 0.5156113921810063
Loss in iteration 20 : 0.5126013245511579
Loss in iteration 21 : 0.5099866661947241
Loss in iteration 22 : 0.5073499834642357
Loss in iteration 23 : 0.5045668294337194
Loss in iteration 24 : 0.5016376127338615
Loss in iteration 25 : 0.49858864385039225
Loss in iteration 26 : 0.4954598727295735
Loss in iteration 27 : 0.49229130442770375
Loss in iteration 28 : 0.48912664710240494
Loss in iteration 29 : 0.48601204513431473
Loss in iteration 30 : 0.48299578595997134
Loss in iteration 31 : 0.4801269729000892
Loss in iteration 32 : 0.47745189603337956
Loss in iteration 33 : 0.475011907535279
Loss in iteration 34 : 0.472838682001957
Loss in iteration 35 : 0.47095306300443773
Loss in iteration 36 : 0.4693616819607526
Loss in iteration 37 : 0.4680587703535148
Loss in iteration 38 : 0.46702534225380454
Loss in iteration 39 : 0.46623403452433115
Loss in iteration 40 : 0.46564976296017485
Loss in iteration 41 : 0.4652357901891551
Loss in iteration 42 : 0.46495388110142655
Loss in iteration 43 : 0.46476986411858223
Loss in iteration 44 : 0.46465230234460037
Loss in iteration 45 : 0.4645770410084975
Loss in iteration 46 : 0.46452506766782675
Loss in iteration 47 : 0.46448695878670276
Loss in iteration 48 : 0.4644632103405308
Loss in iteration 49 : 0.4644725025335738
Loss in iteration 50 : 0.46456707268111697
Loss in iteration 51 : 0.4648597833013267
Loss in iteration 52 : 0.4656143936784971
Loss in iteration 53 : 0.46733371200597107
Loss in iteration 54 : 0.4712436551832103
Loss in iteration 55 : 0.4793062361198344
Loss in iteration 56 : 0.49675955085060375
Loss in iteration 57 : 0.5276572085583539
Loss in iteration 58 : 0.5867717004372273
Loss in iteration 59 : 0.6501645603029621
Loss in iteration 60 : 0.7342256375697052
Loss in iteration 61 : 0.7343765515332482
Loss in iteration 62 : 0.7532407557681483
Loss in iteration 63 : 0.7028029576798271
Loss in iteration 64 : 0.6887390378257128
Loss in iteration 65 : 0.644854763988331
Loss in iteration 66 : 0.6232080658502049
Loss in iteration 67 : 0.5904625848614612
Loss in iteration 68 : 0.5706431804273859
Loss in iteration 69 : 0.5487491591055004
Loss in iteration 70 : 0.5342875686250814
Loss in iteration 71 : 0.520788844943786
Loss in iteration 72 : 0.5114236513252701
Loss in iteration 73 : 0.5034977485287475
Loss in iteration 74 : 0.4978302100906686
Loss in iteration 75 : 0.493309181293852
Loss in iteration 76 : 0.49006088359657674
Loss in iteration 77 : 0.48761046607934794
Loss in iteration 78 : 0.4859550842759784
Loss in iteration 79 : 0.4848843058048335
Loss in iteration 80 : 0.4844509562733055
Loss in iteration 81 : 0.4845640855489104
Loss in iteration 82 : 0.48543983651905753
Loss in iteration 83 : 0.48701310841334694
Loss in iteration 84 : 0.4898228114652723
Loss in iteration 85 : 0.4936494443936033
Loss in iteration 86 : 0.4996906380670059
Loss in iteration 87 : 0.507012768026642
Loss in iteration 88 : 0.5181619701814412
Loss in iteration 89 : 0.5298206251667289
Loss in iteration 90 : 0.5471955130556742
Loss in iteration 91 : 0.5610181468900953
Loss in iteration 92 : 0.5817627399628178
Loss in iteration 93 : 0.5907293836365581
Loss in iteration 94 : 0.6077158794506001
Loss in iteration 95 : 0.6058864261498568
Loss in iteration 96 : 0.6144658445124883
Loss in iteration 97 : 0.6036152609291939
Loss in iteration 98 : 0.6047515874397632
Loss in iteration 99 : 0.5902758384529581
Loss in iteration 100 : 0.5870478779923064
Loss in iteration 101 : 0.5729236593284825
Loss in iteration 102 : 0.5680283855262988
Loss in iteration 103 : 0.5561369668952486
Loss in iteration 104 : 0.5513180791399419
Loss in iteration 105 : 0.5421774999590803
Loss in iteration 106 : 0.5383207017674082
Loss in iteration 107 : 0.5317723690488648
Loss in iteration 108 : 0.5292074222002876
Loss in iteration 109 : 0.5248546649662671
Loss in iteration 110 : 0.5236360997793359
Loss in iteration 111 : 0.5210702032117847
Loss in iteration 112 : 0.5211615740630429
Loss in iteration 113 : 0.5200460037383723
Loss in iteration 114 : 0.5214111989372875
Loss in iteration 115 : 0.5214784234271764
Loss in iteration 116 : 0.5240944402432194
Loss in iteration 117 : 0.5250849053694732
Loss in iteration 118 : 0.5288892472649878
Loss in iteration 119 : 0.5304600845800219
Loss in iteration 120 : 0.5352607739545692
Loss in iteration 121 : 0.536917425599599
Loss in iteration 122 : 0.542325568645022
Loss in iteration 123 : 0.5434522095300521
Loss in iteration 124 : 0.5489127972814024
Loss in iteration 125 : 0.5489367505723294
Loss in iteration 126 : 0.5538798594432529
Loss in iteration 127 : 0.5524871311125612
Loss in iteration 128 : 0.5565153003175074
Loss in iteration 129 : 0.5537592103360635
Loss in iteration 130 : 0.5567472573779487
Loss in iteration 131 : 0.5529779570931731
Loss in iteration 132 : 0.5550433881505967
Loss in iteration 133 : 0.5507349223779578
Loss in iteration 134 : 0.5521341778881497
Loss in iteration 135 : 0.5477278078970423
Loss in iteration 136 : 0.5487517680788444
Loss in iteration 137 : 0.5445727051225749
Loss in iteration 138 : 0.5454787817701771
Loss in iteration 139 : 0.5417192551610706
Loss in iteration 140 : 0.5427023104715769
Loss in iteration 141 : 0.5394410873520711
Loss in iteration 142 : 0.5406302636244865
Loss in iteration 143 : 0.5378634419172525
Loss in iteration 144 : 0.5393310438452817
Loss in iteration 145 : 0.5370005175716661
Loss in iteration 146 : 0.5387736266581349
Loss in iteration 147 : 0.536788773939474
Loss in iteration 148 : 0.5388593335596684
Loss in iteration 149 : 0.5371126061845042
Loss in iteration 150 : 0.5394454230115086
Loss in iteration 151 : 0.5378244147988016
Loss in iteration 152 : 0.5403643833126597
Loss in iteration 153 : 0.538762606384857
Loss in iteration 154 : 0.5414425665288536
Loss in iteration 155 : 0.5397695478942952
Loss in iteration 156 : 0.5425191005041362
Loss in iteration 157 : 0.5407086869285902
Loss in iteration 158 : 0.5434629638492091
Loss in iteration 159 : 0.5414779740800127
Loss in iteration 160 : 0.5441846916505738
Loss in iteration 161 : 0.5420166965622802
Loss in iteration 162 : 0.5446401626017878
Loss in iteration 163 : 0.5423046921191843
Loss in iteration 164 : 0.5448264471891082
Loss in iteration 165 : 0.5423553161723107
Loss in iteration 166 : 0.5447720983530314
Loss in iteration 167 : 0.5422050798275323
Loss in iteration 168 : 0.5445253164105184
Loss in iteration 169 : 0.5419030061932965
Loss in iteration 170 : 0.5441429802903931
Loss in iteration 171 : 0.5415018219356615
Loss in iteration 172 : 0.5436822650822866
Loss in iteration 173 : 0.5410518401674548
Loss in iteration 174 : 0.5431952499200702
Loss in iteration 175 : 0.5405973799274417
Loss in iteration 176 : 0.5427260362930688
Loss in iteration 177 : 0.5401750387118366
Loss in iteration 178 : 0.5423095438602066
Loss in iteration 179 : 0.5398130420850857
Loss in iteration 180 : 0.5419711975844589
Loss in iteration 181 : 0.5395310705882503
Loss in iteration 182 : 0.5417269716139497
Loss in iteration 183 : 0.5393402395038387
Loss in iteration 184 : 0.5415835517545067
Loss in iteration 185 : 0.5392431646644728
Loss in iteration 186 : 0.5415386237991852
Loss in iteration 187 : 0.539234228165883
Loss in iteration 188 : 0.5415814473897085
Loss in iteration 189 : 0.5393002422380139
Loss in iteration 190 : 0.5416939241806095
Loss in iteration 191 : 0.5394216987771483
Loss in iteration 192 : 0.541852319989554
Loss in iteration 193 : 0.5395746976090287
Loss in iteration 194 : 0.5420296694586588
Loss in iteration 195 : 0.539733490655167
Loss in iteration 196 : 0.5421987095491325
Loss in iteration 197 : 0.5398733993300593
Loss in iteration 198 : 0.5423350039358433
Loss in iteration 199 : 0.5399737106286098
Loss in iteration 200 : 0.5424197939466825
Testing accuracy  of updater 2 on alg 0 with rate 1.0 = 0.7885, training accuracy 0.787, time elapsed: 5815 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6839617669630189
Loss in iteration 3 : 0.6760909594261063
Loss in iteration 4 : 0.6680109399659574
Loss in iteration 5 : 0.6586683759720918
Loss in iteration 6 : 0.648455629303341
Loss in iteration 7 : 0.6381864457992905
Loss in iteration 8 : 0.6282892130227828
Loss in iteration 9 : 0.6187991309855126
Loss in iteration 10 : 0.6096646851465796
Loss in iteration 11 : 0.6009073878930333
Loss in iteration 12 : 0.592595169854483
Loss in iteration 13 : 0.5847773046070234
Loss in iteration 14 : 0.5774643669672369
Loss in iteration 15 : 0.5706436570107121
Loss in iteration 16 : 0.5642971258998936
Loss in iteration 17 : 0.558407653411892
Loss in iteration 18 : 0.5529573676419243
Loss in iteration 19 : 0.5479252529308146
Loss in iteration 20 : 0.5432870845320393
Loss in iteration 21 : 0.5390169075937251
Loss in iteration 22 : 0.535088474215232
Loss in iteration 23 : 0.531475973426667
Loss in iteration 24 : 0.5281542653509884
Loss in iteration 25 : 0.525099026057153
Loss in iteration 26 : 0.5222869855558229
Loss in iteration 27 : 0.5196962120922348
Loss in iteration 28 : 0.5173063329482654
Loss in iteration 29 : 0.5150986366144199
Loss in iteration 30 : 0.5130560697574374
Loss in iteration 31 : 0.5111631709570728
Loss in iteration 32 : 0.5094059755886396
Loss in iteration 33 : 0.5077719080048295
Loss in iteration 34 : 0.5062496658524187
Loss in iteration 35 : 0.5048290994581305
Loss in iteration 36 : 0.5035010913054335
Loss in iteration 37 : 0.50225744163968
Loss in iteration 38 : 0.5010907647946262
Loss in iteration 39 : 0.49999439816994223
Loss in iteration 40 : 0.4989623235302444
Loss in iteration 41 : 0.4979890991577862
Loss in iteration 42 : 0.4970698011760974
Loss in iteration 43 : 0.49619997254435394
Loss in iteration 44 : 0.49537557842647906
Loss in iteration 45 : 0.49459296676561576
Loss in iteration 46 : 0.4938488330045416
Loss in iteration 47 : 0.49314018805462767
Loss in iteration 48 : 0.4924643288362188
Loss in iteration 49 : 0.49181881094794494
Loss in iteration 50 : 0.49120142322182203
Loss in iteration 51 : 0.4906101640593382
Loss in iteration 52 : 0.4900432195222154
Loss in iteration 53 : 0.4894989431857199
Loss in iteration 54 : 0.4889758377682842
Loss in iteration 55 : 0.4884725385396592
Loss in iteration 56 : 0.48798779848663765
Loss in iteration 57 : 0.4875204751850635
Loss in iteration 58 : 0.48706951929384673
Loss in iteration 59 : 0.48663396455599195
Loss in iteration 60 : 0.4862129191672906
Loss in iteration 61 : 0.48580555835774303
Loss in iteration 62 : 0.48541111802454573
Loss in iteration 63 : 0.4850288892576975
Loss in iteration 64 : 0.48465821360840283
Loss in iteration 65 : 0.4842984789647091
Loss in iteration 66 : 0.4839491159162071
Loss in iteration 67 : 0.4836095945087897
Loss in iteration 68 : 0.4832794213096364
Loss in iteration 69 : 0.4829581367207961
Loss in iteration 70 : 0.482645312495888
Loss in iteration 71 : 0.4823405494280752
Loss in iteration 72 : 0.48204347518826524
Loss in iteration 73 : 0.4817537423005453
Loss in iteration 74 : 0.48147102624731974
Loss in iteration 75 : 0.48119502369999373
Loss in iteration 76 : 0.48092545087257665
Loss in iteration 77 : 0.48066204199591794
Loss in iteration 78 : 0.48040454790971737
Loss in iteration 79 : 0.48015273476841874
Loss in iteration 80 : 0.4799063828558867
Loss in iteration 81 : 0.47966528550263443
Loss in iteration 82 : 0.4794292480984766
Loss in iteration 83 : 0.4791980871928534
Loss in iteration 84 : 0.4789716296748501
Loss in iteration 85 : 0.4787497120249932
Loss in iteration 86 : 0.47853217963127553
Loss in iteration 87 : 0.47831888616241136
Loss in iteration 88 : 0.4781096929920475
Loss in iteration 89 : 0.47790446866844327
Loss in iteration 90 : 0.47770308842489423
Loss in iteration 91 : 0.47750543372695575
Loss in iteration 92 : 0.4773113918531648
Loss in iteration 93 : 0.4771208555065129
Loss in iteration 94 : 0.4769337224544179
Loss in iteration 95 : 0.476749895195237
Loss in iteration 96 : 0.4765692806496447
Loss in iteration 97 : 0.4763917898753294
Loss in iteration 98 : 0.47621733780359216
Loss in iteration 99 : 0.47604584299644226
Loss in iteration 100 : 0.47587722742283095
Loss in iteration 101 : 0.47571141625266966
Loss in iteration 102 : 0.4755483376672639
Loss in iteration 103 : 0.47538792268484176
Loss in iteration 104 : 0.4752301049998616
Loss in iteration 105 : 0.4750748208348438
Loss in iteration 106 : 0.4749220088035239
Loss in iteration 107 : 0.4747716097842175
Loss in iteration 108 : 0.474623566802357
Loss in iteration 109 : 0.4744778249212793
Loss in iteration 110 : 0.47433433114042034
Loss in iteration 111 : 0.47419303430018644
Loss in iteration 112 : 0.4740538849928648
Loss in iteration 113 : 0.47391683547900904
Loss in iteration 114 : 0.47378183960883385
Loss in iteration 115 : 0.4736488527482115
Loss in iteration 116 : 0.47351783170890394
Loss in iteration 117 : 0.47338873468274856
Loss in iteration 118 : 0.47326152117952547
Loss in iteration 119 : 0.4731361519682653
Loss in iteration 120 : 0.47301258902179516
Loss in iteration 121 : 0.4728907954643343
Loss in iteration 122 : 0.47277073552195026
Loss in iteration 123 : 0.4726523744757161
Loss in iteration 124 : 0.47253567861740553
Loss in iteration 125 : 0.4724206152075795
Loss in iteration 126 : 0.47230715243591864
Loss in iteration 127 : 0.4721952593836591
Loss in iteration 128 : 0.4720849059880087
Loss in iteration 129 : 0.4719760630084187
Loss in iteration 130 : 0.47186870199459174
Loss in iteration 131 : 0.47176279525612486
Loss in iteration 132 : 0.4716583158336714
Loss in iteration 133 : 0.4715552374715489
Loss in iteration 134 : 0.47145353459169065
Loss in iteration 135 : 0.4713531822688597
Loss in iteration 136 : 0.47125415620706457
Loss in iteration 137 : 0.4711564327170921
Loss in iteration 138 : 0.47105998869509863
Loss in iteration 139 : 0.4709648016022099
Loss in iteration 140 : 0.47087084944505886
Loss in iteration 141 : 0.4707781107572179
Loss in iteration 142 : 0.4706865645814765
Loss in iteration 143 : 0.47059619045292
Loss in iteration 144 : 0.4705069683827588
Loss in iteration 145 : 0.4704188788428769
Loss in iteration 146 : 0.47033190275105596
Loss in iteration 147 : 0.47024602145683814
Loss in iteration 148 : 0.47016121672799693
Loss in iteration 149 : 0.4700774707375769
Loss in iteration 150 : 0.4699947660514783
Loss in iteration 151 : 0.46991308561655754
Loss in iteration 152 : 0.46983241274921056
Loss in iteration 153 : 0.469752731124412
Loss in iteration 154 : 0.4696740247652065
Loss in iteration 155 : 0.46959627803259296
Loss in iteration 156 : 0.4695194756158226
Loss in iteration 157 : 0.46944360252305317
Loss in iteration 158 : 0.46936864407237044
Loss in iteration 159 : 0.4692945858831417
Loss in iteration 160 : 0.4692214138676911
Loss in iteration 161 : 0.4691491142232807
Loss in iteration 162 : 0.4690776734243875
Loss in iteration 163 : 0.4690070782152537
Loss in iteration 164 : 0.4689373156027066
Loss in iteration 165 : 0.46886837284923044
Loss in iteration 166 : 0.4688002374662822
Loss in iteration 167 : 0.4687328972078365
Loss in iteration 168 : 0.4686663400641572
Loss in iteration 169 : 0.4686005542557761
Loss in iteration 170 : 0.46853552822767675
Loss in iteration 171 : 0.46847125064367606
Loss in iteration 172 : 0.4684077103809832
Loss in iteration 173 : 0.46834489652494676
Loss in iteration 174 : 0.46828279836396575
Loss in iteration 175 : 0.4682214053845704
Loss in iteration 176 : 0.4681607072666543
Loss in iteration 177 : 0.46810069387886005
Loss in iteration 178 : 0.46804135527411056
Loss in iteration 179 : 0.46798268168527846
Loss in iteration 180 : 0.46792466352098966
Loss in iteration 181 : 0.46786729136155064
Loss in iteration 182 : 0.46781055595500787
Loss in iteration 183 : 0.46775444821331724
Loss in iteration 184 : 0.4676989592086326
Loss in iteration 185 : 0.46764408016970055
Loss in iteration 186 : 0.46758980247836357
Loss in iteration 187 : 0.4675361176661666
Loss in iteration 188 : 0.4674830174110522
Loss in iteration 189 : 0.4674304935341634
Loss in iteration 190 : 0.4673785379967243
Loss in iteration 191 : 0.46732714289701954
Loss in iteration 192 : 0.46727630046745205
Loss in iteration 193 : 0.46722600307167944
Loss in iteration 194 : 0.4671762432018374
Loss in iteration 195 : 0.4671270134758314
Loss in iteration 196 : 0.4670783066347093
Loss in iteration 197 : 0.4670301155400922
Loss in iteration 198 : 0.46698243317168786
Loss in iteration 199 : 0.4669352526248608
Loss in iteration 200 : 0.46688856710827054
Testing accuracy  of updater 2 on alg 0 with rate 0.09999999999999998 = 0.7865, training accuracy 0.788875, time elapsed: 6696 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 19.998616144858993
Loss in iteration 3 : 27.601506627795978
Loss in iteration 4 : 9.938174287340932
Loss in iteration 5 : 11.106207851814618
Loss in iteration 6 : 7.491720966129539
Loss in iteration 7 : 7.967658525853576
Loss in iteration 8 : 6.552448045373025
Loss in iteration 9 : 5.986946808556067
Loss in iteration 10 : 5.211545294583748
Loss in iteration 11 : 4.834898569203404
Loss in iteration 12 : 4.334936535605955
Loss in iteration 13 : 4.1615275872091235
Loss in iteration 14 : 3.8032519955082567
Loss in iteration 15 : 3.743621517021385
Loss in iteration 16 : 3.390202323257722
Loss in iteration 17 : 3.40404431173709
Loss in iteration 18 : 3.090429751891935
Loss in iteration 19 : 3.140609731094307
Loss in iteration 20 : 2.8707146496490896
Loss in iteration 21 : 2.947224550217715
Loss in iteration 22 : 2.7094191522488216
Loss in iteration 23 : 2.8021991893471783
Loss in iteration 24 : 2.587310760084994
Loss in iteration 25 : 2.692087623477606
Loss in iteration 26 : 2.4784760386544185
Loss in iteration 27 : 2.596513381851466
Loss in iteration 28 : 2.3736041589783325
Loss in iteration 29 : 2.5134813251587023
Loss in iteration 30 : 2.264130094005739
Loss in iteration 31 : 2.445983281355484
Loss in iteration 32 : 2.1297988066688633
Loss in iteration 33 : 2.3306158322750115
Loss in iteration 34 : 1.9622470157320782
Loss in iteration 35 : 2.1364128501552484
Loss in iteration 36 : 1.8405640051290493
Loss in iteration 37 : 2.0116993299663553
Loss in iteration 38 : 1.7917621006382862
Loss in iteration 39 : 1.9711821949338229
Loss in iteration 40 : 1.8149027180072208
Loss in iteration 41 : 2.0140150835053743
Loss in iteration 42 : 1.9581126422823563
Loss in iteration 43 : 2.190528991852554
Loss in iteration 44 : 2.1453900501793113
Loss in iteration 45 : 2.363725504207515
Loss in iteration 46 : 2.189652444507532
Loss in iteration 47 : 2.3517752878048923
Loss in iteration 48 : 2.1289134208101514
Loss in iteration 49 : 2.2518145331747754
Loss in iteration 50 : 2.0398683134989923
Loss in iteration 51 : 2.1427676611639637
Loss in iteration 52 : 1.9584896739004156
Loss in iteration 53 : 2.052302436749191
Loss in iteration 54 : 1.8917626852043259
Loss in iteration 55 : 1.982416513344353
Loss in iteration 56 : 1.8385842416364737
Loss in iteration 57 : 1.9295434593989576
Loss in iteration 58 : 1.7960843446987624
Loss in iteration 59 : 1.8894395727252096
Loss in iteration 60 : 1.7614441596642387
Loss in iteration 61 : 1.8584296319917926
Loss in iteration 62 : 1.7323844603838934
Loss in iteration 63 : 1.8336631238548846
Loss in iteration 64 : 1.7072415247454509
Loss in iteration 65 : 1.8130679081787577
Loss in iteration 66 : 1.6849206583655605
Loss in iteration 67 : 1.7952296324009982
Loss in iteration 68 : 1.6647969507205578
Loss in iteration 69 : 1.779248797603643
Loss in iteration 70 : 1.6465920033211507
Loss in iteration 71 : 1.7645989760519256
Loss in iteration 72 : 1.6302509184450547
Loss in iteration 73 : 1.7510049925760762
Loss in iteration 74 : 1.61583246795962
Loss in iteration 75 : 1.7383449686534294
Loss in iteration 76 : 1.6034102789956153
Loss in iteration 77 : 1.7265646393913299
Loss in iteration 78 : 1.5929772935759765
Loss in iteration 79 : 1.7155915758345999
Loss in iteration 80 : 1.584355197831327
Loss in iteration 81 : 1.7052532235872218
Loss in iteration 82 : 1.5771281220666578
Loss in iteration 83 : 1.6952231391563863
Loss in iteration 84 : 1.570634511598506
Loss in iteration 85 : 1.6850289461614139
Loss in iteration 86 : 1.5640494872496842
Loss in iteration 87 : 1.674141630874577
Loss in iteration 88 : 1.5565575378268248
Loss in iteration 89 : 1.6621255916075708
Loss in iteration 90 : 1.547556816846258
Loss in iteration 91 : 1.6487825825502898
Loss in iteration 92 : 1.5367975197862334
Loss in iteration 93 : 1.6342128951751014
Loss in iteration 94 : 1.5243877604506024
Loss in iteration 95 : 1.6187646855427908
Loss in iteration 96 : 1.5106829487452473
Loss in iteration 97 : 1.6029098942643127
Loss in iteration 98 : 1.4961334189717252
Loss in iteration 99 : 1.5871149481843922
Loss in iteration 100 : 1.481159848858829
Loss in iteration 101 : 1.571753929843944
Loss in iteration 102 : 1.466085031634632
Loss in iteration 103 : 1.557074217711419
Loss in iteration 104 : 1.45111636249132
Loss in iteration 105 : 1.5432003349521037
Loss in iteration 106 : 1.4363608897039222
Loss in iteration 107 : 1.5301562880666617
Loss in iteration 108 : 1.4218572071947333
Loss in iteration 109 : 1.5178916936043092
Loss in iteration 110 : 1.4076156231983208
Loss in iteration 111 : 1.506304769796226
Loss in iteration 112 : 1.3936637180203826
Loss in iteration 113 : 1.4952627195056014
Loss in iteration 114 : 1.3800960403127838
Loss in iteration 115 : 1.4846267031479967
Loss in iteration 116 : 1.3671239854731296
Loss in iteration 117 : 1.4742930299384367
Loss in iteration 118 : 1.3551160595737899
Loss in iteration 119 : 1.4642580639800078
Loss in iteration 120 : 1.3446113626979475
Loss in iteration 121 : 1.4546920938237895
Loss in iteration 122 : 1.3362817959456839
Loss in iteration 123 : 1.4459705418862976
Loss in iteration 124 : 1.3308155626802787
Loss in iteration 125 : 1.4385935012233906
Loss in iteration 126 : 1.3287044870035318
Loss in iteration 127 : 1.4329652647545776
Loss in iteration 128 : 1.3299485153141122
Loss in iteration 129 : 1.4290911014788148
Loss in iteration 130 : 1.3337543269590837
Loss in iteration 131 : 1.4263292015409275
Loss in iteration 132 : 1.3384163227432386
Loss in iteration 133 : 1.4233870878705404
Loss in iteration 134 : 1.341651666937491
Loss in iteration 135 : 1.4187178219826948
Loss in iteration 136 : 1.3414493318178782
Loss in iteration 137 : 1.4112140317856972
Loss in iteration 138 : 1.3369296816009624
Loss in iteration 139 : 1.4007288041119665
Loss in iteration 140 : 1.3285230110712327
Loss in iteration 141 : 1.3880109476299833
Loss in iteration 142 : 1.3174375104991363
Loss in iteration 143 : 1.3741994531443198
Loss in iteration 144 : 1.304985660001345
Loss in iteration 145 : 1.3603331814312518
Loss in iteration 146 : 1.2921875790662265
Loss in iteration 147 : 1.3471194499423214
Loss in iteration 148 : 1.2796745591300915
Loss in iteration 149 : 1.3349226940796817
Loss in iteration 150 : 1.2677513318300735
Loss in iteration 151 : 1.3238509995181682
Loss in iteration 152 : 1.2565007371746562
Loss in iteration 153 : 1.3138560541348765
Loss in iteration 154 : 1.245876818239235
Loss in iteration 155 : 1.3048117903722236
Loss in iteration 156 : 1.2357722133142879
Loss in iteration 157 : 1.2965651880209261
Loss in iteration 158 : 1.226062573161218
Loss in iteration 159 : 1.2889635788516123
Loss in iteration 160 : 1.2166354851927235
Loss in iteration 161 : 1.2818654183716736
Loss in iteration 162 : 1.207411307285618
Loss in iteration 163 : 1.2751415261438992
Loss in iteration 164 : 1.1983616232406993
Loss in iteration 165 : 1.2686739680539307
Loss in iteration 166 : 1.189528698174021
Loss in iteration 167 : 1.2623607531622951
Loss in iteration 168 : 1.1810461713980718
Loss in iteration 169 : 1.2561346119231862
Loss in iteration 170 : 1.1731566301864056
Loss in iteration 171 : 1.2499988279767547
Loss in iteration 172 : 1.1662154766085768
Loss in iteration 173 : 1.244068440622031
Loss in iteration 174 : 1.1606645730781386
Loss in iteration 175 : 1.2385859507905899
Loss in iteration 176 : 1.1569587721130077
Loss in iteration 177 : 1.2338748744676031
Loss in iteration 178 : 1.1554398752065471
Loss in iteration 179 : 1.2302190489104958
Loss in iteration 180 : 1.1561770317067195
Loss in iteration 181 : 1.2277025304477462
Loss in iteration 182 : 1.1588228548657848
Loss in iteration 183 : 1.2260823292996061
Loss in iteration 184 : 1.1625589150893847
Loss in iteration 185 : 1.2247697363604841
Loss in iteration 186 : 1.1662062543701923
Loss in iteration 187 : 1.2229633805135904
Loss in iteration 188 : 1.1685243523087514
Loss in iteration 189 : 1.2199089610469742
Loss in iteration 190 : 1.168597444611146
Loss in iteration 191 : 1.2151702577569456
Loss in iteration 192 : 1.1660980079762158
Loss in iteration 193 : 1.2087582480249615
Loss in iteration 194 : 1.1612786037420146
Loss in iteration 195 : 1.2010550696718798
Loss in iteration 196 : 1.1547456066979926
Loss in iteration 197 : 1.1926143362514037
Loss in iteration 198 : 1.1471903738205387
Loss in iteration 199 : 1.1839702222019852
Loss in iteration 200 : 1.1392062604558402
Testing accuracy  of updater 3 on alg 0 with rate 10.0 = 0.743, training accuracy 0.740875, time elapsed: 6359 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6769718176660141
Loss in iteration 3 : 0.6680531143354539
Loss in iteration 4 : 0.6603309659450786
Loss in iteration 5 : 0.6532987849362029
Loss in iteration 6 : 0.6467888919763726
Loss in iteration 7 : 0.6407178333034774
Loss in iteration 8 : 0.635032880988759
Loss in iteration 9 : 0.6296946311021402
Loss in iteration 10 : 0.6246707618785164
Loss in iteration 11 : 0.6199335354954248
Loss in iteration 12 : 0.6154586387639758
Loss in iteration 13 : 0.6112245359274654
Loss in iteration 14 : 0.6072120375649108
Loss in iteration 15 : 0.6034039751398014
Loss in iteration 16 : 0.5997849374355632
Loss in iteration 17 : 0.5963410496814587
Loss in iteration 18 : 0.5930597855120812
Loss in iteration 19 : 0.5899298056743764
Loss in iteration 20 : 0.5869408191044907
Loss in iteration 21 : 0.584083462921577
Loss in iteration 22 : 0.5813491984931445
Loss in iteration 23 : 0.5787302211858996
Loss in iteration 24 : 0.5762193817899478
Loss in iteration 25 : 0.573810117917372
Loss in iteration 26 : 0.5714963939404817
Loss in iteration 27 : 0.5692726482580089
Loss in iteration 28 : 0.5671337468651563
Loss in iteration 29 : 0.5650749423609183
Loss in iteration 30 : 0.5630918376581774
Loss in iteration 31 : 0.5611803537726751
Loss in iteration 32 : 0.5593367011596861
Loss in iteration 33 : 0.5575573541450115
Loss in iteration 34 : 0.5558390280622498
Loss in iteration 35 : 0.5541786587633113
Loss in iteration 36 : 0.5525733842155908
Loss in iteration 37 : 0.5510205279384587
Loss in iteration 38 : 0.5495175840650478
Loss in iteration 39 : 0.5480622038436369
Loss in iteration 40 : 0.5466521834170479
Loss in iteration 41 : 0.5452854527391762
Loss in iteration 42 : 0.5439600655054069
Loss in iteration 43 : 0.5426741899889446
Loss in iteration 44 : 0.5414261006881528
Loss in iteration 45 : 0.5402141707013585
Loss in iteration 46 : 0.5390368647553707
Loss in iteration 47 : 0.5378927328225092
Loss in iteration 48 : 0.5367804042683294
Loss in iteration 49 : 0.535698582478738
Loss in iteration 50 : 0.5346460399208075
Loss in iteration 51 : 0.5336216135966051
Loss in iteration 52 : 0.5326242008536723
Loss in iteration 53 : 0.5316527555196472
Loss in iteration 54 : 0.5307062843318737
Loss in iteration 55 : 0.5297838436358625
Loss in iteration 56 : 0.5288845363290801
Loss in iteration 57 : 0.528007509028894
Loss in iteration 58 : 0.5271519494456047
Loss in iteration 59 : 0.5263170839433017
Loss in iteration 60 : 0.5255021752729725
Loss in iteration 61 : 0.524706520463752
Loss in iteration 62 : 0.5239294488595063
Loss in iteration 63 : 0.5231703202891309
Loss in iteration 64 : 0.5224285233600147
Loss in iteration 65 : 0.5217034738650518
Loss in iteration 66 : 0.5209946132944301
Loss in iteration 67 : 0.5203014074442265
Loss in iteration 68 : 0.5196233451145035
Loss in iteration 69 : 0.518959936890231
Loss in iteration 70 : 0.518310713998938
Loss in iteration 71 : 0.5176752272394918
Loss in iteration 72 : 0.5170530459768786
Loss in iteration 73 : 0.5164437571982798
Loss in iteration 74 : 0.5158469646261015
Loss in iteration 75 : 0.5152622878840064
Loss in iteration 76 : 0.5146893617122439
Loss in iteration 77 : 0.5141278352289421
Loss in iteration 78 : 0.5135773712342201
Loss in iteration 79 : 0.5130376455542595
Loss in iteration 80 : 0.5125083464226785
Loss in iteration 81 : 0.5119891738967546
Loss in iteration 82 : 0.5114798393062183
Loss in iteration 83 : 0.5109800647325276
Loss in iteration 84 : 0.510489582516664
Loss in iteration 85 : 0.5100081347936442
Loss in iteration 86 : 0.5095354730520766
Loss in iteration 87 : 0.5090713577171981
Loss in iteration 88 : 0.5086155577559454
Loss in iteration 89 : 0.5081678503027174
Loss in iteration 90 : 0.5077280203045721
Loss in iteration 91 : 0.5072958601846929
Loss in iteration 92 : 0.506871169523033
Loss in iteration 93 : 0.5064537547531354
Loss in iteration 94 : 0.5060434288741634
Loss in iteration 95 : 0.5056400111772807
Loss in iteration 96 : 0.5052433269855349
Loss in iteration 97 : 0.5048532074064914
Loss in iteration 98 : 0.5044694890968805
Loss in iteration 99 : 0.5040920140385949
Loss in iteration 100 : 0.5037206293254015
Loss in iteration 101 : 0.5033551869597713
Loss in iteration 102 : 0.5029955436592733
Loss in iteration 103 : 0.5026415606720294
Loss in iteration 104 : 0.5022931036007073
Loss in iteration 105 : 0.5019500422346248
Loss in iteration 106 : 0.5016122503895242
Loss in iteration 107 : 0.5012796057546
Loss in iteration 108 : 0.5009519897464239
Loss in iteration 109 : 0.5006292873693844
Loss in iteration 110 : 0.5003113870823236
Loss in iteration 111 : 0.499998180671043
Loss in iteration 112 : 0.4996895631263763
Loss in iteration 113 : 0.49938543252756784
Loss in iteration 114 : 0.4990856899306667
Loss in iteration 115 : 0.4987902392617016
Loss in iteration 116 : 0.49849898721439173
Loss in iteration 117 : 0.4982118431521857
Loss in iteration 118 : 0.4979287190143925
Loss in iteration 119 : 0.49764952922623507
Loss in iteration 120 : 0.4973741906126115
Loss in iteration 121 : 0.49710262231540614
Loss in iteration 122 : 0.49683474571417924
Loss in iteration 123 : 0.4965704843500546
Loss in iteration 124 : 0.4963097638526956
Loss in iteration 125 : 0.4960525118701809
Loss in iteration 126 : 0.49579865800168726
Loss in iteration 127 : 0.4955481337328134
Loss in iteration 128 : 0.49530087237345755
Loss in iteration 129 : 0.495056808998107
Loss in iteration 130 : 0.4948158803884459
Loss in iteration 131 : 0.49457802497817466
Loss in iteration 132 : 0.49434318279993666
Loss in iteration 133 : 0.4941112954342676
Loss in iteration 134 : 0.49388230596047333
Loss in iteration 135 : 0.4936561589093543
Loss in iteration 136 : 0.49343280021769115
Loss in iteration 137 : 0.4932121771844278
Loss in iteration 138 : 0.49299423842846235
Loss in iteration 139 : 0.4927789338479953
Loss in iteration 140 : 0.4925662145813506
Loss in iteration 141 : 0.49235603296922636
Loss in iteration 142 : 0.49214834251829775
Loss in iteration 143 : 0.4919430978661287
Loss in iteration 144 : 0.4917402547473372
Loss in iteration 145 : 0.49153976996094567
Loss in iteration 146 : 0.4913416013388979
Loss in iteration 147 : 0.49114570771566646
Loss in iteration 148 : 0.49095204889892374
Loss in iteration 149 : 0.49076058564122876
Loss in iteration 150 : 0.49057127961268465
Loss in iteration 151 : 0.4903840933745397
Loss in iteration 152 : 0.49019899035368014
Loss in iteration 153 : 0.4900159348179926
Loss in iteration 154 : 0.4898348918525514
Loss in iteration 155 : 0.48965582733660845
Loss in iteration 156 : 0.4894787079213501
Loss in iteration 157 : 0.4893035010083836
Loss in iteration 158 : 0.4891301747289431
Loss in iteration 159 : 0.4889586979237747
Loss in iteration 160 : 0.4887890401236702
Loss in iteration 161 : 0.48862117153064505
Loss in iteration 162 : 0.4884550629997131
Loss in iteration 163 : 0.48829068602125514
Loss in iteration 164 : 0.4881280127039471
Loss in iteration 165 : 0.4879670157582252
Loss in iteration 166 : 0.48780766848028867
Loss in iteration 167 : 0.4876499447365931
Loss in iteration 168 : 0.4874938189488332
Loss in iteration 169 : 0.48733926607939354
Loss in iteration 170 : 0.4871862616172543
Loss in iteration 171 : 0.48703478156432695
Loss in iteration 172 : 0.4868848024222074
Loss in iteration 173 : 0.4867363011793418
Loss in iteration 174 : 0.4865892552985756
Loss in iteration 175 : 0.486443642705083
Loss in iteration 176 : 0.4862994417746561
Loss in iteration 177 : 0.4861566313223498
Loss in iteration 178 : 0.48601519059146125
Loss in iteration 179 : 0.4858750992428397
Loss in iteration 180 : 0.485736337344509
Loss in iteration 181 : 0.485598885361594
Loss in iteration 182 : 0.4854627241465507
Loss in iteration 183 : 0.4853278349296699
Loss in iteration 184 : 0.4851941993098612
Loss in iteration 185 : 0.4850617992457036
Loss in iteration 186 : 0.4849306170467516
Loss in iteration 187 : 0.48480063536509194
Loss in iteration 188 : 0.4846718371871364
Loss in iteration 189 : 0.4845442058256525
Loss in iteration 190 : 0.4844177249120165
Loss in iteration 191 : 0.4842923783886812
Loss in iteration 192 : 0.48416815050186196
Loss in iteration 193 : 0.4840450257944159
Loss in iteration 194 : 0.4839229890989288
Loss in iteration 195 : 0.48380202553098517
Loss in iteration 196 : 0.48368212048262804
Loss in iteration 197 : 0.48356325961599134
Loss in iteration 198 : 0.4834454288571167
Loss in iteration 199 : 0.48332861438992497
Loss in iteration 200 : 0.4832128026503606
Testing accuracy  of updater 3 on alg 0 with rate 1.0 = 0.781, training accuracy 0.781625, time elapsed: 6703 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6909137671598481
Loss in iteration 3 : 0.6889360633507094
Loss in iteration 4 : 0.6871646398128305
Loss in iteration 5 : 0.6855601416515771
Loss in iteration 6 : 0.6840912484645494
Loss in iteration 7 : 0.682733027789172
Loss in iteration 8 : 0.6814656152824146
Loss in iteration 9 : 0.6802731624139592
Loss in iteration 10 : 0.6791430007640903
Loss in iteration 11 : 0.6780649802128051
Loss in iteration 12 : 0.6770309458298795
Loss in iteration 13 : 0.6760343248683911
Loss in iteration 14 : 0.67506980085954
Loss in iteration 15 : 0.6741330564478236
Loss in iteration 16 : 0.6732205703922214
Loss in iteration 17 : 0.6723294572111529
Loss in iteration 18 : 0.6714573403875165
Loss in iteration 19 : 0.6706022519859798
Loss in iteration 20 : 0.6697625530645414
Loss in iteration 21 : 0.6689368704674972
Loss in iteration 22 : 0.6681240465341779
Loss in iteration 23 : 0.6673230990013818
Loss in iteration 24 : 0.6665331889605671
Loss in iteration 25 : 0.6657535951881667
Loss in iteration 26 : 0.664983693525812
Loss in iteration 27 : 0.6642229402684909
Loss in iteration 28 : 0.6634708587391642
Loss in iteration 29 : 0.6627270284016301
Loss in iteration 30 : 0.6619910759994297
Loss in iteration 31 : 0.6612626683155772
Loss in iteration 32 : 0.6605415062320823
Loss in iteration 33 : 0.6598273198345651
Loss in iteration 34 : 0.659119864359526
Loss in iteration 35 : 0.6584189168231209
Loss in iteration 36 : 0.6577242732029499
Loss in iteration 37 : 0.6570357460700988
Loss in iteration 38 : 0.6563531625892104
Loss in iteration 39 : 0.6556763628205142
Loss in iteration 40 : 0.6550051982706868
Loss in iteration 41 : 0.6543395306496292
Loss in iteration 42 : 0.6536792307984506
Loss in iteration 43 : 0.6530241777604756
Loss in iteration 44 : 0.6523742579722845
Loss in iteration 45 : 0.6517293645560366
Loss in iteration 46 : 0.6510893966976435
Loss in iteration 47 : 0.6504542590981015
Loss in iteration 48 : 0.6498238614874603
Loss in iteration 49 : 0.6491981181927134
Loss in iteration 50 : 0.6485769477522749
Loss in iteration 51 : 0.6479602725709702
Loss in iteration 52 : 0.6473480186103459
Loss in iteration 53 : 0.6467401151099331
Loss in iteration 54 : 0.646136494335761
Loss in iteration 55 : 0.6455370913529248
Loss in iteration 56 : 0.6449418438194702
Loss in iteration 57 : 0.6443506917992504
Loss in iteration 58 : 0.6437635775916946
Loss in iteration 59 : 0.6431804455767091
Loss in iteration 60 : 0.6426012420731501
Loss in iteration 61 : 0.6420259152095014
Loss in iteration 62 : 0.6414544148055451
Loss in iteration 63 : 0.6408866922639626
Loss in iteration 64 : 0.6403227004709163
Loss in iteration 65 : 0.6397623937047565
Loss in iteration 66 : 0.6392057275521273
Loss in iteration 67 : 0.6386526588307712
Loss in iteration 68 : 0.6381031455184368
Loss in iteration 69 : 0.6375571466873514
Loss in iteration 70 : 0.637014622443764
Loss in iteration 71 : 0.6364755338721122
Loss in iteration 72 : 0.6359398429834228
Loss in iteration 73 : 0.6354075126675789
Loss in iteration 74 : 0.63487850664912
Loss in iteration 75 : 0.6343527894462957
Loss in iteration 76 : 0.6338303263330739
Loss in iteration 77 : 0.6333110833038915
Loss in iteration 78 : 0.6327950270408891
Loss in iteration 79 : 0.6322821248834533
Loss in iteration 80 : 0.6317723447998594
Loss in iteration 81 : 0.6312656553608624
Loss in iteration 82 : 0.630762025715066
Loss in iteration 83 : 0.6302614255659348
Loss in iteration 84 : 0.629763825150318
Loss in iteration 85 : 0.6292691952183692
Loss in iteration 86 : 0.6287775070147437
Loss in iteration 87 : 0.6282887322609898
Loss in iteration 88 : 0.6278028431390219
Loss in iteration 89 : 0.6273198122756103
Loss in iteration 90 : 0.6268396127278002
Loss in iteration 91 : 0.6263622179691979
Loss in iteration 92 : 0.6258876018770404
Loss in iteration 93 : 0.6254157387200158
Loss in iteration 94 : 0.6249466031467629
Loss in iteration 95 : 0.6244801701749989
Loss in iteration 96 : 0.6240164151812354
Loss in iteration 97 : 0.623555313891041
Loss in iteration 98 : 0.6230968423698033
Loss in iteration 99 : 0.6226409770139684
Loss in iteration 100 : 0.6221876945427012
Loss in iteration 101 : 0.621736971989967
Loss in iteration 102 : 0.6212887866969801
Loss in iteration 103 : 0.6208431163050062
Loss in iteration 104 : 0.6203999387484869
Loss in iteration 105 : 0.6199592322484831
Loss in iteration 106 : 0.6195209753063884
Loss in iteration 107 : 0.6190851466979123
Loss in iteration 108 : 0.6186517254673185
Loss in iteration 109 : 0.6182206909218904
Loss in iteration 110 : 0.6177920226266133
Loss in iteration 111 : 0.6173657003990624
Loss in iteration 112 : 0.6169417043044905
Loss in iteration 113 : 0.6165200146510786
Loss in iteration 114 : 0.6161006119853693
Loss in iteration 115 : 0.6156834770878583
Loss in iteration 116 : 0.6152685909687253
Loss in iteration 117 : 0.6148559348637214
Loss in iteration 118 : 0.6144454902301734
Loss in iteration 119 : 0.6140372387431282
Loss in iteration 120 : 0.6136311622916114
Loss in iteration 121 : 0.613227242974986
Loss in iteration 122 : 0.6128254630994441
Loss in iteration 123 : 0.6124258051745729
Loss in iteration 124 : 0.6120282519100313
Loss in iteration 125 : 0.6116327862123155
Loss in iteration 126 : 0.6112393911816106
Loss in iteration 127 : 0.6108480501087233
Loss in iteration 128 : 0.6104587464720928
Loss in iteration 129 : 0.6100714639348822
Loss in iteration 130 : 0.6096861863421341
Loss in iteration 131 : 0.609302897718002
Loss in iteration 132 : 0.6089215822630406
Loss in iteration 133 : 0.608542224351564
Loss in iteration 134 : 0.6081648085290615
Loss in iteration 135 : 0.6077893195096746
Loss in iteration 136 : 0.607415742173722
Loss in iteration 137 : 0.6070440615652859
Loss in iteration 138 : 0.6066742628898502
Loss in iteration 139 : 0.6063063315119785
Loss in iteration 140 : 0.6059402529530491
Loss in iteration 141 : 0.6055760128890373
Loss in iteration 142 : 0.6052135971483293
Loss in iteration 143 : 0.6048529917095962
Loss in iteration 144 : 0.6044941826996978
Loss in iteration 145 : 0.6041371563916271
Loss in iteration 146 : 0.6037818992025046
Loss in iteration 147 : 0.6034283976915958
Loss in iteration 148 : 0.6030766385583729
Loss in iteration 149 : 0.6027266086406169
Loss in iteration 150 : 0.60237829491254
Loss in iteration 151 : 0.6020316844829592
Loss in iteration 152 : 0.6016867645934861
Loss in iteration 153 : 0.6013435226167603
Loss in iteration 154 : 0.6010019460547084
Loss in iteration 155 : 0.6006620225368334
Loss in iteration 156 : 0.6003237398185358
Loss in iteration 157 : 0.5999870857794593
Loss in iteration 158 : 0.5996520484218666
Loss in iteration 159 : 0.5993186158690451
Loss in iteration 160 : 0.5989867763637367
Loss in iteration 161 : 0.598656518266586
Loss in iteration 162 : 0.5983278300546335
Loss in iteration 163 : 0.5980007003198125
Loss in iteration 164 : 0.5976751177674774
Loss in iteration 165 : 0.597351071214966
Loss in iteration 166 : 0.5970285495901652
Loss in iteration 167 : 0.596707541930124
Loss in iteration 168 : 0.5963880373796607
Loss in iteration 169 : 0.5960700251900146
Loss in iteration 170 : 0.5957534947175073
Loss in iteration 171 : 0.5954384354222306
Loss in iteration 172 : 0.5951248368667498
Loss in iteration 173 : 0.5948126887148266
Loss in iteration 174 : 0.5945019807301726
Loss in iteration 175 : 0.5941927027752067
Loss in iteration 176 : 0.5938848448098375
Loss in iteration 177 : 0.5935783968902698
Loss in iteration 178 : 0.5932733491678244
Loss in iteration 179 : 0.5929696918877683
Loss in iteration 180 : 0.5926674153881799
Loss in iteration 181 : 0.5923665100988134
Loss in iteration 182 : 0.5920669665399889
Loss in iteration 183 : 0.5917687753215022
Loss in iteration 184 : 0.5914719271415413
Loss in iteration 185 : 0.5911764127856247
Loss in iteration 186 : 0.5908822231255585
Loss in iteration 187 : 0.5905893491183982
Loss in iteration 188 : 0.5902977818054406
Loss in iteration 189 : 0.5900075123112088
Loss in iteration 190 : 0.5897185318424822
Loss in iteration 191 : 0.589430831687308
Loss in iteration 192 : 0.5891444032140501
Loss in iteration 193 : 0.5888592378704443
Loss in iteration 194 : 0.5885753271826615
Loss in iteration 195 : 0.5882926627543953
Loss in iteration 196 : 0.5880112362659538
Loss in iteration 197 : 0.5877310394733647
Loss in iteration 198 : 0.5874520642074995
Loss in iteration 199 : 0.5871743023732067
Loss in iteration 200 : 0.5868977459484511
Testing accuracy  of updater 3 on alg 0 with rate 0.09999999999999998 = 0.7585, training accuracy 0.7665, time elapsed: 5725 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6929235752666216
Loss in iteration 3 : 0.6926960665968459
Loss in iteration 4 : 0.692466673002798
Loss in iteration 5 : 0.6922363372674492
Loss in iteration 6 : 0.6920055789324585
Loss in iteration 7 : 0.69177471205339
Loss in iteration 8 : 0.6915439381601188
Loss in iteration 9 : 0.6913133922107215
Loss in iteration 10 : 0.691083167597145
Loss in iteration 11 : 0.6908533306884844
Loss in iteration 12 : 0.6906239297064454
Loss in iteration 13 : 0.690395000341158
Loss in iteration 14 : 0.6901665694061496
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231967
Loss in iteration 17 : 0.6894844480963954
Loss in iteration 18 : 0.6892581720925248
Loss in iteration 19 : 0.6890324583520112
Loss in iteration 20 : 0.6888073118783552
Loss in iteration 21 : 0.6885827361540363
Loss in iteration 22 : 0.6883587333817472
Loss in iteration 23 : 0.6881353046728775
Loss in iteration 24 : 0.6879124501982642
Loss in iteration 25 : 0.6876901693114452
Loss in iteration 26 : 0.6874684606515441
Loss in iteration 27 : 0.6872473222307213
Loss in iteration 28 : 0.6870267515096308
Loss in iteration 29 : 0.6868067454633647
Loss in iteration 30 : 0.6865873006395931
Loss in iteration 31 : 0.6863684132102226
Loss in iteration 32 : 0.6861500790174879
Loss in iteration 33 : 0.6859322936152161
Loss in iteration 34 : 0.6857150523058196
Loss in iteration 35 : 0.6854983501734838
Loss in iteration 36 : 0.6852821821139227
Loss in iteration 37 : 0.6850665428610211
Loss in iteration 38 : 0.6848514270106489
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359665
Loss in iteration 41 : 0.6842091641928563
Loss in iteration 42 : 0.6839960858461507
Loss in iteration 43 : 0.6837835024759527
Loss in iteration 44 : 0.6835714082201896
Loss in iteration 45 : 0.6833597971844099
Loss in iteration 46 : 0.6831486634501976
Loss in iteration 47 : 0.6829380010823383
Loss in iteration 48 : 0.6827278041348106
Loss in iteration 49 : 0.6825180666557314
Loss in iteration 50 : 0.6823087826913213
Loss in iteration 51 : 0.6820999462889611
Loss in iteration 52 : 0.6818915514994244
Loss in iteration 53 : 0.6816835923783227
Loss in iteration 54 : 0.681476062986818
Loss in iteration 55 : 0.681268957391646
Loss in iteration 56 : 0.6810622696644756
Loss in iteration 57 : 0.6808559938806241
Loss in iteration 58 : 0.6806501241171421
Loss in iteration 59 : 0.6804446544502752
Loss in iteration 60 : 0.6802395789522903
Loss in iteration 61 : 0.6800348916876392
Loss in iteration 62 : 0.6798305867084534
Loss in iteration 63 : 0.6796266580492966
Loss in iteration 64 : 0.6794230997211296
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.679017069941194
Loss in iteration 67 : 0.6788145863262015
Loss in iteration 68 : 0.6786124486965575
Loss in iteration 69 : 0.6784106508201545
Loss in iteration 70 : 0.6782091863823767
Loss in iteration 71 : 0.6780080489709516
Loss in iteration 72 : 0.677807232058712
Loss in iteration 73 : 0.6776067289840477
Loss in iteration 74 : 0.6774065329288739
Loss in iteration 75 : 0.6772066368941017
Loss in iteration 76 : 0.677007033672807
Loss in iteration 77 : 0.6768077158217204
Loss in iteration 78 : 0.6766086756322006
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.6762113959162673
Loss in iteration 81 : 0.6760131394261117
Loss in iteration 82 : 0.6758151266569101
Loss in iteration 83 : 0.6756173483255633
Loss in iteration 84 : 0.6754197948878221
Loss in iteration 85 : 0.6752224566127074
Loss in iteration 86 : 0.6750253236822996
Loss in iteration 87 : 0.6748283863086908
Loss in iteration 88 : 0.6746316348550656
Loss in iteration 89 : 0.6744350599463959
Loss in iteration 90 : 0.6742386525578968
Loss in iteration 91 : 0.6740424040750551
Loss in iteration 92 : 0.6738463063253899
Loss in iteration 93 : 0.6736503515866578
Loss in iteration 94 : 0.6734545325782223
Loss in iteration 95 : 0.6732588424421753
Loss in iteration 96 : 0.6730632747194032
Loss in iteration 97 : 0.6728678233241574
Loss in iteration 98 : 0.6726724825192106
Loss in iteration 99 : 0.6724772468926725
Loss in iteration 100 : 0.6722821113368843
Loss in iteration 101 : 0.6720870710293817
Loss in iteration 102 : 0.6718921214158105
Loss in iteration 103 : 0.671697258194545
Loss in iteration 104 : 0.6715024773027843
Loss in iteration 105 : 0.6713077749039102
Loss in iteration 106 : 0.6711131473759093
Loss in iteration 107 : 0.6709185913007187
Loss in iteration 108 : 0.6707241034543497
Loss in iteration 109 : 0.6705296807976902
Loss in iteration 110 : 0.6703353204679124
Loss in iteration 111 : 0.670141019770401
Loss in iteration 112 : 0.6699467761711535
Loss in iteration 113 : 0.6697525872896092
Loss in iteration 114 : 0.6695584508918571
Loss in iteration 115 : 0.6693643648842087
Loss in iteration 116 : 0.6691703273070829
Loss in iteration 117 : 0.6689763363292
Loss in iteration 118 : 0.6687823902420595
Loss in iteration 119 : 0.6685884874546625
Loss in iteration 120 : 0.6683946264884943
Loss in iteration 121 : 0.6682008059727267
Loss in iteration 122 : 0.6680070246396386
Loss in iteration 123 : 0.6678132813202301
Loss in iteration 124 : 0.6676195749400383
Loss in iteration 125 : 0.6674259045151227
Loss in iteration 126 : 0.6672322691482263
Loss in iteration 127 : 0.6670386680250863
Loss in iteration 128 : 0.666845100410913
Loss in iteration 129 : 0.6666515656469991
Loss in iteration 130 : 0.6664580631474695
Loss in iteration 131 : 0.6662645923961655
Loss in iteration 132 : 0.666071152943642
Loss in iteration 133 : 0.6658777444042951
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254213
Loss in iteration 136 : 0.6652977013095167
Loss in iteration 137 : 0.6651044137490169
Loss in iteration 138 : 0.6649111560380934
Loss in iteration 139 : 0.6647179281196691
Loss in iteration 140 : 0.6645247299832375
Loss in iteration 141 : 0.6643315616627489
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155865
Loss in iteration 144 : 0.663752236561218
Loss in iteration 145 : 0.6635591886637079
Loss in iteration 146 : 0.6633661713503329
Loss in iteration 147 : 0.6631731848817287
Loss in iteration 148 : 0.6629802295502826
Loss in iteration 149 : 0.6627873056785705
Loss in iteration 150 : 0.6625944136178619
Loss in iteration 151 : 0.6624015537466772
Loss in iteration 152 : 0.6622087264694023
Loss in iteration 153 : 0.662015932214951
Loss in iteration 154 : 0.661823171435479
Loss in iteration 155 : 0.6616304446051507
Loss in iteration 156 : 0.66143775221895
Loss in iteration 157 : 0.6612450947915361
Loss in iteration 158 : 0.6610524728561491
Loss in iteration 159 : 0.6608598869635478
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913315
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939861
Loss in iteration 164 : 0.659897518521434
Loss in iteration 165 : 0.6597051613103248
Loss in iteration 166 : 0.6595128444079473
Loss in iteration 167 : 0.6593205684721009
Loss in iteration 168 : 0.6591283341703855
Loss in iteration 169 : 0.6589361421795211
Loss in iteration 170 : 0.658743993184703
Loss in iteration 171 : 0.6585518878789903
Loss in iteration 172 : 0.6583598269627238
Loss in iteration 173 : 0.6581678111429801
Loss in iteration 174 : 0.6579758411330453
Loss in iteration 175 : 0.6577839176519346
Loss in iteration 176 : 0.6575920414239279
Loss in iteration 177 : 0.6574002131781415
Loss in iteration 178 : 0.6572084336481163
Loss in iteration 179 : 0.6570167035714489
Loss in iteration 180 : 0.6568250236894333
Loss in iteration 181 : 0.656633394746732
Loss in iteration 182 : 0.6564418174910627
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452745
Loss in iteration 185 : 0.6558674033633612
Loss in iteration 186 : 0.6556760403843811
Loss in iteration 187 : 0.655484732867282
Loss in iteration 188 : 0.6552934815725102
Loss in iteration 189 : 0.6551022872617625
Loss in iteration 190 : 0.6549111506977265
Loss in iteration 191 : 0.6547200726438126
Loss in iteration 192 : 0.6545290538638501
Loss in iteration 193 : 0.6543380951217604
Loss in iteration 194 : 0.6541471971811891
Loss in iteration 195 : 0.6539563608050971
Loss in iteration 196 : 0.6537655867552801
Loss in iteration 197 : 0.6535748757918564
Loss in iteration 198 : 0.6533842286726638
Loss in iteration 199 : 0.6531936461525888
Loss in iteration 200 : 0.6530031289828341
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.666, training accuracy 0.6705, time elapsed: 4920 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6929235752666216
Loss in iteration 3 : 0.6926960665968458
Loss in iteration 4 : 0.692466673002798
Loss in iteration 5 : 0.692236337267449
Loss in iteration 6 : 0.6920055789324584
Loss in iteration 7 : 0.6917747120533901
Loss in iteration 8 : 0.6915439381601188
Loss in iteration 9 : 0.6913133922107215
Loss in iteration 10 : 0.691083167597145
Loss in iteration 11 : 0.6908533306884844
Loss in iteration 12 : 0.6906239297064454
Loss in iteration 13 : 0.6903950003411579
Loss in iteration 14 : 0.6901665694061496
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231967
Loss in iteration 17 : 0.6894844480963954
Loss in iteration 18 : 0.6892581720925248
Loss in iteration 19 : 0.6890324583520112
Loss in iteration 20 : 0.6888073118783553
Loss in iteration 21 : 0.6885827361540363
Loss in iteration 22 : 0.6883587333817472
Loss in iteration 23 : 0.6881353046728775
Loss in iteration 24 : 0.6879124501982642
Loss in iteration 25 : 0.6876901693114452
Loss in iteration 26 : 0.687468460651544
Loss in iteration 27 : 0.6872473222307213
Loss in iteration 28 : 0.6870267515096307
Loss in iteration 29 : 0.6868067454633647
Loss in iteration 30 : 0.686587300639593
Loss in iteration 31 : 0.6863684132102226
Loss in iteration 32 : 0.6861500790174879
Loss in iteration 33 : 0.6859322936152161
Loss in iteration 34 : 0.6857150523058196
Loss in iteration 35 : 0.6854983501734838
Loss in iteration 36 : 0.6852821821139227
Loss in iteration 37 : 0.6850665428610211
Loss in iteration 38 : 0.6848514270106489
Loss in iteration 39 : 0.6846368290419
Loss in iteration 40 : 0.6844227433359665
Loss in iteration 41 : 0.6842091641928564
Loss in iteration 42 : 0.6839960858461508
Loss in iteration 43 : 0.6837835024759527
Loss in iteration 44 : 0.6835714082201896
Loss in iteration 45 : 0.6833597971844099
Loss in iteration 46 : 0.6831486634501978
Loss in iteration 47 : 0.6829380010823383
Loss in iteration 48 : 0.6827278041348106
Loss in iteration 49 : 0.6825180666557314
Loss in iteration 50 : 0.6823087826913212
Loss in iteration 51 : 0.6820999462889611
Loss in iteration 52 : 0.6818915514994244
Loss in iteration 53 : 0.6816835923783223
Loss in iteration 54 : 0.681476062986818
Loss in iteration 55 : 0.681268957391646
Loss in iteration 56 : 0.6810622696644757
Loss in iteration 57 : 0.6808559938806242
Loss in iteration 58 : 0.6806501241171421
Loss in iteration 59 : 0.6804446544502752
Loss in iteration 60 : 0.6802395789522903
Loss in iteration 61 : 0.6800348916876392
Loss in iteration 62 : 0.6798305867084533
Loss in iteration 63 : 0.6796266580492966
Loss in iteration 64 : 0.6794230997211296
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.679017069941194
Loss in iteration 67 : 0.6788145863262015
Loss in iteration 68 : 0.6786124486965575
Loss in iteration 69 : 0.6784106508201545
Loss in iteration 70 : 0.6782091863823767
Loss in iteration 71 : 0.6780080489709516
Loss in iteration 72 : 0.677807232058712
Loss in iteration 73 : 0.6776067289840477
Loss in iteration 74 : 0.6774065329288738
Loss in iteration 75 : 0.6772066368941015
Loss in iteration 76 : 0.6770070336728071
Loss in iteration 77 : 0.6768077158217204
Loss in iteration 78 : 0.6766086756322006
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.6762113959162673
Loss in iteration 81 : 0.6760131394261117
Loss in iteration 82 : 0.6758151266569101
Loss in iteration 83 : 0.6756173483255633
Loss in iteration 84 : 0.6754197948878221
Loss in iteration 85 : 0.6752224566127075
Loss in iteration 86 : 0.6750253236822996
Loss in iteration 87 : 0.6748283863086908
Loss in iteration 88 : 0.6746316348550655
Loss in iteration 89 : 0.674435059946396
Loss in iteration 90 : 0.6742386525578968
Loss in iteration 91 : 0.6740424040750551
Loss in iteration 92 : 0.6738463063253899
Loss in iteration 93 : 0.6736503515866578
Loss in iteration 94 : 0.6734545325782223
Loss in iteration 95 : 0.6732588424421753
Loss in iteration 96 : 0.6730632747194031
Loss in iteration 97 : 0.6728678233241574
Loss in iteration 98 : 0.6726724825192106
Loss in iteration 99 : 0.6724772468926725
Loss in iteration 100 : 0.6722821113368843
Loss in iteration 101 : 0.6720870710293817
Loss in iteration 102 : 0.6718921214158105
Loss in iteration 103 : 0.671697258194545
Loss in iteration 104 : 0.6715024773027843
Loss in iteration 105 : 0.6713077749039102
Loss in iteration 106 : 0.6711131473759093
Loss in iteration 107 : 0.6709185913007186
Loss in iteration 108 : 0.6707241034543497
Loss in iteration 109 : 0.6705296807976902
Loss in iteration 110 : 0.6703353204679124
Loss in iteration 111 : 0.670141019770401
Loss in iteration 112 : 0.6699467761711536
Loss in iteration 113 : 0.6697525872896092
Loss in iteration 114 : 0.669558450891857
Loss in iteration 115 : 0.6693643648842087
Loss in iteration 116 : 0.6691703273070829
Loss in iteration 117 : 0.6689763363291998
Loss in iteration 118 : 0.6687823902420595
Loss in iteration 119 : 0.6685884874546625
Loss in iteration 120 : 0.6683946264884943
Loss in iteration 121 : 0.6682008059727267
Loss in iteration 122 : 0.6680070246396387
Loss in iteration 123 : 0.6678132813202301
Loss in iteration 124 : 0.6676195749400383
Loss in iteration 125 : 0.6674259045151226
Loss in iteration 126 : 0.6672322691482263
Loss in iteration 127 : 0.6670386680250863
Loss in iteration 128 : 0.666845100410913
Loss in iteration 129 : 0.6666515656469991
Loss in iteration 130 : 0.6664580631474694
Loss in iteration 131 : 0.6662645923961655
Loss in iteration 132 : 0.6660711529436422
Loss in iteration 133 : 0.6658777444042951
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254212
Loss in iteration 136 : 0.6652977013095167
Loss in iteration 137 : 0.6651044137490169
Loss in iteration 138 : 0.6649111560380934
Loss in iteration 139 : 0.6647179281196691
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627488
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155865
Loss in iteration 144 : 0.6637522365612181
Loss in iteration 145 : 0.6635591886637078
Loss in iteration 146 : 0.6633661713503329
Loss in iteration 147 : 0.6631731848817287
Loss in iteration 148 : 0.6629802295502826
Loss in iteration 149 : 0.6627873056785705
Loss in iteration 150 : 0.6625944136178619
Loss in iteration 151 : 0.6624015537466772
Loss in iteration 152 : 0.6622087264694023
Loss in iteration 153 : 0.6620159322149509
Loss in iteration 154 : 0.661823171435479
Loss in iteration 155 : 0.6616304446051507
Loss in iteration 156 : 0.66143775221895
Loss in iteration 157 : 0.6612450947915361
Loss in iteration 158 : 0.661052472856149
Loss in iteration 159 : 0.6608598869635478
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913315
Loss in iteration 162 : 0.6602823512919482
Loss in iteration 163 : 0.660089915393986
Loss in iteration 164 : 0.659897518521434
Loss in iteration 165 : 0.6597051613103248
Loss in iteration 166 : 0.6595128444079473
Loss in iteration 167 : 0.659320568472101
Loss in iteration 168 : 0.6591283341703855
Loss in iteration 169 : 0.6589361421795211
Loss in iteration 170 : 0.658743993184703
Loss in iteration 171 : 0.6585518878789903
Loss in iteration 172 : 0.6583598269627239
Loss in iteration 173 : 0.6581678111429801
Loss in iteration 174 : 0.6579758411330453
Loss in iteration 175 : 0.6577839176519347
Loss in iteration 176 : 0.657592041423928
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481164
Loss in iteration 179 : 0.6570167035714489
Loss in iteration 180 : 0.6568250236894335
Loss in iteration 181 : 0.656633394746732
Loss in iteration 182 : 0.6564418174910626
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452745
Loss in iteration 185 : 0.6558674033633612
Loss in iteration 186 : 0.6556760403843812
Loss in iteration 187 : 0.655484732867282
Loss in iteration 188 : 0.6552934815725103
Loss in iteration 189 : 0.6551022872617625
Loss in iteration 190 : 0.6549111506977265
Loss in iteration 191 : 0.6547200726438127
Loss in iteration 192 : 0.6545290538638501
Loss in iteration 193 : 0.6543380951217604
Loss in iteration 194 : 0.6541471971811891
Loss in iteration 195 : 0.653956360805097
Loss in iteration 196 : 0.65376558675528
Loss in iteration 197 : 0.6535748757918564
Loss in iteration 198 : 0.6533842286726638
Loss in iteration 199 : 0.6531936461525888
Loss in iteration 200 : 0.6530031289828341
Testing accuracy  of updater 4 on alg 0 with rate 1.0 = 0.666, training accuracy 0.6705, time elapsed: 5201 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6929235752666216
Loss in iteration 3 : 0.6926960665968459
Loss in iteration 4 : 0.692466673002798
Loss in iteration 5 : 0.692236337267449
Loss in iteration 6 : 0.6920055789324585
Loss in iteration 7 : 0.6917747120533901
Loss in iteration 8 : 0.6915439381601188
Loss in iteration 9 : 0.6913133922107215
Loss in iteration 10 : 0.6910831675971452
Loss in iteration 11 : 0.6908533306884844
Loss in iteration 12 : 0.6906239297064454
Loss in iteration 13 : 0.690395000341158
Loss in iteration 14 : 0.6901665694061496
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231967
Loss in iteration 17 : 0.6894844480963954
Loss in iteration 18 : 0.6892581720925248
Loss in iteration 19 : 0.6890324583520112
Loss in iteration 20 : 0.6888073118783552
Loss in iteration 21 : 0.6885827361540363
Loss in iteration 22 : 0.6883587333817472
Loss in iteration 23 : 0.6881353046728775
Loss in iteration 24 : 0.6879124501982642
Loss in iteration 25 : 0.6876901693114452
Loss in iteration 26 : 0.6874684606515441
Loss in iteration 27 : 0.6872473222307213
Loss in iteration 28 : 0.6870267515096307
Loss in iteration 29 : 0.6868067454633647
Loss in iteration 30 : 0.6865873006395931
Loss in iteration 31 : 0.6863684132102226
Loss in iteration 32 : 0.6861500790174879
Loss in iteration 33 : 0.6859322936152161
Loss in iteration 34 : 0.6857150523058196
Loss in iteration 35 : 0.6854983501734838
Loss in iteration 36 : 0.6852821821139226
Loss in iteration 37 : 0.6850665428610211
Loss in iteration 38 : 0.6848514270106488
Loss in iteration 39 : 0.6846368290419003
Loss in iteration 40 : 0.6844227433359665
Loss in iteration 41 : 0.6842091641928563
Loss in iteration 42 : 0.6839960858461507
Loss in iteration 43 : 0.6837835024759527
Loss in iteration 44 : 0.6835714082201896
Loss in iteration 45 : 0.6833597971844099
Loss in iteration 46 : 0.6831486634501978
Loss in iteration 47 : 0.6829380010823383
Loss in iteration 48 : 0.6827278041348106
Loss in iteration 49 : 0.6825180666557314
Loss in iteration 50 : 0.6823087826913213
Loss in iteration 51 : 0.6820999462889611
Loss in iteration 52 : 0.6818915514994244
Loss in iteration 53 : 0.6816835923783224
Loss in iteration 54 : 0.681476062986818
Loss in iteration 55 : 0.681268957391646
Loss in iteration 56 : 0.6810622696644757
Loss in iteration 57 : 0.6808559938806242
Loss in iteration 58 : 0.6806501241171421
Loss in iteration 59 : 0.6804446544502752
Loss in iteration 60 : 0.6802395789522903
Loss in iteration 61 : 0.6800348916876392
Loss in iteration 62 : 0.6798305867084534
Loss in iteration 63 : 0.6796266580492966
Loss in iteration 64 : 0.6794230997211296
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.679017069941194
Loss in iteration 67 : 0.6788145863262015
Loss in iteration 68 : 0.6786124486965573
Loss in iteration 69 : 0.6784106508201545
Loss in iteration 70 : 0.6782091863823767
Loss in iteration 71 : 0.6780080489709516
Loss in iteration 72 : 0.677807232058712
Loss in iteration 73 : 0.6776067289840477
Loss in iteration 74 : 0.6774065329288739
Loss in iteration 75 : 0.6772066368941017
Loss in iteration 76 : 0.677007033672807
Loss in iteration 77 : 0.6768077158217204
Loss in iteration 78 : 0.6766086756322007
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.6762113959162673
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569101
Loss in iteration 83 : 0.6756173483255632
Loss in iteration 84 : 0.6754197948878221
Loss in iteration 85 : 0.6752224566127074
Loss in iteration 86 : 0.6750253236822996
Loss in iteration 87 : 0.6748283863086908
Loss in iteration 88 : 0.6746316348550656
Loss in iteration 89 : 0.6744350599463959
Loss in iteration 90 : 0.6742386525578968
Loss in iteration 91 : 0.6740424040750551
Loss in iteration 92 : 0.6738463063253899
Loss in iteration 93 : 0.6736503515866578
Loss in iteration 94 : 0.6734545325782223
Loss in iteration 95 : 0.6732588424421752
Loss in iteration 96 : 0.6730632747194031
Loss in iteration 97 : 0.6728678233241574
Loss in iteration 98 : 0.6726724825192106
Loss in iteration 99 : 0.6724772468926725
Loss in iteration 100 : 0.6722821113368843
Loss in iteration 101 : 0.6720870710293817
Loss in iteration 102 : 0.6718921214158105
Loss in iteration 103 : 0.671697258194545
Loss in iteration 104 : 0.6715024773027843
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759093
Loss in iteration 107 : 0.6709185913007186
Loss in iteration 108 : 0.6707241034543497
Loss in iteration 109 : 0.6705296807976902
Loss in iteration 110 : 0.6703353204679126
Loss in iteration 111 : 0.670141019770401
Loss in iteration 112 : 0.6699467761711535
Loss in iteration 113 : 0.6697525872896092
Loss in iteration 114 : 0.6695584508918571
Loss in iteration 115 : 0.6693643648842087
Loss in iteration 116 : 0.6691703273070828
Loss in iteration 117 : 0.6689763363291998
Loss in iteration 118 : 0.6687823902420594
Loss in iteration 119 : 0.6685884874546625
Loss in iteration 120 : 0.6683946264884942
Loss in iteration 121 : 0.6682008059727267
Loss in iteration 122 : 0.6680070246396386
Loss in iteration 123 : 0.6678132813202301
Loss in iteration 124 : 0.6676195749400383
Loss in iteration 125 : 0.6674259045151227
Loss in iteration 126 : 0.6672322691482262
Loss in iteration 127 : 0.6670386680250863
Loss in iteration 128 : 0.666845100410913
Loss in iteration 129 : 0.6666515656469991
Loss in iteration 130 : 0.6664580631474695
Loss in iteration 131 : 0.6662645923961655
Loss in iteration 132 : 0.666071152943642
Loss in iteration 133 : 0.6658777444042951
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254212
Loss in iteration 136 : 0.6652977013095167
Loss in iteration 137 : 0.6651044137490169
Loss in iteration 138 : 0.6649111560380934
Loss in iteration 139 : 0.6647179281196691
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627489
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155866
Loss in iteration 144 : 0.6637522365612181
Loss in iteration 145 : 0.6635591886637079
Loss in iteration 146 : 0.6633661713503328
Loss in iteration 147 : 0.6631731848817287
Loss in iteration 148 : 0.6629802295502825
Loss in iteration 149 : 0.6627873056785704
Loss in iteration 150 : 0.6625944136178619
Loss in iteration 151 : 0.6624015537466772
Loss in iteration 152 : 0.6622087264694023
Loss in iteration 153 : 0.6620159322149509
Loss in iteration 154 : 0.6618231714354789
Loss in iteration 155 : 0.6616304446051507
Loss in iteration 156 : 0.66143775221895
Loss in iteration 157 : 0.6612450947915361
Loss in iteration 158 : 0.6610524728561491
Loss in iteration 159 : 0.6608598869635478
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913316
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939861
Loss in iteration 164 : 0.659897518521434
Loss in iteration 165 : 0.6597051613103248
Loss in iteration 166 : 0.6595128444079473
Loss in iteration 167 : 0.6593205684721009
Loss in iteration 168 : 0.6591283341703855
Loss in iteration 169 : 0.6589361421795211
Loss in iteration 170 : 0.658743993184703
Loss in iteration 171 : 0.6585518878789903
Loss in iteration 172 : 0.6583598269627238
Loss in iteration 173 : 0.6581678111429801
Loss in iteration 174 : 0.6579758411330453
Loss in iteration 175 : 0.6577839176519346
Loss in iteration 176 : 0.657592041423928
Loss in iteration 177 : 0.6574002131781415
Loss in iteration 178 : 0.6572084336481163
Loss in iteration 179 : 0.6570167035714489
Loss in iteration 180 : 0.6568250236894333
Loss in iteration 181 : 0.656633394746732
Loss in iteration 182 : 0.6564418174910626
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452745
Loss in iteration 185 : 0.6558674033633612
Loss in iteration 186 : 0.6556760403843812
Loss in iteration 187 : 0.655484732867282
Loss in iteration 188 : 0.6552934815725103
Loss in iteration 189 : 0.6551022872617625
Loss in iteration 190 : 0.6549111506977265
Loss in iteration 191 : 0.6547200726438126
Loss in iteration 192 : 0.6545290538638501
Loss in iteration 193 : 0.6543380951217604
Loss in iteration 194 : 0.6541471971811891
Loss in iteration 195 : 0.6539563608050971
Loss in iteration 196 : 0.65376558675528
Loss in iteration 197 : 0.6535748757918564
Loss in iteration 198 : 0.6533842286726637
Loss in iteration 199 : 0.6531936461525888
Loss in iteration 200 : 0.6530031289828341
Testing accuracy  of updater 4 on alg 0 with rate 0.09999999999999998 = 0.666, training accuracy 0.6705, time elapsed: 4666 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 65.93856858740774
Loss in iteration 3 : 84.855426556232
Loss in iteration 4 : 34.27691872039005
Loss in iteration 5 : 40.09256124550931
Loss in iteration 6 : 28.278066021264955
Loss in iteration 7 : 29.248996869359594
Loss in iteration 8 : 25.402685776933694
Loss in iteration 9 : 23.852585622720596
Loss in iteration 10 : 22.67777914049521
Loss in iteration 11 : 21.49826642351746
Loss in iteration 12 : 20.728604530229084
Loss in iteration 13 : 19.857287706840435
Loss in iteration 14 : 18.69437548795496
Loss in iteration 15 : 18.961392619818923
Loss in iteration 16 : 17.37213854585894
Loss in iteration 17 : 18.251118560721928
Loss in iteration 18 : 17.098513658999757
Loss in iteration 19 : 18.121731624418548
Loss in iteration 20 : 16.49372483381754
Loss in iteration 21 : 18.046254887381647
Loss in iteration 22 : 15.310892049383238
Loss in iteration 23 : 17.81189620038464
Loss in iteration 24 : 14.025791290785078
Loss in iteration 25 : 17.58015845889642
Loss in iteration 26 : 15.502604889700553
Loss in iteration 27 : 18.43708989176377
Loss in iteration 28 : 16.452054720749306
Loss in iteration 29 : 18.076336835244916
Loss in iteration 30 : 15.988827838594927
Loss in iteration 31 : 17.465143213334102
Loss in iteration 32 : 15.21495616411177
Loss in iteration 33 : 17.091671909351973
Loss in iteration 34 : 14.95920823625356
Loss in iteration 35 : 17.041640736742433
Loss in iteration 36 : 14.791083230152418
Loss in iteration 37 : 17.086119093720573
Loss in iteration 38 : 14.46261465635054
Loss in iteration 39 : 17.311381758147693
Loss in iteration 40 : 13.976097676197991
Loss in iteration 41 : 17.83744740869134
Loss in iteration 42 : 12.691749160892918
Loss in iteration 43 : 16.891580983596388
Loss in iteration 44 : 14.39124845515655
Loss in iteration 45 : 18.19509374874919
Loss in iteration 46 : 15.614546507149134
Loss in iteration 47 : 18.155127113721434
Loss in iteration 48 : 15.403832119152176
Loss in iteration 49 : 17.57801854988745
Loss in iteration 50 : 14.627180422834215
Loss in iteration 51 : 17.00007908793064
Loss in iteration 52 : 14.333609098941478
Loss in iteration 53 : 16.902434318003273
Loss in iteration 54 : 13.913479753537997
Loss in iteration 55 : 17.107402064557967
Loss in iteration 56 : 13.58907254671177
Loss in iteration 57 : 17.187703310521858
Loss in iteration 58 : 12.560079185570304
Loss in iteration 59 : 16.66403638408114
Loss in iteration 60 : 14.002883919241743
Loss in iteration 61 : 18.046446484268305
Loss in iteration 62 : 15.307472776847884
Loss in iteration 63 : 18.194745728554125
Loss in iteration 64 : 15.128786932900638
Loss in iteration 65 : 17.562019187240264
Loss in iteration 66 : 14.313437951405747
Loss in iteration 67 : 16.886995545535843
Loss in iteration 68 : 14.05491650165137
Loss in iteration 69 : 16.837067442828303
Loss in iteration 70 : 13.77283681312683
Loss in iteration 71 : 17.275763085999863
Loss in iteration 72 : 13.008544165370248
Loss in iteration 73 : 16.634964813463785
Loss in iteration 74 : 12.552309469036627
Loss in iteration 75 : 16.740005791398563
Loss in iteration 76 : 14.339885715975015
Loss in iteration 77 : 18.253731569555473
Loss in iteration 78 : 15.234819154574927
Loss in iteration 79 : 18.06124904030075
Loss in iteration 80 : 14.761682842274062
Loss in iteration 81 : 17.236330772569254
Loss in iteration 82 : 14.259970649474704
Loss in iteration 83 : 16.82232585685962
Loss in iteration 84 : 13.77491771667207
Loss in iteration 85 : 16.868698029772546
Loss in iteration 86 : 13.486493675264207
Loss in iteration 87 : 17.070351391669785
Loss in iteration 88 : 12.66167597320692
Loss in iteration 89 : 16.361933201666904
Loss in iteration 90 : 13.485655727483703
Loss in iteration 91 : 17.51129627948984
Loss in iteration 92 : 14.797305048854849
Loss in iteration 93 : 17.985240873691964
Loss in iteration 94 : 14.829805195376764
Loss in iteration 95 : 17.426129443204225
Loss in iteration 96 : 14.299104661769656
Loss in iteration 97 : 16.79921861451509
Loss in iteration 98 : 14.031688792403488
Loss in iteration 99 : 16.79299409740542
Loss in iteration 100 : 13.722299569920075
Loss in iteration 101 : 17.058337077070544
Loss in iteration 102 : 12.924506023645488
Loss in iteration 103 : 16.432461981140154
Loss in iteration 104 : 13.091534207918256
Loss in iteration 105 : 17.039428061437313
Loss in iteration 106 : 14.418869251532648
Loss in iteration 107 : 17.70700008919936
Loss in iteration 108 : 14.809126951455308
Loss in iteration 109 : 17.44477774745624
Loss in iteration 110 : 14.412869921377636
Loss in iteration 111 : 16.887204556030746
Loss in iteration 112 : 14.037764312304327
Loss in iteration 113 : 16.657272261767364
Loss in iteration 114 : 13.79490625882698
Loss in iteration 115 : 17.00163782371371
Loss in iteration 116 : 13.216188660711047
Loss in iteration 117 : 16.561929512257375
Loss in iteration 118 : 12.68702886711702
Loss in iteration 119 : 16.425423916749605
Loss in iteration 120 : 13.975252691903567
Loss in iteration 121 : 17.566368093595752
Loss in iteration 122 : 14.92349559738765
Loss in iteration 123 : 17.651291672761587
Loss in iteration 124 : 14.691900350361667
Loss in iteration 125 : 17.086565816289653
Loss in iteration 126 : 14.07310877406789
Loss in iteration 127 : 16.49022405832909
Loss in iteration 128 : 13.926959700344698
Loss in iteration 129 : 16.72409712087278
Loss in iteration 130 : 13.486733880258917
Loss in iteration 131 : 16.747831940079106
Loss in iteration 132 : 12.64891857081775
Loss in iteration 133 : 16.180889616899616
Loss in iteration 134 : 13.198335797158746
Loss in iteration 135 : 17.087556433376545
Loss in iteration 136 : 14.737562004711426
Loss in iteration 137 : 17.758755860420617
Loss in iteration 138 : 14.979989668121762
Loss in iteration 139 : 17.333985772833707
Loss in iteration 140 : 14.259686231087311
Loss in iteration 141 : 16.51693391106698
Loss in iteration 142 : 14.026322316221984
Loss in iteration 143 : 16.50398269845493
Loss in iteration 144 : 13.66269942771115
Loss in iteration 145 : 16.799183699131177
Loss in iteration 146 : 12.971055705512677
Loss in iteration 147 : 16.334949248415622
Loss in iteration 148 : 12.64806158302048
Loss in iteration 149 : 16.386505132749328
Loss in iteration 150 : 14.278495527112046
Loss in iteration 151 : 17.6548662677052
Loss in iteration 152 : 14.918334963205655
Loss in iteration 153 : 17.362389105438556
Loss in iteration 154 : 14.623551072331237
Loss in iteration 155 : 16.833559105807332
Loss in iteration 156 : 14.092391406579328
Loss in iteration 157 : 16.381384619112936
Loss in iteration 158 : 13.81091261213846
Loss in iteration 159 : 16.555134509782416
Loss in iteration 160 : 13.504308934895958
Loss in iteration 161 : 16.728383275195405
Loss in iteration 162 : 12.552808732977287
Loss in iteration 163 : 16.013090655324408
Loss in iteration 164 : 13.136138528182482
Loss in iteration 165 : 16.904018171685063
Loss in iteration 166 : 14.723640594825538
Loss in iteration 167 : 17.669780991514358
Loss in iteration 168 : 15.022993292491646
Loss in iteration 169 : 17.190133069497723
Loss in iteration 170 : 14.350422297711477
Loss in iteration 171 : 16.45275064931419
Loss in iteration 172 : 13.990682560477376
Loss in iteration 173 : 16.32080038583238
Loss in iteration 174 : 13.636752111275758
Loss in iteration 175 : 16.62515089613376
Loss in iteration 176 : 13.07151740251034
Loss in iteration 177 : 16.329117907119944
Loss in iteration 178 : 12.57123288865326
Loss in iteration 179 : 16.207589707218617
Loss in iteration 180 : 14.252745894643486
Loss in iteration 181 : 17.53291281809702
Loss in iteration 182 : 14.89641490152057
Loss in iteration 183 : 17.20123461969298
Loss in iteration 184 : 14.58397792996818
Loss in iteration 185 : 16.7151710434027
Loss in iteration 186 : 14.0512872010139
Loss in iteration 187 : 16.189420055609173
Loss in iteration 188 : 13.897149950786167
Loss in iteration 189 : 16.351628961774615
Loss in iteration 190 : 13.672109088770751
Loss in iteration 191 : 16.809783897846142
Loss in iteration 192 : 12.601304515900985
Loss in iteration 193 : 15.896975577127428
Loss in iteration 194 : 12.860935001641552
Loss in iteration 195 : 16.61074788272234
Loss in iteration 196 : 14.726263458458291
Loss in iteration 197 : 17.629935144905616
Loss in iteration 198 : 15.018864896176423
Loss in iteration 199 : 17.078662359046895
Loss in iteration 200 : 14.398090260430697
Testing accuracy  of updater 5 on alg 0 with rate 10.0 = 0.72, training accuracy 0.7115, time elapsed: 4553 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 1.064805985558032
Loss in iteration 3 : 6.554385356256291
Loss in iteration 4 : 3.404083146300398
Loss in iteration 5 : 3.7806886791491183
Loss in iteration 6 : 3.644371574299546
Loss in iteration 7 : 2.461017272470781
Loss in iteration 8 : 3.5901918443594867
Loss in iteration 9 : 1.8231944172871832
Loss in iteration 10 : 3.594571422426248
Loss in iteration 11 : 1.3756710988573349
Loss in iteration 12 : 3.0246896833573746
Loss in iteration 13 : 1.6498899906077193
Loss in iteration 14 : 2.834882528847995
Loss in iteration 15 : 1.642789974342573
Loss in iteration 16 : 2.090768362454279
Loss in iteration 17 : 1.6228999984598103
Loss in iteration 18 : 2.2549257608036686
Loss in iteration 19 : 1.9940957426596084
Loss in iteration 20 : 2.472680452950336
Loss in iteration 21 : 1.6669317324564354
Loss in iteration 22 : 2.1844035689694286
Loss in iteration 23 : 1.7051448118829937
Loss in iteration 24 : 2.035689545623813
Loss in iteration 25 : 1.6926102668069258
Loss in iteration 26 : 1.915852368831678
Loss in iteration 27 : 1.6866274756633972
Loss in iteration 28 : 1.8234863724582717
Loss in iteration 29 : 1.6872854840732348
Loss in iteration 30 : 1.749300260759041
Loss in iteration 31 : 1.6934932309129975
Loss in iteration 32 : 1.6916501835276758
Loss in iteration 33 : 1.7006646613458334
Loss in iteration 34 : 1.6622498037494189
Loss in iteration 35 : 1.7138322196213958
Loss in iteration 36 : 1.6541300178093088
Loss in iteration 37 : 1.7763470656941953
Loss in iteration 38 : 1.5407034969598
Loss in iteration 39 : 1.6626209751247119
Loss in iteration 40 : 1.4337742952245929
Loss in iteration 41 : 1.774446320397037
Loss in iteration 42 : 1.695254442227805
Loss in iteration 43 : 1.8340723313832037
Loss in iteration 44 : 1.7652093631073487
Loss in iteration 45 : 1.7652912215616574
Loss in iteration 46 : 1.683509114626419
Loss in iteration 47 : 1.7221849424686648
Loss in iteration 48 : 1.6255795432851856
Loss in iteration 49 : 1.7054928125166686
Loss in iteration 50 : 1.5904437938954405
Loss in iteration 51 : 1.7028940494707312
Loss in iteration 52 : 1.5677495003290953
Loss in iteration 53 : 1.7050134325747839
Loss in iteration 54 : 1.5589302273030061
Loss in iteration 55 : 1.7122422701193618
Loss in iteration 56 : 1.558078711572446
Loss in iteration 57 : 1.7530718010712933
Loss in iteration 58 : 1.5402355363836513
Loss in iteration 59 : 1.7553181245940341
Loss in iteration 60 : 1.3340093401921231
Loss in iteration 61 : 1.6867152152102172
Loss in iteration 62 : 1.5431974017579797
Loss in iteration 63 : 1.83123461663125
Loss in iteration 64 : 1.6790640492649542
Loss in iteration 65 : 1.8105749009138277
Loss in iteration 66 : 1.6373770952764222
Loss in iteration 67 : 1.7479630785766724
Loss in iteration 68 : 1.5754771390910502
Loss in iteration 69 : 1.7119525702946083
Loss in iteration 70 : 1.5428453740571313
Loss in iteration 71 : 1.7027860700659931
Loss in iteration 72 : 1.5238236437877162
Loss in iteration 73 : 1.702789756229262
Loss in iteration 74 : 1.516913055843032
Loss in iteration 75 : 1.7090223198768388
Loss in iteration 76 : 1.5189120886121086
Loss in iteration 77 : 1.7489931608849292
Loss in iteration 78 : 1.5140863097521537
Loss in iteration 79 : 1.7578435428474455
Loss in iteration 80 : 1.314631994573693
Loss in iteration 81 : 1.657260894006441
Loss in iteration 82 : 1.5155253631118693
Loss in iteration 83 : 1.825067007587187
Loss in iteration 84 : 1.65218788541447
Loss in iteration 85 : 1.8201328469241354
Loss in iteration 86 : 1.611817897262762
Loss in iteration 87 : 1.7492372815420691
Loss in iteration 88 : 1.5485792033022932
Loss in iteration 89 : 1.7040780940496312
Loss in iteration 90 : 1.5165594113703458
Loss in iteration 91 : 1.6923038096454148
Loss in iteration 92 : 1.4996224549040897
Loss in iteration 93 : 1.6937684469478957
Loss in iteration 94 : 1.4956763036984202
Loss in iteration 95 : 1.7189697428169233
Loss in iteration 96 : 1.5026318250530284
Loss in iteration 97 : 1.7725274736574175
Loss in iteration 98 : 1.343236788831866
Loss in iteration 99 : 1.6550830206136455
Loss in iteration 100 : 1.4646882663348668
Loss in iteration 101 : 1.7792193103276364
Loss in iteration 102 : 1.613504714940739
Loss in iteration 103 : 1.8147260155824798
Loss in iteration 104 : 1.607574300409928
Loss in iteration 105 : 1.7537458478022923
Loss in iteration 106 : 1.5461975994537498
Loss in iteration 107 : 1.701341170757229
Loss in iteration 108 : 1.510302588346375
Loss in iteration 109 : 1.6857296227792622
Loss in iteration 110 : 1.4915953070099852
Loss in iteration 111 : 1.6897084943343308
Loss in iteration 112 : 1.4880059274576292
Loss in iteration 113 : 1.7366364495091902
Loss in iteration 114 : 1.4580043314307356
Loss in iteration 115 : 1.7173518844242488
Loss in iteration 116 : 1.3614713232709328
Loss in iteration 117 : 1.6744696740030132
Loss in iteration 118 : 1.5298113064773584
Loss in iteration 119 : 1.7984150041519638
Loss in iteration 120 : 1.616111549838838
Loss in iteration 121 : 1.7839678390114417
Loss in iteration 122 : 1.5737748523813484
Loss in iteration 123 : 1.71998698751081
Loss in iteration 124 : 1.5224735465538106
Loss in iteration 125 : 1.6836228194660061
Loss in iteration 126 : 1.4950787588964436
Loss in iteration 127 : 1.678363321640405
Loss in iteration 128 : 1.482878978838203
Loss in iteration 129 : 1.7021495262868926
Loss in iteration 130 : 1.4820473678034598
Loss in iteration 131 : 1.745119314714952
Loss in iteration 132 : 1.3607785539851824
Loss in iteration 133 : 1.651687250076735
Loss in iteration 134 : 1.468382203707797
Loss in iteration 135 : 1.757422332526577
Loss in iteration 136 : 1.5954005186355718
Loss in iteration 137 : 1.7893124125721933
Loss in iteration 138 : 1.588854266806749
Loss in iteration 139 : 1.7335465370346221
Loss in iteration 140 : 1.5343336834557544
Loss in iteration 141 : 1.6849219248342704
Loss in iteration 142 : 1.5000298563530943
Loss in iteration 143 : 1.6717244106618776
Loss in iteration 144 : 1.482276500274384
Loss in iteration 145 : 1.687639913940751
Loss in iteration 146 : 1.4800869001915444
Loss in iteration 147 : 1.7397773192978674
Loss in iteration 148 : 1.3752621997851258
Loss in iteration 149 : 1.6516903836820156
Loss in iteration 150 : 1.4437946809030007
Loss in iteration 151 : 1.731229741238306
Loss in iteration 152 : 1.58115177442775
Loss in iteration 153 : 1.7841950467516092
Loss in iteration 154 : 1.592885214281327
Loss in iteration 155 : 1.7353739263837886
Loss in iteration 156 : 1.5396428618470084
Loss in iteration 157 : 1.6827688383772104
Loss in iteration 158 : 1.5016450951073657
Loss in iteration 159 : 1.6652835407924491
Loss in iteration 160 : 1.4815444601702068
Loss in iteration 161 : 1.6779483535355344
Loss in iteration 162 : 1.4782885400847432
Loss in iteration 163 : 1.7318229126877662
Loss in iteration 164 : 1.3850039798361213
Loss in iteration 165 : 1.6517550696957852
Loss in iteration 166 : 1.4335164873560597
Loss in iteration 167 : 1.7153260005441087
Loss in iteration 168 : 1.5723266081770462
Loss in iteration 169 : 1.7769959962909028
Loss in iteration 170 : 1.593495344745759
Loss in iteration 171 : 1.7329958406958672
Loss in iteration 172 : 1.541943897800394
Loss in iteration 173 : 1.6791904366549086
Loss in iteration 174 : 1.5018808858001775
Loss in iteration 175 : 1.6596722820654697
Loss in iteration 176 : 1.480586201261948
Loss in iteration 177 : 1.6720611984756342
Loss in iteration 178 : 1.4763586069164227
Loss in iteration 179 : 1.7243423090739207
Loss in iteration 180 : 1.386385499167408
Loss in iteration 181 : 1.6478154042223263
Loss in iteration 182 : 1.4363538046463482
Loss in iteration 183 : 1.7104760203386449
Loss in iteration 184 : 1.5702450242043322
Loss in iteration 185 : 1.7690248278576561
Loss in iteration 186 : 1.5908570644695301
Loss in iteration 187 : 1.7261870325015307
Loss in iteration 188 : 1.5403497521556302
Loss in iteration 189 : 1.6736765563497016
Loss in iteration 190 : 1.500284606670997
Loss in iteration 191 : 1.6550579232713951
Loss in iteration 192 : 1.4791116019636286
Loss in iteration 193 : 1.671012187915478
Loss in iteration 194 : 1.47323214083134
Loss in iteration 195 : 1.7163138143028556
Loss in iteration 196 : 1.3795893687957987
Loss in iteration 197 : 1.6397483826430435
Loss in iteration 198 : 1.4543681773153383
Loss in iteration 199 : 1.7175472470152762
Loss in iteration 200 : 1.5758503705702707
Testing accuracy  of updater 5 on alg 0 with rate 1.0 = 0.7165, training accuracy 0.7065, time elapsed: 5696 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.674960800493683
Loss in iteration 3 : 0.6632724319965575
Loss in iteration 4 : 0.6531344668216418
Loss in iteration 5 : 0.6436863233975594
Loss in iteration 6 : 0.6347462944420347
Loss in iteration 7 : 0.6262593755373332
Loss in iteration 8 : 0.6181930306435333
Loss in iteration 9 : 0.6105244347976758
Loss in iteration 10 : 0.6032304144667944
Loss in iteration 11 : 0.5962920433398781
Loss in iteration 12 : 0.5896896003906787
Loss in iteration 13 : 0.5834071115434507
Loss in iteration 14 : 0.5774277012392273
Loss in iteration 15 : 0.5717387179604326
Loss in iteration 16 : 0.566327000529155
Loss in iteration 17 : 0.5611883581886257
Loss in iteration 18 : 0.5563294142172853
Loss in iteration 19 : 0.5518152663376142
Loss in iteration 20 : 0.5479096018176282
Loss in iteration 21 : 0.5457196092464371
Loss in iteration 22 : 0.5501346166338951
Loss in iteration 23 : 0.5774225203992673
Loss in iteration 24 : 0.6526319740509875
Loss in iteration 25 : 0.6569187414432428
Loss in iteration 26 : 0.6340368923626719
Loss in iteration 27 : 0.5677057880997137
Loss in iteration 28 : 0.5529024798631149
Loss in iteration 29 : 0.5389867254612684
Loss in iteration 30 : 0.5342654501126601
Loss in iteration 31 : 0.5299654738523368
Loss in iteration 32 : 0.5288174956170264
Loss in iteration 33 : 0.5278048051398488
Loss in iteration 34 : 0.529378623126047
Loss in iteration 35 : 0.5311282369300336
Loss in iteration 36 : 0.5367828217581527
Loss in iteration 37 : 0.5411764929343592
Loss in iteration 38 : 0.5501685155314608
Loss in iteration 39 : 0.5497631109080905
Loss in iteration 40 : 0.5527834654040206
Loss in iteration 41 : 0.5429912230966825
Loss in iteration 42 : 0.5401548540736647
Loss in iteration 43 : 0.5311810329421656
Loss in iteration 44 : 0.5288301035344775
Loss in iteration 45 : 0.5239164394986447
Loss in iteration 46 : 0.523630938855295
Loss in iteration 47 : 0.5216060848427714
Loss in iteration 48 : 0.523067523567913
Loss in iteration 49 : 0.522433668906021
Loss in iteration 50 : 0.5250568022638813
Loss in iteration 51 : 0.5246784444988802
Loss in iteration 52 : 0.5276812309854443
Loss in iteration 53 : 0.525977684784114
Loss in iteration 54 : 0.5276766654889127
Loss in iteration 55 : 0.5240119300831432
Loss in iteration 56 : 0.5241692424602519
Loss in iteration 57 : 0.5200670595718095
Loss in iteration 58 : 0.5199285932356559
Loss in iteration 59 : 0.5167906694474523
Loss in iteration 60 : 0.5172887217197841
Loss in iteration 61 : 0.5153592700235187
Loss in iteration 62 : 0.516538350545989
Loss in iteration 63 : 0.5151700962469975
Loss in iteration 64 : 0.5165813729980935
Loss in iteration 65 : 0.5152355529781509
Loss in iteration 66 : 0.5167531914947053
Loss in iteration 67 : 0.5150943345717519
Loss in iteration 68 : 0.5163066545181725
Loss in iteration 69 : 0.5140974900761742
Loss in iteration 70 : 0.5148149844453389
Loss in iteration 71 : 0.512377513043488
Loss in iteration 72 : 0.5129092022274795
Loss in iteration 73 : 0.5107591703388331
Loss in iteration 74 : 0.5114922013202111
Loss in iteration 75 : 0.5098591141957768
Loss in iteration 76 : 0.5108364155635406
Loss in iteration 77 : 0.5094511667749729
Loss in iteration 78 : 0.5104125646277942
Loss in iteration 79 : 0.509018731040799
Loss in iteration 80 : 0.5100350503668534
Loss in iteration 81 : 0.5086015814751256
Loss in iteration 82 : 0.5095981132484062
Loss in iteration 83 : 0.5079891405494434
Loss in iteration 84 : 0.5088032438499926
Loss in iteration 85 : 0.5070609033071376
Loss in iteration 86 : 0.5077502140558369
Loss in iteration 87 : 0.5060991686872658
Loss in iteration 88 : 0.5068439614597194
Loss in iteration 89 : 0.5054559836276407
Loss in iteration 90 : 0.50629457796022
Loss in iteration 91 : 0.5050538896177375
Loss in iteration 92 : 0.5058254000491484
Loss in iteration 93 : 0.5045761114858386
Loss in iteration 94 : 0.5053815652178205
Loss in iteration 95 : 0.5041609741704526
Loss in iteration 96 : 0.5050342021700969
Loss in iteration 97 : 0.5037517988524475
Loss in iteration 98 : 0.5045517361919876
Loss in iteration 99 : 0.5031677592589604
Loss in iteration 100 : 0.5038675115752269
Loss in iteration 101 : 0.5025007495386561
Loss in iteration 102 : 0.50320144232675
Loss in iteration 103 : 0.5019968287770803
Loss in iteration 104 : 0.5027413764654797
Loss in iteration 105 : 0.5016531172329665
Loss in iteration 106 : 0.5023279613329493
Loss in iteration 107 : 0.5012347019643532
Loss in iteration 108 : 0.5019291706431744
Loss in iteration 109 : 0.5008770224253856
Loss in iteration 110 : 0.501667012815539
Loss in iteration 111 : 0.5005910432506843
Loss in iteration 112 : 0.5013531348924727
Loss in iteration 113 : 0.500185955346162
Loss in iteration 114 : 0.5008594646314107
Loss in iteration 115 : 0.49967491546544884
Loss in iteration 116 : 0.5003268736804174
Loss in iteration 117 : 0.49925867028285337
Loss in iteration 118 : 0.4999385471116916
Loss in iteration 119 : 0.49897838892457774
Loss in iteration 120 : 0.4995950320381135
Loss in iteration 121 : 0.4986321028856582
Loss in iteration 122 : 0.4992554266259588
Loss in iteration 123 : 0.4983313062733891
Loss in iteration 124 : 0.49906026550144666
Loss in iteration 125 : 0.49812738583611177
Loss in iteration 126 : 0.49885251775865647
Loss in iteration 127 : 0.4978316574224477
Loss in iteration 128 : 0.49847466968319937
Loss in iteration 129 : 0.49741616405550065
Loss in iteration 130 : 0.49802649006777916
Loss in iteration 131 : 0.4970599426162656
Loss in iteration 132 : 0.49769412924328493
Loss in iteration 133 : 0.49683612802870036
Loss in iteration 134 : 0.49741380276582925
Loss in iteration 135 : 0.4965539456890989
Loss in iteration 136 : 0.49712468077747807
Loss in iteration 137 : 0.4962978425092094
Loss in iteration 138 : 0.4969800982159134
Loss in iteration 139 : 0.4961530003452727
Loss in iteration 140 : 0.49684845277556616
Loss in iteration 141 : 0.4959338507657391
Loss in iteration 142 : 0.49655079232754
Loss in iteration 143 : 0.49558320314389936
Loss in iteration 144 : 0.49615974034371957
Loss in iteration 145 : 0.49526854614439764
Loss in iteration 146 : 0.4958694524156924
Loss in iteration 147 : 0.49509239023174484
Loss in iteration 148 : 0.49564303300831286
Loss in iteration 149 : 0.4948646977505671
Loss in iteration 150 : 0.4953924836763213
Loss in iteration 151 : 0.49464036882614376
Loss in iteration 152 : 0.4952854733271365
Loss in iteration 153 : 0.4945405871871311
Loss in iteration 154 : 0.49521446312719386
Loss in iteration 155 : 0.4943812988301952
Loss in iteration 156 : 0.4949782717609343
Loss in iteration 157 : 0.4940776551948378
Loss in iteration 158 : 0.4946265464245527
Loss in iteration 159 : 0.4937900752587803
Loss in iteration 160 : 0.49436583799773554
Loss in iteration 161 : 0.49365336819133165
Loss in iteration 162 : 0.4941861773771769
Loss in iteration 163 : 0.4934739505801963
Loss in iteration 164 : 0.4939638444997835
Loss in iteration 165 : 0.49326961970148214
Loss in iteration 166 : 0.49388340470318826
Loss in iteration 167 : 0.4932051635529519
Loss in iteration 168 : 0.49386485237253785
Loss in iteration 169 : 0.49309720420400543
Loss in iteration 170 : 0.4936802442945908
Loss in iteration 171 : 0.4928294846645878
Loss in iteration 172 : 0.493355036534009
Loss in iteration 173 : 0.49255627135951785
Loss in iteration 174 : 0.4931118804229275
Loss in iteration 175 : 0.4924511449714511
Loss in iteration 176 : 0.49297479642905506
Loss in iteration 177 : 0.49231874536334186
Loss in iteration 178 : 0.49277312355619646
Loss in iteration 179 : 0.4921235888854111
Loss in iteration 180 : 0.4927083606870148
Loss in iteration 181 : 0.49208614946575924
Loss in iteration 182 : 0.49273805526106684
Loss in iteration 183 : 0.49202604118849125
Loss in iteration 184 : 0.49260107256516106
Loss in iteration 185 : 0.49178782865667925
Loss in iteration 186 : 0.49229294696855025
Loss in iteration 187 : 0.4915169252287652
Loss in iteration 188 : 0.4920544888281759
Loss in iteration 189 : 0.49143418239544157
Loss in iteration 190 : 0.49195824194846466
Loss in iteration 191 : 0.491353749396139
Loss in iteration 192 : 0.49177381877092874
Loss in iteration 193 : 0.49115781618449283
Loss in iteration 194 : 0.49171179959332867
Loss in iteration 195 : 0.4911379917014981
Loss in iteration 196 : 0.49178748467991396
Loss in iteration 197 : 0.49112601075962686
Loss in iteration 198 : 0.49169942609139866
Loss in iteration 199 : 0.49091533383031805
Loss in iteration 200 : 0.4914021917902752
Testing accuracy  of updater 5 on alg 0 with rate 0.09999999999999998 = 0.7735, training accuracy 0.766875, time elapsed: 5404 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 3.0166175422620536
Loss in iteration 3 : 2.4686069150061822
Loss in iteration 4 : 1.839756115281386
Loss in iteration 5 : 1.26529374297366
Loss in iteration 6 : 1.4080173861407153
Loss in iteration 7 : 0.7223093056718862
Loss in iteration 8 : 1.392939909802436
Loss in iteration 9 : 1.1209836984496553
Loss in iteration 10 : 0.7978223405699179
Loss in iteration 11 : 1.1713136180632502
Loss in iteration 12 : 0.9406699714211951
Loss in iteration 13 : 0.6524642584034593
Loss in iteration 14 : 0.9278276752219855
Loss in iteration 15 : 0.8278419622838826
Loss in iteration 16 : 0.665976110721066
Loss in iteration 17 : 0.8685010221846596
Loss in iteration 18 : 0.8062859743013503
Loss in iteration 19 : 0.6641875453761638
Loss in iteration 20 : 0.7921104715473564
Loss in iteration 21 : 0.7407351539948374
Loss in iteration 22 : 0.5953801747093556
Loss in iteration 23 : 0.7035187050469115
Loss in iteration 24 : 0.6831287096403903
Loss in iteration 25 : 0.5912355107702942
Loss in iteration 26 : 0.6793436453779833
Loss in iteration 27 : 0.6404537141497467
Loss in iteration 28 : 0.5718385362773755
Loss in iteration 29 : 0.6250141272450315
Loss in iteration 30 : 0.56303396347972
Loss in iteration 31 : 0.5515256697411454
Loss in iteration 32 : 0.587075233935624
Loss in iteration 33 : 0.5273216252009595
Loss in iteration 34 : 0.5626451201095928
Loss in iteration 35 : 0.5415109423159122
Loss in iteration 36 : 0.5104608887367236
Loss in iteration 37 : 0.5390041482402654
Loss in iteration 38 : 0.49291919814323715
Loss in iteration 39 : 0.5210282347977014
Loss in iteration 40 : 0.5008740127146781
Loss in iteration 41 : 0.5083286232754226
Loss in iteration 42 : 0.49910413763096256
Loss in iteration 43 : 0.4920671974726937
Loss in iteration 44 : 0.49698671250837206
Loss in iteration 45 : 0.48489349573082385
Loss in iteration 46 : 0.4976511648924843
Loss in iteration 47 : 0.48539072671670197
Loss in iteration 48 : 0.4910590904879077
Loss in iteration 49 : 0.48128206027033665
Loss in iteration 50 : 0.4843995681354625
Loss in iteration 51 : 0.4811600392579486
Loss in iteration 52 : 0.4813002814141633
Loss in iteration 53 : 0.4805344083768848
Loss in iteration 54 : 0.47503100996449804
Loss in iteration 55 : 0.4772587205666604
Loss in iteration 56 : 0.47050241474504295
Loss in iteration 57 : 0.4765530278380029
Loss in iteration 58 : 0.4677780421509211
Loss in iteration 59 : 0.47277889088149455
Loss in iteration 60 : 0.4653373135549625
Loss in iteration 61 : 0.4682819248002347
Loss in iteration 62 : 0.46633116604149705
Loss in iteration 63 : 0.4649982405803992
Loss in iteration 64 : 0.466581555961949
Loss in iteration 65 : 0.46219945144201563
Loss in iteration 66 : 0.4653580998144513
Loss in iteration 67 : 0.4624500215370404
Loss in iteration 68 : 0.46382568761115506
Loss in iteration 69 : 0.4633408845333879
Loss in iteration 70 : 0.46166273977416566
Loss in iteration 71 : 0.4634262140712753
Loss in iteration 72 : 0.4615102347934744
Loss in iteration 73 : 0.46259983568374025
Loss in iteration 74 : 0.46212953181929767
Loss in iteration 75 : 0.46102911534815566
Loss in iteration 76 : 0.4620089342787354
Loss in iteration 77 : 0.46102242334947524
Loss in iteration 78 : 0.4610562721683208
Loss in iteration 79 : 0.4613990707402281
Loss in iteration 80 : 0.46036216268981167
Loss in iteration 81 : 0.46079722218037716
Loss in iteration 82 : 0.46098350174677494
Loss in iteration 83 : 0.4602571293740215
Loss in iteration 84 : 0.46075782437302054
Loss in iteration 85 : 0.46079382481769227
Loss in iteration 86 : 0.46066463038783306
Loss in iteration 87 : 0.461645294869763
Loss in iteration 88 : 0.4627305194053095
Loss in iteration 89 : 0.46404779217287834
Loss in iteration 90 : 0.4654968703167876
Loss in iteration 91 : 0.4639621941231178
Loss in iteration 92 : 0.46090243946943144
Loss in iteration 93 : 0.45990291120497634
Loss in iteration 94 : 0.4616906682829005
Loss in iteration 95 : 0.4629030789087291
Loss in iteration 96 : 0.4613612109135099
Loss in iteration 97 : 0.4597676351618601
Loss in iteration 98 : 0.46030994795565994
Loss in iteration 99 : 0.46142099078139676
Loss in iteration 100 : 0.4609419044883681
Loss in iteration 101 : 0.4596592671223084
Loss in iteration 102 : 0.45979951594955926
Loss in iteration 103 : 0.4607035510396869
Loss in iteration 104 : 0.4604447774640003
Loss in iteration 105 : 0.4595371553155166
Loss in iteration 106 : 0.4595962894696588
Loss in iteration 107 : 0.460210033333112
Loss in iteration 108 : 0.4600743632561992
Loss in iteration 109 : 0.45946778564801255
Loss in iteration 110 : 0.45945955339135325
Loss in iteration 111 : 0.45983158778866173
Loss in iteration 112 : 0.4598072506362189
Loss in iteration 113 : 0.45945736305128504
Loss in iteration 114 : 0.45935081875768413
Loss in iteration 115 : 0.4595278987463119
Loss in iteration 116 : 0.45960392960398044
Loss in iteration 117 : 0.45945923364266583
Loss in iteration 118 : 0.4593258281763945
Loss in iteration 119 : 0.45932305476766616
Loss in iteration 120 : 0.459382387205217
Loss in iteration 121 : 0.4593960504234311
Loss in iteration 122 : 0.4593513912950233
Loss in iteration 123 : 0.4592871679104847
Loss in iteration 124 : 0.45924105977424096
Loss in iteration 125 : 0.4592373118721639
Loss in iteration 126 : 0.4592681218034684
Loss in iteration 127 : 0.4592893717843183
Loss in iteration 128 : 0.4592631391509341
Loss in iteration 129 : 0.4591999749119791
Loss in iteration 130 : 0.4591487419411926
Loss in iteration 131 : 0.45914729435348967
Loss in iteration 132 : 0.4591832627812395
Loss in iteration 133 : 0.4592135723954277
Loss in iteration 134 : 0.45920516562639385
Loss in iteration 135 : 0.459163562204685
Loss in iteration 136 : 0.45911824057805045
Loss in iteration 137 : 0.4590955244716305
Loss in iteration 138 : 0.45909833374330594
Loss in iteration 139 : 0.4591135261963725
Loss in iteration 140 : 0.45912638339708534
Loss in iteration 141 : 0.459130538169798
Loss in iteration 142 : 0.459128150063279
Loss in iteration 143 : 0.45912282893890893
Loss in iteration 144 : 0.45911899642262727
Loss in iteration 145 : 0.459115675518534
Loss in iteration 146 : 0.4591131563419108
Loss in iteration 147 : 0.4591089749734709
Loss in iteration 148 : 0.45910448882498084
Loss in iteration 149 : 0.45910079873648524
Loss in iteration 150 : 0.45910130372915403
Loss in iteration 151 : 0.45910828187416114
Loss in iteration 152 : 0.45912780780259616
Loss in iteration 153 : 0.45916623294653663
Loss in iteration 154 : 0.45924214931270346
Loss in iteration 155 : 0.4593807674348469
Loss in iteration 156 : 0.4596559017156589
Loss in iteration 157 : 0.46016991959969505
Loss in iteration 158 : 0.4612440757938422
Loss in iteration 159 : 0.4632178352174425
Loss in iteration 160 : 0.4674153815035099
Loss in iteration 161 : 0.4739882306288575
Loss in iteration 162 : 0.48647446585722876
Loss in iteration 163 : 0.5008789196251776
Loss in iteration 164 : 0.5206212092359678
Loss in iteration 165 : 0.5453751273176968
Loss in iteration 166 : 0.5440897927693675
Loss in iteration 167 : 0.5251961761905238
Loss in iteration 168 : 0.4873046674605196
Loss in iteration 169 : 0.4652563544991007
Loss in iteration 170 : 0.46498487963781654
Loss in iteration 171 : 0.4786338860657057
Loss in iteration 172 : 0.4947528411042573
Loss in iteration 173 : 0.49947571054998036
Loss in iteration 174 : 0.48647849827589473
Loss in iteration 175 : 0.4698405756089437
Loss in iteration 176 : 0.4612442969609022
Loss in iteration 177 : 0.46299454090621567
Loss in iteration 178 : 0.4706307440915591
Loss in iteration 179 : 0.4786916201275582
Loss in iteration 180 : 0.48421271200679367
Loss in iteration 181 : 0.4862021333217654
Loss in iteration 182 : 0.48054666447521743
Loss in iteration 183 : 0.47377378571525874
Loss in iteration 184 : 0.46752909746002386
Loss in iteration 185 : 0.4631433946911776
Loss in iteration 186 : 0.4604315994457303
Loss in iteration 187 : 0.45930864720298065
Loss in iteration 188 : 0.45999121153759137
Loss in iteration 189 : 0.4620484445277261
Loss in iteration 190 : 0.46522218817332717
Loss in iteration 191 : 0.4711243340402332
Loss in iteration 192 : 0.4833080236503206
Loss in iteration 193 : 0.5040118515385084
Loss in iteration 194 : 0.5410596500653001
Loss in iteration 195 : 0.5666498063377912
Loss in iteration 196 : 0.5783450280674653
Loss in iteration 197 : 0.5299913711438138
Loss in iteration 198 : 0.4817782761219189
Loss in iteration 199 : 0.4616669274722297
Loss in iteration 200 : 0.47671009491727195
Testing accuracy  of updater 6 on alg 0 with rate 2.0 = 0.752, training accuracy 0.759375, time elapsed: 4919 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.681319483389277
Loss in iteration 3 : 0.6757019580859882
Loss in iteration 4 : 0.6580656819369965
Loss in iteration 5 : 0.6362264441265377
Loss in iteration 6 : 0.62367453698155
Loss in iteration 7 : 0.6089500000890661
Loss in iteration 8 : 0.5879037274530567
Loss in iteration 9 : 0.5721448937997963
Loss in iteration 10 : 0.5613653071362417
Loss in iteration 11 : 0.5484744282859431
Loss in iteration 12 : 0.5358395090720973
Loss in iteration 13 : 0.5281184581813753
Loss in iteration 14 : 0.52248434960533
Loss in iteration 15 : 0.5153235779156907
Loss in iteration 16 : 0.508642738315813
Loss in iteration 17 : 0.5046981830321858
Loss in iteration 18 : 0.5018182037823147
Loss in iteration 19 : 0.49810705384499315
Loss in iteration 20 : 0.49448482371792973
Loss in iteration 21 : 0.4923406287504905
Loss in iteration 22 : 0.49101836310210834
Loss in iteration 23 : 0.4892304942368123
Loss in iteration 24 : 0.4871732793100273
Loss in iteration 25 : 0.4857908037016042
Loss in iteration 26 : 0.4850459005230015
Loss in iteration 27 : 0.48413471691767473
Loss in iteration 28 : 0.48284753309538164
Loss in iteration 29 : 0.4817316899829302
Loss in iteration 30 : 0.4810773279137137
Loss in iteration 31 : 0.4805150428283757
Loss in iteration 32 : 0.4797053820788117
Loss in iteration 33 : 0.47884151657469515
Loss in iteration 34 : 0.47823823688443867
Loss in iteration 35 : 0.47781867373069586
Loss in iteration 36 : 0.4772953696003665
Loss in iteration 37 : 0.4766322487142849
Loss in iteration 38 : 0.476039397591858
Loss in iteration 39 : 0.47560245587601707
Loss in iteration 40 : 0.4751743166081313
Loss in iteration 41 : 0.4746411672498138
Loss in iteration 42 : 0.47408846594367343
Loss in iteration 43 : 0.473634183882683
Loss in iteration 44 : 0.47324531756042293
Loss in iteration 45 : 0.4728183879929381
Loss in iteration 46 : 0.4723507389256147
Loss in iteration 47 : 0.4719253601901262
Loss in iteration 48 : 0.47156759911142676
Loss in iteration 49 : 0.4712161935668732
Loss in iteration 50 : 0.4708322001464453
Loss in iteration 51 : 0.4704539036097095
Loss in iteration 52 : 0.4701234915057161
Loss in iteration 53 : 0.469821877232377
Loss in iteration 54 : 0.46951025967471754
Loss in iteration 55 : 0.4691942105434037
Loss in iteration 56 : 0.46890688468767283
Loss in iteration 57 : 0.46865202612683193
Loss in iteration 58 : 0.4684025953806362
Loss in iteration 59 : 0.46814736705016397
Loss in iteration 60 : 0.46790438019362157
Loss in iteration 61 : 0.4676862621851509
Loss in iteration 62 : 0.46748045200926325
Loss in iteration 63 : 0.46727267942998746
Loss in iteration 64 : 0.4670690170689267
Loss in iteration 65 : 0.4668820663863961
Loss in iteration 66 : 0.46670945331419944
Loss in iteration 67 : 0.4665399041100102
Loss in iteration 68 : 0.46637227369460027
Loss in iteration 69 : 0.4662149643943527
Loss in iteration 70 : 0.4660701751886239
Loss in iteration 71 : 0.4659308358643825
Loss in iteration 72 : 0.4657928645615552
Loss in iteration 73 : 0.46566042635118077
Loss in iteration 74 : 0.46553708970424873
Loss in iteration 75 : 0.46541965518372963
Loss in iteration 76 : 0.465304094985342
Loss in iteration 77 : 0.4651918045501716
Loss in iteration 78 : 0.465085956766222
Loss in iteration 79 : 0.4649856155137289
Loss in iteration 80 : 0.46488757091449656
Loss in iteration 81 : 0.4647915535083248
Loss in iteration 82 : 0.4646997491260446
Loss in iteration 83 : 0.46461241801992015
Loss in iteration 84 : 0.4645274580498478
Loss in iteration 85 : 0.46444393755449187
Loss in iteration 86 : 0.4643631304736351
Loss in iteration 87 : 0.4642858028879394
Loss in iteration 88 : 0.4642108492609341
Loss in iteration 89 : 0.46413725887225654
Loss in iteration 90 : 0.464065584214761
Loss in iteration 91 : 0.4639965878740236
Loss in iteration 92 : 0.4639297932720669
Loss in iteration 93 : 0.46386433305774244
Loss in iteration 94 : 0.4638002839616042
Loss in iteration 95 : 0.463738210656391
Loss in iteration 96 : 0.4636780068181629
Loss in iteration 97 : 0.46361907665901675
Loss in iteration 98 : 0.4635613062766689
Loss in iteration 99 : 0.463505066831322
Loss in iteration 100 : 0.46345041913218826
Loss in iteration 101 : 0.4633969852587608
Loss in iteration 102 : 0.46334457769635834
Loss in iteration 103 : 0.4632933994396418
Loss in iteration 104 : 0.46324355915108195
Loss in iteration 105 : 0.463194834971272
Loss in iteration 106 : 0.46314703681908964
Loss in iteration 107 : 0.46310025376449854
Loss in iteration 108 : 0.4630545942786458
Loss in iteration 109 : 0.46300994638005954
Loss in iteration 110 : 0.46296615499627464
Loss in iteration 111 : 0.46292324200571794
Loss in iteration 112 : 0.4628812911757797
Loss in iteration 113 : 0.46284025172826876
Loss in iteration 114 : 0.46280000442147806
Loss in iteration 115 : 0.46276053003957224
Loss in iteration 116 : 0.4627218784610205
Loss in iteration 117 : 0.4626840287796889
Loss in iteration 118 : 0.46264689738305614
Loss in iteration 119 : 0.4626104524533582
Loss in iteration 120 : 0.46257472089258284
Loss in iteration 121 : 0.4625396959696585
Loss in iteration 122 : 0.4625053209035989
Loss in iteration 123 : 0.4624715617949246
Loss in iteration 124 : 0.4624384287517731
Loss in iteration 125 : 0.46240591872929165
Loss in iteration 126 : 0.46237399228346404
Loss in iteration 127 : 0.4623426177193504
Loss in iteration 128 : 0.4623117945594976
Loss in iteration 129 : 0.46228152001663564
Loss in iteration 130 : 0.46225176671065576
Loss in iteration 131 : 0.46222250792263253
Loss in iteration 132 : 0.46219373808697084
Loss in iteration 133 : 0.46216545393810354
Loss in iteration 134 : 0.4621376359224099
Loss in iteration 135 : 0.4621102618137108
Loss in iteration 136 : 0.4620833230181562
Loss in iteration 137 : 0.4620568148790466
Loss in iteration 138 : 0.46203072263761125
Loss in iteration 139 : 0.4620050282618157
Loss in iteration 140 : 0.46197972245447533
Loss in iteration 141 : 0.46195479998798744
Loss in iteration 142 : 0.4619302495288138
Loss in iteration 143 : 0.46190605667041057
Loss in iteration 144 : 0.46188221240997857
Loss in iteration 145 : 0.4618587112113078
Loss in iteration 146 : 0.4618355439206164
Loss in iteration 147 : 0.46181269885735393
Loss in iteration 148 : 0.4617901676291343
Loss in iteration 149 : 0.46176794456573694
Loss in iteration 150 : 0.4617460219390615
Loss in iteration 151 : 0.4617243901760921
Loss in iteration 152 : 0.4617030417795211
Loss in iteration 153 : 0.4616819713319845
Loss in iteration 154 : 0.4616611722548523
Loss in iteration 155 : 0.4616406366221198
Loss in iteration 156 : 0.4616203577829761
Loss in iteration 157 : 0.4616003306073865
Loss in iteration 158 : 0.46158054932913567
Loss in iteration 159 : 0.4615610072487643
Loss in iteration 160 : 0.4615416984897535
Loss in iteration 161 : 0.4615226183049088
Loss in iteration 162 : 0.4615037616361996
Loss in iteration 163 : 0.4614851227983478
Loss in iteration 164 : 0.4614666966355943
Loss in iteration 165 : 0.4614484788079842
Loss in iteration 166 : 0.46143046484306055
Loss in iteration 167 : 0.46141264986166786
Loss in iteration 168 : 0.4613950293312356
Loss in iteration 169 : 0.46137759929647015
Loss in iteration 170 : 0.4613603557633891
Loss in iteration 171 : 0.46134329449555705
Loss in iteration 172 : 0.461326411507817
Loss in iteration 173 : 0.46130970323230314
Loss in iteration 174 : 0.4612931661107986
Loss in iteration 175 : 0.46127679644926683
Loss in iteration 176 : 0.4612605907421856
Loss in iteration 177 : 0.4612445457844205
Loss in iteration 178 : 0.46122865839906196
Loss in iteration 179 : 0.46121292534016933
Loss in iteration 180 : 0.4611973435105202
Loss in iteration 181 : 0.4611819100354898
Loss in iteration 182 : 0.4611666220795436
Loss in iteration 183 : 0.461151476781969
Loss in iteration 184 : 0.4611364714026067
Loss in iteration 185 : 0.4611216033685896
Loss in iteration 186 : 0.4611068701504778
Loss in iteration 187 : 0.46109226922085195
Loss in iteration 188 : 0.4610777981504283
Loss in iteration 189 : 0.4610634546350412
Loss in iteration 190 : 0.4610492364129316
Loss in iteration 191 : 0.46103514124128475
Loss in iteration 192 : 0.4610211669605203
Loss in iteration 193 : 0.4610073115079429
Loss in iteration 194 : 0.46099357286180737
Loss in iteration 195 : 0.460979949028927
Loss in iteration 196 : 0.4609664380870055
Loss in iteration 197 : 0.4609530381892824
Loss in iteration 198 : 0.46093974752654787
Loss in iteration 199 : 0.46092656432211554
Loss in iteration 200 : 0.4609134868601171
Testing accuracy  of updater 6 on alg 0 with rate 0.2 = 0.7855, training accuracy 0.788625, time elapsed: 5990 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6908889318572128
Loss in iteration 3 : 0.6871471061954912
Loss in iteration 4 : 0.6827850254251293
Loss in iteration 5 : 0.6784444536942126
Loss in iteration 6 : 0.6743762662972105
Loss in iteration 7 : 0.6704775067275601
Loss in iteration 8 : 0.6664693342888737
Loss in iteration 9 : 0.6620991652342475
Loss in iteration 10 : 0.6572728594922415
Loss in iteration 11 : 0.6520817742786067
Loss in iteration 12 : 0.6467422560854221
Loss in iteration 13 : 0.6414935012111006
Loss in iteration 14 : 0.6365040114312962
Loss in iteration 15 : 0.6318230826286915
Loss in iteration 16 : 0.6273892483191128
Loss in iteration 17 : 0.6230820727630881
Loss in iteration 18 : 0.6187876893800268
Loss in iteration 19 : 0.6144480384830208
Loss in iteration 20 : 0.6100766485786366
Loss in iteration 21 : 0.6057415601416921
Loss in iteration 22 : 0.6015295838132302
Loss in iteration 23 : 0.5975105382901151
Loss in iteration 24 : 0.593715737163984
Loss in iteration 25 : 0.5901358405866031
Loss in iteration 26 : 0.5867342876436694
Loss in iteration 27 : 0.5834674358384422
Loss in iteration 28 : 0.5803022246963981
Loss in iteration 29 : 0.5772254160918526
Loss in iteration 30 : 0.5742430299849428
Loss in iteration 31 : 0.5713724819510112
Loss in iteration 32 : 0.5686319724220532
Loss in iteration 33 : 0.5660316468133239
Loss in iteration 34 : 0.5635694333795463
Loss in iteration 35 : 0.5612321641115902
Loss in iteration 36 : 0.5590005374637445
Loss in iteration 37 : 0.5568553711596781
Loss in iteration 38 : 0.5547826386827042
Loss in iteration 39 : 0.552775736081676
Loss in iteration 40 : 0.5508347472834095
Loss in iteration 41 : 0.54896360052131
Loss in iteration 42 : 0.5471665666190555
Loss in iteration 43 : 0.5454454654044398
Loss in iteration 44 : 0.5437983960656574
Loss in iteration 45 : 0.5422200942192879
Loss in iteration 46 : 0.540703431719594
Loss in iteration 47 : 0.5392412894932357
Loss in iteration 48 : 0.5378280767780264
Loss in iteration 49 : 0.5364604507042849
Loss in iteration 50 : 0.5351371587611379
Loss in iteration 51 : 0.533858240078571
Loss in iteration 52 : 0.532623987799293
Loss in iteration 53 : 0.531434071114625
Loss in iteration 54 : 0.5302870793592473
Loss in iteration 55 : 0.5291805548502496
Loss in iteration 56 : 0.5281114033445128
Loss in iteration 57 : 0.5270764668031268
Loss in iteration 58 : 0.5260730324995236
Loss in iteration 59 : 0.5250991198938454
Loss in iteration 60 : 0.5241534934874964
Loss in iteration 61 : 0.5232354520580699
Loss in iteration 62 : 0.5223445089092954
Loss in iteration 63 : 0.5214800904539767
Loss in iteration 64 : 0.5206413481450117
Loss in iteration 65 : 0.5198271216811422
Loss in iteration 66 : 0.5190360336722647
Loss in iteration 67 : 0.5182666567396383
Loss in iteration 68 : 0.5175176823303568
Loss in iteration 69 : 0.5167880343981
Loss in iteration 70 : 0.5160769006810991
Loss in iteration 71 : 0.5153836867203586
Loss in iteration 72 : 0.5147079219166901
Loss in iteration 73 : 0.5140491567002664
Loss in iteration 74 : 0.5134068850455243
Loss in iteration 75 : 0.5127805116197368
Loss in iteration 76 : 0.5121693646542095
Loss in iteration 77 : 0.5115727406938574
Loss in iteration 78 : 0.5109899598799138
Loss in iteration 79 : 0.5104204112930218
Loss in iteration 80 : 0.5098635751834596
Loss in iteration 81 : 0.5093190190097443
Loss in iteration 82 : 0.5087863732866884
Loss in iteration 83 : 0.508265298587639
Loss in iteration 84 : 0.5077554556794788
Loss in iteration 85 : 0.5072564874294824
Loss in iteration 86 : 0.5067680156743836
Loss in iteration 87 : 0.5062896508494901
Loss in iteration 88 : 0.5058210085512128
Loss in iteration 89 : 0.5053617261538995
Loss in iteration 90 : 0.504911473944887
Loss in iteration 91 : 0.504469958078018
Loss in iteration 92 : 0.5040369157798683
Loss in iteration 93 : 0.5036121056229362
Loss in iteration 94 : 0.5031952967016764
Loss in iteration 95 : 0.5027862601549834
Loss in iteration 96 : 0.5023847650743658
Loss in iteration 97 : 0.5019905790626749
Loss in iteration 98 : 0.5016034721965886
Loss in iteration 99 : 0.5012232223227466
Loss in iteration 100 : 0.5008496196077541
Loss in iteration 101 : 0.5004824689101545
Loss in iteration 102 : 0.5001215895151978
Loss in iteration 103 : 0.4997668127006367
Loss in iteration 104 : 0.49941797820062084
Loss in iteration 105 : 0.49907493078065096
Loss in iteration 106 : 0.4987375178722503
Loss in iteration 107 : 0.49840558870482576
Loss in iteration 108 : 0.49807899482316925
Loss in iteration 109 : 0.4977575914719246
Loss in iteration 110 : 0.49744123916330246
Loss in iteration 111 : 0.4971298048255658
Loss in iteration 112 : 0.49682316218389894
Loss in iteration 113 : 0.49652119133858813
Loss in iteration 114 : 0.49622377776833154
Loss in iteration 115 : 0.4959308111263171
Loss in iteration 116 : 0.49564218419370776
Loss in iteration 117 : 0.49535779223802917
Loss in iteration 118 : 0.4950775328516238
Loss in iteration 119 : 0.49480130618250456
Loss in iteration 120 : 0.49452901536587185
Loss in iteration 121 : 0.4942605669404116
Loss in iteration 122 : 0.4939958710815551
Loss in iteration 123 : 0.49373484157498226
Loss in iteration 124 : 0.49347739555120024
Loss in iteration 125 : 0.49322345307410836
Loss in iteration 126 : 0.49297293670556874
Loss in iteration 127 : 0.492725771153316
Loss in iteration 128 : 0.4924818830641435
Loss in iteration 129 : 0.4922412009683166
Loss in iteration 130 : 0.4920036553342019
Loss in iteration 131 : 0.4917691786669834
Loss in iteration 132 : 0.491537705585855
Loss in iteration 133 : 0.49130917283490205
Loss in iteration 134 : 0.4910835192137547
Loss in iteration 135 : 0.49086068544322176
Loss in iteration 136 : 0.4906406139998275
Loss in iteration 137 : 0.4904232489575176
Loss in iteration 138 : 0.49020853586622365
Loss in iteration 139 : 0.4899964216805849
Loss in iteration 140 : 0.4897868547346725
Loss in iteration 141 : 0.4895797847455532
Loss in iteration 142 : 0.4893751628234161
Loss in iteration 143 : 0.48917294146871665
Loss in iteration 144 : 0.48897307454510885
Loss in iteration 145 : 0.4887755172271903
Loss in iteration 146 : 0.48858022593065575
Loss in iteration 147 : 0.48838715823703593
Loss in iteration 148 : 0.48819627282506467
Loss in iteration 149 : 0.4880075294168581
Loss in iteration 150 : 0.4878208887413705
Loss in iteration 151 : 0.48763631251215633
Loss in iteration 152 : 0.4874537634130381
Loss in iteration 153 : 0.4872732050844131
Loss in iteration 154 : 0.48709460210454303
Loss in iteration 155 : 0.4869179199632325
Loss in iteration 156 : 0.4867431250285181
Loss in iteration 157 : 0.4865701845094968
Loss in iteration 158 : 0.4863990664193411
Loss in iteration 159 : 0.48622973954209275
Loss in iteration 160 : 0.48606217340526375
Loss in iteration 161 : 0.48589633825837814
Loss in iteration 162 : 0.4857322050559672
Loss in iteration 163 : 0.48556974544268106
Loss in iteration 164 : 0.48540893173818006
Loss in iteration 165 : 0.48524973692023254
Loss in iteration 166 : 0.4850921346054869
Loss in iteration 167 : 0.4849360990284416
Loss in iteration 168 : 0.48478160501980055
Loss in iteration 169 : 0.48462862798554324
Loss in iteration 170 : 0.4844771438877901
Loss in iteration 171 : 0.48432712922793014
Loss in iteration 172 : 0.48417856103188267
Loss in iteration 173 : 0.4840314168368909
Loss in iteration 174 : 0.48388567467904847
Loss in iteration 175 : 0.48374131308085466
Loss in iteration 176 : 0.4835983110383618
Loss in iteration 177 : 0.48345664800785837
Loss in iteration 178 : 0.48331630389232116
Loss in iteration 179 : 0.4831772590280309
Loss in iteration 180 : 0.48303949417177494
Loss in iteration 181 : 0.4829029904888881
Loss in iteration 182 : 0.48276772954221925
Loss in iteration 183 : 0.4826336932818991
Loss in iteration 184 : 0.4825008640356775
Loss in iteration 185 : 0.4823692244995535
Loss in iteration 186 : 0.482238757728497
Loss in iteration 187 : 0.48210944712714715
Loss in iteration 188 : 0.48198127644051636
Loss in iteration 189 : 0.4818542297447886
Loss in iteration 190 : 0.4817282914383564
Loss in iteration 191 : 0.48160344623321244
Loss in iteration 192 : 0.48147967914676826
Loss in iteration 193 : 0.4813569754940906
Loss in iteration 194 : 0.4812353208805082
Loss in iteration 195 : 0.48111470119449046
Loss in iteration 196 : 0.4809951026007201
Loss in iteration 197 : 0.48087651153330185
Loss in iteration 198 : 0.48075891468906573
Loss in iteration 199 : 0.48064229902100725
Loss in iteration 200 : 0.48052665173187026
Testing accuracy  of updater 6 on alg 0 with rate 0.01999999999999999 = 0.7835, training accuracy 0.781875, time elapsed: 6056 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 20.902439943570485
Loss in iteration 3 : 12.517217681051939
Loss in iteration 4 : 12.194135457128466
Loss in iteration 5 : 6.689634168295111
Loss in iteration 6 : 7.6721489556689235
Loss in iteration 7 : 2.9876194179866515
Loss in iteration 8 : 6.7080881402748185
Loss in iteration 9 : 5.701037920375063
Loss in iteration 10 : 5.415723511937644
Loss in iteration 11 : 5.344570548341498
Loss in iteration 12 : 3.5911615065830085
Loss in iteration 13 : 3.727330084781799
Loss in iteration 14 : 4.357274171408016
Loss in iteration 15 : 3.7578721334698733
Loss in iteration 16 : 3.4722586201249404
Loss in iteration 17 : 3.442361411770051
Loss in iteration 18 : 3.2277178660685326
Loss in iteration 19 : 3.474263329052351
Loss in iteration 20 : 3.6025759322950943
Loss in iteration 21 : 3.031646441822655
Loss in iteration 22 : 2.4875756558645015
Loss in iteration 23 : 2.583220504061085
Loss in iteration 24 : 2.5967153893571377
Loss in iteration 25 : 2.5996784612260297
Loss in iteration 26 : 2.6529127957189105
Loss in iteration 27 : 2.269139624166022
Loss in iteration 28 : 2.172436660311636
Loss in iteration 29 : 2.1653741279886582
Loss in iteration 30 : 1.9281047838589471
Loss in iteration 31 : 1.9303509436778026
Loss in iteration 32 : 1.7322635755721623
Loss in iteration 33 : 1.760736596852741
Loss in iteration 34 : 1.732396943023086
Loss in iteration 35 : 1.6080456779959194
Loss in iteration 36 : 1.5851269284250127
Loss in iteration 37 : 1.4452720524539553
Loss in iteration 38 : 1.4200118073928651
Loss in iteration 39 : 1.3167431228563469
Loss in iteration 40 : 1.3867726880473423
Loss in iteration 41 : 1.3099323016930717
Loss in iteration 42 : 1.2495815452936385
Loss in iteration 43 : 1.1697296380618032
Loss in iteration 44 : 1.1675372948839116
Loss in iteration 45 : 1.1683680037844706
Loss in iteration 46 : 1.0475345291027816
Loss in iteration 47 : 1.089495866239854
Loss in iteration 48 : 1.038837215002268
Loss in iteration 49 : 0.9638454012953582
Loss in iteration 50 : 0.9479003879314462
Loss in iteration 51 : 0.9554563399452377
Loss in iteration 52 : 0.92424418203294
Loss in iteration 53 : 0.8573120597244169
Loss in iteration 54 : 0.8247409506800185
Loss in iteration 55 : 0.8005714799016797
Loss in iteration 56 : 0.7376934159102666
Loss in iteration 57 : 0.7221141220825766
Loss in iteration 58 : 0.6915448821231622
Loss in iteration 59 : 0.6253458107613902
Loss in iteration 60 : 0.6421145455262309
Loss in iteration 61 : 0.6291511466709239
Loss in iteration 62 : 0.7496253075581701
Loss in iteration 63 : 1.670461163594957
Loss in iteration 64 : 3.358499936853105
Loss in iteration 65 : 0.9142584208747632
Loss in iteration 66 : 1.2768032365383706
Loss in iteration 67 : 2.4463956685246977
Loss in iteration 68 : 0.793799493866517
Loss in iteration 69 : 1.4624992858636068
Loss in iteration 70 : 1.6372053860321452
Loss in iteration 71 : 0.8863566073485933
Loss in iteration 72 : 1.6925698511932823
Loss in iteration 73 : 1.0252796077690296
Loss in iteration 74 : 1.1519731970705924
Loss in iteration 75 : 1.3023346932350948
Loss in iteration 76 : 0.8285261031508789
Loss in iteration 77 : 1.167515707224758
Loss in iteration 78 : 0.8983001881238994
Loss in iteration 79 : 0.800838275754864
Loss in iteration 80 : 1.054064536041226
Loss in iteration 81 : 0.819370888214195
Loss in iteration 82 : 0.6880921888195918
Loss in iteration 83 : 0.8582453884657133
Loss in iteration 84 : 0.8599372072462502
Loss in iteration 85 : 0.7527240631142992
Loss in iteration 86 : 0.6583496299729104
Loss in iteration 87 : 0.5819799404520879
Loss in iteration 88 : 0.5919833472640444
Loss in iteration 89 : 0.5737233406685537
Loss in iteration 90 : 0.6973170481582701
Loss in iteration 91 : 1.2641989610192754
Loss in iteration 92 : 2.579362265921127
Loss in iteration 93 : 1.834668094678032
Loss in iteration 94 : 0.5761139012887808
Loss in iteration 95 : 1.103126351318086
Loss in iteration 96 : 1.9589890425462364
Loss in iteration 97 : 0.9953288556549776
Loss in iteration 98 : 0.7916959999594801
Loss in iteration 99 : 0.9267630081527861
Loss in iteration 100 : 1.1073695744646341
Loss in iteration 101 : 0.9107166551415549
Loss in iteration 102 : 0.6621463280105235
Loss in iteration 103 : 0.8507593242362305
Loss in iteration 104 : 0.9066969308459484
Loss in iteration 105 : 0.724361671495895
Loss in iteration 106 : 0.8461176807635991
Loss in iteration 107 : 0.62344063237104
Loss in iteration 108 : 0.7494277715263692
Loss in iteration 109 : 0.6718157682059129
Loss in iteration 110 : 0.6603502843657381
Loss in iteration 111 : 0.7187100586250333
Loss in iteration 112 : 0.6751762693913456
Loss in iteration 113 : 0.8405246283123617
Loss in iteration 114 : 0.942411208788584
Loss in iteration 115 : 1.1495912959359667
Loss in iteration 116 : 1.6147991684569156
Loss in iteration 117 : 0.874542558504044
Loss in iteration 118 : 1.0994300686347864
Loss in iteration 119 : 0.8050017647562422
Loss in iteration 120 : 0.9686046275026061
Loss in iteration 121 : 1.0362656469057907
Loss in iteration 122 : 1.2271784706345028
Loss in iteration 123 : 1.283902824843944
Loss in iteration 124 : 1.2088924262606604
Loss in iteration 125 : 0.7070888520808848
Loss in iteration 126 : 0.753652858622215
Loss in iteration 127 : 0.6172881340878149
Loss in iteration 128 : 0.639888815416308
Loss in iteration 129 : 0.6596425943558981
Loss in iteration 130 : 0.5896858656024689
Loss in iteration 131 : 0.7430754226812635
Loss in iteration 132 : 0.8672115397956461
Loss in iteration 133 : 1.5820561498029193
Loss in iteration 134 : 1.6016789052938825
Loss in iteration 135 : 1.0119764870881676
Loss in iteration 136 : 0.71452230003222
Loss in iteration 137 : 0.6269770698797652
Loss in iteration 138 : 0.5895569438237624
Loss in iteration 139 : 0.6411606168487242
Loss in iteration 140 : 0.6750734262648259
Loss in iteration 141 : 0.949124889799943
Loss in iteration 142 : 1.5857058103171657
Loss in iteration 143 : 1.7623550126181682
Loss in iteration 144 : 1.1988012257916936
Loss in iteration 145 : 0.7384402965957034
Loss in iteration 146 : 0.6672153152770575
Loss in iteration 147 : 0.6172623239176405
Loss in iteration 148 : 0.6267631128121438
Loss in iteration 149 : 0.6342501732446406
Loss in iteration 150 : 0.6688437036024486
Loss in iteration 151 : 0.8459465878897077
Loss in iteration 152 : 1.3065817193897031
Loss in iteration 153 : 1.8186286774105558
Loss in iteration 154 : 1.2844172510900849
Loss in iteration 155 : 0.794480362392386
Loss in iteration 156 : 0.6674895655104396
Loss in iteration 157 : 0.6030752678048713
Loss in iteration 158 : 0.5974906999572157
Loss in iteration 159 : 0.5743226321623185
Loss in iteration 160 : 0.5609179229251899
Loss in iteration 161 : 0.5598790611260799
Loss in iteration 162 : 0.6038657023818714
Loss in iteration 163 : 0.97197821493422
Loss in iteration 164 : 2.3423744533180297
Loss in iteration 165 : 1.8161598398532328
Loss in iteration 166 : 0.827420158685131
Loss in iteration 167 : 0.5848071230635338
Loss in iteration 168 : 0.5699156694543339
Loss in iteration 169 : 0.6325605933202222
Loss in iteration 170 : 0.7854134207218582
Loss in iteration 171 : 1.0280714106503752
Loss in iteration 172 : 1.232593430844032
Loss in iteration 173 : 1.1453265745994914
Loss in iteration 174 : 0.9337096974252509
Loss in iteration 175 : 0.7920670533039301
Loss in iteration 176 : 0.7291589843673755
Loss in iteration 177 : 0.745989389412407
Loss in iteration 178 : 0.8199353432738438
Loss in iteration 179 : 0.9507935784680265
Loss in iteration 180 : 1.0775735976752427
Loss in iteration 181 : 1.0587471129635133
Loss in iteration 182 : 0.9296083868173685
Loss in iteration 183 : 0.8000226805884532
Loss in iteration 184 : 0.699555247649559
Loss in iteration 185 : 0.6612420889751247
Loss in iteration 186 : 0.6461950775114533
Loss in iteration 187 : 0.7007487912723621
Loss in iteration 188 : 0.8973367908373165
Loss in iteration 189 : 1.4371520119702375
Loss in iteration 190 : 1.761251736842483
Loss in iteration 191 : 1.212883648132269
Loss in iteration 192 : 0.7863709209192886
Loss in iteration 193 : 0.6529804589205846
Loss in iteration 194 : 0.6026846429097003
Loss in iteration 195 : 0.5903592110088542
Loss in iteration 196 : 0.5715868751027422
Loss in iteration 197 : 0.5663891326759943
Loss in iteration 198 : 0.5790681312430505
Loss in iteration 199 : 0.7171197977709547
Loss in iteration 200 : 1.3778495637705004
Testing accuracy  of updater 7 on alg 0 with rate 20.0 = 0.471, training accuracy 0.481625, time elapsed: 5722 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.8192825517699764
Loss in iteration 3 : 0.7571814716075921
Loss in iteration 4 : 0.6600690523926501
Loss in iteration 5 : 0.7324455467946965
Loss in iteration 6 : 0.5897626197835978
Loss in iteration 7 : 0.6336769668923246
Loss in iteration 8 : 0.5966396051410779
Loss in iteration 9 : 0.533275879520251
Loss in iteration 10 : 0.5759170417112177
Loss in iteration 11 : 0.526698911875692
Loss in iteration 12 : 0.5091695600065103
Loss in iteration 13 : 0.5345868866271262
Loss in iteration 14 : 0.5037337442071571
Loss in iteration 15 : 0.4930869256903392
Loss in iteration 16 : 0.5131786534555846
Loss in iteration 17 : 0.49624258930559706
Loss in iteration 18 : 0.48476638550148665
Loss in iteration 19 : 0.49931176499814145
Loss in iteration 20 : 0.494344878920664
Loss in iteration 21 : 0.481714669517787
Loss in iteration 22 : 0.4893423821941716
Loss in iteration 23 : 0.4919729732775208
Loss in iteration 24 : 0.4814615715962523
Loss in iteration 25 : 0.4819379651334104
Loss in iteration 26 : 0.48725525180408885
Loss in iteration 27 : 0.4814641180644524
Loss in iteration 28 : 0.4772957667414388
Loss in iteration 29 : 0.4814032613078781
Loss in iteration 30 : 0.4802254715142195
Loss in iteration 31 : 0.47501766201517315
Loss in iteration 32 : 0.47611109620972875
Loss in iteration 33 : 0.4773976406367665
Loss in iteration 34 : 0.47351190187605546
Loss in iteration 35 : 0.4720614876955706
Loss in iteration 36 : 0.4736931992211218
Loss in iteration 37 : 0.47185791925127035
Loss in iteration 38 : 0.469386141517439
Loss in iteration 39 : 0.4701877668366429
Loss in iteration 40 : 0.4699237780250244
Loss in iteration 41 : 0.46767670782769233
Loss in iteration 42 : 0.46747774989430285
Loss in iteration 43 : 0.4679121112615414
Loss in iteration 44 : 0.4664392305108052
Loss in iteration 45 : 0.46563556780277837
Loss in iteration 46 : 0.4661387888694791
Loss in iteration 47 : 0.46548197512482725
Loss in iteration 48 : 0.4645394739225419
Loss in iteration 49 : 0.4648205566271749
Loss in iteration 50 : 0.4647149344584416
Loss in iteration 51 : 0.46392539136931477
Loss in iteration 52 : 0.4639376002917425
Loss in iteration 53 : 0.4640858730277283
Loss in iteration 54 : 0.46355535609813703
Loss in iteration 55 : 0.4633831031620194
Loss in iteration 56 : 0.4635911973900211
Loss in iteration 57 : 0.4633101414964685
Loss in iteration 58 : 0.4630644539579631
Loss in iteration 59 : 0.46322299740648554
Loss in iteration 60 : 0.46311712277141653
Loss in iteration 61 : 0.46287083255007905
Loss in iteration 62 : 0.46294559978748273
Loss in iteration 63 : 0.4629369733743091
Loss in iteration 64 : 0.4627311560837568
Loss in iteration 65 : 0.4627311962147141
Loss in iteration 66 : 0.46276183590266273
Loss in iteration 67 : 0.4626100818395766
Loss in iteration 68 : 0.4625576536632795
Loss in iteration 69 : 0.4625885502216015
Loss in iteration 70 : 0.4624833738165885
Loss in iteration 71 : 0.46240239703492103
Loss in iteration 72 : 0.46241608965162734
Loss in iteration 73 : 0.4623474021256391
Loss in iteration 74 : 0.46225876650491227
Loss in iteration 75 : 0.46225217581145506
Loss in iteration 76 : 0.46220839530420277
Loss in iteration 77 : 0.4621255647403978
Loss in iteration 78 : 0.4621016353711647
Loss in iteration 79 : 0.4620716981443166
Loss in iteration 80 : 0.46200057070254147
Loss in iteration 81 : 0.46196500666887197
Loss in iteration 82 : 0.4619416953908395
Loss in iteration 83 : 0.46188433884802566
Loss in iteration 84 : 0.46184337519927005
Loss in iteration 85 : 0.4618218990493631
Loss in iteration 86 : 0.4617768456071019
Loss in iteration 87 : 0.4617349511893753
Loss in iteration 88 : 0.4617127580663956
Loss in iteration 89 : 0.46167714248921576
Loss in iteration 90 : 0.46163729880084364
Loss in iteration 91 : 0.4616137435811547
Loss in iteration 92 : 0.46158486547364286
Loss in iteration 93 : 0.46154874493381787
Loss in iteration 94 : 0.4615240648200945
Loss in iteration 95 : 0.4614993265288934
Loss in iteration 96 : 0.4614671382351633
Loss in iteration 97 : 0.46144179496244503
Loss in iteration 98 : 0.46141924847174753
Loss in iteration 99 : 0.46139058686874435
Loss in iteration 100 : 0.4613651883615335
Loss in iteration 101 : 0.461343669771367
Loss in iteration 102 : 0.46131789941738127
Loss in iteration 103 : 0.4612929760603897
Loss in iteration 104 : 0.4612718428113207
Loss in iteration 105 : 0.46124822524751563
Loss in iteration 106 : 0.461224098857184
Loss in iteration 107 : 0.46120313121322976
Loss in iteration 108 : 0.4611810997708438
Loss in iteration 109 : 0.4611579959446548
Loss in iteration 110 : 0.4611372384448187
Loss in iteration 111 : 0.46111638669025856
Loss in iteration 112 : 0.46109438741185954
Loss in iteration 113 : 0.4610739560663091
Loss in iteration 114 : 0.461053985309735
Loss in iteration 115 : 0.46103305164996256
Loss in iteration 116 : 0.4610130716354327
Loss in iteration 117 : 0.46099381275069873
Loss in iteration 118 : 0.46097387987495836
Loss in iteration 119 : 0.46095446530479284
Loss in iteration 120 : 0.4609358275149555
Loss in iteration 121 : 0.46091680041670585
Loss in iteration 122 : 0.4608980153724367
Loss in iteration 123 : 0.4608799459689765
Loss in iteration 124 : 0.4608617206081462
Loss in iteration 125 : 0.4608435858636699
Loss in iteration 126 : 0.46082605870187276
Loss in iteration 127 : 0.46080854655065534
Loss in iteration 128 : 0.46079105708159496
Loss in iteration 129 : 0.4607740534657578
Loss in iteration 130 : 0.46075717198941213
Loss in iteration 131 : 0.460740295818435
Loss in iteration 132 : 0.4607237959950683
Loss in iteration 133 : 0.46070747755532665
Loss in iteration 134 : 0.46069117483137395
Loss in iteration 135 : 0.4606751612319878
Loss in iteration 136 : 0.460659355670346
Loss in iteration 137 : 0.4606435885529434
Loss in iteration 138 : 0.46062804548902964
Loss in iteration 139 : 0.4606127142836196
Loss in iteration 140 : 0.4605974453246202
Loss in iteration 141 : 0.46058235542608866
Loss in iteration 142 : 0.46056746967327855
Loss in iteration 143 : 0.46055266728465716
Loss in iteration 144 : 0.460538014503392
Loss in iteration 145 : 0.460523552395221
Loss in iteration 146 : 0.4605091897405092
Loss in iteration 147 : 0.46049495831294074
Loss in iteration 148 : 0.4604809020976501
Loss in iteration 149 : 0.46046695585157715
Loss in iteration 150 : 0.4604531298576011
Loss in iteration 151 : 0.4604394645067438
Loss in iteration 152 : 0.46042591542725336
Loss in iteration 153 : 0.4604124803616029
Loss in iteration 154 : 0.4603991929422572
Loss in iteration 155 : 0.4603860244265318
Loss in iteration 156 : 0.4603729662507159
Loss in iteration 157 : 0.4603600446331841
Loss in iteration 158 : 0.4603472418627986
Loss in iteration 159 : 0.4603345469641855
Loss in iteration 160 : 0.46032197942791436
Loss in iteration 161 : 0.46030952906075123
Loss in iteration 162 : 0.46029718470809017
Loss in iteration 163 : 0.46028496014225323
Loss in iteration 164 : 0.4602728499406974
Loss in iteration 165 : 0.46026084394378364
Loss in iteration 166 : 0.46024895148938605
Loss in iteration 167 : 0.4602371700715298
Loss in iteration 168 : 0.46022549099955323
Loss in iteration 169 : 0.4602139202994052
Loss in iteration 170 : 0.46020245711928454
Loss in iteration 171 : 0.46019109431196253
Loss in iteration 172 : 0.46017983550952746
Loss in iteration 173 : 0.46016868068038785
Loss in iteration 174 : 0.4601576240914562
Loss in iteration 175 : 0.46014666773133833
Loss in iteration 176 : 0.46013581191296204
Loss in iteration 177 : 0.46012505210614657
Loss in iteration 178 : 0.4601143891992036
Loss in iteration 179 : 0.46010382357513036
Loss in iteration 180 : 0.4600933516817652
Loss in iteration 181 : 0.4600829736956776
Loss in iteration 182 : 0.46007268991752515
Loss in iteration 183 : 0.4600624975523861
Loss in iteration 184 : 0.4600523963404786
Loss in iteration 185 : 0.46004238645227646
Loss in iteration 186 : 0.4600324656748915
Loss in iteration 187 : 0.4600226334891488
Loss in iteration 188 : 0.46001288991356787
Loss in iteration 189 : 0.46000323316619646
Loss in iteration 190 : 0.45999366258619784
Loss in iteration 191 : 0.45998417805262976
Loss in iteration 192 : 0.4599747781022576
Loss in iteration 193 : 0.4599654620069978
Loss in iteration 194 : 0.4599562295284219
Loss in iteration 195 : 0.45994707943738283
Loss in iteration 196 : 0.45993801098285153
Loss in iteration 197 : 0.45992902383212303
Loss in iteration 198 : 0.45992011692745194
Loss in iteration 199 : 0.45991128952098015
Loss in iteration 200 : 0.4599025412079141
Testing accuracy  of updater 7 on alg 0 with rate 2.0 = 0.787, training accuracy 0.789125, time elapsed: 4591 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6896969666077134
Loss in iteration 3 : 0.6843853708816384
Loss in iteration 4 : 0.6788270674875401
Loss in iteration 5 : 0.6737041904115787
Loss in iteration 6 : 0.6686713722673672
Loss in iteration 7 : 0.6630027134962417
Loss in iteration 8 : 0.6563157259000117
Loss in iteration 9 : 0.648857412557121
Loss in iteration 10 : 0.6412853457943221
Loss in iteration 11 : 0.6342048948958786
Loss in iteration 12 : 0.6278140128084514
Loss in iteration 13 : 0.6218751456727419
Loss in iteration 14 : 0.6159777656033848
Loss in iteration 15 : 0.6098601711723618
Loss in iteration 16 : 0.6035630241086638
Loss in iteration 17 : 0.5973506244577363
Loss in iteration 18 : 0.5915056097770555
Loss in iteration 19 : 0.5861594887489393
Loss in iteration 20 : 0.5812562653828401
Loss in iteration 21 : 0.5766364190654231
Loss in iteration 22 : 0.5721586784322158
Loss in iteration 23 : 0.5677782892644878
Loss in iteration 24 : 0.5635483067263282
Loss in iteration 25 : 0.5595626752864755
Loss in iteration 26 : 0.5558875182133604
Loss in iteration 27 : 0.5525226626466406
Loss in iteration 28 : 0.5494089559275246
Loss in iteration 29 : 0.5464674895282006
Loss in iteration 30 : 0.5436416284588389
Loss in iteration 31 : 0.5409175410472609
Loss in iteration 32 : 0.5383165708380215
Loss in iteration 33 : 0.5358701365153727
Loss in iteration 34 : 0.5335950424789452
Loss in iteration 35 : 0.5314825849755563
Loss in iteration 36 : 0.5295041203914133
Loss in iteration 37 : 0.5276262961179383
Loss in iteration 38 : 0.5258256729647941
Loss in iteration 39 : 0.524095228071046
Loss in iteration 40 : 0.5224411736722202
Loss in iteration 41 : 0.5208738484800868
Loss in iteration 42 : 0.5193986279816535
Loss in iteration 43 : 0.5180114929792005
Loss in iteration 44 : 0.5167005221525097
Loss in iteration 45 : 0.515451279535427
Loss in iteration 46 : 0.5142525092308117
Loss in iteration 47 : 0.5130991568300847
Loss in iteration 48 : 0.5119917371459096
Loss in iteration 49 : 0.5109331363419459
Loss in iteration 50 : 0.509925002719147
Loss in iteration 51 : 0.5089656243725932
Loss in iteration 52 : 0.5080500377319538
Loss in iteration 53 : 0.5071718385401854
Loss in iteration 54 : 0.5063254355789019
Loss in iteration 55 : 0.5055075369320734
Loss in iteration 56 : 0.5047172907192675
Loss in iteration 57 : 0.5039552811122515
Loss in iteration 58 : 0.503222096956211
Loss in iteration 59 : 0.5025172508919948
Loss in iteration 60 : 0.5018388979695232
Loss in iteration 61 : 0.5011843238112359
Loss in iteration 62 : 0.5005508056856706
Loss in iteration 63 : 0.4999363498934803
Loss in iteration 64 : 0.4993399696493504
Loss in iteration 65 : 0.4987614580821048
Loss in iteration 66 : 0.4982008640487776
Loss in iteration 67 : 0.4976579796666138
Loss in iteration 68 : 0.49713208039713774
Loss in iteration 69 : 0.4966219888916459
Loss in iteration 70 : 0.49612636489931494
Loss in iteration 71 : 0.49564403582293215
Loss in iteration 72 : 0.49517420092676506
Loss in iteration 73 : 0.49471643614060923
Loss in iteration 74 : 0.4942705359051154
Loss in iteration 75 : 0.49383629864039863
Loss in iteration 76 : 0.4934133680389532
Loss in iteration 77 : 0.49300119379463286
Loss in iteration 78 : 0.49259910633353515
Loss in iteration 79 : 0.492206447257101
Loss in iteration 80 : 0.49182268227972803
Loss in iteration 81 : 0.4914474459937878
Loss in iteration 82 : 0.4910805097959336
Loss in iteration 83 : 0.4907217021381284
Loss in iteration 84 : 0.49037082695613887
Loss in iteration 85 : 0.4900276180255631
Loss in iteration 86 : 0.4896917428179127
Loss in iteration 87 : 0.48936284368792976
Loss in iteration 88 : 0.4890405892601053
Loss in iteration 89 : 0.4887247095386171
Loss in iteration 90 : 0.4884150011323161
Loss in iteration 91 : 0.48811130565060784
Loss in iteration 92 : 0.4878134762791326
Loss in iteration 93 : 0.48752135015322046
Loss in iteration 94 : 0.4872347380352346
Loss in iteration 95 : 0.48695343252106116
Loss in iteration 96 : 0.4866772271480024
Loss in iteration 97 : 0.48640593519021036
Loss in iteration 98 : 0.4861393992513972
Loss in iteration 99 : 0.4858774887695146
Loss in iteration 100 : 0.4856200887298309
Loss in iteration 101 : 0.4853670863370358
Loss in iteration 102 : 0.4851183620505503
Loss in iteration 103 : 0.48487378813863447
Loss in iteration 104 : 0.4846332338245344
Loss in iteration 105 : 0.48439657323356017
Loss in iteration 106 : 0.48416369179256885
Loss in iteration 107 : 0.4839344882936653
Loss in iteration 108 : 0.4837088723813189
Loss in iteration 109 : 0.4834867593894503
Loss in iteration 110 : 0.48326806531545297
Loss in iteration 111 : 0.48305270412667284
Loss in iteration 112 : 0.4828405881041938
Loss in iteration 113 : 0.48263163039936896
Loss in iteration 114 : 0.4824257481230857
Loss in iteration 115 : 0.4822228643708043
Loss in iteration 116 : 0.4820229083892088
Loss in iteration 117 : 0.48182581410420944
Loss in iteration 118 : 0.4816315179472432
Loss in iteration 119 : 0.48143995706367926
Loss in iteration 120 : 0.48125106860584044
Loss in iteration 121 : 0.48106479018080106
Loss in iteration 122 : 0.48088106097952327
Loss in iteration 123 : 0.48069982289245416
Loss in iteration 124 : 0.48052102105789857
Loss in iteration 125 : 0.4803446036587638
Loss in iteration 126 : 0.48017052116710285
Loss in iteration 127 : 0.47999872545460964
Loss in iteration 128 : 0.47982916917156776
Loss in iteration 129 : 0.47966180559860244
Loss in iteration 130 : 0.4794965889207517
Loss in iteration 131 : 0.47933347468997806
Loss in iteration 132 : 0.4791724202011054
Loss in iteration 133 : 0.4790133845992159
Loss in iteration 134 : 0.4788563286955026
Loss in iteration 135 : 0.4787012146079177
Loss in iteration 136 : 0.47854800540223447
Loss in iteration 137 : 0.47839666487651245
Loss in iteration 138 : 0.47824715753974834
Loss in iteration 139 : 0.478099448737595
Loss in iteration 140 : 0.47795350482045346
Loss in iteration 141 : 0.47780929325085425
Loss in iteration 142 : 0.477666782595724
Loss in iteration 143 : 0.47752594241358814
Loss in iteration 144 : 0.47738674309425105
Loss in iteration 145 : 0.47724915572025695
Loss in iteration 146 : 0.47711315199702226
Loss in iteration 147 : 0.47697870425849315
Loss in iteration 148 : 0.4768457855195563
Loss in iteration 149 : 0.47671436953067914
Loss in iteration 150 : 0.47658443079795343
Loss in iteration 151 : 0.4764559445549944
Loss in iteration 152 : 0.47632888669818724
Loss in iteration 153 : 0.4762032337118862
Loss in iteration 154 : 0.4760789626101483
Loss in iteration 155 : 0.4759560509093487
Loss in iteration 156 : 0.47583447662945855
Loss in iteration 157 : 0.47571421830941174
Loss in iteration 158 : 0.47559525501862915
Loss in iteration 159 : 0.47547756635232863
Loss in iteration 160 : 0.47536113240845607
Loss in iteration 161 : 0.47524593375329804
Loss in iteration 162 : 0.4751319513870574
Loss in iteration 163 : 0.4750191667188331
Loss in iteration 164 : 0.47490756155458463
Loss in iteration 165 : 0.4747971180952052
Loss in iteration 166 : 0.4746878189379052
Loss in iteration 167 : 0.47457964707407024
Loss in iteration 168 : 0.47447258587984953
Loss in iteration 169 : 0.47436661909999667
Loss in iteration 170 : 0.4742617308286805
Loss in iteration 171 : 0.4741579054918678
Loss in iteration 172 : 0.4740551278344415
Loss in iteration 173 : 0.47395338291261696
Loss in iteration 174 : 0.4738526560897992
Loss in iteration 175 : 0.4737529330329587
Loss in iteration 176 : 0.4736541997070535
Loss in iteration 177 : 0.47355644236656214
Loss in iteration 178 : 0.4734596475448525
Loss in iteration 179 : 0.47336380204311435
Loss in iteration 180 : 0.47326889292060337
Loss in iteration 181 : 0.47317490748712054
Loss in iteration 182 : 0.4730818332975651
Loss in iteration 183 : 0.4729896581475667
Loss in iteration 184 : 0.47289837006899055
Loss in iteration 185 : 0.4728079573244919
Loss in iteration 186 : 0.472718408400992
Loss in iteration 187 : 0.4726297120025407
Loss in iteration 188 : 0.4725418570433462
Loss in iteration 189 : 0.47245483264157817
Loss in iteration 190 : 0.4723686281141893
Loss in iteration 191 : 0.4722832329725317
Loss in iteration 192 : 0.47219863691831887
Loss in iteration 193 : 0.4721148298394765
Loss in iteration 194 : 0.47203180180562865
Loss in iteration 195 : 0.47194954306328457
Loss in iteration 196 : 0.471868044030958
Loss in iteration 197 : 0.4717872952945329
Loss in iteration 198 : 0.4717072876030602
Loss in iteration 199 : 0.47162801186501224
Loss in iteration 200 : 0.4715494591448299
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999996 = 0.782, training accuracy 0.783875, time elapsed: 4621 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 4.855845284144978
Loss in iteration 3 : 4.932588061608754
Loss in iteration 4 : 1.480491367804903
Loss in iteration 5 : 1.1069842418577835
Loss in iteration 6 : 0.7976234227949938
Loss in iteration 7 : 0.8172371760950755
Loss in iteration 8 : 0.857296416147338
Loss in iteration 9 : 0.8436630400673936
Loss in iteration 10 : 0.7858796440778238
Loss in iteration 11 : 0.7165514312599903
Loss in iteration 12 : 0.6618541843399646
Loss in iteration 13 : 0.631089822429313
Loss in iteration 14 : 0.6285544225302216
Loss in iteration 15 : 0.6348439335914197
Loss in iteration 16 : 0.6474860388341638
Loss in iteration 17 : 0.6511573874959241
Loss in iteration 18 : 0.6712594616115503
Loss in iteration 19 : 0.6857601463553219
Loss in iteration 20 : 0.7383678023389098
Loss in iteration 21 : 0.7535748885309254
Loss in iteration 22 : 0.7830334340617199
Loss in iteration 23 : 0.7548960761957584
Loss in iteration 24 : 0.7320765210040617
Loss in iteration 25 : 0.7010767868717392
Loss in iteration 26 : 0.6792776755744442
Loss in iteration 27 : 0.6603315097316281
Loss in iteration 28 : 0.6520374746877994
Loss in iteration 29 : 0.6439141501931781
Loss in iteration 30 : 0.6479278066149669
Loss in iteration 31 : 0.6478249747712954
Loss in iteration 32 : 0.6634967057905008
Loss in iteration 33 : 0.6670704311162208
Loss in iteration 34 : 0.6884518265210597
Loss in iteration 35 : 0.6898856540710239
Loss in iteration 36 : 0.705553127163588
Loss in iteration 37 : 0.7100767846666575
Loss in iteration 38 : 0.7126944963821161
Loss in iteration 39 : 0.7130377966462991
Loss in iteration 40 : 0.7015705036840029
Loss in iteration 41 : 0.692853996388418
Loss in iteration 42 : 0.6771844703092712
Loss in iteration 43 : 0.6643259479473866
Loss in iteration 44 : 0.652006181135361
Loss in iteration 45 : 0.6394317341600736
Loss in iteration 46 : 0.6328914541042979
Loss in iteration 47 : 0.6226387153692853
Loss in iteration 48 : 0.6220785841059999
Loss in iteration 49 : 0.6145926396442056
Loss in iteration 50 : 0.6196880569845034
Loss in iteration 51 : 0.6142298851889627
Loss in iteration 52 : 0.6241194608893013
Loss in iteration 53 : 0.6183227563074857
Loss in iteration 54 : 0.6300137818740951
Loss in iteration 55 : 0.623323004913398
Loss in iteration 56 : 0.6342137746404434
Loss in iteration 57 : 0.6324133022363333
Loss in iteration 58 : 0.6423619264715976
Loss in iteration 59 : 0.6459004613603203
Loss in iteration 60 : 0.652358080519524
Loss in iteration 61 : 0.6560860986057698
Loss in iteration 62 : 0.6567290025295766
Loss in iteration 63 : 0.6574902137175547
Loss in iteration 64 : 0.6536021909510373
Loss in iteration 65 : 0.6516571399867799
Loss in iteration 66 : 0.6463901351390273
Loss in iteration 67 : 0.6434173853476398
Loss in iteration 68 : 0.639277216177954
Loss in iteration 69 : 0.636729812245597
Loss in iteration 70 : 0.6349207685196038
Loss in iteration 71 : 0.6335252363846822
Loss in iteration 72 : 0.6342273756926755
Loss in iteration 73 : 0.634011750136512
Loss in iteration 74 : 0.636716701861987
Loss in iteration 75 : 0.6371365502577725
Loss in iteration 76 : 0.6409366241580796
Loss in iteration 77 : 0.6410725429181925
Loss in iteration 78 : 0.6450336586095914
Loss in iteration 79 : 0.643937479388926
Loss in iteration 80 : 0.6474999910564981
Loss in iteration 81 : 0.6446116285453499
Loss in iteration 82 : 0.6477484469574923
Loss in iteration 83 : 0.6431825061302646
Loss in iteration 84 : 0.646207721065848
Loss in iteration 85 : 0.6407516927852773
Loss in iteration 86 : 0.6440141434694321
Loss in iteration 87 : 0.638790759559301
Loss in iteration 88 : 0.6424749030139546
Loss in iteration 89 : 0.6384251030018003
Loss in iteration 90 : 0.6424490130882942
Loss in iteration 91 : 0.639966273902633
Loss in iteration 92 : 0.6439495942021947
Loss in iteration 93 : 0.6428375242820921
Loss in iteration 94 : 0.6462014063912488
Loss in iteration 95 : 0.6458742066076836
Loss in iteration 96 : 0.648090408111916
Loss in iteration 97 : 0.6478771934855699
Loss in iteration 98 : 0.6487410329557582
Loss in iteration 99 : 0.648158497793876
Loss in iteration 100 : 0.6478966312561817
Loss in iteration 101 : 0.6467731402247048
Loss in iteration 102 : 0.6459232727507517
Loss in iteration 103 : 0.6443503400448465
Loss in iteration 104 : 0.6435287898042918
Loss in iteration 105 : 0.6417174216546634
Loss in iteration 106 : 0.6414248258945056
Loss in iteration 107 : 0.6395637675481455
Loss in iteration 108 : 0.6400913981922531
Loss in iteration 109 : 0.6382606257349276
Loss in iteration 110 : 0.6396822514952194
Loss in iteration 111 : 0.6378327670930309
Loss in iteration 112 : 0.6400473019194818
Loss in iteration 113 : 0.6380403092560728
Loss in iteration 114 : 0.6408392513831505
Loss in iteration 115 : 0.6385289154586626
Loss in iteration 116 : 0.6416695168016222
Loss in iteration 117 : 0.6389968302884351
Loss in iteration 118 : 0.6422621972685187
Loss in iteration 119 : 0.6393101639454462
Loss in iteration 120 : 0.6425403675836161
Loss in iteration 121 : 0.6395123013945611
Loss in iteration 122 : 0.6426023233007128
Loss in iteration 123 : 0.6397351967453367
Loss in iteration 124 : 0.6426118620477529
Loss in iteration 125 : 0.6400820084722446
Loss in iteration 126 : 0.642681367426817
Loss in iteration 127 : 0.640554290320291
Loss in iteration 128 : 0.6428157649196975
Loss in iteration 129 : 0.641050952355304
Loss in iteration 130 : 0.642930164378524
Loss in iteration 131 : 0.6414207458854033
Loss in iteration 132 : 0.642910498593913
Loss in iteration 133 : 0.6415314106502364
Loss in iteration 134 : 0.6426764595385218
Loss in iteration 135 : 0.6413222013425371
Loss in iteration 136 : 0.6422182873476354
Loss in iteration 137 : 0.6408215817050414
Loss in iteration 138 : 0.6415983544365113
Loss in iteration 139 : 0.6401300656201442
Loss in iteration 140 : 0.6409250748275198
Loss in iteration 141 : 0.6393817997633795
Loss in iteration 142 : 0.6403150326412145
Loss in iteration 143 : 0.6387028831760141
Loss in iteration 144 : 0.6398589075701195
Loss in iteration 145 : 0.6381806569190885
Loss in iteration 146 : 0.6396012108236503
Loss in iteration 147 : 0.6378507962783021
Loss in iteration 148 : 0.6395370619837087
Loss in iteration 149 : 0.6377019377808523
Loss in iteration 150 : 0.6396235811357278
Loss in iteration 151 : 0.6376925037390127
Loss in iteration 152 : 0.6397997067168231
Loss in iteration 153 : 0.6377717586892705
Loss in iteration 154 : 0.6400066733442498
Loss in iteration 155 : 0.6378971588069282
Loss in iteration 156 : 0.6402023007667783
Loss in iteration 157 : 0.6380427280197143
Loss in iteration 158 : 0.6403654875165649
Loss in iteration 159 : 0.638197583142942
Loss in iteration 160 : 0.6404916558654647
Loss in iteration 161 : 0.6383579184137608
Loss in iteration 162 : 0.6405834039668583
Loss in iteration 163 : 0.6385178690136611
Loss in iteration 164 : 0.6406417427768806
Loss in iteration 165 : 0.6386640437322303
Loss in iteration 166 : 0.6406618526146304
Loss in iteration 167 : 0.6387759346604391
Loss in iteration 168 : 0.640634412794452
Loss in iteration 169 : 0.6388313812357521
Loss in iteration 170 : 0.6405508086096348
Loss in iteration 171 : 0.6388141320176818
Loss in iteration 172 : 0.6404090598081627
Loss in iteration 173 : 0.6387200053335964
Loss in iteration 174 : 0.6402174908969693
Loss in iteration 175 : 0.6385591388240914
Loss in iteration 176 : 0.6399945672912141
Loss in iteration 177 : 0.6383536738467118
Loss in iteration 178 : 0.6397651209178447
Loss in iteration 179 : 0.6381320347418298
Loss in iteration 180 : 0.6395545701748979
Loss in iteration 181 : 0.6379220177462341
Loss in iteration 182 : 0.6393832494599838
Loss in iteration 183 : 0.6377449534641912
Loss in iteration 184 : 0.6392626080139344
Loss in iteration 185 : 0.6376124614985388
Loss in iteration 186 : 0.6391941434820112
Loss in iteration 187 : 0.6375262235744417
Loss in iteration 188 : 0.6391709291759349
Loss in iteration 189 : 0.6374802050476739
Loss in iteration 190 : 0.6391808289430108
Loss in iteration 191 : 0.6374641488496157
Loss in iteration 192 : 0.6392101651990625
Loss in iteration 193 : 0.6374670548775668
Loss in iteration 194 : 0.6392467334716739
Loss in iteration 195 : 0.6374796725573696
Loss in iteration 196 : 0.6392815106451788
Loss in iteration 197 : 0.6374955854518077
Loss in iteration 198 : 0.6393089683982205
Loss in iteration 199 : 0.6375110238340245
Loss in iteration 200 : 0.6393263643162037
Testing accuracy  of updater 8 on alg 0 with rate 2.0 = 0.7075, training accuracy 0.7165, time elapsed: 5241 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6815803696607013
Loss in iteration 3 : 0.6754203524830003
Loss in iteration 4 : 0.6656415211672124
Loss in iteration 5 : 0.6533641362983676
Loss in iteration 6 : 0.6415251817228482
Loss in iteration 7 : 0.6306308687977787
Loss in iteration 8 : 0.6199402251876752
Loss in iteration 9 : 0.6092035068678351
Loss in iteration 10 : 0.598725197520835
Loss in iteration 11 : 0.5888153598016665
Loss in iteration 12 : 0.5795908091025552
Loss in iteration 13 : 0.5710610464854569
Loss in iteration 14 : 0.5632150729058728
Loss in iteration 15 : 0.5560359550680396
Loss in iteration 16 : 0.5494910709490001
Loss in iteration 17 : 0.5435331633874625
Loss in iteration 18 : 0.538110615134844
Loss in iteration 19 : 0.5331755036972794
Loss in iteration 20 : 0.5286855949611893
Loss in iteration 21 : 0.5246029917435522
Loss in iteration 22 : 0.5208925888612586
Loss in iteration 23 : 0.5175213696357929
Loss in iteration 24 : 0.5144582298632211
Loss in iteration 25 : 0.5116739141045098
Loss in iteration 26 : 0.5091409667975938
Loss in iteration 27 : 0.5068337543080967
Loss in iteration 28 : 0.5047285687003779
Loss in iteration 29 : 0.5028037499216329
Loss in iteration 30 : 0.5010397544308124
Loss in iteration 31 : 0.4994191380278946
Loss in iteration 32 : 0.4979264600854774
Loss in iteration 33 : 0.49654813270093723
Loss in iteration 34 : 0.4952722371693462
Loss in iteration 35 : 0.4940883248364454
Loss in iteration 36 : 0.49298721637168547
Loss in iteration 37 : 0.4919608121298661
Loss in iteration 38 : 0.4910019239787746
Loss in iteration 39 : 0.4901041349165235
Loss in iteration 40 : 0.48926168800526076
Loss in iteration 41 : 0.4884694020503267
Loss in iteration 42 : 0.4877226088734992
Loss in iteration 43 : 0.4870171060320754
Loss in iteration 44 : 0.48634911908585216
Loss in iteration 45 : 0.4857152685871528
Loss in iteration 46 : 0.48511253847105035
Loss in iteration 47 : 0.4845382441026727
Loss in iteration 48 : 0.48398999960308237
Loss in iteration 49 : 0.4834656850308633
Loss in iteration 50 : 0.4829634144786037
Loss in iteration 51 : 0.48248150620614116
Loss in iteration 52 : 0.4820184557036088
Loss in iteration 53 : 0.48157291220347326
Loss in iteration 54 : 0.48114365876644555
Loss in iteration 55 : 0.48072959573490176
Loss in iteration 56 : 0.4803297271207507
Loss in iteration 57 : 0.4799431493805805
Loss in iteration 58 : 0.4795690420151494
Loss in iteration 59 : 0.47920665948708524
Loss in iteration 60 : 0.47885532405102677
Loss in iteration 61 : 0.4785144192079848
Loss in iteration 62 : 0.4781833836094548
Loss in iteration 63 : 0.4778617053326793
Loss in iteration 64 : 0.47754891651860687
Loss in iteration 65 : 0.47724458840592465
Loss in iteration 66 : 0.47694832681018173
Loss in iteration 67 : 0.4766597680911913
Loss in iteration 68 : 0.4763785756315791
Loss in iteration 69 : 0.47610443682194087
Loss in iteration 70 : 0.4758370605203837
Loss in iteration 71 : 0.475576174931726
Loss in iteration 72 : 0.4753215258374454
Loss in iteration 73 : 0.4750728751028058
Loss in iteration 74 : 0.47482999939151105
Loss in iteration 75 : 0.4745926890286686
Loss in iteration 76 : 0.4743607469668564
Loss in iteration 77 : 0.47413398782503063
Loss in iteration 78 : 0.47391223698345397
Loss in iteration 79 : 0.4736953297283488
Loss in iteration 80 : 0.4734831104469102
Loss in iteration 81 : 0.47327543187674403
Loss in iteration 82 : 0.4730721544143266
Loss in iteration 83 : 0.4728731454855832
Loss in iteration 84 : 0.47267827897922077
Loss in iteration 85 : 0.4724874347406309
Loss in iteration 86 : 0.4723004981218683
Loss in iteration 87 : 0.472117359581565
Loss in iteration 88 : 0.47193791432790266
Loss in iteration 89 : 0.4717620619978511
Loss in iteration 90 : 0.471589706366538
Loss in iteration 91 : 0.4714207550817539
Loss in iteration 92 : 0.4712551194197737
Loss in iteration 93 : 0.47109271405986636
Loss in iteration 94 : 0.47093345687579463
Loss in iteration 95 : 0.4707772687432724
Loss in iteration 96 : 0.47062407336270085
Loss in iteration 97 : 0.4704737970966119
Loss in iteration 98 : 0.47032636882114287
Loss in iteration 99 : 0.47018171979070267
Loss in iteration 100 : 0.47003978351475667
Loss in iteration 101 : 0.46990049564550274
Loss in iteration 102 : 0.46976379387511336
Loss in iteration 103 : 0.46962961784123575
Loss in iteration 104 : 0.4694979090395086
Loss in iteration 105 : 0.46936861074205927
Loss in iteration 106 : 0.4692416679210795
Loss in iteration 107 : 0.46911702717684617
Loss in iteration 108 : 0.4689946366696952
Loss in iteration 109 : 0.4688744460556291
Loss in iteration 110 : 0.46875640642533734
Loss in iteration 111 : 0.46864047024650285
Loss in iteration 112 : 0.4685265913092502
Loss in iteration 113 : 0.4684147246746466
Loss in iteration 114 : 0.4683048266261078
Loss in iteration 115 : 0.46819685462356175
Loss in iteration 116 : 0.46809076726020504
Loss in iteration 117 : 0.46798652422164916
Loss in iteration 118 : 0.4678840862472895
Loss in iteration 119 : 0.46778341509369087
Loss in iteration 120 : 0.4676844734998275
Loss in iteration 121 : 0.4675872251540322
Loss in iteration 122 : 0.4674916346624979
Loss in iteration 123 : 0.46739766751924666
Loss in iteration 124 : 0.46730529007744454
Loss in iteration 125 : 0.4672144695219948
Loss in iteration 126 : 0.4671251738433267
Loss in iteration 127 : 0.46703737181232347
Loss in iteration 128 : 0.4669510329563152
Loss in iteration 129 : 0.4668661275360846
Loss in iteration 130 : 0.46678262652382535
Loss in iteration 131 : 0.4667005015819973
Loss in iteration 132 : 0.46661972504302174
Loss in iteration 133 : 0.466540269889767
Loss in iteration 134 : 0.46646210973678776
Loss in iteration 135 : 0.4663852188122544
Loss in iteration 136 : 0.4663095719405555
Loss in iteration 137 : 0.46623514452552905
Loss in iteration 138 : 0.4661619125342951
Loss in iteration 139 : 0.4660898524816523
Loss in iteration 140 : 0.4660189414150386
Loss in iteration 141 : 0.46594915690000427
Loss in iteration 142 : 0.46588047700619234
Loss in iteration 143 : 0.46581288029380286
Loss in iteration 144 : 0.4657463458005159
Loss in iteration 145 : 0.4656808530288618
Loss in iteration 146 : 0.4656163819340083
Loss in iteration 147 : 0.4655529129119546
Loss in iteration 148 : 0.4654904267881126
Loss in iteration 149 : 0.46542890480625726
Loss in iteration 150 : 0.46536832861782695
Loss in iteration 151 : 0.46530868027156164
Loss in iteration 152 : 0.46524994220346994
Loss in iteration 153 : 0.4651920972270957
Loss in iteration 154 : 0.4651351285240921
Loss in iteration 155 : 0.4650790196350736
Loss in iteration 156 : 0.4650237544507434
Loss in iteration 157 : 0.4649693172032879
Loss in iteration 158 : 0.46491569245801667
Loss in iteration 159 : 0.4648628651052564
Loss in iteration 160 : 0.46481082035247284
Loss in iteration 161 : 0.4647595437166191
Loss in iteration 162 : 0.46470902101670686
Loss in iteration 163 : 0.46465923836658424
Loss in iteration 164 : 0.4646101821679175
Loss in iteration 165 : 0.4645618391033714
Loss in iteration 166 : 0.464514196129974
Loss in iteration 167 : 0.46446724047267396
Loss in iteration 168 : 0.4644209596180659
Loss in iteration 169 : 0.46437534130829144
Loss in iteration 170 : 0.4643303735351076
Loss in iteration 171 : 0.46428604453411226
Loss in iteration 172 : 0.464242342779132
Loss in iteration 173 : 0.4641992569767505
Loss in iteration 174 : 0.46415677606099365
Loss in iteration 175 : 0.4641148891881503
Loss in iteration 176 : 0.46407358573172847
Loss in iteration 177 : 0.46403285527754934
Loss in iteration 178 : 0.46399268761896895
Loss in iteration 179 : 0.46395307275222014
Loss in iteration 180 : 0.4639140008718806
Loss in iteration 181 : 0.46387546236645655
Loss in iteration 182 : 0.4638374478140781
Loss in iteration 183 : 0.4637999479783083
Loss in iteration 184 : 0.46376295380405586
Loss in iteration 185 : 0.4637264564135951
Loss in iteration 186 : 0.46369044710268376
Loss in iteration 187 : 0.46365491733678027
Loss in iteration 188 : 0.46361985874735484
Loss in iteration 189 : 0.4635852631282946
Loss in iteration 190 : 0.46355112243239444
Loss in iteration 191 : 0.46351742876793883
Loss in iteration 192 : 0.46348417439536715
Loss in iteration 193 : 0.46345135172401813
Loss in iteration 194 : 0.4634189533089586
Loss in iteration 195 : 0.463386971847884
Loss in iteration 196 : 0.4633554001780993
Loss in iteration 197 : 0.4633242312735711
Loss in iteration 198 : 0.46329345824204854
Loss in iteration 199 : 0.46326307432225633
Loss in iteration 200 : 0.46323307288115584
Testing accuracy  of updater 8 on alg 0 with rate 0.2 = 0.7875, training accuracy 0.791, time elapsed: 4834 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6909328184924474
Loss in iteration 3 : 0.6881353535586163
Loss in iteration 4 : 0.6850617910315081
Loss in iteration 5 : 0.6819393774176052
Loss in iteration 6 : 0.6788999529153493
Loss in iteration 7 : 0.6759886267480456
Loss in iteration 8 : 0.6731878511354008
Loss in iteration 9 : 0.6704467340399116
Loss in iteration 10 : 0.6677073400890697
Loss in iteration 11 : 0.6649232212285284
Loss in iteration 12 : 0.6620688431190407
Loss in iteration 13 : 0.659141016373028
Loss in iteration 14 : 0.6561547221114374
Loss in iteration 15 : 0.6531360479664685
Loss in iteration 16 : 0.6501146397743841
Loss in iteration 17 : 0.6471174148197879
Loss in iteration 18 : 0.6441645017836981
Loss in iteration 19 : 0.6412676390069445
Loss in iteration 20 : 0.6384306918921786
Loss in iteration 21 : 0.6356516046855128
Loss in iteration 22 : 0.6329249903012515
Loss in iteration 23 : 0.6302446461076229
Loss in iteration 24 : 0.6276054945528717
Loss in iteration 25 : 0.6250047064083478
Loss in iteration 26 : 0.6224420026486879
Loss in iteration 27 : 0.6199193031126988
Loss in iteration 28 : 0.6174399778479909
Loss in iteration 29 : 0.6150079659214076
Loss in iteration 30 : 0.6126269776106265
Loss in iteration 31 : 0.6102999167087696
Loss in iteration 32 : 0.6080285758855978
Loss in iteration 33 : 0.6058135884892125
Loss in iteration 34 : 0.6036545748222427
Loss in iteration 35 : 0.6015504015732084
Loss in iteration 36 : 0.599499475583267
Loss in iteration 37 : 0.5975000102081662
Loss in iteration 38 : 0.5955502263150253
Loss in iteration 39 : 0.5936484737353461
Loss in iteration 40 : 0.591793278200892
Loss in iteration 41 : 0.5899833311589257
Loss in iteration 42 : 0.5882174451437845
Loss in iteration 43 : 0.5864944967356722
Loss in iteration 44 : 0.58481337448497
Loss in iteration 45 : 0.5831729426108754
Loss in iteration 46 : 0.5815720246187496
Loss in iteration 47 : 0.580009405519557
Loss in iteration 48 : 0.5784838477659136
Loss in iteration 49 : 0.5769941144721324
Loss in iteration 50 : 0.5755389936683958
Loss in iteration 51 : 0.5741173187154128
Loss in iteration 52 : 0.5727279819661992
Loss in iteration 53 : 0.5713699407600569
Loss in iteration 54 : 0.5700422164739009
Loss in iteration 55 : 0.568743888422698
Loss in iteration 56 : 0.5674740848500662
Loss in iteration 57 : 0.5662319731677142
Loss in iteration 58 : 0.5650167511507179
Loss in iteration 59 : 0.5638276401605421
Loss in iteration 60 : 0.562663880816967
Loss in iteration 61 : 0.5615247309968299
Loss in iteration 62 : 0.560409465670182
Loss in iteration 63 : 0.5593173779096157
Loss in iteration 64 : 0.5582477804028811
Loss in iteration 65 : 0.5572000069147911
Loss in iteration 66 : 0.5561734133258288
Loss in iteration 67 : 0.5551673780699146
Loss in iteration 68 : 0.5541813019630385
Loss in iteration 69 : 0.5532146075346636
Loss in iteration 70 : 0.5522667380370992
Loss in iteration 71 : 0.5513371563195851
Loss in iteration 72 : 0.5504253437263609
Loss in iteration 73 : 0.5495307991281223
Loss in iteration 74 : 0.5486530381395983
Loss in iteration 75 : 0.5477915925253232
Loss in iteration 76 : 0.5469460097586037
Loss in iteration 77 : 0.5461158526786775
Loss in iteration 78 : 0.5453006991869037
Loss in iteration 79 : 0.5445001419312316
Loss in iteration 80 : 0.5437137879439993
Loss in iteration 81 : 0.5429412582166057
Loss in iteration 82 : 0.5421821872115251
Loss in iteration 83 : 0.5414362223248934
Loss in iteration 84 : 0.5407030233202644
Loss in iteration 85 : 0.53998226175614
Loss in iteration 86 : 0.5392736204276207
Loss in iteration 87 : 0.538576792837442
Loss in iteration 88 : 0.5378914827053924
Loss in iteration 89 : 0.5372174035190469
Loss in iteration 90 : 0.536554278123836
Loss in iteration 91 : 0.535901838347218
Loss in iteration 92 : 0.5352598246502857
Loss in iteration 93 : 0.5346279858001228
Loss in iteration 94 : 0.5340060785573514
Loss in iteration 95 : 0.5333938673749539
Loss in iteration 96 : 0.532791124106231
Loss in iteration 97 : 0.5321976277212728
Loss in iteration 98 : 0.5316131640323786
Loss in iteration 99 : 0.5310375254294528
Loss in iteration 100 : 0.5304705106264469
Loss in iteration 101 : 0.5299119244196958
Loss in iteration 102 : 0.5293615774584891
Loss in iteration 103 : 0.5288192860277428
Loss in iteration 104 : 0.5282848718421241
Loss in iteration 105 : 0.5277581618506971
Loss in iteration 106 : 0.5272389880509677
Loss in iteration 107 : 0.5267271873112156
Loss in iteration 108 : 0.5262226012001133
Loss in iteration 109 : 0.525725075822834
Loss in iteration 110 : 0.5252344616630602
Loss in iteration 111 : 0.5247506134305383
Loss in iteration 112 : 0.5242733899139773
Loss in iteration 113 : 0.5238026538391931
Loss in iteration 114 : 0.5233382717325001
Loss in iteration 115 : 0.522880113789297
Loss in iteration 116 : 0.5224280537478134
Loss in iteration 117 : 0.5219819687679306
Loss in iteration 118 : 0.5215417393149091
Loss in iteration 119 : 0.5211072490478695
Loss in iteration 120 : 0.5206783847127773
Loss in iteration 121 : 0.5202550360397364
Loss in iteration 122 : 0.5198370956443373
Loss in iteration 123 : 0.5194244589328776
Loss in iteration 124 : 0.5190170240112396
Loss in iteration 125 : 0.5186146915972677
Loss in iteration 126 : 0.518217364936501
Loss in iteration 127 : 0.5178249497211166
Loss in iteration 128 : 0.5174373540119717
Loss in iteration 129 : 0.5170544881636361
Loss in iteration 130 : 0.5166762647522803
Loss in iteration 131 : 0.5163025985063435
Loss in iteration 132 : 0.5159334062398223
Loss in iteration 133 : 0.515568606788115
Loss in iteration 134 : 0.515208120946268
Loss in iteration 135 : 0.5148518714095356
Loss in iteration 136 : 0.5144997827161342
Loss in iteration 137 : 0.5141517811920985
Loss in iteration 138 : 0.5138077948981301
Loss in iteration 139 : 0.5134677535783578
Loss in iteration 140 : 0.5131315886109281
Loss in iteration 141 : 0.5127992329603326
Loss in iteration 142 : 0.5124706211314248
Loss in iteration 143 : 0.5121456891250327
Loss in iteration 144 : 0.5118243743951205
Loss in iteration 145 : 0.5115066158074343
Loss in iteration 146 : 0.5111923535995659
Loss in iteration 147 : 0.5108815293423875
Loss in iteration 148 : 0.5105740859027975
Loss in iteration 149 : 0.5102699674077324
Loss in iteration 150 : 0.5099691192093777
Loss in iteration 151 : 0.509671487851553
Loss in iteration 152 : 0.5093770210372084
Loss in iteration 153 : 0.5090856675969927
Loss in iteration 154 : 0.508797377458853
Loss in iteration 155 : 0.5085121016186291
Loss in iteration 156 : 0.5082297921115937
Loss in iteration 157 : 0.5079504019849135
Loss in iteration 158 : 0.5076738852709936
Loss in iteration 159 : 0.5074001969616669
Loss in iteration 160 : 0.5071292929832037
Loss in iteration 161 : 0.5068611301721058
Loss in iteration 162 : 0.5065956662516587
Loss in iteration 163 : 0.5063328598092149
Loss in iteration 164 : 0.506072670274175
Loss in iteration 165 : 0.5058150578966506
Loss in iteration 166 : 0.5055599837267809
Loss in iteration 167 : 0.5053074095946665
Loss in iteration 168 : 0.5050572980909244
Loss in iteration 169 : 0.5048096125478176
Loss in iteration 170 : 0.5045643170209512
Loss in iteration 171 : 0.5043213762715146
Loss in iteration 172 : 0.5040807557490435
Loss in iteration 173 : 0.503842421574695
Loss in iteration 174 : 0.5036063405250077
Loss in iteration 175 : 0.5033724800161367
Loss in iteration 176 : 0.5031408080885436
Loss in iteration 177 : 0.5029112933921277
Loss in iteration 178 : 0.5026839051717839
Loss in iteration 179 : 0.5024586132533749
Loss in iteration 180 : 0.5022353880300945
Loss in iteration 181 : 0.5020142004492236
Loss in iteration 182 : 0.5017950219992519
Loss in iteration 183 : 0.5015778246973633
Loss in iteration 184 : 0.5013625810772696
Loss in iteration 185 : 0.5011492641773739
Loss in iteration 186 : 0.5009378475292698
Loss in iteration 187 : 0.5007283051465498
Loss in iteration 188 : 0.5005206115139178
Loss in iteration 189 : 0.500314741576599
Loss in iteration 190 : 0.5001106707300369
Loss in iteration 191 : 0.49990837480986566
Loss in iteration 192 : 0.4997078300821463
Loss in iteration 193 : 0.4995090132338702
Loss in iteration 194 : 0.49931190136370995
Loss in iteration 195 : 0.49911647197301334
Loss in iteration 196 : 0.4989227029570352
Loss in iteration 197 : 0.49873057259639847
Loss in iteration 198 : 0.4985400595487762
Loss in iteration 199 : 0.49835114284078535
Loss in iteration 200 : 0.49816380186010156
Testing accuracy  of updater 8 on alg 0 with rate 0.01999999999999999 = 0.776, training accuracy 0.776375, time elapsed: 4999 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 12.569829085734185
Loss in iteration 3 : 13.880316658153781
Loss in iteration 4 : 8.01656966282435
Loss in iteration 5 : 9.18155126256362
Loss in iteration 6 : 6.868710291161581
Loss in iteration 7 : 4.384014147899585
Loss in iteration 8 : 7.925858092046503
Loss in iteration 9 : 3.0896315889362636
Loss in iteration 10 : 5.819007554891202
Loss in iteration 11 : 5.613804939091495
Loss in iteration 12 : 2.4261894468437006
Loss in iteration 13 : 4.861626330568006
Loss in iteration 14 : 4.518143093545736
Loss in iteration 15 : 2.53065583176503
Loss in iteration 16 : 4.344273712842111
Loss in iteration 17 : 3.858451090814337
Loss in iteration 18 : 2.616707257922228
Loss in iteration 19 : 3.8518407146376954
Loss in iteration 20 : 3.348841244578297
Loss in iteration 21 : 2.429400420533823
Loss in iteration 22 : 3.338534442129395
Loss in iteration 23 : 2.81602394549041
Loss in iteration 24 : 2.4040462982645248
Loss in iteration 25 : 3.0186591741522086
Loss in iteration 26 : 2.477913607341642
Loss in iteration 27 : 2.1415486861230493
Loss in iteration 28 : 2.6069465804773455
Loss in iteration 29 : 1.96819038444245
Loss in iteration 30 : 2.117205887788249
Loss in iteration 31 : 2.213607462771375
Loss in iteration 32 : 1.7436470477457073
Loss in iteration 33 : 2.0219463487107
Loss in iteration 34 : 1.7109765525889866
Loss in iteration 35 : 1.6676263810083582
Loss in iteration 36 : 1.722434661959266
Loss in iteration 37 : 1.4093322169279618
Loss in iteration 38 : 1.643953071887427
Loss in iteration 39 : 1.2994242826258335
Loss in iteration 40 : 1.4896755472158238
Loss in iteration 41 : 1.2055869881545591
Loss in iteration 42 : 1.3928948830010772
Loss in iteration 43 : 1.1267215890734532
Loss in iteration 44 : 1.3301920292898468
Loss in iteration 45 : 1.0706676849527983
Loss in iteration 46 : 1.2027638800284728
Loss in iteration 47 : 1.1661931785755262
Loss in iteration 48 : 1.051019376583735
Loss in iteration 49 : 1.1349543349958484
Loss in iteration 50 : 1.0561146896309028
Loss in iteration 51 : 1.0784635428287477
Loss in iteration 52 : 1.0027546075805016
Loss in iteration 53 : 1.0766585179781634
Loss in iteration 54 : 0.9775281764454614
Loss in iteration 55 : 1.2013451625661804
Loss in iteration 56 : 1.0738930547874992
Loss in iteration 57 : 0.9049619013903204
Loss in iteration 58 : 1.1069739350632226
Loss in iteration 59 : 1.0093260713225978
Loss in iteration 60 : 0.8321899692326442
Loss in iteration 61 : 0.688698730799164
Loss in iteration 62 : 0.7178221783767744
Loss in iteration 63 : 0.8599585151097598
Loss in iteration 64 : 0.9624942176145662
Loss in iteration 65 : 0.9658661590936818
Loss in iteration 66 : 0.7720208659211012
Loss in iteration 67 : 0.7922655685716203
Loss in iteration 68 : 0.6127731112217288
Loss in iteration 69 : 0.7737537636289009
Loss in iteration 70 : 0.5727806140122182
Loss in iteration 71 : 0.8294480654077988
Loss in iteration 72 : 0.9900884412852055
Loss in iteration 73 : 1.7564896249777386
Loss in iteration 74 : 1.1535386638632903
Loss in iteration 75 : 0.7409687962862246
Loss in iteration 76 : 0.7528990111522235
Loss in iteration 77 : 0.742064732891964
Loss in iteration 78 : 1.5115977833582914
Loss in iteration 79 : 1.1425188266424628
Loss in iteration 80 : 0.9147490706397652
Loss in iteration 81 : 0.5847423571279834
Loss in iteration 82 : 0.7775405556706694
Loss in iteration 83 : 0.8099058347403202
Loss in iteration 84 : 0.9406797010382802
Loss in iteration 85 : 0.7422198198201703
Loss in iteration 86 : 0.6574848663259787
Loss in iteration 87 : 0.6727888643481529
Loss in iteration 88 : 0.7056092819319828
Loss in iteration 89 : 0.920734806710005
Loss in iteration 90 : 1.0119561403813238
Loss in iteration 91 : 0.90891103317088
Loss in iteration 92 : 0.7852233493903031
Loss in iteration 93 : 0.5884784322295963
Loss in iteration 94 : 0.6476251310157332
Loss in iteration 95 : 0.5323446707863406
Loss in iteration 96 : 0.6474023276678552
Loss in iteration 97 : 0.5394867755669297
Loss in iteration 98 : 0.6437833480357849
Loss in iteration 99 : 0.6288882714966225
Loss in iteration 100 : 0.8093950511650414
Loss in iteration 101 : 1.2428605731650122
Loss in iteration 102 : 1.102555677192017
Loss in iteration 103 : 1.154956760578147
Loss in iteration 104 : 0.5921826544192533
Loss in iteration 105 : 0.7059504634724373
Loss in iteration 106 : 0.9463952330982282
Loss in iteration 107 : 0.766034062872533
Loss in iteration 108 : 0.761058339736724
Loss in iteration 109 : 0.5542308872504166
Loss in iteration 110 : 0.6744452617451413
Loss in iteration 111 : 0.7015001177197293
Loss in iteration 112 : 0.6470619260494711
Loss in iteration 113 : 0.7231736413586397
Loss in iteration 114 : 0.5975057046065468
Loss in iteration 115 : 0.6329194034336495
Loss in iteration 116 : 0.5851314315555485
Loss in iteration 117 : 0.5900328466203274
Loss in iteration 118 : 0.5724837277730005
Loss in iteration 119 : 0.6012667845750559
Loss in iteration 120 : 0.5632178390089219
Loss in iteration 121 : 0.6828822828192654
Loss in iteration 122 : 0.7019793784323632
Loss in iteration 123 : 1.1003313541082793
Loss in iteration 124 : 1.0016651124725857
Loss in iteration 125 : 0.899308271227097
Loss in iteration 126 : 0.692675142867349
Loss in iteration 127 : 0.5619112338337869
Loss in iteration 128 : 0.5114388489562286
Loss in iteration 129 : 0.5165340428180818
Loss in iteration 130 : 0.5374810067596348
Loss in iteration 131 : 0.6201771882956559
Loss in iteration 132 : 0.7656726006870287
Loss in iteration 133 : 0.8711446853979115
Loss in iteration 134 : 0.893911412880825
Loss in iteration 135 : 0.6314428466396389
Loss in iteration 136 : 0.5323374637223149
Loss in iteration 137 : 0.5117328159356976
Loss in iteration 138 : 0.5715011372964045
Loss in iteration 139 : 0.6637445662299714
Loss in iteration 140 : 0.6693549804169372
Loss in iteration 141 : 0.6518336091821213
Loss in iteration 142 : 0.575294275913389
Loss in iteration 143 : 0.55103111237297
Loss in iteration 144 : 0.5232499637284083
Loss in iteration 145 : 0.5340755714036927
Loss in iteration 146 : 0.5560131841678858
Loss in iteration 147 : 0.6697185398585914
Loss in iteration 148 : 0.8509661779163341
Loss in iteration 149 : 1.1108728606998495
Loss in iteration 150 : 0.7151517163761956
Loss in iteration 151 : 0.573105219413319
Loss in iteration 152 : 0.49929954389714437
Loss in iteration 153 : 0.5947832403869348
Loss in iteration 154 : 0.6490852492713519
Loss in iteration 155 : 0.6605821062727988
Loss in iteration 156 : 0.6452147120103421
Loss in iteration 157 : 0.5585487100402973
Loss in iteration 158 : 0.5768607636692761
Loss in iteration 159 : 0.5027442046368047
Loss in iteration 160 : 0.5563079266985215
Loss in iteration 161 : 0.49317872052420453
Loss in iteration 162 : 0.5740317210205915
Loss in iteration 163 : 0.5590852469974299
Loss in iteration 164 : 0.7259818690521317
Loss in iteration 165 : 0.7360649135460223
Loss in iteration 166 : 0.8081568618756981
Loss in iteration 167 : 0.626622980368741
Loss in iteration 168 : 0.5446812856722049
Loss in iteration 169 : 0.4890762262764889
Loss in iteration 170 : 0.4930115830959903
Loss in iteration 171 : 0.5230001791883174
Loss in iteration 172 : 0.5985232372318896
Loss in iteration 173 : 0.7664417937121872
Loss in iteration 174 : 0.8267558319943032
Loss in iteration 175 : 0.8417938146816967
Loss in iteration 176 : 0.5975102638586112
Loss in iteration 177 : 0.5270093248849282
Loss in iteration 178 : 0.4898361153674099
Loss in iteration 179 : 0.5509803176662088
Loss in iteration 180 : 0.5719583875234295
Loss in iteration 181 : 0.5658245771219277
Loss in iteration 182 : 0.5464318416375158
Loss in iteration 183 : 0.49448166901132834
Loss in iteration 184 : 0.5170675219620173
Loss in iteration 185 : 0.4749934311584244
Loss in iteration 186 : 0.5045753576119316
Loss in iteration 187 : 0.47202950922397097
Loss in iteration 188 : 0.49874509423560537
Loss in iteration 189 : 0.47753476465189126
Loss in iteration 190 : 0.5214420690477437
Loss in iteration 191 : 0.5953014800653935
Loss in iteration 192 : 0.8813785528448305
Loss in iteration 193 : 0.9928674240742925
Loss in iteration 194 : 0.8967485774354508
Loss in iteration 195 : 0.5696290958763208
Loss in iteration 196 : 0.491423161413993
Loss in iteration 197 : 0.6191429236217634
Loss in iteration 198 : 0.7384790909519265
Loss in iteration 199 : 0.667681753839879
Loss in iteration 200 : 0.5139399529286431
Testing accuracy  of updater 9 on alg 0 with rate 2.0 = 0.789, training accuracy 0.784, time elapsed: 4893 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6775654358183358
Loss in iteration 3 : 0.6667698180414757
Loss in iteration 4 : 0.6478185412729558
Loss in iteration 5 : 0.6269478780098184
Loss in iteration 6 : 0.6115534194052704
Loss in iteration 7 : 0.5920591232964466
Loss in iteration 8 : 0.5718629171929451
Loss in iteration 9 : 0.5578847813712109
Loss in iteration 10 : 0.5453959775867726
Loss in iteration 11 : 0.5328626797318885
Loss in iteration 12 : 0.5239845778172608
Loss in iteration 13 : 0.517580047759391
Loss in iteration 14 : 0.5105429041774788
Loss in iteration 15 : 0.504159172427489
Loss in iteration 16 : 0.5001022867281735
Loss in iteration 17 : 0.4969416313077019
Loss in iteration 18 : 0.49347236383223786
Loss in iteration 19 : 0.49063428709992213
Loss in iteration 20 : 0.4890062348165791
Loss in iteration 21 : 0.48760075615439985
Loss in iteration 22 : 0.48578956291388015
Loss in iteration 23 : 0.4841837427000639
Loss in iteration 24 : 0.4832003523808912
Loss in iteration 25 : 0.48234663537530753
Loss in iteration 26 : 0.48123918524716114
Loss in iteration 27 : 0.4802072918821887
Loss in iteration 28 : 0.47956460770468456
Loss in iteration 29 : 0.47905997714573717
Loss in iteration 30 : 0.4783794763697016
Loss in iteration 31 : 0.4776439059334639
Loss in iteration 32 : 0.4770883575986412
Loss in iteration 33 : 0.4766309933787335
Loss in iteration 34 : 0.4760613681875674
Loss in iteration 35 : 0.4754086086640555
Loss in iteration 36 : 0.47484961240706736
Loss in iteration 37 : 0.4743959183956057
Loss in iteration 38 : 0.4739103616128415
Loss in iteration 39 : 0.4733608070253445
Loss in iteration 40 : 0.4728512244395603
Loss in iteration 41 : 0.4724237692854805
Loss in iteration 42 : 0.4719997964197049
Loss in iteration 43 : 0.4715295548866597
Loss in iteration 44 : 0.4710674380812905
Loss in iteration 45 : 0.47066499969316583
Loss in iteration 46 : 0.47029010621859996
Loss in iteration 47 : 0.4698982379138109
Loss in iteration 48 : 0.4695101612135692
Loss in iteration 49 : 0.4691672510194749
Loss in iteration 50 : 0.4688599981415932
Loss in iteration 51 : 0.4685515459784515
Loss in iteration 52 : 0.4682405900674835
Loss in iteration 53 : 0.46795441414239225
Loss in iteration 54 : 0.4676970554212584
Loss in iteration 55 : 0.4674459202915074
Loss in iteration 56 : 0.46719335669685574
Loss in iteration 57 : 0.4669563819953702
Loss in iteration 58 : 0.46674410952822337
Loss in iteration 59 : 0.4665439615124384
Loss in iteration 60 : 0.4663457374725648
Loss in iteration 61 : 0.4661569579708942
Loss in iteration 62 : 0.4659859517622724
Loss in iteration 63 : 0.46582666196895883
Loss in iteration 64 : 0.4656700759142661
Loss in iteration 65 : 0.4655185902711201
Loss in iteration 66 : 0.46537905114731587
Loss in iteration 67 : 0.46524991517002773
Loss in iteration 68 : 0.4651247932711584
Loss in iteration 69 : 0.4650033428729301
Loss in iteration 70 : 0.464890017122313
Loss in iteration 71 : 0.46478499767864095
Loss in iteration 72 : 0.46468384861510287
Loss in iteration 73 : 0.4645849013926653
Loss in iteration 74 : 0.46449072092546123
Loss in iteration 75 : 0.4644023714783392
Loss in iteration 76 : 0.4643173825795638
Loss in iteration 77 : 0.46423408336448985
Loss in iteration 78 : 0.4641538655087869
Loss in iteration 79 : 0.46407797013654667
Loss in iteration 80 : 0.46400515160841477
Loss in iteration 81 : 0.46393392255021354
Loss in iteration 82 : 0.46386476684391953
Loss in iteration 83 : 0.46379864304933494
Loss in iteration 84 : 0.46373498868550517
Loss in iteration 85 : 0.4636726643586714
Loss in iteration 86 : 0.4636117216267568
Loss in iteration 87 : 0.46355289664358224
Loss in iteration 88 : 0.4634960968333155
Loss in iteration 89 : 0.46344057904222247
Loss in iteration 90 : 0.4633861873397752
Loss in iteration 91 : 0.4633333769300109
Loss in iteration 92 : 0.46328221242632817
Loss in iteration 93 : 0.4632322089307435
Loss in iteration 94 : 0.46318312736020495
Loss in iteration 95 : 0.4631352225056537
Loss in iteration 96 : 0.463088635950808
Loss in iteration 97 : 0.4630431066903915
Loss in iteration 98 : 0.46299841912303513
Loss in iteration 99 : 0.4629546988028856
Loss in iteration 100 : 0.46291208820651825
Loss in iteration 101 : 0.4628704571038001
Loss in iteration 102 : 0.4628296214784438
Loss in iteration 103 : 0.46278960913307904
Loss in iteration 104 : 0.4627505213421037
Loss in iteration 105 : 0.4627122993470623
Loss in iteration 106 : 0.4626748046350244
Loss in iteration 107 : 0.4626380209002925
Loss in iteration 108 : 0.4626020181707228
Loss in iteration 109 : 0.4625667831550046
Loss in iteration 110 : 0.4625322225937737
Loss in iteration 111 : 0.4624983000252647
Loss in iteration 112 : 0.46246505212743705
Loss in iteration 113 : 0.46243248020049676
Loss in iteration 114 : 0.46240052157345224
Loss in iteration 115 : 0.46236913376982414
Loss in iteration 116 : 0.4623383301318579
Loss in iteration 117 : 0.4623081170796505
Loss in iteration 118 : 0.4622784564286462
Loss in iteration 119 : 0.4622493107168097
Loss in iteration 120 : 0.46222067988412596
Loss in iteration 121 : 0.4621925693830453
Loss in iteration 122 : 0.462164955561221
Loss in iteration 123 : 0.46213780637028107
Loss in iteration 124 : 0.4621111123713005
Loss in iteration 125 : 0.4620848745610762
Loss in iteration 126 : 0.46205907780848277
Loss in iteration 127 : 0.46203369717837267
Loss in iteration 128 : 0.4620087204178594
Loss in iteration 129 : 0.46198414593651815
Loss in iteration 130 : 0.4619599641725686
Loss in iteration 131 : 0.46193615649545267
Loss in iteration 132 : 0.46191271007716295
Loss in iteration 133 : 0.46188962081685336
Loss in iteration 134 : 0.46186688167067513
Loss in iteration 135 : 0.4618444786521734
Loss in iteration 136 : 0.46182239975177697
Loss in iteration 137 : 0.4618006394351397
Loss in iteration 138 : 0.4617791920412493
Loss in iteration 139 : 0.4617580472161672
Loss in iteration 140 : 0.46173719461749496
Loss in iteration 141 : 0.4617166282269091
Loss in iteration 142 : 0.4616963430716144
Loss in iteration 143 : 0.46167633122304913
Loss in iteration 144 : 0.4616565838763894
Loss in iteration 145 : 0.4616370948722487
Loss in iteration 146 : 0.4616178594938125
Loss in iteration 147 : 0.4615988714508481
Loss in iteration 148 : 0.4615801234463217
Loss in iteration 149 : 0.4615616096932241
Loss in iteration 150 : 0.46154332577981055
Loss in iteration 151 : 0.46152526658782267
Loss in iteration 152 : 0.4615074260978506
Loss in iteration 153 : 0.4614897990295763
Loss in iteration 154 : 0.46147238120023576
Loss in iteration 155 : 0.4614551682278293
Loss in iteration 156 : 0.4614381550589133
Loss in iteration 157 : 0.46142133695280463
Loss in iteration 158 : 0.4614047099911905
Loss in iteration 159 : 0.4613882703425431
Loss in iteration 160 : 0.4613720137449652
Loss in iteration 161 : 0.4613559360245261
Loss in iteration 162 : 0.46134003357001213
Loss in iteration 163 : 0.46132430296317195
Loss in iteration 164 : 0.46130874053768484
Loss in iteration 165 : 0.4612933426164295
Loss in iteration 166 : 0.46127810589488805
Loss in iteration 167 : 0.46126302729334223
Loss in iteration 168 : 0.4612481036227812
Loss in iteration 169 : 0.46123333165769803
Loss in iteration 170 : 0.46121870841090307
Loss in iteration 171 : 0.4612042311034139
Loss in iteration 172 : 0.4611898969324828
Loss in iteration 173 : 0.461175703062072
Loss in iteration 174 : 0.46116164680470134
Loss in iteration 175 : 0.46114772564646345
Loss in iteration 176 : 0.46113393709789174
Loss in iteration 177 : 0.461120278651509
Loss in iteration 178 : 0.4611067478943158
Loss in iteration 179 : 0.46109334255267653
Loss in iteration 180 : 0.46108006040322663
Loss in iteration 181 : 0.4610668992224214
Loss in iteration 182 : 0.4610538568494213
Loss in iteration 183 : 0.4610409312310415
Loss in iteration 184 : 0.4610281203726081
Loss in iteration 185 : 0.4610154222924491
Loss in iteration 186 : 0.4610028350534975
Loss in iteration 187 : 0.46099035680120565
Loss in iteration 188 : 0.4609779857396909
Loss in iteration 189 : 0.4609657200956648
Loss in iteration 190 : 0.46095355813119804
Loss in iteration 191 : 0.4609414981720869
Loss in iteration 192 : 0.4609295385983471
Loss in iteration 193 : 0.46091767781769133
Loss in iteration 194 : 0.4609059142678601
Loss in iteration 195 : 0.46089424643615085
Loss in iteration 196 : 0.46088267285756634
Loss in iteration 197 : 0.4608711920967635
Loss in iteration 198 : 0.46085980274564176
Loss in iteration 199 : 0.46084850343592126
Loss in iteration 200 : 0.46083729284070407
Testing accuracy  of updater 9 on alg 0 with rate 0.2 = 0.786, training accuracy 0.788375, time elapsed: 4481 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6910505960040544
Loss in iteration 3 : 0.6875206638392688
Loss in iteration 4 : 0.6833259613971904
Loss in iteration 5 : 0.6790835990852695
Loss in iteration 6 : 0.675096557469224
Loss in iteration 7 : 0.6713507316190158
Loss in iteration 8 : 0.66763646183405
Loss in iteration 9 : 0.6637112535625094
Loss in iteration 10 : 0.6594267998330577
Loss in iteration 11 : 0.6547819930528329
Loss in iteration 12 : 0.6499032242272501
Loss in iteration 13 : 0.6449784344441039
Loss in iteration 14 : 0.6401802601175258
Loss in iteration 15 : 0.6356092687328421
Loss in iteration 16 : 0.6312748145690793
Loss in iteration 17 : 0.6271137929872416
Loss in iteration 18 : 0.6230330224151422
Loss in iteration 19 : 0.6189544551332802
Loss in iteration 20 : 0.6148453465051442
Loss in iteration 21 : 0.6107249716891829
Loss in iteration 22 : 0.606650275433848
Loss in iteration 23 : 0.6026903405597517
Loss in iteration 24 : 0.5989015376087051
Loss in iteration 25 : 0.5953122002271951
Loss in iteration 26 : 0.5919200636554149
Loss in iteration 27 : 0.5887003264664226
Loss in iteration 28 : 0.5856190058283683
Loss in iteration 29 : 0.5826458035060231
Loss in iteration 30 : 0.579762351962883
Loss in iteration 31 : 0.5769642915530537
Loss in iteration 32 : 0.5742580298367503
Loss in iteration 33 : 0.5716545462574388
Loss in iteration 34 : 0.5691629987575555
Loss in iteration 35 : 0.5667863198385885
Loss in iteration 36 : 0.5645198543357591
Loss in iteration 37 : 0.5623528657852839
Loss in iteration 38 : 0.5602718296011997
Loss in iteration 39 : 0.5582640670221023
Loss in iteration 40 : 0.5563204607930272
Loss in iteration 41 : 0.5544365552219338
Loss in iteration 42 : 0.5526120144192653
Loss in iteration 43 : 0.5508489479891243
Loss in iteration 44 : 0.5491498672322468
Loss in iteration 45 : 0.5475159874014697
Loss in iteration 46 : 0.5459463265668348
Loss in iteration 47 : 0.5444377040045306
Loss in iteration 48 : 0.542985439922816
Loss in iteration 49 : 0.5415843870817524
Loss in iteration 50 : 0.540229905963378
Loss in iteration 51 : 0.538918498734672
Loss in iteration 52 : 0.5376479835116376
Loss in iteration 53 : 0.5364172551294414
Loss in iteration 54 : 0.5352257930915657
Loss in iteration 55 : 0.5340731189079791
Loss in iteration 56 : 0.5329583773523009
Loss in iteration 57 : 0.5318801423712314
Loss in iteration 58 : 0.5308364600851402
Loss in iteration 59 : 0.5298250677195181
Loss in iteration 60 : 0.5288436871691893
Loss in iteration 61 : 0.5278902901908802
Loss in iteration 62 : 0.5269632614786708
Loss in iteration 63 : 0.5260614309796335
Loss in iteration 64 : 0.5251839910788126
Loss in iteration 65 : 0.5243303447999391
Loss in iteration 66 : 0.5234999418192329
Loss in iteration 67 : 0.522692150934164
Loss in iteration 68 : 0.521906197034474
Loss in iteration 69 : 0.5211411662304606
Loss in iteration 70 : 0.5203960624850252
Loss in iteration 71 : 0.5196698878629601
Loss in iteration 72 : 0.5189617176460286
Loss in iteration 73 : 0.5182707491037088
Loss in iteration 74 : 0.517596314718226
Loss in iteration 75 : 0.5169378627762896
Loss in iteration 76 : 0.5162949169993261
Loss in iteration 77 : 0.5156670305116896
Loss in iteration 78 : 0.5150537479863639
Loss in iteration 79 : 0.5144545847263569
Loss in iteration 80 : 0.5138690249164138
Loss in iteration 81 : 0.5132965354470534
Loss in iteration 82 : 0.5127365880705433
Loss in iteration 83 : 0.5121886817730927
Loss in iteration 84 : 0.5116523588213093
Loss in iteration 85 : 0.5111272110245912
Loss in iteration 86 : 0.5106128761790703
Loss in iteration 87 : 0.5101090273978988
Loss in iteration 88 : 0.5096153594640395
Loss in iteration 89 : 0.5091315763345468
Loss in iteration 90 : 0.5086573827764662
Loss in iteration 91 : 0.5081924813761594
Loss in iteration 92 : 0.5077365744379039
Loss in iteration 93 : 0.5072893690479069
Loss in iteration 94 : 0.5068505830684151
Loss in iteration 95 : 0.50641995003479
Loss in iteration 96 : 0.5059972216518566
Loss in iteration 97 : 0.5055821675205164
Loss in iteration 98 : 0.5051745725724162
Loss in iteration 99 : 0.5047742332351357
Loss in iteration 100 : 0.5043809535051469
Loss in iteration 101 : 0.5039945419070082
Loss in iteration 102 : 0.5036148098902027
Loss in iteration 103 : 0.503241571721943
Loss in iteration 104 : 0.5028746455224399
Loss in iteration 105 : 0.5025138548543683
Loss in iteration 106 : 0.5021590302489035
Loss in iteration 107 : 0.5018100101938417
Loss in iteration 108 : 0.5014666413518565
Loss in iteration 109 : 0.501128778033645
Loss in iteration 110 : 0.5007962811494768
Loss in iteration 111 : 0.5004690169615695
Loss in iteration 112 : 0.500146855952118
Loss in iteration 113 : 0.49982967203094486
Loss in iteration 114 : 0.4995173421738557
Loss in iteration 115 : 0.4992097464516944
Loss in iteration 116 : 0.49890676831582764
Loss in iteration 117 : 0.4986082949660482
Loss in iteration 118 : 0.4983142176415975
Loss in iteration 119 : 0.49802443173024075
Loss in iteration 120 : 0.4977388366619016
Loss in iteration 121 : 0.49745733561954003
Loss in iteration 122 : 0.4971798351443259
Loss in iteration 123 : 0.4969062447271232
Loss in iteration 124 : 0.4966364764656771
Loss in iteration 125 : 0.49637044483557397
Loss in iteration 126 : 0.4961080665849223
Loss in iteration 127 : 0.49584926072933105
Loss in iteration 128 : 0.4955939486030079
Loss in iteration 129 : 0.49534205391671926
Loss in iteration 130 : 0.4950935027822012
Loss in iteration 131 : 0.4948482236803555
Loss in iteration 132 : 0.49460614737076963
Loss in iteration 133 : 0.4943672067569464
Loss in iteration 134 : 0.49413133673139464
Loss in iteration 135 : 0.4938984740261772
Loss in iteration 136 : 0.4936685570889348
Loss in iteration 137 : 0.4934415259946218
Loss in iteration 138 : 0.4932173223925906
Loss in iteration 139 : 0.4929958894802306
Loss in iteration 140 : 0.4927771719897933
Loss in iteration 141 : 0.4925611161748946
Loss in iteration 142 : 0.4923476697864945
Loss in iteration 143 : 0.4921367820335339
Loss in iteration 144 : 0.4919284035288791
Loss in iteration 145 : 0.49172248622552056
Loss in iteration 146 : 0.49151898335002603
Loss in iteration 147 : 0.49131784934010203
Loss in iteration 148 : 0.4911190397911684
Loss in iteration 149 : 0.49092251141395526
Loss in iteration 150 : 0.49072822200229027
Loss in iteration 151 : 0.4905361304081059
Loss in iteration 152 : 0.490346196519747
Loss in iteration 153 : 0.49015838123988986
Loss in iteration 154 : 0.489972646460511
Loss in iteration 155 : 0.48978895503388464
Loss in iteration 156 : 0.4896072707401421
Loss in iteration 157 : 0.4894275582529269
Loss in iteration 158 : 0.48924978310522677
Loss in iteration 159 : 0.48907391165719777
Loss in iteration 160 : 0.4888999110672488
Loss in iteration 161 : 0.48872774926678064
Loss in iteration 162 : 0.4885573949382058
Loss in iteration 163 : 0.48838881749534363
Loss in iteration 164 : 0.48822198706505165
Loss in iteration 165 : 0.4880568744690941
Loss in iteration 166 : 0.4878934512055731
Loss in iteration 167 : 0.48773168942968487
Loss in iteration 168 : 0.4875715619339977
Loss in iteration 169 : 0.48741304212867076
Loss in iteration 170 : 0.4872561040221707
Loss in iteration 171 : 0.4871007222029649
Loss in iteration 172 : 0.48694687182244095
Loss in iteration 173 : 0.48679452857915656
Loss in iteration 174 : 0.486643668704213
Loss in iteration 175 : 0.4864942689475008
Loss in iteration 176 : 0.48634630656446354
Loss in iteration 177 : 0.4861997593031096
Loss in iteration 178 : 0.486054605391094
Loss in iteration 179 : 0.4859108235228057
Loss in iteration 180 : 0.4857683928465348
Loss in iteration 181 : 0.4856272929518433
Loss in iteration 182 : 0.48548750385728934
Loss in iteration 183 : 0.4853490059986387
Loss in iteration 184 : 0.485211780217628
Loss in iteration 185 : 0.48507580775128645
Loss in iteration 186 : 0.4849410702217757
Loss in iteration 187 : 0.4848075496266412
Loss in iteration 188 : 0.4846752283294052
Loss in iteration 189 : 0.4845440890503932
Loss in iteration 190 : 0.4844141148577618
Loss in iteration 191 : 0.48428528915869884
Loss in iteration 192 : 0.4841575956908139
Loss in iteration 193 : 0.4840310185137451
Loss in iteration 194 : 0.48390554200101205
Loss in iteration 195 : 0.4837811508321571
Loss in iteration 196 : 0.48365782998515855
Loss in iteration 197 : 0.4835355647291385
Loss in iteration 198 : 0.4834143406173224
Loss in iteration 199 : 0.48329414348022204
Loss in iteration 200 : 0.4831749594190235
Testing accuracy  of updater 9 on alg 0 with rate 0.01999999999999999 = 0.7795, training accuracy 0.780875, time elapsed: 4752 millisecond.
