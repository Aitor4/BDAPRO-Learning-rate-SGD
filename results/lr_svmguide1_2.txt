objc[2683]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x10d6384c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10d6bc4e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 21:10:59 INFO SparkContext: Running Spark version 2.0.0
18/02/26 21:10:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 21:11:00 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 21:11:00 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 21:11:00 INFO SecurityManager: Changing view acls groups to: 
18/02/26 21:11:00 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 21:11:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 21:11:00 INFO Utils: Successfully started service 'sparkDriver' on port 51491.
18/02/26 21:11:00 INFO SparkEnv: Registering MapOutputTracker
18/02/26 21:11:01 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 21:11:01 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-31e7699d-c339-4956-b8e6-76e01ae913eb
18/02/26 21:11:01 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 21:11:01 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 21:11:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 21:11:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 21:11:01 INFO Executor: Starting executor ID driver on host localhost
18/02/26 21:11:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51492.
18/02/26 21:11:01 INFO NettyBlockTransferService: Server created on 192.168.2.140:51492
18/02/26 21:11:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 51492)
18/02/26 21:11:01 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:51492 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 51492)
18/02/26 21:11:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 51492)
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 13.753468193395502
Loss in iteration 3 : 2.8731164885783342
Loss in iteration 4 : 5.934790819908981
Loss in iteration 5 : 12.078671575612997
Loss in iteration 6 : 2.164951132969383
Loss in iteration 7 : 1.7461102577832277
Loss in iteration 8 : 1.8386379722840651
Loss in iteration 9 : 1.6497547514932578
Loss in iteration 10 : 1.9007542649531308
Loss in iteration 11 : 1.833803088211361
Loss in iteration 12 : 2.653274914831811
Loss in iteration 13 : 2.9860093836662567
Loss in iteration 14 : 5.411565289504908
Loss in iteration 15 : 2.665164584288548
Loss in iteration 16 : 3.5688249834765573
Loss in iteration 17 : 2.6654612865157614
Loss in iteration 18 : 3.2543955550121515
Loss in iteration 19 : 2.3031512337235465
Loss in iteration 20 : 2.575084548445875
Loss in iteration 21 : 1.8866548821008944
Loss in iteration 22 : 2.0607406139475546
Loss in iteration 23 : 1.6974948585410166
Loss in iteration 24 : 1.9366562030500074
Loss in iteration 25 : 1.7340113145053386
Loss in iteration 26 : 2.2604101517409414
Loss in iteration 27 : 2.1957387775661763
Loss in iteration 28 : 3.6353956483158876
Loss in iteration 29 : 3.397174962642034
Loss in iteration 30 : 5.528570006096163
Loss in iteration 31 : 2.359132629670231
Loss in iteration 32 : 2.6343819262764314
Loss in iteration 33 : 1.8537653876882674
Loss in iteration 34 : 1.99505575610305
Loss in iteration 35 : 1.633839168426967
Loss in iteration 36 : 1.8241157832238346
Loss in iteration 37 : 1.6311805886522686
Loss in iteration 38 : 2.0513059391483983
Loss in iteration 39 : 1.9622591936327158
Loss in iteration 40 : 3.128103544383247
Loss in iteration 41 : 3.2446034175831913
Loss in iteration 42 : 5.734723263256398
Loss in iteration 43 : 2.278888348551951
Loss in iteration 44 : 2.6805402867183012
Loss in iteration 45 : 1.9297431746075464
Loss in iteration 46 : 2.1989779635320743
Loss in iteration 47 : 1.7457544681571362
Loss in iteration 48 : 2.0543680922371785
Loss in iteration 49 : 1.7700155245793892
Loss in iteration 50 : 2.308730598757707
Loss in iteration 51 : 2.1061291633773505
Loss in iteration 52 : 3.306086018154294
Loss in iteration 53 : 3.049767160890208
Loss in iteration 54 : 4.981991331136652
Loss in iteration 55 : 2.5732825190833277
Loss in iteration 56 : 3.119715116269591
Loss in iteration 57 : 2.08000511039173
Loss in iteration 58 : 2.2788149623523917
Loss in iteration 59 : 1.699758200138421
Loss in iteration 60 : 1.8530141995268408
Loss in iteration 61 : 1.583244821225464
Loss in iteration 62 : 1.8414890459468312
Loss in iteration 63 : 1.6865070070923882
Loss in iteration 64 : 2.2803568425024254
Loss in iteration 65 : 2.2216966552486705
Loss in iteration 66 : 3.8762929008373996
Loss in iteration 67 : 3.3230118419924484
Loss in iteration 68 : 5.445446563709316
Loss in iteration 69 : 2.326156901520118
Loss in iteration 70 : 2.6236664392164037
Loss in iteration 71 : 1.7945553148742264
Loss in iteration 72 : 1.9221495807114994
Loss in iteration 73 : 1.5821944119113804
Loss in iteration 74 : 1.7660243246870315
Loss in iteration 75 : 1.5803284509889008
Loss in iteration 76 : 1.960096392794984
Loss in iteration 77 : 1.8542470394909634
Loss in iteration 78 : 2.8848532310582815
Loss in iteration 79 : 2.9725962124017364
Loss in iteration 80 : 5.388417781889063
Loss in iteration 81 : 2.484154933930217
Loss in iteration 82 : 3.21716414384107
Loss in iteration 83 : 2.2342558501711247
Loss in iteration 84 : 2.6394788209767848
Loss in iteration 85 : 1.8631171192373162
Loss in iteration 86 : 2.093460103651445
Loss in iteration 87 : 1.6857062538395755
Loss in iteration 88 : 1.969414091778942
Loss in iteration 89 : 1.7178230561096346
Loss in iteration 90 : 2.2288652535040017
Loss in iteration 91 : 2.0265976355441606
Loss in iteration 92 : 3.2029221114999875
Loss in iteration 93 : 2.989251852901036
Loss in iteration 94 : 5.001968254615825
Loss in iteration 95 : 2.5708843474892666
Loss in iteration 96 : 3.1995941231919183
Loss in iteration 97 : 2.1152597041651764
Loss in iteration 98 : 2.3479828312906164
Loss in iteration 99 : 1.7114860756312291
Loss in iteration 100 : 1.875272577376148
Loss in iteration 101 : 1.5860866977792272
Loss in iteration 102 : 1.8440108057281286
Loss in iteration 103 : 1.6696488868148942
Loss in iteration 104 : 2.2165579649918916
Loss in iteration 105 : 2.0956473378747926
Loss in iteration 106 : 3.5314888765688828
Loss in iteration 107 : 3.2217437375972304
Loss in iteration 108 : 5.414112386546317
Loss in iteration 109 : 2.3521528748746916
Loss in iteration 110 : 2.7555351198283837
Loss in iteration 111 : 1.8574436375759558
Loss in iteration 112 : 2.0167051902187327
Loss in iteration 113 : 1.6204135106776152
Loss in iteration 114 : 1.8233418025055486
Loss in iteration 115 : 1.6034313129889397
Loss in iteration 116 : 1.9760468456658675
Loss in iteration 117 : 1.8274262735531615
Loss in iteration 118 : 2.7464867606151957
Loss in iteration 119 : 2.73505350776642
Loss in iteration 120 : 4.8571068486280256
Loss in iteration 121 : 2.775528432925964
Loss in iteration 122 : 3.825725143017532
Loss in iteration 123 : 2.432881568256998
Loss in iteration 124 : 2.8653293681458245
Loss in iteration 125 : 1.8890743795894613
Loss in iteration 126 : 2.0297352270383753
Loss in iteration 127 : 1.6121893160110623
Loss in iteration 128 : 1.7851421799593232
Loss in iteration 129 : 1.5657105361507417
Loss in iteration 130 : 1.879260229243011
Loss in iteration 131 : 1.7456356764747423
Loss in iteration 132 : 2.5157438621499404
Loss in iteration 133 : 2.4946806818663028
Loss in iteration 134 : 4.466180170092458
Loss in iteration 135 : 3.0378196748916104
Loss in iteration 136 : 4.56512530219717
Loss in iteration 137 : 2.5194480819026666
Loss in iteration 138 : 2.9441684100433165
Loss in iteration 139 : 1.8923419731911495
Loss in iteration 140 : 1.9986920762457567
Loss in iteration 141 : 1.5863822531869483
Loss in iteration 142 : 1.7217422907754263
Loss in iteration 143 : 1.5195847253471277
Loss in iteration 144 : 1.7754573106861604
Loss in iteration 145 : 1.6605497830929306
Loss in iteration 146 : 2.2987793290672154
Loss in iteration 147 : 2.268422992215248
Loss in iteration 148 : 4.077796202910278
Loss in iteration 149 : 3.2741606102189147
Loss in iteration 150 : 5.320288223031888
Loss in iteration 151 : 2.3458968005169427
Loss in iteration 152 : 2.6789874548919212
Loss in iteration 153 : 1.7908431160760823
Loss in iteration 154 : 1.9155198727473726
Loss in iteration 155 : 1.5697895015839667
Loss in iteration 156 : 1.7416547045970046
Loss in iteration 157 : 1.5546843037219558
Loss in iteration 158 : 1.8940581396867777
Loss in iteration 159 : 1.7836524716051332
Loss in iteration 160 : 2.686190360472599
Loss in iteration 161 : 2.7258096560168754
Loss in iteration 162 : 4.952123848210314
Loss in iteration 163 : 2.7583971214167438
Loss in iteration 164 : 3.854054344952478
Loss in iteration 165 : 2.4563512757925925
Loss in iteration 166 : 2.9291788374345553
Loss in iteration 167 : 1.924327836960723
Loss in iteration 168 : 2.076634830500947
Loss in iteration 169 : 1.6278605122613337
Loss in iteration 170 : 1.800648634725079
Loss in iteration 171 : 1.5686604409468325
Loss in iteration 172 : 1.8692826990576623
Loss in iteration 173 : 1.7263526551464372
Loss in iteration 174 : 2.441402944167079
Loss in iteration 175 : 2.3827757892573005
Loss in iteration 176 : 4.216144641571102
Loss in iteration 177 : 3.1351739851441778
Loss in iteration 178 : 4.857089524442639
Loss in iteration 179 : 2.4722955644524807
Loss in iteration 180 : 2.8671057306501173
Loss in iteration 181 : 1.85406093867531
Loss in iteration 182 : 1.9618957096535417
Loss in iteration 183 : 1.5751839524576559
Loss in iteration 184 : 1.7155356454044461
Loss in iteration 185 : 1.5219556761037631
Loss in iteration 186 : 1.7928763405291077
Loss in iteration 187 : 1.6832955310312612
Loss in iteration 188 : 2.374834844718572
Loss in iteration 189 : 2.362003322355453
Loss in iteration 190 : 4.283067690852015
Loss in iteration 191 : 3.182781048275535
Loss in iteration 192 : 5.027316074628762
Loss in iteration 193 : 2.434344211384815
Loss in iteration 194 : 2.815064211123186
Loss in iteration 195 : 1.8361140408275398
Loss in iteration 196 : 1.9502926268908616
Loss in iteration 197 : 1.5744530762776965
Loss in iteration 198 : 1.724330654877958
Loss in iteration 199 : 1.5314892837092218
Loss in iteration 200 : 1.8210423662893704
Testing accuracy  of updater 0 on alg 0 with rate 0.010000000000000002 = 0.783, training accuracy 0.7821301392036257, time elapsed: 5918 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 9.627432113361973
Loss in iteration 3 : 2.0258538014929073
Loss in iteration 4 : 3.9809369775961345
Loss in iteration 5 : 8.213622186970802
Loss in iteration 6 : 1.4080221057736775
Loss in iteration 7 : 1.194860911339089
Loss in iteration 8 : 1.2678541834067902
Loss in iteration 9 : 1.169815517196293
Loss in iteration 10 : 1.3897004078238246
Loss in iteration 11 : 1.388115916081245
Loss in iteration 12 : 2.132116627781219
Loss in iteration 13 : 2.36408493270697
Loss in iteration 14 : 4.134866532288369
Loss in iteration 15 : 1.6101422643638117
Loss in iteration 16 : 1.888586746520404
Loss in iteration 17 : 1.4586178406541166
Loss in iteration 18 : 1.6985074670460316
Loss in iteration 19 : 1.379026149142944
Loss in iteration 20 : 1.6627995249336573
Loss in iteration 21 : 1.4250844160495237
Loss in iteration 22 : 1.8707773581871303
Loss in iteration 23 : 1.6784293189595898
Loss in iteration 24 : 2.4649517798132323
Loss in iteration 25 : 2.0363936888573226
Loss in iteration 26 : 2.898521853200737
Loss in iteration 27 : 1.8775049130871184
Loss in iteration 28 : 2.224893800162489
Loss in iteration 29 : 1.501843214807214
Loss in iteration 30 : 1.6116732853099034
Loss in iteration 31 : 1.2202401014656374
Loss in iteration 32 : 1.3008164997627836
Loss in iteration 33 : 1.1243746458561688
Loss in iteration 34 : 1.2695368951621973
Loss in iteration 35 : 1.1775417459724624
Loss in iteration 36 : 1.5526011669953061
Loss in iteration 37 : 1.558049989701558
Loss in iteration 38 : 2.6838019280093093
Loss in iteration 39 : 2.375663655477991
Loss in iteration 40 : 3.9044882827356857
Loss in iteration 41 : 1.6203808678086267
Loss in iteration 42 : 1.818033385377431
Loss in iteration 43 : 1.2963115986160483
Loss in iteration 44 : 1.4029517171128072
Loss in iteration 45 : 1.1570225572062174
Loss in iteration 46 : 1.2955675375112659
Loss in iteration 47 : 1.1614357354468416
Loss in iteration 48 : 1.4533434558476979
Loss in iteration 49 : 1.3814523709977904
Loss in iteration 50 : 2.1536396672239984
Loss in iteration 51 : 2.145044248037196
Loss in iteration 52 : 3.7182829317710846
Loss in iteration 53 : 1.752547090930511
Loss in iteration 54 : 2.1831429068995054
Loss in iteration 55 : 1.5187051003021694
Loss in iteration 56 : 1.735961482057277
Loss in iteration 57 : 1.28208289485378
Loss in iteration 58 : 1.4371366483574202
Loss in iteration 59 : 1.1925127403420104
Loss in iteration 60 : 1.4024427764081815
Loss in iteration 61 : 1.246313103243397
Loss in iteration 62 : 1.6577258655059328
Loss in iteration 63 : 1.5508169727138688
Loss in iteration 64 : 2.5039270575727057
Loss in iteration 65 : 2.160652592947684
Loss in iteration 66 : 3.4271846589271613
Loss in iteration 67 : 1.7819122068550577
Loss in iteration 68 : 2.1165130431802144
Loss in iteration 69 : 1.4071648709912508
Loss in iteration 70 : 1.5223615863649471
Loss in iteration 71 : 1.1768663882765316
Loss in iteration 72 : 1.2777943222662556
Loss in iteration 73 : 1.1145433247501189
Loss in iteration 74 : 1.2941172025570724
Loss in iteration 75 : 1.198729350055597
Loss in iteration 76 : 1.6285130575134592
Loss in iteration 77 : 1.593476775627532
Loss in iteration 78 : 2.7472607175554793
Loss in iteration 79 : 2.2546826390010954
Loss in iteration 80 : 3.6135349683147084
Loss in iteration 81 : 1.7000327833236024
Loss in iteration 82 : 1.9619043994543477
Loss in iteration 83 : 1.322924718835442
Loss in iteration 84 : 1.4180099130123993
Loss in iteration 85 : 1.143228931438369
Loss in iteration 86 : 1.252057921270743
Loss in iteration 87 : 1.1130560164422922
Loss in iteration 88 : 1.3254950396698928
Loss in iteration 89 : 1.241965537587272
Loss in iteration 90 : 1.7770154512356642
Loss in iteration 91 : 1.7666839378186765
Loss in iteration 92 : 3.122216427967171
Loss in iteration 93 : 2.1113670365236037
Loss in iteration 94 : 3.1455499495443657
Loss in iteration 95 : 1.7744391138982958
Loss in iteration 96 : 2.0729016711263704
Loss in iteration 97 : 1.3572142859276222
Loss in iteration 98 : 1.4428354608570984
Loss in iteration 99 : 1.1429823871649096
Loss in iteration 100 : 1.2318754367388165
Loss in iteration 101 : 1.0913907649549042
Loss in iteration 102 : 1.2613190587212622
Loss in iteration 103 : 1.1807765753697865
Loss in iteration 104 : 1.6086828454930397
Loss in iteration 105 : 1.5823994461708668
Loss in iteration 106 : 2.759307947663365
Loss in iteration 107 : 2.260860201735721
Loss in iteration 108 : 3.651906871492839
Loss in iteration 109 : 1.6811665240358848
Loss in iteration 110 : 1.9424276847559185
Loss in iteration 111 : 1.310500122616615
Loss in iteration 112 : 1.4070772278049928
Loss in iteration 113 : 1.1394902782641056
Loss in iteration 114 : 1.252268409328864
Loss in iteration 115 : 1.1149548774626619
Loss in iteration 116 : 1.3332702791178292
Loss in iteration 117 : 1.2472053772014342
Loss in iteration 118 : 1.792969927158098
Loss in iteration 119 : 1.7685218213225595
Loss in iteration 120 : 3.1106477853909964
Loss in iteration 121 : 2.0969023199172194
Loss in iteration 122 : 3.1039832230142923
Loss in iteration 123 : 1.7663355760672634
Loss in iteration 124 : 2.0650196974571986
Loss in iteration 125 : 1.349470189729064
Loss in iteration 126 : 1.4349388936561562
Loss in iteration 127 : 1.1398267287508954
Loss in iteration 128 : 1.229392707195645
Loss in iteration 129 : 1.0908644253903206
Loss in iteration 130 : 1.2612305970870827
Loss in iteration 131 : 1.1805251874187415
Loss in iteration 132 : 1.6082372175301258
Loss in iteration 133 : 1.575121661814396
Loss in iteration 134 : 2.7384385806278932
Loss in iteration 135 : 2.2509727233314725
Loss in iteration 136 : 3.636038352694125
Loss in iteration 137 : 1.684637748740132
Loss in iteration 138 : 1.953090352598738
Loss in iteration 139 : 1.3126773558239504
Loss in iteration 140 : 1.4104680847484963
Loss in iteration 141 : 1.1399353810670243
Loss in iteration 142 : 1.2518744771402284
Loss in iteration 143 : 1.1137429494083018
Loss in iteration 144 : 1.3279654132060985
Loss in iteration 145 : 1.2404802425273047
Loss in iteration 146 : 1.7723913939252889
Loss in iteration 147 : 1.7392892049470654
Loss in iteration 148 : 3.0487713986994995
Loss in iteration 149 : 2.1179466020832725
Loss in iteration 150 : 3.16884962925045
Loss in iteration 151 : 1.7616924581478008
Loss in iteration 152 : 2.057233669661871
Loss in iteration 153 : 1.343143768594312
Loss in iteration 154 : 1.427533142659323
Loss in iteration 155 : 1.1370134038733986
Loss in iteration 156 : 1.2260597959991353
Loss in iteration 157 : 1.0896486406883386
Loss in iteration 158 : 1.2597020448538365
Loss in iteration 159 : 1.1798871437072518
Loss in iteration 160 : 1.6079281609401939
Loss in iteration 161 : 1.573006938733127
Loss in iteration 162 : 2.734428403816356
Loss in iteration 163 : 2.247130666885339
Loss in iteration 164 : 3.629236975322641
Loss in iteration 165 : 1.6857640013200903
Loss in iteration 166 : 1.9567937798616273
Loss in iteration 167 : 1.3129902473516726
Loss in iteration 168 : 1.4111529421189566
Loss in iteration 169 : 1.1398261791305966
Loss in iteration 170 : 1.251213110836267
Loss in iteration 171 : 1.113019305938778
Loss in iteration 172 : 1.3253548680915743
Loss in iteration 173 : 1.2374827860020239
Loss in iteration 174 : 1.7635044159411397
Loss in iteration 175 : 1.7267761945542335
Loss in iteration 176 : 3.0225787453063724
Loss in iteration 177 : 2.1258135448726416
Loss in iteration 178 : 3.1948507279274216
Loss in iteration 179 : 1.7591335419458416
Loss in iteration 180 : 2.0536949479025033
Loss in iteration 181 : 1.3405487217353216
Loss in iteration 182 : 1.4248844793325885
Loss in iteration 183 : 1.1360720058062204
Loss in iteration 184 : 1.2251357924335562
Loss in iteration 185 : 1.0894450958026733
Loss in iteration 186 : 1.2596622832482474
Loss in iteration 187 : 1.1800227336550977
Loss in iteration 188 : 1.6085875470928532
Loss in iteration 189 : 1.572437728350874
Loss in iteration 190 : 2.7327711160787738
Loss in iteration 191 : 2.2445246770268343
Loss in iteration 192 : 3.623472671473952
Loss in iteration 193 : 1.6870460838034442
Loss in iteration 194 : 1.9596540969590832
Loss in iteration 195 : 1.3134803417574221
Loss in iteration 196 : 1.4117319333114844
Loss in iteration 197 : 1.1397702076435487
Loss in iteration 198 : 1.2506631261354335
Loss in iteration 199 : 1.112429563776864
Loss in iteration 200 : 1.3234074526088253
Testing accuracy  of updater 0 on alg 0 with rate 0.007 = 0.7825, training accuracy 0.7811589511168663, time elapsed: 3736 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 5.501460003634972
Loss in iteration 3 : 1.1921359106078941
Loss in iteration 4 : 2.0193640088653724
Loss in iteration 5 : 4.253776387568349
Loss in iteration 6 : 0.6685728372226688
Loss in iteration 7 : 0.6347065557232717
Loss in iteration 8 : 0.655020255803362
Loss in iteration 9 : 0.6531919033590209
Loss in iteration 10 : 0.7823237053714353
Loss in iteration 11 : 0.869678404191824
Loss in iteration 12 : 1.4780964496473186
Loss in iteration 13 : 1.4796176711203073
Loss in iteration 14 : 2.546328041406533
Loss in iteration 15 : 0.8453445097015223
Loss in iteration 16 : 0.9285271152880792
Loss in iteration 17 : 0.7758224421796189
Loss in iteration 18 : 0.8713773959234891
Loss in iteration 19 : 0.7761745451512562
Loss in iteration 20 : 0.941483293162572
Loss in iteration 21 : 0.8731493384036769
Loss in iteration 22 : 1.2068241103238602
Loss in iteration 23 : 1.0974926664808404
Loss in iteration 24 : 1.6300300552019513
Loss in iteration 25 : 1.140539077470072
Loss in iteration 26 : 1.4793820044082884
Loss in iteration 27 : 0.9987304636221509
Loss in iteration 28 : 1.1325569383392349
Loss in iteration 29 : 0.8257356355722054
Loss in iteration 30 : 0.8875771349887088
Loss in iteration 31 : 0.7311492797483318
Loss in iteration 32 : 0.7946950366370129
Loss in iteration 33 : 0.7121053809910258
Loss in iteration 34 : 0.8285310199566366
Loss in iteration 35 : 0.7787414393237161
Loss in iteration 36 : 1.0466688536932556
Loss in iteration 37 : 1.007006277126442
Loss in iteration 38 : 1.5825292449335269
Loss in iteration 39 : 1.1961213827298924
Loss in iteration 40 : 1.7162944074091644
Loss in iteration 41 : 1.0456295300639316
Loss in iteration 42 : 1.2249956251665055
Loss in iteration 43 : 0.8514574637098046
Loss in iteration 44 : 0.9184485266876141
Loss in iteration 45 : 0.7310398042604803
Loss in iteration 46 : 0.7844330556059458
Loss in iteration 47 : 0.6936414450894711
Loss in iteration 48 : 0.7818790142160368
Loss in iteration 49 : 0.7288207461503964
Loss in iteration 50 : 0.9273937668015947
Loss in iteration 51 : 0.8922713879056684
Loss in iteration 52 : 1.3609684068566217
Loss in iteration 53 : 1.1861129324405526
Loss in iteration 54 : 1.8385316335418933
Loss in iteration 55 : 1.0673063324597962
Loss in iteration 56 : 1.3149085687929902
Loss in iteration 57 : 0.896154303565287
Loss in iteration 58 : 0.9943709875724318
Loss in iteration 59 : 0.7577368913989914
Loss in iteration 60 : 0.8196725775390528
Loss in iteration 61 : 0.703996627995761
Loss in iteration 62 : 0.7867822137037365
Loss in iteration 63 : 0.7184910420257905
Loss in iteration 64 : 0.8826754269974966
Loss in iteration 65 : 0.8314274554318136
Loss in iteration 66 : 1.198406104482979
Loss in iteration 67 : 1.0924520661263424
Loss in iteration 68 : 1.7112614744435672
Loss in iteration 69 : 1.114497254216404
Loss in iteration 70 : 1.4720381184508242
Loss in iteration 71 : 0.9639800360778192
Loss in iteration 72 : 1.1035163382978093
Loss in iteration 73 : 0.7943046890226146
Loss in iteration 74 : 0.8592031394679613
Loss in iteration 75 : 0.7112658709686709
Loss in iteration 76 : 0.7783016553625659
Loss in iteration 77 : 0.6990056812051196
Loss in iteration 78 : 0.8182057562307768
Loss in iteration 79 : 0.7641984386087889
Loss in iteration 80 : 1.0265866813773104
Loss in iteration 81 : 0.9654040438606449
Loss in iteration 82 : 1.5034072460357486
Loss in iteration 83 : 1.1632077029902295
Loss in iteration 84 : 1.6940831856663898
Loss in iteration 85 : 1.0327380085044
Loss in iteration 86 : 1.2338936656948232
Loss in iteration 87 : 0.8448198363651813
Loss in iteration 88 : 0.9211922840774436
Loss in iteration 89 : 0.7272458039274634
Loss in iteration 90 : 0.7848842038415816
Loss in iteration 91 : 0.6910880857893765
Loss in iteration 92 : 0.7800641058717654
Loss in iteration 93 : 0.7220947728087431
Loss in iteration 94 : 0.9104438255200614
Loss in iteration 95 : 0.8606838748785572
Loss in iteration 96 : 1.2812846069810317
Loss in iteration 97 : 1.1267941230112575
Loss in iteration 98 : 1.7494130293484909
Loss in iteration 99 : 1.080297121658737
Loss in iteration 100 : 1.3778576614882823
Loss in iteration 101 : 0.916521593344143
Loss in iteration 102 : 1.0331966252411264
Loss in iteration 103 : 0.7661409107002188
Loss in iteration 104 : 0.8296311800450888
Loss in iteration 105 : 0.7030795598794886
Loss in iteration 106 : 0.7788858814526495
Loss in iteration 107 : 0.706208255807837
Loss in iteration 108 : 0.8477655696719171
Loss in iteration 109 : 0.7921365776987604
Loss in iteration 110 : 1.1004938098868813
Loss in iteration 111 : 1.0141696643330231
Loss in iteration 112 : 1.5830233478106703
Loss in iteration 113 : 1.135747674534955
Loss in iteration 114 : 1.583198148451597
Loss in iteration 115 : 0.9994499958244676
Loss in iteration 116 : 1.173695012480907
Loss in iteration 117 : 0.8170390389991744
Loss in iteration 118 : 0.8873896714750947
Loss in iteration 119 : 0.7166594579270535
Loss in iteration 120 : 0.7775938401247747
Loss in iteration 121 : 0.6916006557750539
Loss in iteration 122 : 0.791373858137486
Loss in iteration 123 : 0.7349989766524194
Loss in iteration 124 : 0.9484397540175028
Loss in iteration 125 : 0.8920507206072599
Loss in iteration 126 : 1.3505820672057975
Loss in iteration 127 : 1.1391785475832923
Loss in iteration 128 : 1.7317584274559292
Loss in iteration 129 : 1.0588504523814344
Loss in iteration 130 : 1.3184115819702036
Loss in iteration 131 : 0.8848768183694604
Loss in iteration 132 : 0.9854229010746501
Loss in iteration 133 : 0.7484555736179312
Loss in iteration 134 : 0.8102186416693118
Loss in iteration 135 : 0.6977564883832358
Loss in iteration 136 : 0.7790754946230222
Loss in iteration 137 : 0.7116191109583779
Loss in iteration 138 : 0.8698036591820042
Loss in iteration 139 : 0.8138967482475863
Loss in iteration 140 : 1.15811753339677
Loss in iteration 141 : 1.0499248497512927
Loss in iteration 142 : 1.636918955334231
Loss in iteration 143 : 1.114772776820806
Loss in iteration 144 : 1.5062851457171735
Loss in iteration 145 : 0.9703521043870608
Loss in iteration 146 : 1.1244870849060278
Loss in iteration 147 : 0.7974212465526138
Loss in iteration 148 : 0.8651394658804887
Loss in iteration 149 : 0.7108665153321384
Loss in iteration 150 : 0.7762800376584861
Loss in iteration 151 : 0.6950030571915374
Loss in iteration 152 : 0.8066273332467045
Loss in iteration 153 : 0.7501346050747809
Loss in iteration 154 : 0.9894570984324962
Loss in iteration 155 : 0.9245811973028429
Loss in iteration 156 : 1.4162308647694484
Loss in iteration 157 : 1.1433780924788008
Loss in iteration 158 : 1.6975236786829142
Loss in iteration 159 : 1.0411164755000744
Loss in iteration 160 : 1.2714126484726413
Loss in iteration 161 : 0.8604916348804439
Loss in iteration 162 : 0.9490707584926307
Loss in iteration 163 : 0.735766697013971
Loss in iteration 164 : 0.7962188205665253
Loss in iteration 165 : 0.6940547282954193
Loss in iteration 166 : 0.7799455403557678
Loss in iteration 167 : 0.7166979160156266
Loss in iteration 168 : 0.8895294528639887
Loss in iteration 169 : 0.8337067509210222
Loss in iteration 170 : 1.20977984099269
Loss in iteration 171 : 1.0790980321235064
Loss in iteration 172 : 1.676583669380837
Loss in iteration 173 : 1.096960322903236
Loss in iteration 174 : 1.4450337900656436
Loss in iteration 175 : 0.9442469569547782
Loss in iteration 176 : 1.0816602083250029
Loss in iteration 177 : 0.7815723988440357
Loss in iteration 178 : 0.8476000658845954
Loss in iteration 179 : 0.7066765861930435
Loss in iteration 180 : 0.7767215962231518
Loss in iteration 181 : 0.6992649155048145
Loss in iteration 182 : 0.8231699828482504
Loss in iteration 183 : 0.766220353546753
Loss in iteration 184 : 1.032185890167723
Loss in iteration 185 : 0.9575669509360868
Loss in iteration 186 : 1.4786486062484288
Loss in iteration 187 : 1.1411641334521991
Loss in iteration 188 : 1.6528349059724514
Loss in iteration 189 : 1.0239152775046034
Loss in iteration 190 : 1.2303527966724277
Loss in iteration 191 : 0.8405739110458706
Loss in iteration 192 : 0.9205971958817841
Loss in iteration 193 : 0.7264062818431999
Loss in iteration 194 : 0.7865332848159196
Loss in iteration 195 : 0.691987494390284
Loss in iteration 196 : 0.782745355228346
Loss in iteration 197 : 0.7225773300981124
Loss in iteration 198 : 0.9099308644701185
Loss in iteration 199 : 0.8535658472023013
Loss in iteration 200 : 1.2593920275865802
Testing accuracy  of updater 0 on alg 0 with rate 0.004 = 0.76325, training accuracy 0.7361605697636776, time elapsed: 4311 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.3896664660825948
Loss in iteration 3 : 0.4713776023236357
Loss in iteration 4 : 0.41990645578428715
Loss in iteration 5 : 0.46278734838873964
Loss in iteration 6 : 0.3927738077420322
Loss in iteration 7 : 0.4025152188589516
Loss in iteration 8 : 0.36895536196077505
Loss in iteration 9 : 0.3677005591725164
Loss in iteration 10 : 0.353368905328423
Loss in iteration 11 : 0.3505896443801199
Loss in iteration 12 : 0.3447685653092544
Loss in iteration 13 : 0.34280612641325336
Loss in iteration 14 : 0.3404370987953765
Loss in iteration 15 : 0.3393256544364279
Loss in iteration 16 : 0.3382993212044811
Loss in iteration 17 : 0.33768784590532996
Loss in iteration 18 : 0.3371767052405078
Loss in iteration 19 : 0.3368088249733965
Testing accuracy  of updater 0 on alg 0 with rate 0.001 = 0.78, training accuracy 0.832308190352865, time elapsed: 299 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.9961903707673733
Loss in iteration 3 : 0.44215993013868515
Loss in iteration 4 : 0.3870963146042838
Loss in iteration 5 : 0.3783608645767582
Loss in iteration 6 : 0.3649236947047112
Loss in iteration 7 : 0.35934664547400486
Loss in iteration 8 : 0.3549348350332629
Loss in iteration 9 : 0.35217499762012416
Loss in iteration 10 : 0.35007229853927585
Loss in iteration 11 : 0.34840481058086786
Loss in iteration 12 : 0.34700602025689925
Testing accuracy  of updater 0 on alg 0 with rate 7.000000000000001E-4 = 0.7735, training accuracy 0.830042084817093, time elapsed: 286 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6427414632225813
Loss in iteration 3 : 0.44551441542822173
Loss in iteration 4 : 0.4124851389141893
Loss in iteration 5 : 0.4019085679906379
Loss in iteration 6 : 0.3935753727922455
Loss in iteration 7 : 0.3867835762303029
Loss in iteration 8 : 0.3811480398969846
Loss in iteration 9 : 0.3764072801340163
Loss in iteration 10 : 0.37237217566846303
Loss in iteration 11 : 0.3689027694958427
Loss in iteration 12 : 0.3658933403566717
Loss in iteration 13 : 0.36326259977980596
Testing accuracy  of updater 0 on alg 0 with rate 4.000000000000001E-4 = 0.762, training accuracy 0.828099708643574, time elapsed: 218 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5138915865062822
Loss in iteration 3 : 0.49347603878612195
Loss in iteration 4 : 0.4827481535422838
Testing accuracy  of updater 0 on alg 0 with rate 9.999999999999994E-5 = 0.5, training accuracy 0.6474587245063127, time elapsed: 77 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.9961903707673728
Loss in iteration 3 : 1.1407129594855674
Loss in iteration 4 : 0.640051276336788
Loss in iteration 5 : 0.40488631998447444
Loss in iteration 6 : 0.8340489195466431
Loss in iteration 7 : 0.6982433372180092
Loss in iteration 8 : 0.4685118452099379
Loss in iteration 9 : 0.5618862760484415
Loss in iteration 10 : 0.7150993810726299
Loss in iteration 11 : 0.7327963951165887
Loss in iteration 12 : 0.6365283232641419
Loss in iteration 13 : 0.559526696661113
Loss in iteration 14 : 0.5921122961775332
Loss in iteration 15 : 0.6736696369394742
Loss in iteration 16 : 0.6719919551748134
Loss in iteration 17 : 0.592882359684666
Loss in iteration 18 : 0.5481130990803045
Loss in iteration 19 : 0.5703543952384998
Loss in iteration 20 : 0.5985656737038928
Loss in iteration 21 : 0.5785566496535485
Loss in iteration 22 : 0.5195102016648088
Loss in iteration 23 : 0.4789254588455557
Loss in iteration 24 : 0.49022137118045367
Loss in iteration 25 : 0.49931053932049846
Loss in iteration 26 : 0.45310133070225306
Loss in iteration 27 : 0.4097803417121703
Loss in iteration 28 : 0.4156058426821214
Loss in iteration 29 : 0.420421287839778
Loss in iteration 30 : 0.3842035963838494
Loss in iteration 31 : 0.356734161868686
Loss in iteration 32 : 0.38030320382737914
Loss in iteration 33 : 0.3649521448362078
Loss in iteration 34 : 0.34445916779300945
Loss in iteration 35 : 0.3737951198466598
Loss in iteration 36 : 0.36068496367444997
Loss in iteration 37 : 0.3605077234828341
Loss in iteration 38 : 0.38007363694868207
Loss in iteration 39 : 0.3568814251052543
Loss in iteration 40 : 0.3752581702562215
Loss in iteration 41 : 0.3540204671965455
Loss in iteration 42 : 0.354278136300807
Loss in iteration 43 : 0.3520948771808086
Loss in iteration 44 : 0.338159191310624
Loss in iteration 45 : 0.3460734496788163
Loss in iteration 46 : 0.3387995276533605
Loss in iteration 47 : 0.3361026688305956
Loss in iteration 48 : 0.34290506466257653
Loss in iteration 49 : 0.3375982919782269
Loss in iteration 50 : 0.3382012593624913
Loss in iteration 51 : 0.3424233641081971
Loss in iteration 52 : 0.3387300477471443
Loss in iteration 53 : 0.33827094541084346
Loss in iteration 54 : 0.3408594027766192
Loss in iteration 55 : 0.3371893036359753
Loss in iteration 56 : 0.33639889962932656
Loss in iteration 57 : 0.3375556805843574
Loss in iteration 58 : 0.33433730631187153
Loss in iteration 59 : 0.33385916661386855
Loss in iteration 60 : 0.3345916097859396
Loss in iteration 61 : 0.3321254277037675
Loss in iteration 62 : 0.3330371503864124
Loss in iteration 63 : 0.3332154130369635
Loss in iteration 64 : 0.332084825024101
Loss in iteration 65 : 0.3336849629449444
Loss in iteration 66 : 0.3330086474234743
Loss in iteration 67 : 0.3331740710908007
Testing accuracy  of updater 1 on alg 0 with rate 6.999999999999999E-4 = 0.77525, training accuracy 0.8355454839753965, time elapsed: 1082 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.7404080951166329
Loss in iteration 3 : 0.8735814483415397
Loss in iteration 4 : 0.5940356760345846
Loss in iteration 5 : 0.3436470465100678
Loss in iteration 6 : 0.5747550159056335
Loss in iteration 7 : 0.6110556652536872
Loss in iteration 8 : 0.42755288747133624
Loss in iteration 9 : 0.4145527239635892
Loss in iteration 10 : 0.5148884499337277
Loss in iteration 11 : 0.5697239184733502
Loss in iteration 12 : 0.535726612540348
Loss in iteration 13 : 0.47219920597597337
Loss in iteration 14 : 0.4553607885145371
Loss in iteration 15 : 0.4981169572741335
Loss in iteration 16 : 0.5341136990969775
Loss in iteration 17 : 0.5115362085750285
Loss in iteration 18 : 0.4650534520184403
Loss in iteration 19 : 0.4502655014274859
Loss in iteration 20 : 0.46780607850955874
Loss in iteration 21 : 0.48058751968706387
Loss in iteration 22 : 0.4642446232431751
Loss in iteration 23 : 0.42973988377424954
Loss in iteration 24 : 0.40882052315445633
Loss in iteration 25 : 0.41507618372965066
Loss in iteration 26 : 0.42068788293694587
Loss in iteration 27 : 0.39881989700592985
Loss in iteration 28 : 0.3720106913551638
Loss in iteration 29 : 0.36905755710493027
Loss in iteration 30 : 0.37582762895812055
Loss in iteration 31 : 0.36581125277855714
Loss in iteration 32 : 0.3454052805699864
Loss in iteration 33 : 0.34389201400780534
Loss in iteration 34 : 0.35446930334515775
Loss in iteration 35 : 0.3441545741878017
Loss in iteration 36 : 0.336206053033278
Loss in iteration 37 : 0.34804492819052835
Loss in iteration 38 : 0.3490654654046184
Loss in iteration 39 : 0.34057837909392236
Loss in iteration 40 : 0.34912240126031274
Loss in iteration 41 : 0.35106979308137065
Loss in iteration 42 : 0.34267173241111126
Loss in iteration 43 : 0.3475413727556751
Loss in iteration 44 : 0.3467027168808608
Loss in iteration 45 : 0.3389926417650341
Loss in iteration 46 : 0.3406130506077984
Loss in iteration 47 : 0.3402982329468087
Loss in iteration 48 : 0.33464992922047565
Loss in iteration 49 : 0.3353284976734832
Loss in iteration 50 : 0.3364883603860472
Loss in iteration 51 : 0.33360339769000213
Loss in iteration 52 : 0.33316976042103774
Loss in iteration 53 : 0.3352569426307108
Loss in iteration 54 : 0.3344283607547241
Loss in iteration 55 : 0.333305655258282
Loss in iteration 56 : 0.3346219803079961
Testing accuracy  of updater 1 on alg 0 with rate 4.9E-4 = 0.77975, training accuracy 0.8358692133376497, time elapsed: 877 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5367600975803626
Loss in iteration 3 : 0.6435189907816746
Loss in iteration 4 : 0.5767948085102635
Loss in iteration 5 : 0.403166461687853
Loss in iteration 6 : 0.3543295225371903
Loss in iteration 7 : 0.4578564649414917
Loss in iteration 8 : 0.46394578261397423
Loss in iteration 9 : 0.38105054238548575
Loss in iteration 10 : 0.3517394055213718
Loss in iteration 11 : 0.3859554644100329
Loss in iteration 12 : 0.42389299077161985
Loss in iteration 13 : 0.4286940623430356
Loss in iteration 14 : 0.4051359705625973
Loss in iteration 15 : 0.38120943746325836
Loss in iteration 16 : 0.37966092646700034
Loss in iteration 17 : 0.3981439763432002
Loss in iteration 18 : 0.4134462831841987
Loss in iteration 19 : 0.40863004420221566
Loss in iteration 20 : 0.3908953508211676
Loss in iteration 21 : 0.3787566172810028
Loss in iteration 22 : 0.38007009998937685
Loss in iteration 23 : 0.3875949751531542
Loss in iteration 24 : 0.3898878642808996
Loss in iteration 25 : 0.38229247935972926
Loss in iteration 26 : 0.3697810276740353
Loss in iteration 27 : 0.3614492006392749
Loss in iteration 28 : 0.36141424958644425
Loss in iteration 29 : 0.36432906103448215
Loss in iteration 30 : 0.36176424132276214
Loss in iteration 31 : 0.3529747360990542
Loss in iteration 32 : 0.34526754707197166
Loss in iteration 33 : 0.34373383393225804
Loss in iteration 34 : 0.3454521073279548
Loss in iteration 35 : 0.344481419443037
Loss in iteration 36 : 0.33943584483961386
Loss in iteration 37 : 0.3347510808369686
Loss in iteration 38 : 0.3346113861339882
Loss in iteration 39 : 0.33693195382522856
Loss in iteration 40 : 0.3365056129466209
Loss in iteration 41 : 0.3334252242680148
Loss in iteration 42 : 0.3323735753051831
Loss in iteration 43 : 0.33440571064323493
Loss in iteration 44 : 0.33589127205349917
Loss in iteration 45 : 0.3348330923573242
Loss in iteration 46 : 0.3336022898389448
Loss in iteration 47 : 0.3345585550295896
Testing accuracy  of updater 1 on alg 0 with rate 2.8E-4 = 0.788, training accuracy 0.8345742958886371, time elapsed: 761 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.545475733585198
Loss in iteration 3 : 0.4875429614468288
Loss in iteration 4 : 0.5019975355377355
Loss in iteration 5 : 0.5210100330761144
Loss in iteration 6 : 0.5173678999227783
Loss in iteration 7 : 0.4877682970635352
Loss in iteration 8 : 0.44161728585398563
Loss in iteration 9 : 0.3958217642357257
Loss in iteration 10 : 0.36784030528804024
Loss in iteration 11 : 0.36493834065667996
Loss in iteration 12 : 0.37815073348165557
Loss in iteration 13 : 0.3898827051244578
Loss in iteration 14 : 0.388433049117929
Loss in iteration 15 : 0.37470634335352415
Loss in iteration 16 : 0.3574437159383266
Loss in iteration 17 : 0.34491067473415354
Loss in iteration 18 : 0.3403887037616103
Loss in iteration 19 : 0.342469528643485
Loss in iteration 20 : 0.34753313420788456
Loss in iteration 21 : 0.3520556886940927
Loss in iteration 22 : 0.35388261188867237
Loss in iteration 23 : 0.35253732334261384
Loss in iteration 24 : 0.34889712716691934
Loss in iteration 25 : 0.3445469635137303
Loss in iteration 26 : 0.3410625066998279
Loss in iteration 27 : 0.3394319134471392
Loss in iteration 28 : 0.3397748333248263
Loss in iteration 29 : 0.3414264373296753
Testing accuracy  of updater 1 on alg 0 with rate 7.0E-5 = 0.77875, training accuracy 0.8271285205568145, time elapsed: 542 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5778678515108919
Loss in iteration 3 : 0.49880036248169957
Loss in iteration 4 : 0.48783196409852964
Loss in iteration 5 : 0.5025347264607357
Loss in iteration 6 : 0.5134224774776184
Loss in iteration 7 : 0.5089341626897135
Loss in iteration 8 : 0.4878146899297587
Loss in iteration 9 : 0.4547740655089983
Loss in iteration 10 : 0.41818679897579725
Loss in iteration 11 : 0.3877029610141185
Loss in iteration 12 : 0.37066941672624876
Loss in iteration 13 : 0.3684507230632689
Loss in iteration 14 : 0.3755962456216836
Loss in iteration 15 : 0.38331836333674685
Loss in iteration 16 : 0.38483281351944304
Loss in iteration 17 : 0.3785152894829376
Loss in iteration 18 : 0.3672558995043803
Loss in iteration 19 : 0.3555832818787069
Loss in iteration 20 : 0.34701222910352125
Loss in iteration 21 : 0.34290088115767664
Loss in iteration 22 : 0.342688921263954
Loss in iteration 23 : 0.3447728689236063
Loss in iteration 24 : 0.3473818672161918
Loss in iteration 25 : 0.34915982346326857
Loss in iteration 26 : 0.3494166725240059
Testing accuracy  of updater 1 on alg 0 with rate 4.9E-5 = 0.756, training accuracy 0.8245386856587893, time elapsed: 455 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6200648685568554
Loss in iteration 3 : 0.5394732926228495
Loss in iteration 4 : 0.49569736124985325
Loss in iteration 5 : 0.48565294937283293
Loss in iteration 6 : 0.49143977795726784
Loss in iteration 7 : 0.49945065593603805
Loss in iteration 8 : 0.5023935885564058
Loss in iteration 9 : 0.4974078171146079
Loss in iteration 10 : 0.48441509053329324
Loss in iteration 11 : 0.46510341688900014
Loss in iteration 12 : 0.4422773054784178
Loss in iteration 13 : 0.41930235251830805
Loss in iteration 14 : 0.39946735223180363
Loss in iteration 15 : 0.3852416083615313
Loss in iteration 16 : 0.3776081058781907
Loss in iteration 17 : 0.37578668820121935
Loss in iteration 18 : 0.3775706672769278
Loss in iteration 19 : 0.38019014500852827
Loss in iteration 20 : 0.38130937697393713
Loss in iteration 21 : 0.3797073874681349
Loss in iteration 22 : 0.37540990063994384
Loss in iteration 23 : 0.3693483376794725
Loss in iteration 24 : 0.3628102009814687
Loss in iteration 25 : 0.3569465791603125
Loss in iteration 26 : 0.35248456193430044
Loss in iteration 27 : 0.34966034737596347
Loss in iteration 28 : 0.34830780864194294
Loss in iteration 29 : 0.34801685785052217
Testing accuracy  of updater 1 on alg 0 with rate 2.7999999999999996E-5 = 0.76075, training accuracy 0.8271285205568145, time elapsed: 404 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6730089272381253
Testing accuracy  of updater 1 on alg 0 with rate 6.999999999999994E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 56 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 2.6150117376811233
Loss in iteration 3 : 1.662906834288797
Loss in iteration 4 : 0.43918411292716625
Loss in iteration 5 : 0.6651983365211819
Loss in iteration 6 : 0.5449589907767509
Loss in iteration 7 : 0.5473150671154264
Loss in iteration 8 : 0.568734754593046
Loss in iteration 9 : 0.5841711092390561
Loss in iteration 10 : 0.5911623522099207
Loss in iteration 11 : 0.5916230706303085
Loss in iteration 12 : 0.586571569874536
Loss in iteration 13 : 0.5764379261020084
Loss in iteration 14 : 0.5618573082825885
Loss in iteration 15 : 0.5436275564126073
Loss in iteration 16 : 0.5225156768408554
Loss in iteration 17 : 0.4992679945407664
Loss in iteration 18 : 0.47466522045845694
Loss in iteration 19 : 0.449529176942373
Loss in iteration 20 : 0.4247269624174817
Loss in iteration 21 : 0.40121841062840125
Loss in iteration 22 : 0.38012945888383615
Loss in iteration 23 : 0.3627657748857423
Loss in iteration 24 : 0.35050744661779587
Loss in iteration 25 : 0.34453396989798024
Loss in iteration 26 : 0.3452473642930284
Loss in iteration 27 : 0.35132349763836623
Loss in iteration 28 : 0.35902529962921464
Loss in iteration 29 : 0.3634775738955459
Loss in iteration 30 : 0.3621078709422554
Loss in iteration 31 : 0.3562657462002593
Loss in iteration 32 : 0.34902455485580997
Loss in iteration 33 : 0.3426944718858658
Loss in iteration 34 : 0.3381978483512791
Loss in iteration 35 : 0.33556085761603605
Loss in iteration 36 : 0.33434721932304606
Loss in iteration 37 : 0.33410072643328864
Loss in iteration 38 : 0.33432249049467594
Loss in iteration 39 : 0.33471641185862455
Loss in iteration 40 : 0.33502679538706553
Loss in iteration 41 : 0.3351594482434434
Loss in iteration 42 : 0.33512081602382554
Loss in iteration 43 : 0.33494143889338046
Loss in iteration 44 : 0.33487659153384763
Loss in iteration 45 : 0.33502356931592886
Loss in iteration 46 : 0.3362364116510043
Loss in iteration 47 : 0.33880385524289586
Loss in iteration 48 : 0.3473693296960269
Loss in iteration 49 : 0.3620280930229549
Loss in iteration 50 : 0.41739969579408437
Loss in iteration 51 : 0.4581426648884221
Loss in iteration 52 : 0.6774353861831395
Loss in iteration 53 : 0.44973640792234254
Loss in iteration 54 : 0.4790597200304583
Loss in iteration 55 : 0.4018802474852281
Loss in iteration 56 : 0.3822278444704536
Loss in iteration 57 : 0.36258797877965954
Loss in iteration 58 : 0.3547514326059475
Loss in iteration 59 : 0.3512074011306845
Loss in iteration 60 : 0.3477982167376714
Loss in iteration 61 : 0.344379091038123
Loss in iteration 62 : 0.3408470239836077
Loss in iteration 63 : 0.33760073795472584
Loss in iteration 64 : 0.33496355196481953
Loss in iteration 65 : 0.3332291694221724
Loss in iteration 66 : 0.3324812805555893
Loss in iteration 67 : 0.3327400915648856
Loss in iteration 68 : 0.3339933318680327
Loss in iteration 69 : 0.3369446238359508
Loss in iteration 70 : 0.3460100068297869
Loss in iteration 71 : 0.3713829060418213
Loss in iteration 72 : 0.48101935357742215
Loss in iteration 73 : 0.5467510736929784
Loss in iteration 74 : 0.9426332275724819
Loss in iteration 75 : 0.35787367569930895
Loss in iteration 76 : 0.34908417538775643
Loss in iteration 77 : 0.35342777320192825
Loss in iteration 78 : 0.355481474888475
Loss in iteration 79 : 0.35624725746705976
Testing accuracy  of updater 2 on alg 0 with rate 0.001 = 0.78, training accuracy 0.8306895435415992, time elapsed: 1219 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.8355255346364894
Loss in iteration 3 : 1.1911326987836386
Loss in iteration 4 : 0.38071874377918796
Loss in iteration 5 : 0.5157181093404665
Loss in iteration 6 : 0.4446395078624231
Loss in iteration 7 : 0.438567822937381
Loss in iteration 8 : 0.45240785413169166
Loss in iteration 9 : 0.4644521455990182
Loss in iteration 10 : 0.4711372217914584
Loss in iteration 11 : 0.4736571022439513
Loss in iteration 12 : 0.4728487886488813
Loss in iteration 13 : 0.4689208828506785
Loss in iteration 14 : 0.46215948599779
Loss in iteration 15 : 0.4530442270773925
Loss in iteration 16 : 0.4420735903515149
Loss in iteration 17 : 0.42971261452517373
Loss in iteration 18 : 0.41643184955949475
Loss in iteration 19 : 0.40272135839072015
Loss in iteration 20 : 0.3890807490585324
Loss in iteration 21 : 0.3760171899216768
Loss in iteration 22 : 0.36404831387991615
Loss in iteration 23 : 0.35369497486185014
Loss in iteration 24 : 0.3454522076231719
Loss in iteration 25 : 0.33972758641685546
Loss in iteration 26 : 0.3367422797943782
Loss in iteration 27 : 0.3364049351080535
Loss in iteration 28 : 0.33819786398753704
Loss in iteration 29 : 0.34116192241609333
Loss in iteration 30 : 0.3440844956418785
Loss in iteration 31 : 0.34589290325652045
Loss in iteration 32 : 0.3460497272080055
Loss in iteration 33 : 0.3446732220044445
Loss in iteration 34 : 0.3423243609928325
Loss in iteration 35 : 0.3396661675166723
Loss in iteration 36 : 0.33721807679212706
Loss in iteration 37 : 0.3352717076937479
Loss in iteration 38 : 0.33391706027037604
Loss in iteration 39 : 0.33310939672209106
Loss in iteration 40 : 0.3327343011058896
Loss in iteration 41 : 0.3326555063476533
Loss in iteration 42 : 0.33274451184554343
Loss in iteration 43 : 0.3328960516678184
Loss in iteration 44 : 0.3330340182101188
Loss in iteration 45 : 0.33311150187403427
Loss in iteration 46 : 0.33310746337643654
Testing accuracy  of updater 2 on alg 0 with rate 7.000000000000001E-4 = 0.7805, training accuracy 0.834250566526384, time elapsed: 732 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.073201657954043
Loss in iteration 3 : 0.7656956767849009
Loss in iteration 4 : 0.37019571078504465
Loss in iteration 5 : 0.38678579735363133
Loss in iteration 6 : 0.3771680291845718
Loss in iteration 7 : 0.36014464678033875
Loss in iteration 8 : 0.36194679933267004
Loss in iteration 9 : 0.3687061078873937
Loss in iteration 10 : 0.37414562877282725
Loss in iteration 11 : 0.3776417127171067
Loss in iteration 12 : 0.37981733677134544
Loss in iteration 13 : 0.3808934423416699
Loss in iteration 14 : 0.38080794786121486
Loss in iteration 15 : 0.37958473220505035
Loss in iteration 16 : 0.3773851785690078
Loss in iteration 17 : 0.3744022734630092
Loss in iteration 18 : 0.370806393517998
Loss in iteration 19 : 0.36675688118453714
Loss in iteration 20 : 0.36241762764794966
Loss in iteration 21 : 0.3579533748914332
Loss in iteration 22 : 0.3535203443484475
Loss in iteration 23 : 0.3492623054598184
Loss in iteration 24 : 0.345309369032657
Loss in iteration 25 : 0.3417747649230768
Loss in iteration 26 : 0.3387497983408277
Loss in iteration 27 : 0.3362987583565974
Loss in iteration 28 : 0.33445425298143605
Loss in iteration 29 : 0.33321329959354634
Loss in iteration 30 : 0.332535338939804
Loss in iteration 31 : 0.33234359409925357
Loss in iteration 32 : 0.33253083575698017
Loss in iteration 33 : 0.33296989259643583
Loss in iteration 34 : 0.33352804095351407
Loss in iteration 35 : 0.3340829882024189
Loss in iteration 36 : 0.3345372100093449
Testing accuracy  of updater 2 on alg 0 with rate 4.0E-4 = 0.78575, training accuracy 0.8391065069601813, time elapsed: 571 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4929761023111823
Loss in iteration 3 : 0.49584581971414426
Loss in iteration 4 : 0.475228892298021
Loss in iteration 5 : 0.43657625863305505
Loss in iteration 6 : 0.3992385368454529
Loss in iteration 7 : 0.3769740358109618
Loss in iteration 8 : 0.36801216396615166
Loss in iteration 9 : 0.36312394633762024
Loss in iteration 10 : 0.35742250392266395
Loss in iteration 11 : 0.35124457674951853
Loss in iteration 12 : 0.3461492299976222
Loss in iteration 13 : 0.3428058749205513
Loss in iteration 14 : 0.3409648025989394
Loss in iteration 15 : 0.3400381441680323
Loss in iteration 16 : 0.33952682963871766
Loss in iteration 17 : 0.3391631428336356
Loss in iteration 18 : 0.3388700311154351
Loss in iteration 19 : 0.33866022764276865
Loss in iteration 20 : 0.33855357509347306
Loss in iteration 21 : 0.33854037353630745
Loss in iteration 22 : 0.33858363545421016
Loss in iteration 23 : 0.33863880379776296
Testing accuracy  of updater 2 on alg 0 with rate 1.0E-4 = 0.77875, training accuracy 0.832308190352865, time elapsed: 596 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4954413300215582
Loss in iteration 3 : 0.48553748610697905
Loss in iteration 4 : 0.4817693533839836
Loss in iteration 5 : 0.4652726837176546
Loss in iteration 6 : 0.43858000869209707
Loss in iteration 7 : 0.41040239576035786
Loss in iteration 8 : 0.3887126303335827
Loss in iteration 9 : 0.37611775141743437
Loss in iteration 10 : 0.3698084067937395
Loss in iteration 11 : 0.36552909635789377
Loss in iteration 12 : 0.3609140959881073
Loss in iteration 13 : 0.35580799669223684
Loss in iteration 14 : 0.3509975714392001
Loss in iteration 15 : 0.347147632397907
Loss in iteration 16 : 0.34444683402431303
Loss in iteration 17 : 0.342710089152994
Loss in iteration 18 : 0.3416069092263843
Loss in iteration 19 : 0.3408375507782329
Loss in iteration 20 : 0.34021183995516674
Loss in iteration 21 : 0.3396507114961059
Loss in iteration 22 : 0.3391481466758621
Loss in iteration 23 : 0.33872596111192965
Loss in iteration 24 : 0.3384008817407916
Loss in iteration 25 : 0.33817029295905127
Testing accuracy  of updater 2 on alg 0 with rate 7.000000000000001E-5 = 0.7795, training accuracy 0.8329556490773713, time elapsed: 374 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5378608758802431
Loss in iteration 3 : 0.4907605997543612
Loss in iteration 4 : 0.48161517104602763
Loss in iteration 5 : 0.47946051888966035
Loss in iteration 6 : 0.47346689992959606
Loss in iteration 7 : 0.4611073428995746
Loss in iteration 8 : 0.44373478189539606
Loss in iteration 9 : 0.424435670687333
Loss in iteration 10 : 0.4065446984650425
Loss in iteration 11 : 0.39244757437823913
Loss in iteration 12 : 0.38288289440945333
Loss in iteration 13 : 0.3770342816894408
Loss in iteration 14 : 0.3732976512645113
Loss in iteration 15 : 0.3701833546826216
Loss in iteration 16 : 0.3668456731041453
Loss in iteration 17 : 0.36311862449150306
Loss in iteration 18 : 0.3592550058891403
Loss in iteration 19 : 0.3556218472012967
Loss in iteration 20 : 0.3524980218421005
Loss in iteration 21 : 0.35000016852707505
Loss in iteration 22 : 0.3480991320702236
Loss in iteration 23 : 0.34667673899172735
Loss in iteration 24 : 0.3455857269150479
Loss in iteration 25 : 0.34469373325447134
Testing accuracy  of updater 2 on alg 0 with rate 4.0E-5 = 0.77025, training accuracy 0.8303658141793461, time elapsed: 505 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6413977969667161
Testing accuracy  of updater 2 on alg 0 with rate 9.999999999999999E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 87 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 31.962770895146242
Loss in iteration 3 : 12.946699102545496
Loss in iteration 4 : 1.2843700773610647
Loss in iteration 5 : 1.2725744899655231
Loss in iteration 6 : 1.5689200427643377
Loss in iteration 7 : 4.030875529860718
Loss in iteration 8 : 7.3222410989759235
Loss in iteration 9 : 11.77811137578084
Loss in iteration 10 : 3.070399604000381
Loss in iteration 11 : 1.5644611310449903
Loss in iteration 12 : 1.4956752906815787
Loss in iteration 13 : 1.428452967868804
Loss in iteration 14 : 1.3641943964452088
Loss in iteration 15 : 1.3005248781013015
Loss in iteration 16 : 1.2377867915553378
Loss in iteration 17 : 1.1756443384691853
Loss in iteration 18 : 1.1146064351162426
Loss in iteration 19 : 1.0548530033747716
Loss in iteration 20 : 0.998869727423021
Loss in iteration 21 : 0.9501871258009292
Loss in iteration 22 : 0.9362911928019687
Loss in iteration 23 : 0.9972266685688421
Loss in iteration 24 : 1.5897395729923505
Loss in iteration 25 : 2.6023152463959383
Loss in iteration 26 : 7.259999346252773
Loss in iteration 27 : 1.0312163672165664
Loss in iteration 28 : 0.9927131813641601
Loss in iteration 29 : 1.0558637073615738
Loss in iteration 30 : 1.0786975637452456
Loss in iteration 31 : 1.592649269638417
Loss in iteration 32 : 1.6806408082387998
Loss in iteration 33 : 3.805138484405367
Loss in iteration 34 : 1.5615118087087299
Loss in iteration 35 : 2.352890984426426
Loss in iteration 36 : 1.3191130526526498
Loss in iteration 37 : 1.6481412747453155
Loss in iteration 38 : 1.1338222860240232
Loss in iteration 39 : 1.364438874871505
Loss in iteration 40 : 1.1035118410900862
Loss in iteration 41 : 1.453637821761423
Loss in iteration 42 : 1.1826750878227974
Loss in iteration 43 : 1.819593094593875
Loss in iteration 44 : 1.3382182085316057
Loss in iteration 45 : 2.234046783119807
Loss in iteration 46 : 1.3420328694416739
Loss in iteration 47 : 1.9883574173653442
Loss in iteration 48 : 1.1441270473813447
Loss in iteration 49 : 1.4530082178711214
Loss in iteration 50 : 1.030796223549992
Loss in iteration 51 : 1.3010841735816758
Loss in iteration 52 : 1.0311408775538353
Loss in iteration 53 : 1.4435452874610384
Loss in iteration 54 : 1.1163126860361245
Loss in iteration 55 : 1.7793666773328412
Loss in iteration 56 : 1.2167403020627645
Loss in iteration 57 : 1.9771066670755424
Loss in iteration 58 : 1.1564317768628776
Loss in iteration 59 : 1.652552848658963
Loss in iteration 60 : 1.0177280800284176
Loss in iteration 61 : 1.3211184278228725
Loss in iteration 62 : 0.9646210842598204
Loss in iteration 63 : 1.2924420535337575
Loss in iteration 64 : 0.9916288531912866
Loss in iteration 65 : 1.4721491463888645
Loss in iteration 66 : 1.0647468881452589
Loss in iteration 67 : 1.7060784365503903
Loss in iteration 68 : 1.0912985481327977
Loss in iteration 69 : 1.6829356172014294
Loss in iteration 70 : 1.0149710887302892
Loss in iteration 71 : 1.4180378100050628
Loss in iteration 72 : 0.9404799023817884
Loss in iteration 73 : 1.2710997583414327
Loss in iteration 74 : 0.9274702979062517
Loss in iteration 75 : 1.3183576636211527
Loss in iteration 76 : 0.9612690478729551
Loss in iteration 77 : 1.4686920470340783
Loss in iteration 78 : 0.997423919147823
Loss in iteration 79 : 1.551720947067637
Loss in iteration 80 : 0.978460816098254
Loss in iteration 81 : 1.444155756887793
Loss in iteration 82 : 0.9253133768495031
Loss in iteration 83 : 1.2994342751294568
Loss in iteration 84 : 0.8953479939015533
Loss in iteration 85 : 1.2644089043692286
Loss in iteration 86 : 0.9003686388072218
Loss in iteration 87 : 1.3265257697980537
Loss in iteration 88 : 0.9204052983515294
Loss in iteration 89 : 1.3978565251112312
Loss in iteration 90 : 0.9225637818274876
Loss in iteration 91 : 1.3837525741014471
Loss in iteration 92 : 0.897997226863077
Loss in iteration 93 : 1.3029230078170573
Loss in iteration 94 : 0.8721768547774035
Loss in iteration 95 : 1.2487858008824255
Loss in iteration 96 : 0.8633591187412334
Loss in iteration 97 : 1.2542781196152712
Loss in iteration 98 : 0.8678756823974686
Loss in iteration 99 : 1.2872357676420842
Loss in iteration 100 : 0.8705858483162829
Loss in iteration 101 : 1.296463999225134
Loss in iteration 102 : 0.861415502914112
Loss in iteration 103 : 1.2660251660012232
Loss in iteration 104 : 0.8461170266182051
Loss in iteration 105 : 1.2284140280779392
Loss in iteration 106 : 0.8353030040460785
Loss in iteration 107 : 1.2132500373414048
Loss in iteration 108 : 0.8316650253129846
Loss in iteration 109 : 1.218272020079859
Loss in iteration 110 : 0.8302997343552727
Loss in iteration 111 : 1.2227674657331506
Loss in iteration 112 : 0.8256452633431828
Loss in iteration 113 : 1.2125849970463207
Loss in iteration 114 : 0.8173441002189205
Loss in iteration 115 : 1.1931511512341815
Loss in iteration 116 : 0.809047544622254
Loss in iteration 117 : 1.1777603302548583
Loss in iteration 118 : 0.8032627983597828
Loss in iteration 119 : 1.171202940337353
Loss in iteration 120 : 0.7993120309847732
Loss in iteration 121 : 1.1682374814564263
Loss in iteration 122 : 0.7951123626344944
Loss in iteration 123 : 1.162118311083184
Loss in iteration 124 : 0.7896831859949146
Loss in iteration 125 : 1.1517040269238024
Loss in iteration 126 : 0.783734310706895
Loss in iteration 127 : 1.1406671994323816
Loss in iteration 128 : 0.7783401929208642
Loss in iteration 129 : 1.13208862526121
Loss in iteration 130 : 0.7737508573421237
Loss in iteration 131 : 1.1256870477312269
Loss in iteration 132 : 0.7694648371864458
Loss in iteration 133 : 1.1193693534978377
Loss in iteration 134 : 0.7649919815855791
Loss in iteration 135 : 1.11191348590569
Loss in iteration 136 : 0.7603152333940842
Loss in iteration 137 : 1.1038040084947447
Loss in iteration 138 : 0.7557177965290109
Loss in iteration 139 : 1.0960961785530543
Loss in iteration 140 : 0.7513895924461422
Loss in iteration 141 : 1.0891792386055663
Loss in iteration 142 : 0.7472824887635328
Loss in iteration 143 : 1.0826753626749865
Loss in iteration 144 : 0.7432498548615458
Loss in iteration 145 : 1.0761033398099003
Loss in iteration 146 : 0.7392214672852435
Loss in iteration 147 : 1.0693676265193632
Loss in iteration 148 : 0.7352343467345139
Loss in iteration 149 : 1.0626835592353678
Loss in iteration 150 : 0.7313502908934109
Loss in iteration 151 : 1.056242687778499
Loss in iteration 152 : 0.7275854749899227
Loss in iteration 153 : 1.0500424188352517
Loss in iteration 154 : 0.7239127148222192
Loss in iteration 155 : 1.0439686885617199
Loss in iteration 156 : 0.7203028592176033
Loss in iteration 157 : 1.037945766274718
Loss in iteration 158 : 0.7167500366827508
Loss in iteration 159 : 1.0319857509020167
Loss in iteration 160 : 0.7132645694192952
Loss in iteration 161 : 1.0261360371110524
Loss in iteration 162 : 0.7098543421928049
Loss in iteration 163 : 1.0204179640572426
Loss in iteration 164 : 0.7065168049546496
Loss in iteration 165 : 1.0148166285158293
Loss in iteration 166 : 0.7032441026191815
Loss in iteration 167 : 1.0093081444756344
Loss in iteration 168 : 0.7000307367301988
Loss in iteration 169 : 1.0038827306650286
Loss in iteration 170 : 0.6968756854268447
Loss in iteration 171 : 0.9985446956015142
Loss in iteration 172 : 0.6937796122654355
Loss in iteration 173 : 0.993299912381302
Loss in iteration 174 : 0.6907419283800859
Loss in iteration 175 : 0.9881477645498842
Loss in iteration 176 : 0.6877603955665705
Loss in iteration 177 : 0.983082668862935
Loss in iteration 178 : 0.6848324332795359
Loss in iteration 179 : 0.9780993473854094
Loss in iteration 180 : 0.6819561683662968
Loss in iteration 181 : 0.9731954184796057
Loss in iteration 182 : 0.6791304196468417
Loss in iteration 183 : 0.968370283858677
Loss in iteration 184 : 0.6763541339927557
Loss in iteration 185 : 0.9636230447727111
Loss in iteration 186 : 0.6736260275767066
Loss in iteration 187 : 0.9589517330806174
Loss in iteration 188 : 0.6709446451780281
Loss in iteration 189 : 0.9543538959478742
Loss in iteration 190 : 0.6683085825780375
Loss in iteration 191 : 0.949827371697367
Loss in iteration 192 : 0.6657165982681912
Loss in iteration 193 : 0.9453704896458011
Loss in iteration 194 : 0.6631675697645802
Loss in iteration 195 : 0.9409817989149033
Loss in iteration 196 : 0.6606604061954916
Loss in iteration 197 : 0.9366597874939615
Loss in iteration 198 : 0.6581940120205716
Loss in iteration 199 : 0.9324028356773205
Loss in iteration 200 : 0.6557673062873425
Testing accuracy  of updater 3 on alg 0 with rate 0.7000000000000002 = 0.679, training accuracy 0.7665911298154743, time elapsed: 3301 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 3.8978808763077675
Loss in iteration 3 : 1.4803191176187307
Loss in iteration 4 : 0.4576229702078763
Loss in iteration 5 : 0.4828968834076675
Loss in iteration 6 : 0.45402755410659285
Loss in iteration 7 : 0.5102789044264397
Loss in iteration 8 : 0.48245073797501836
Loss in iteration 9 : 0.6134854077338684
Loss in iteration 10 : 0.5371746015390082
Loss in iteration 11 : 0.7638997792831704
Loss in iteration 12 : 0.5544789371266238
Loss in iteration 13 : 0.763274396114047
Loss in iteration 14 : 0.5263095028506829
Loss in iteration 15 : 0.6645055573744716
Loss in iteration 16 : 0.5020092575115244
Loss in iteration 17 : 0.6108313990245324
Loss in iteration 18 : 0.4927817669524537
Loss in iteration 19 : 0.6022433067928237
Loss in iteration 20 : 0.4950064625841451
Loss in iteration 21 : 0.6192582731477855
Loss in iteration 22 : 0.5016266859023684
Loss in iteration 23 : 0.6405346538761091
Loss in iteration 24 : 0.5049865328047329
Loss in iteration 25 : 0.6472225521251472
Loss in iteration 26 : 0.5025377987897516
Loss in iteration 27 : 0.6381289429651871
Loss in iteration 28 : 0.4974771543727841
Loss in iteration 29 : 0.6254478784403226
Loss in iteration 30 : 0.4933460827086313
Loss in iteration 31 : 0.6177922713928106
Loss in iteration 32 : 0.49127737284478074
Loss in iteration 33 : 0.6159958950986929
Loss in iteration 34 : 0.49056377057040695
Loss in iteration 35 : 0.6168429867356712
Loss in iteration 36 : 0.4899747845496243
Loss in iteration 37 : 0.6169158675412136
Loss in iteration 38 : 0.4887771101872828
Loss in iteration 39 : 0.614868600109297
Loss in iteration 40 : 0.4870126875626063
Loss in iteration 41 : 0.6114476426169154
Loss in iteration 42 : 0.4851097266447502
Loss in iteration 43 : 0.6080031466214705
Loss in iteration 44 : 0.48340402723881126
Loss in iteration 45 : 0.6052742069414311
Loss in iteration 46 : 0.48196038174845335
Loss in iteration 47 : 0.6031988768659818
Loss in iteration 48 : 0.48066068962307956
Loss in iteration 49 : 0.6013420481369068
Loss in iteration 50 : 0.47936723914917684
Loss in iteration 51 : 0.5993542725983413
Loss in iteration 52 : 0.47802101853100976
Loss in iteration 53 : 0.5971577376053052
Loss in iteration 54 : 0.47664207123039215
Loss in iteration 55 : 0.594866941547073
Loss in iteration 56 : 0.47527788868468224
Loss in iteration 57 : 0.5926211003938967
Loss in iteration 58 : 0.47395888355948096
Loss in iteration 59 : 0.5904816343811014
Loss in iteration 60 : 0.4726866294079961
Loss in iteration 61 : 0.5884300070374526
Loss in iteration 62 : 0.47144638898971464
Loss in iteration 63 : 0.5864179316834627
Loss in iteration 64 : 0.4702240033949163
Loss in iteration 65 : 0.5844121535177026
Loss in iteration 66 : 0.46901422889340716
Loss in iteration 67 : 0.5824079160024181
Loss in iteration 68 : 0.4678193713627275
Loss in iteration 69 : 0.5804177964528504
Loss in iteration 70 : 0.46664382207059973
Loss in iteration 71 : 0.5784550401004714
Loss in iteration 72 : 0.46548996571251144
Loss in iteration 73 : 0.5765246485902203
Loss in iteration 74 : 0.4643573673772468
Loss in iteration 75 : 0.5746240244278917
Loss in iteration 76 : 0.46324413376686313
Loss in iteration 77 : 0.5727480555244547
Loss in iteration 78 : 0.4621485254082741
Loss in iteration 79 : 0.5708932736589584
Loss in iteration 80 : 0.4610696913119532
Loss in iteration 81 : 0.5690589567726315
Loss in iteration 82 : 0.46000750259078355
Loss in iteration 83 : 0.5672460011308131
Loss in iteration 84 : 0.45896203230023463
Loss in iteration 85 : 0.5654553618460221
Loss in iteration 86 : 0.45793317599506544
Loss in iteration 87 : 0.5636872334178823
Loss in iteration 88 : 0.456920575286827
Loss in iteration 89 : 0.561941104131709
Loss in iteration 90 : 0.4559237403418554
Loss in iteration 91 : 0.5602162166714755
Loss in iteration 92 : 0.4549421977533058
Loss in iteration 93 : 0.5585119530616435
Loss in iteration 94 : 0.45397556136964506
Loss in iteration 95 : 0.556827948623151
Loss in iteration 96 : 0.45302352141963304
Loss in iteration 97 : 0.5551640010706775
Loss in iteration 98 : 0.45208579887129513
Loss in iteration 99 : 0.5535199311647401
Loss in iteration 100 : 0.4511621093944746
Loss in iteration 101 : 0.5518955021053747
Loss in iteration 102 : 0.45025215338291635
Loss in iteration 103 : 0.5502904161049436
Loss in iteration 104 : 0.4493556247604903
Loss in iteration 105 : 0.5487043514696069
Loss in iteration 106 : 0.44847222382230056
Loss in iteration 107 : 0.5471369979585623
Loss in iteration 108 : 0.4476016643869339
Loss in iteration 109 : 0.5455880704306648
Loss in iteration 110 : 0.4467436737691267
Loss in iteration 111 : 0.5440573035954989
Loss in iteration 112 : 0.4458979890425112
Loss in iteration 113 : 0.5425444403278389
Loss in iteration 114 : 0.4450643534900899
Loss in iteration 115 : 0.5410492234910315
Loss in iteration 116 : 0.44424251507912316
Loss in iteration 117 : 0.5395713941787298
Loss in iteration 118 : 0.4434322267091106
Loss in iteration 119 : 0.5381106941170669
Loss in iteration 120 : 0.4426332470922666
Loss in iteration 121 : 0.5366668687185959
Loss in iteration 122 : 0.4418453413419144
Loss in iteration 123 : 0.5352396686785562
Loss in iteration 124 : 0.44106828098074735
Loss in iteration 125 : 0.5338288499023587
Loss in iteration 126 : 0.44030184355979374
Loss in iteration 127 : 0.532434172610063
Loss in iteration 128 : 0.43954581220683564
Loss in iteration 129 : 0.5310554005041707
Loss in iteration 130 : 0.438799975305477
Loss in iteration 131 : 0.5296923004054013
Loss in iteration 132 : 0.43806412633425573
Loss in iteration 133 : 0.5283446422953337
Loss in iteration 134 : 0.43733806379368356
Loss in iteration 135 : 0.52701219950936
Loss in iteration 136 : 0.43662159113907795
Loss in iteration 137 : 0.5256947488702223
Loss in iteration 138 : 0.4359145166777857
Loss in iteration 139 : 0.5243920706920726
Loss in iteration 140 : 0.43521665343231986
Loss in iteration 141 : 0.523103948692592
Loss in iteration 142 : 0.4345278189911764
Loss in iteration 143 : 0.5218301698827678
Loss in iteration 144 : 0.4338478353670268
Loss in iteration 145 : 0.5205705244815212
Loss in iteration 146 : 0.4331765288700441
Loss in iteration 147 : 0.5193248058653246
Loss in iteration 148 : 0.43251372999394755
Loss in iteration 149 : 0.5180928105392186
Loss in iteration 150 : 0.43185927330854496
Loss in iteration 151 : 0.5168743381112421
Loss in iteration 152 : 0.43121299735398677
Loss in iteration 153 : 0.5156691912598367
Loss in iteration 154 : 0.4305747445351576
Loss in iteration 155 : 0.5144771756931076
Loss in iteration 156 : 0.429944361017042
Loss in iteration 157 : 0.5132981001040272
Loss in iteration 158 : 0.42932169662252034
Loss in iteration 159 : 0.5121317761259623
Loss in iteration 160 : 0.4287066047336197
Loss in iteration 161 : 0.5109780182907636
Loss in iteration 162 : 0.42809894219633765
Loss in iteration 163 : 0.5098366439893878
Loss in iteration 164 : 0.42749856922869583
Loss in iteration 165 : 0.5087074734339067
Loss in iteration 166 : 0.4269053493315122
Loss in iteration 167 : 0.5075903296198028
Loss in iteration 168 : 0.426319149201569
Loss in iteration 169 : 0.5064850382880466
Loss in iteration 170 : 0.4257398386470516
Loss in iteration 171 : 0.5053914278870116
Loss in iteration 172 : 0.4251672905052866
Loss in iteration 173 : 0.5043093295345441
Loss in iteration 174 : 0.42460138056277896
Loss in iteration 175 : 0.503238576980403
Loss in iteration 176 : 0.42404198747755595
Loss in iteration 177 : 0.5021790065691827
Loss in iteration 178 : 0.4234889927037577
Loss in iteration 179 : 0.5011304572037517
Loss in iteration 180 : 0.422942280418381
Loss in iteration 181 : 0.5000927703090483
Loss in iteration 182 : 0.4224017374500865
Loss in iteration 183 : 0.4990657897962135
Loss in iteration 184 : 0.42186725321000756
Loss in iteration 185 : 0.4980493620270455
Loss in iteration 186 : 0.42133871962447994
Loss in iteration 187 : 0.4970433357787424
Loss in iteration 188 : 0.42081603106967375
Loss in iteration 189 : 0.4960475622090039
Loss in iteration 190 : 0.42029908430803453
Loss in iteration 191 : 0.49506189482144397
Loss in iteration 192 : 0.4197877784265244
Loss in iteration 193 : 0.49408618943138427
Loss in iteration 194 : 0.4192820147765885
Loss in iteration 195 : 0.49312030413199487
Loss in iteration 196 : 0.41878169691579575
Loss in iteration 197 : 0.4921640992607724
Loss in iteration 198 : 0.4182867305511212
Loss in iteration 199 : 0.4912174373663694
Loss in iteration 200 : 0.4177970234837978
Testing accuracy  of updater 3 on alg 0 with rate 0.49000000000000005 = 0.71275, training accuracy 0.7879572677241826, time elapsed: 2419 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.729633798196001
Loss in iteration 3 : 0.7320256386157048
Loss in iteration 4 : 0.33586018890378105
Loss in iteration 5 : 0.3349510275673869
Loss in iteration 6 : 0.33354850390231
Loss in iteration 7 : 0.33290007035347935
Loss in iteration 8 : 0.3321395261100743
Loss in iteration 9 : 0.33170052532724986
Loss in iteration 10 : 0.33126154641827826
Loss in iteration 11 : 0.33096285150508176
Loss in iteration 12 : 0.3306868995598514
Loss in iteration 13 : 0.3304748104031444
Loss in iteration 14 : 0.3302835365577121
Loss in iteration 15 : 0.330123139942764
Loss in iteration 16 : 0.3299774955278904
Loss in iteration 17 : 0.32984794865887135
Loss in iteration 18 : 0.3297281673638069
Loss in iteration 19 : 0.32961753442591313
Loss in iteration 20 : 0.32951337514410295
Loss in iteration 21 : 0.3294149062986353
Loss in iteration 22 : 0.32932088634296275
Loss in iteration 23 : 0.3292307236802261
Loss in iteration 24 : 0.32914377844170717
Loss in iteration 25 : 0.3290596509191462
Loss in iteration 26 : 0.3289779762043327
Loss in iteration 27 : 0.3288984885982619
Loss in iteration 28 : 0.3288209610268618
Loss in iteration 29 : 0.3287452134689299
Loss in iteration 30 : 0.3286710945199025
Loss in iteration 31 : 0.3285984783681937
Loss in iteration 32 : 0.328527258668806
Loss in iteration 33 : 0.32845734463354964
Loss in iteration 34 : 0.32838865860358635
Loss in iteration 35 : 0.32832113314220435
Loss in iteration 36 : 0.3282547098123052
Loss in iteration 37 : 0.32818933724083155
Loss in iteration 38 : 0.32812497035996036
Loss in iteration 39 : 0.32806156914198126
Loss in iteration 40 : 0.3279990980673039
Loss in iteration 41 : 0.3279375252878017
Loss in iteration 42 : 0.32787682223325215
Loss in iteration 43 : 0.3278169630465571
Loss in iteration 44 : 0.32775792428679135
Loss in iteration 45 : 0.3276996845399692
Loss in iteration 46 : 0.3276422241936624
Loss in iteration 47 : 0.3275855251638838
Loss in iteration 48 : 0.3275295707238782
Loss in iteration 49 : 0.32747434530965763
Loss in iteration 50 : 0.3274198343900014
Loss in iteration 51 : 0.32736602432635875
Loss in iteration 52 : 0.3273129022742096
Loss in iteration 53 : 0.3272604560811486
Loss in iteration 54 : 0.32720867421206695
Loss in iteration 55 : 0.3271575456744115
Loss in iteration 56 : 0.32710705996141815
Loss in iteration 57 : 0.32705720699690377
Loss in iteration 58 : 0.3270079770921671
Loss in iteration 59 : 0.3269593609049422
Loss in iteration 60 : 0.3269113494066132
Loss in iteration 61 : 0.32686393385151813
Loss in iteration 62 : 0.3268171057519451
Loss in iteration 63 : 0.3267708568550296
Loss in iteration 64 : 0.3267251791236322
Loss in iteration 65 : 0.3266800647188324
Loss in iteration 66 : 0.3266355059852388
Loss in iteration 67 : 0.3265914954376365
Loss in iteration 68 : 0.32654802574964
Loss in iteration 69 : 0.326505089743429
Loss in iteration 70 : 0.3264626803809209
Loss in iteration 71 : 0.32642079075581215
Loss in iteration 72 : 0.3263794140866658
Loss in iteration 73 : 0.32633854371066556
Loss in iteration 74 : 0.3262981730781515
Loss in iteration 75 : 0.32625829574767023
Loss in iteration 76 : 0.32621890538160153
Loss in iteration 77 : 0.3261799957421867
Loss in iteration 78 : 0.32614156068798256
Loss in iteration 79 : 0.32610359417062634
Loss in iteration 80 : 0.3260660902319258
Loss in iteration 81 : 0.32602904300118174
Loss in iteration 82 : 0.32599244669275806
Loss in iteration 83 : 0.3259562956038342
Loss in iteration 84 : 0.32592058411234365
Loss in iteration 85 : 0.3258853066750531
Loss in iteration 86 : 0.32585045782578576
Loss in iteration 87 : 0.3258160321737536
Loss in iteration 88 : 0.32578202440200166
Loss in iteration 89 : 0.3257484292659338
Loss in iteration 90 : 0.3257152415919287
Loss in iteration 91 : 0.32568245627602294
Loss in iteration 92 : 0.32565006828266213
Loss in iteration 93 : 0.32561807264349957
Loss in iteration 94 : 0.32558646445626155
Loss in iteration 95 : 0.32555523888364174
Loss in iteration 96 : 0.32552439115224946
Loss in iteration 97 : 0.3254939165515897
Loss in iteration 98 : 0.3254638104330744
Loss in iteration 99 : 0.325434068209073
Loss in iteration 100 : 0.3254046853519827
Loss in iteration 101 : 0.32537565739332636
Loss in iteration 102 : 0.32534697992288125
Loss in iteration 103 : 0.3253186485878156
Loss in iteration 104 : 0.32529065909186144
Loss in iteration 105 : 0.32526300719449736
Loss in iteration 106 : 0.32523568871014497
Loss in iteration 107 : 0.32520869950739634
Loss in iteration 108 : 0.32518203550824404
Loss in iteration 109 : 0.32515569268732974
Loss in iteration 110 : 0.32512966707121227
Loss in iteration 111 : 0.3251039547376421
Loss in iteration 112 : 0.32507855181485323
Loss in iteration 113 : 0.32505345448086453
Loss in iteration 114 : 0.3250286589627933
Loss in iteration 115 : 0.3250041615361887
Loss in iteration 116 : 0.3249799585243583
Loss in iteration 117 : 0.32495604629772645
Loss in iteration 118 : 0.3249324212731836
Loss in iteration 119 : 0.3249090799134632
Loss in iteration 120 : 0.32488601872651396
Loss in iteration 121 : 0.3248632342648918
Loss in iteration 122 : 0.3248407231251543
Loss in iteration 123 : 0.32481848194726914
Loss in iteration 124 : 0.3247965074140288
Loss in iteration 125 : 0.3247747962504747
Loss in iteration 126 : 0.3247533452233312
Loss in iteration 127 : 0.3247321511404428
Loss in iteration 128 : 0.3247112108502282
Loss in iteration 129 : 0.32469052124113407
Loss in iteration 130 : 0.3246700792411019
Loss in iteration 131 : 0.3246498818170401
Loss in iteration 132 : 0.32462992597430607
Loss in iteration 133 : 0.32461020875619045
Loss in iteration 134 : 0.3245907272434169
Loss in iteration 135 : 0.3245714785536423
Loss in iteration 136 : 0.32455245984096626
Loss in iteration 137 : 0.32453366829544944
Loss in iteration 138 : 0.3245151011426334
Loss in iteration 139 : 0.32449675564307634
Loss in iteration 140 : 0.3244786290918856
Loss in iteration 141 : 0.32446071881826094
Loss in iteration 142 : 0.3244430221850486
Loss in iteration 143 : 0.3244255365882942
Loss in iteration 144 : 0.3244082594568036
Loss in iteration 145 : 0.3243911882517174
Loss in iteration 146 : 0.32437432046608106
Loss in iteration 147 : 0.3243576536244234
Loss in iteration 148 : 0.32434118528234945
Loss in iteration 149 : 0.32432491302613015
Loss in iteration 150 : 0.3243088344722948
Loss in iteration 151 : 0.32429294726724384
Loss in iteration 152 : 0.324277249086853
Loss in iteration 153 : 0.3242617376360851
Loss in iteration 154 : 0.3242464106486166
Loss in iteration 155 : 0.3242312658864573
Loss in iteration 156 : 0.3242163011395822
Loss in iteration 157 : 0.3242015142255682
Loss in iteration 158 : 0.3241869029892324
Loss in iteration 159 : 0.3241724653022796
Loss in iteration 160 : 0.32415819906295207
Loss in iteration 161 : 0.3241441021956822
Loss in iteration 162 : 0.3241301726507566
Loss in iteration 163 : 0.32411640840397743
Loss in iteration 164 : 0.3241028074563338
Loss in iteration 165 : 0.324089367833672
Loss in iteration 166 : 0.32407608758637735
Loss in iteration 167 : 0.3240629647890547
Loss in iteration 168 : 0.3240499975402178
Loss in iteration 169 : 0.32403718396197656
Loss in iteration 170 : 0.32402452219973565
Loss in iteration 171 : 0.32401201042189165
Loss in iteration 172 : 0.3239996468195399
Loss in iteration 173 : 0.32398742960617916
Loss in iteration 174 : 0.3239753570174236
Loss in iteration 175 : 0.3239634273107199
Loss in iteration 176 : 0.3239516387650659
Loss in iteration 177 : 0.3239399896807336
Loss in iteration 178 : 0.3239284783789967
Loss in iteration 179 : 0.3239171032018598
Loss in iteration 180 : 0.3239058625117952
Loss in iteration 181 : 0.32389475469147805
Loss in iteration 182 : 0.32388377814352876
Loss in iteration 183 : 0.3238729312902606
Loss in iteration 184 : 0.3238622125734212
Loss in iteration 185 : 0.3238516204539534
Loss in iteration 186 : 0.32384115341174247
Loss in iteration 187 : 0.3238308099453785
Loss in iteration 188 : 0.323820588571918
Loss in iteration 189 : 0.32381048782664484
Loss in iteration 190 : 0.32380050626284457
Loss in iteration 191 : 0.32379064245156874
Loss in iteration 192 : 0.3237808949814139
Loss in iteration 193 : 0.32377126245829735
Loss in iteration 194 : 0.32376174350523323
Loss in iteration 195 : 0.3237523367621245
Loss in iteration 196 : 0.3237430408855402
Loss in iteration 197 : 0.3237338545485061
Loss in iteration 198 : 0.32372477644029996
Loss in iteration 199 : 0.32371580526624255
Loss in iteration 200 : 0.32370693974749504
Testing accuracy  of updater 3 on alg 0 with rate 0.28 = 0.78775, training accuracy 0.8433149886694723, time elapsed: 2580 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5467829199776958
Loss in iteration 3 : 0.4525507323218727
Loss in iteration 4 : 0.41912537799502253
Loss in iteration 5 : 0.407520093719711
Loss in iteration 6 : 0.39934755576074193
Loss in iteration 7 : 0.39242374153014825
Loss in iteration 8 : 0.38646265924137496
Loss in iteration 9 : 0.3812897431725877
Loss in iteration 10 : 0.37676960465107673
Loss in iteration 11 : 0.3727950197742769
Loss in iteration 12 : 0.3692801613560834
Loss in iteration 13 : 0.3661556748095719
Loss in iteration 14 : 0.3633650149057771
Loss in iteration 15 : 0.36086169147623715
Loss in iteration 16 : 0.35860717919317864
Loss in iteration 17 : 0.35656931684207116
Loss in iteration 18 : 0.35472107047476265
Loss in iteration 19 : 0.3530395693980986
Loss in iteration 20 : 0.3515053485012919
Loss in iteration 21 : 0.3501017479644416
Loss in iteration 22 : 0.3488144340068939
Loss in iteration 23 : 0.3476310134766468
Loss in iteration 24 : 0.3465407217584409
Loss in iteration 25 : 0.3455341683924387
Loss in iteration 26 : 0.34460312844102237
Loss in iteration 27 : 0.34374037036693084
Loss in iteration 28 : 0.3429395132393549
Loss in iteration 29 : 0.3421949076429906
Loss in iteration 30 : 0.3415015358562725
Loss in iteration 31 : 0.3408549277818874
Loss in iteration 32 : 0.3402510898230327
Loss in iteration 33 : 0.33968644445285523
Loss in iteration 34 : 0.33915777865912544
Loss in iteration 35 : 0.33866219978919876
Loss in iteration 36 : 0.33819709759257377
Loss in iteration 37 : 0.33776011147560336
Loss in iteration 38 : 0.33734910215718217
Loss in iteration 39 : 0.33696212705473255
Loss in iteration 40 : 0.3365974188436228
Loss in iteration 41 : 0.33625336672576156
Loss in iteration 42 : 0.3359285000188145
Loss in iteration 43 : 0.33562147373963214
Loss in iteration 44 : 0.33533105590669876
Loss in iteration 45 : 0.33505611632878224
Loss in iteration 46 : 0.3347956166821737
Loss in iteration 47 : 0.3345486017082479
Loss in iteration 48 : 0.334314191387612
Loss in iteration 49 : 0.3340915739677196
Loss in iteration 50 : 0.3338799997381818
Loss in iteration 51 : 0.3336787754626375
Loss in iteration 52 : 0.33348725938849616
Loss in iteration 53 : 0.3333048567663832
Loss in iteration 54 : 0.3331310158201277
Loss in iteration 55 : 0.33296522411581514
Loss in iteration 56 : 0.33280700528498636
Loss in iteration 57 : 0.33265591606276607
Loss in iteration 58 : 0.3325115436064983
Loss in iteration 59 : 0.33237350306474367
Loss in iteration 60 : 0.33224143537006745
Loss in iteration 61 : 0.33211500523223025
Loss in iteration 62 : 0.3319938993111344
Loss in iteration 63 : 0.3318778245512171
Loss in iteration 64 : 0.3317665066611369
Loss in iteration 65 : 0.3316596887243408
Loss in iteration 66 : 0.33155712992774977
Loss in iteration 67 : 0.3314586043971619
Loss in iteration 68 : 0.33136390012922146
Loss in iteration 69 : 0.3312728180108762
Loss in iteration 70 : 0.3311851709182016
Loss in iteration 71 : 0.33110078288730915
Loss in iteration 72 : 0.3310194883508171
Loss in iteration 73 : 0.33094113143400494
Loss in iteration 74 : 0.33086556530537553
Loss in iteration 75 : 0.3307926515768683
Loss in iteration 76 : 0.3307222597494209
Loss in iteration 77 : 0.33065426670001224
Loss in iteration 78 : 0.3305885562066785
Loss in iteration 79 : 0.3305250185083154
Loss in iteration 80 : 0.33046354989640414
Loss in iteration 81 : 0.3304040523360357
Loss in iteration 82 : 0.3303464331138681
Loss in iteration 83 : 0.33029060451085346
Loss in iteration 84 : 0.33023648349777174
Loss in iteration 85 : 0.3301839914517797
Loss in iteration 86 : 0.33013305389233855
Loss in iteration 87 : 0.3300836002350363
Loss in iteration 88 : 0.3300355635619257
Loss in iteration 89 : 0.32998888040714697
Loss in iteration 90 : 0.32994349055668265
Loss in iteration 91 : 0.3298993368612
Loss in iteration 92 : 0.3298563650610124
Loss in iteration 93 : 0.3298145236222985
Loss in iteration 94 : 0.3297737635837431
Loss in iteration 95 : 0.3297340384128726
Loss in iteration 96 : 0.3296953038713872
Loss in iteration 97 : 0.3296575178888689
Loss in iteration 98 : 0.32962064044427386
Loss in iteration 99 : 0.3295846334546731
Loss in iteration 100 : 0.3295494606707506
Loss in iteration 101 : 0.32951508757859466
Loss in iteration 102 : 0.32948148130736166
Loss in iteration 103 : 0.32944861054242036
Loss in iteration 104 : 0.32941644544361054
Loss in iteration 105 : 0.3293849575682853
Loss in iteration 106 : 0.3293541197988202
Loss in iteration 107 : 0.32932390627430264
Loss in iteration 108 : 0.32929429232613233
Loss in iteration 109 : 0.3292652544172855
Loss in iteration 110 : 0.3292367700850064
Loss in iteration 111 : 0.3292088178867211
Loss in iteration 112 : 0.3291813773489541
Loss in iteration 113 : 0.3291544289190861
Loss in iteration 114 : 0.3291279539197591
Loss in iteration 115 : 0.3291019345057748
Loss in iteration 116 : 0.32907635362333676
Loss in iteration 117 : 0.3290511949714923
Loss in iteration 118 : 0.3290264429656415
Loss in iteration 119 : 0.32900208270299713
Loss in iteration 120 : 0.3289780999298744
Loss in iteration 121 : 0.3289544810107013
Loss in iteration 122 : 0.32893121289866034
Loss in iteration 123 : 0.3289082831078548
Loss in iteration 124 : 0.3288856796869184
Loss in iteration 125 : 0.3288633911939862
Loss in iteration 126 : 0.3288414066729484
Loss in iteration 127 : 0.328819715630914
Loss in iteration 128 : 0.32879830801681553
Loss in iteration 129 : 0.3287771742010947
Loss in iteration 130 : 0.3287563049564097
Loss in iteration 131 : 0.32873569143929615
Loss in iteration 132 : 0.3287153251727534
Loss in iteration 133 : 0.3286951980296759
Loss in iteration 134 : 0.32867530221711183
Loss in iteration 135 : 0.3286556302612818
Loss in iteration 136 : 0.32863617499333503
Loss in iteration 137 : 0.328616929535792
Loss in iteration 138 : 0.32859788728963946
Loss in iteration 139 : 0.32857904192204845
Loss in iteration 140 : 0.3285603873546772
Loss in iteration 141 : 0.3285419177525329
Loss in iteration 142 : 0.32852362751335534
Loss in iteration 143 : 0.3285055112575075
Loss in iteration 144 : 0.3284875638183376
Loss in iteration 145 : 0.3284697802329918
Loss in iteration 146 : 0.32845215573365466
Loss in iteration 147 : 0.32843468573919565
Loss in iteration 148 : 0.3284173658472026
Loss in iteration 149 : 0.32840019182637803
Loss in iteration 150 : 0.3283831596092881
Loss in iteration 151 : 0.3283662652854365
Loss in iteration 152 : 0.3283495050946582
Loss in iteration 153 : 0.3283328754208121
Loss in iteration 154 : 0.32831637278575165
Loss in iteration 155 : 0.32829999384357245
Loss in iteration 156 : 0.3282837353751149
Loss in iteration 157 : 0.32826759428271407
Loss in iteration 158 : 0.3282515675851755
Loss in iteration 159 : 0.32823565241298513
Loss in iteration 160 : 0.328219846003719
Loss in iteration 161 : 0.3282041456976596
Loss in iteration 162 : 0.3281885489336067
Loss in iteration 163 : 0.3281730532448648
Loss in iteration 164 : 0.3281576562554097
Loss in iteration 165 : 0.32814235567622063
Loss in iteration 166 : 0.32812714930176795
Loss in iteration 167 : 0.32811203500665514
Loss in iteration 168 : 0.32809701074240294
Loss in iteration 169 : 0.3280820745343719
Loss in iteration 170 : 0.3280672244788185
Loss in iteration 171 : 0.32805245874007044
Loss in iteration 172 : 0.3280377755478249
Loss in iteration 173 : 0.32802317319456525
Loss in iteration 174 : 0.32800865003307766
Loss in iteration 175 : 0.32799420447407795
Loss in iteration 176 : 0.3279798349839395
Loss in iteration 177 : 0.3279655400825081
Loss in iteration 178 : 0.32795131834101754
Loss in iteration 179 : 0.32793716838008513
Loss in iteration 180 : 0.3279230888677869
Loss in iteration 181 : 0.32790907851782564
Loss in iteration 182 : 0.32789513608776055
Loss in iteration 183 : 0.32788126037731546
Loss in iteration 184 : 0.32786745022675917
Loss in iteration 185 : 0.3278537045153419
Loss in iteration 186 : 0.3278400221598089
Loss in iteration 187 : 0.3278264021129645
Loss in iteration 188 : 0.32781284336229416
Loss in iteration 189 : 0.32779934492865476
Loss in iteration 190 : 0.32778590586499673
Loss in iteration 191 : 0.3277725252551573
Loss in iteration 192 : 0.32775920221269134
Loss in iteration 193 : 0.32774593587975187
Loss in iteration 194 : 0.3277327254260147
Loss in iteration 195 : 0.32771957004764624
Loss in iteration 196 : 0.32770646896631406
Loss in iteration 197 : 0.32769342142823077
Loss in iteration 198 : 0.3276804267032446
Loss in iteration 199 : 0.3276674840839597
Loss in iteration 200 : 0.32765459288489057
Testing accuracy  of updater 3 on alg 0 with rate 0.07 = 0.7845, training accuracy 0.8381353188734219, time elapsed: 2510 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.48261690126144025
Loss in iteration 3 : 0.45377807073060866
Loss in iteration 4 : 0.43909494463419557
Loss in iteration 5 : 0.4292042919959906
Loss in iteration 6 : 0.4210447444602264
Loss in iteration 7 : 0.41390877353777267
Loss in iteration 8 : 0.4075769760196304
Loss in iteration 9 : 0.40192389613072843
Loss in iteration 10 : 0.396853789161709
Loss in iteration 11 : 0.39228813037288573
Loss in iteration 12 : 0.3881612977116333
Loss in iteration 13 : 0.38441801536853654
Loss in iteration 14 : 0.3810114645576049
Loss in iteration 15 : 0.37790179524559026
Loss in iteration 16 : 0.37505493065923606
Loss in iteration 17 : 0.37244159925626075
Loss in iteration 18 : 0.3700365469923518
Loss in iteration 19 : 0.3678178936211349
Loss in iteration 20 : 0.3657666045573693
Loss in iteration 21 : 0.3638660557823576
Loss in iteration 22 : 0.36210167391377984
Loss in iteration 23 : 0.36046063721242283
Loss in iteration 24 : 0.35893162617473917
Loss in iteration 25 : 0.3575046146298251
Loss in iteration 26 : 0.3561706940526892
Loss in iteration 27 : 0.35492192522520327
Loss in iteration 28 : 0.3537512125023576
Loss in iteration 29 : 0.35265219683740967
Loss in iteration 30 : 0.3516191644345049
Loss in iteration 31 : 0.3506469684697046
Loss in iteration 32 : 0.3497309617812182
Loss in iteration 33 : 0.34886693880035347
Loss in iteration 34 : 0.3480510852946261
Loss in iteration 35 : 0.3472799347381066
Loss in iteration 36 : 0.34655033032254545
Loss in iteration 37 : 0.34585939178525527
Loss in iteration 38 : 0.3452044863629825
Loss in iteration 39 : 0.34458320329083425
Loss in iteration 40 : 0.34399333135606686
Loss in iteration 41 : 0.34343283909182637
Loss in iteration 42 : 0.3428998572585734
Loss in iteration 43 : 0.34239266331321755
Loss in iteration 44 : 0.3419096676097554
Loss in iteration 45 : 0.34144940111202804
Loss in iteration 46 : 0.34101050443017367
Loss in iteration 47 : 0.340591718018543
Loss in iteration 48 : 0.34019187339504564
Loss in iteration 49 : 0.339809885260747
Loss in iteration 50 : 0.33944474441456507
Loss in iteration 51 : 0.3390955113717017
Loss in iteration 52 : 0.33876131060611536
Loss in iteration 53 : 0.33844132534749544
Loss in iteration 54 : 0.3381347928718223
Loss in iteration 55 : 0.33784100023212216
Loss in iteration 56 : 0.3375592803824719
Loss in iteration 57 : 0.33728900865393263
Loss in iteration 58 : 0.3370295995459349
Loss in iteration 59 : 0.3367805038008974
Loss in iteration 60 : 0.33654120573353613
Loss in iteration 61 : 0.3363112207895502
Loss in iteration 62 : 0.33609009331119055
Loss in iteration 63 : 0.3358773944896938
Loss in iteration 64 : 0.33567272048673913
Loss in iteration 65 : 0.3354756907089949
Loss in iteration 66 : 0.3352859462214972
Loss in iteration 67 : 0.3351031482871184
Loss in iteration 68 : 0.33492697702066804
Loss in iteration 69 : 0.3347571301473654
Loss in iteration 70 : 0.3345933218564184
Loss in iteration 71 : 0.33443528174141607
Loss in iteration 72 : 0.33428275381999883
Loss in iteration 73 : 0.33413549562605716
Loss in iteration 74 : 0.33399327736830947
Loss in iteration 75 : 0.3338558811497387
Loss in iteration 76 : 0.3337231002428419
Loss in iteration 77 : 0.33359473841615145
Loss in iteration 78 : 0.3334706093078756
Loss in iteration 79 : 0.33335053584290086
Loss in iteration 80 : 0.33323434968973814
Loss in iteration 81 : 0.33312189075426496
Loss in iteration 82 : 0.33301300670744954
Loss in iteration 83 : 0.33290755254442445
Loss in iteration 84 : 0.3328053901725652
Loss in iteration 85 : 0.33270638802636343
Loss in iteration 86 : 0.3326104207071451
Loss in iteration 87 : 0.33251736864577214
Loss in iteration 88 : 0.33242711778666945
Loss in iteration 89 : 0.3323395592916543
Loss in iteration 90 : 0.33225458926211987
Loss in iteration 91 : 0.3321721084783145
Loss in iteration 92 : 0.3320920221544872
Loss in iteration 93 : 0.3320142397088196
Loss in iteration 94 : 0.3319386745471131
Loss in iteration 95 : 0.33186524385930954
Loss in iteration 96 : 0.3317938684279633
Loss in iteration 97 : 0.33172447244787656
Loss in iteration 98 : 0.331656983356151
Loss in iteration 99 : 0.33159133167197896
Loss in iteration 100 : 0.33152745084552193
Loss in iteration 101 : 0.3314652771153235
Loss in iteration 102 : 0.33140474937366055
Loss in iteration 103 : 0.33134580903938293
Loss in iteration 104 : 0.33128839993772535
Loss in iteration 105 : 0.3312324681866811
Loss in iteration 106 : 0.3311779620895252
Loss in iteration 107 : 0.3311248320331015
Loss in iteration 108 : 0.33107303039153885
Loss in iteration 109 : 0.33102251143504774
Loss in iteration 110 : 0.3309732312435111
Loss in iteration 111 : 0.33092514762456987
Loss in iteration 112 : 0.3308782200359461
Loss in iteration 113 : 0.33083240951175513
Loss in iteration 114 : 0.3307876785925654
Loss in iteration 115 : 0.3307439912590041
Loss in iteration 116 : 0.3307013128686976
Loss in iteration 117 : 0.3306596100963475
Loss in iteration 118 : 0.33061885087678855
Loss in iteration 119 : 0.33057900435083554
Loss in iteration 120 : 0.3305400408137889
Loss in iteration 121 : 0.33050193166642594
Loss in iteration 122 : 0.33046464936836434
Loss in iteration 123 : 0.33042816739365505
Loss in iteration 124 : 0.33039246018848983
Loss in iteration 125 : 0.330357503130905
Loss in iteration 126 : 0.33032327249238114
Loss in iteration 127 : 0.33028974540123407
Loss in iteration 128 : 0.3302568998077004
Loss in iteration 129 : 0.330224714450639
Loss in iteration 130 : 0.3301931688257478
Loss in iteration 131 : 0.3301622431552392
Loss in iteration 132 : 0.33013191835887756
Loss in iteration 133 : 0.33010217602632524
Loss in iteration 134 : 0.3300729983907223
Loss in iteration 135 : 0.33004436830344386
Loss in iteration 136 : 0.33001626920996596
Loss in iteration 137 : 0.32998868512680246
Loss in iteration 138 : 0.32996160061943947
Loss in iteration 139 : 0.32993500078123295
Loss in iteration 140 : 0.3299088712132186
Loss in iteration 141 : 0.32988319800478577
Loss in iteration 142 : 0.3298579677151819
Loss in iteration 143 : 0.3298331673557951
Loss in iteration 144 : 0.32980878437319405
Loss in iteration 145 : 0.3297848066328771
Loss in iteration 146 : 0.3297612224036964
Loss in iteration 147 : 0.3297380203429368
Loss in iteration 148 : 0.3297151894820118
Loss in iteration 149 : 0.32969271921273996
Loss in iteration 150 : 0.3296705992741941
Loss in iteration 151 : 0.3296488197400789
Loss in iteration 152 : 0.3296273710066237
Loss in iteration 153 : 0.329606243780962
Loss in iteration 154 : 0.329585429069979
Loss in iteration 155 : 0.3295649181696037
Loss in iteration 156 : 0.3295447026545264
Loss in iteration 157 : 0.32952477436832206
Loss in iteration 158 : 0.3295051254139678
Loss in iteration 159 : 0.32948574814472353
Loss in iteration 160 : 0.32946663515537755
Loss in iteration 161 : 0.32944777927382224
Loss in iteration 162 : 0.3294291735529667
Loss in iteration 163 : 0.3294108112629478
Loss in iteration 164 : 0.32939268588365295
Loss in iteration 165 : 0.32937479109751666
Loss in iteration 166 : 0.32935712078259677
Loss in iteration 167 : 0.32933966900591066
Loss in iteration 168 : 0.32932243001702394
Loss in iteration 169 : 0.32930539824187766
Loss in iteration 170 : 0.3292885682768425
Loss in iteration 171 : 0.3292719348830034
Loss in iteration 172 : 0.3292554929806465
Loss in iteration 173 : 0.32923923764395263
Loss in iteration 174 : 0.32922316409588764
Loss in iteration 175 : 0.32920726770327524
Loss in iteration 176 : 0.3291915439720532
Loss in iteration 177 : 0.3291759885426999
Loss in iteration 178 : 0.32916059718582275
Loss in iteration 179 : 0.3291453657979138
Loss in iteration 180 : 0.32913029039724834
Loss in iteration 181 : 0.3291153671199357
Loss in iteration 182 : 0.3291005922161055
Loss in iteration 183 : 0.329085962046238
Loss in iteration 184 : 0.32907147307761414
Loss in iteration 185 : 0.3290571218808925
Loss in iteration 186 : 0.3290429051268118
Loss in iteration 187 : 0.3290288195830045
Loss in iteration 188 : 0.32901486211091907
Loss in iteration 189 : 0.3290010296628542
Loss in iteration 190 : 0.32898731927908986
Loss in iteration 191 : 0.32897372808512154
Loss in iteration 192 : 0.32896025328898854
Loss in iteration 193 : 0.32894689217868905
Loss in iteration 194 : 0.32893364211969045
Loss in iteration 195 : 0.3289205005525213
Loss in iteration 196 : 0.32890746499043816
Loss in iteration 197 : 0.3288945330171835
Loss in iteration 198 : 0.32888170228480973
Loss in iteration 199 : 0.32886897051157754
Loss in iteration 200 : 0.328856335479927
Testing accuracy  of updater 3 on alg 0 with rate 0.049 = 0.782, training accuracy 0.8391065069601813, time elapsed: 2691 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4791509157686055
Loss in iteration 3 : 0.4680009342661439
Loss in iteration 4 : 0.4599228886968083
Loss in iteration 5 : 0.45297152364824056
Loss in iteration 6 : 0.44667499206800315
Loss in iteration 7 : 0.44087393879555514
Loss in iteration 8 : 0.43549448629250326
Loss in iteration 9 : 0.4304904513836367
Loss in iteration 10 : 0.4258264624760087
Loss in iteration 11 : 0.4214725519403014
Loss in iteration 12 : 0.4174022530949284
Loss in iteration 13 : 0.4135918236096889
Loss in iteration 14 : 0.4100198450480132
Loss in iteration 15 : 0.40666695510768786
Loss in iteration 16 : 0.40351563304914156
Loss in iteration 17 : 0.40055001258600986
Loss in iteration 18 : 0.3977557139682214
Loss in iteration 19 : 0.3951196923922298
Loss in iteration 20 : 0.3926301013618795
Loss in iteration 21 : 0.39027616992263103
Loss in iteration 22 : 0.3880480926910228
Loss in iteration 23 : 0.3859369315690296
Loss in iteration 24 : 0.3839345280343769
Loss in iteration 25 : 0.3820334249365176
Loss in iteration 26 : 0.38022679679252813
Loss in iteration 27 : 0.37850838765609385
Loss in iteration 28 : 0.376872455717149
Loss in iteration 29 : 0.3753137238739275
Loss in iteration 30 : 0.3738273355996444
Loss in iteration 31 : 0.3724088155009108
Loss in iteration 32 : 0.37105403403341125
Loss in iteration 33 : 0.3697591759021685
Loss in iteration 34 : 0.36852071172897716
Loss in iteration 35 : 0.3673353726187748
Loss in iteration 36 : 0.366200127300234
Loss in iteration 37 : 0.36511216155429327
Loss in iteration 38 : 0.36406885967818564
Loss in iteration 39 : 0.36306778776226395
Loss in iteration 40 : 0.3621066785830992
Loss in iteration 41 : 0.3611834179392436
Loss in iteration 42 : 0.360296032276242
Loss in iteration 43 : 0.35944267746513736
Loss in iteration 44 : 0.35862162861428026
Loss in iteration 45 : 0.35783127080787885
Loss in iteration 46 : 0.3570700906767499
Loss in iteration 47 : 0.35633666871728314
Loss in iteration 48 : 0.35562967228391235
Loss in iteration 49 : 0.3549478491885935
Loss in iteration 50 : 0.35429002184801556
Loss in iteration 51 : 0.3536550819256372
Loss in iteration 52 : 0.35304198542128046
Loss in iteration 53 : 0.3524497481660127
Loss in iteration 54 : 0.3518774416844495
Loss in iteration 55 : 0.35132418939053006
Loss in iteration 56 : 0.35078916308630576
Loss in iteration 57 : 0.35027157973634365
Testing accuracy  of updater 3 on alg 0 with rate 0.028000000000000004 = 0.76775, training accuracy 0.8310132729038524, time elapsed: 821 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.603984293605351
Loss in iteration 3 : 0.557589544920726
Loss in iteration 4 : 0.5306838801775916
Testing accuracy  of updater 3 on alg 0 with rate 0.006999999999999992 = 0.5, training accuracy 0.6474587245063127, time elapsed: 123 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 100.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 34 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 70.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 46 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 40.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 31 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 29 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 7.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 39 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 4.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 29 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 1.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 37 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 10.107523572796929
Loss in iteration 3 : 3.938962011893691
Loss in iteration 4 : 0.5985842440840353
Loss in iteration 5 : 1.1663464778052226
Loss in iteration 6 : 2.405612846341351
Loss in iteration 7 : 4.221694462115804
Loss in iteration 8 : 1.016888138972091
Loss in iteration 9 : 0.6384469589731621
Loss in iteration 10 : 0.6223191429486722
Loss in iteration 11 : 0.5908416660192559
Loss in iteration 12 : 0.6091978199166697
Loss in iteration 13 : 0.6347157794782223
Loss in iteration 14 : 0.9241446955814516
Loss in iteration 15 : 1.2129867090076547
Loss in iteration 16 : 2.7884042321508518
Loss in iteration 17 : 0.6154911992436833
Loss in iteration 18 : 0.6545276448266045
Loss in iteration 19 : 0.6785226897266154
Loss in iteration 20 : 1.0533174260879066
Loss in iteration 21 : 1.2050911130285238
Loss in iteration 22 : 2.6740602913545666
Loss in iteration 23 : 0.6339334977789896
Loss in iteration 24 : 0.6721032473514212
Loss in iteration 25 : 0.6801649871990065
Loss in iteration 26 : 1.0120582300865573
Loss in iteration 27 : 1.1162056044362574
Loss in iteration 28 : 2.5076256406205832
Loss in iteration 29 : 0.7010180174426832
Loss in iteration 30 : 0.8352526586355566
Loss in iteration 31 : 0.7842305928036523
Loss in iteration 32 : 1.2925846436039519
Loss in iteration 33 : 1.065630422217944
Loss in iteration 34 : 2.0710630209946284
Loss in iteration 35 : 0.7972248663917637
Loss in iteration 36 : 1.0250567713745409
Loss in iteration 37 : 0.7993386265280497
Loss in iteration 38 : 1.1923859612768541
Loss in iteration 39 : 0.9387384427224351
Loss in iteration 40 : 1.6773049602043735
Loss in iteration 41 : 0.9338123374966821
Loss in iteration 42 : 1.451152028514656
Loss in iteration 43 : 0.8483871033847675
Loss in iteration 44 : 1.2074178506525177
Loss in iteration 45 : 0.8582906562126101
Loss in iteration 46 : 1.3315173664138684
Loss in iteration 47 : 0.9289044291183517
Loss in iteration 48 : 1.5449832190307822
Loss in iteration 49 : 0.9147198917131006
Loss in iteration 50 : 1.4043900695234426
Loss in iteration 51 : 0.8697917468660681
Loss in iteration 52 : 1.287812255405028
Loss in iteration 53 : 0.8850780570966684
Loss in iteration 54 : 1.3897207661060078
Loss in iteration 55 : 0.922756630441756
Loss in iteration 56 : 1.4864727062051644
Loss in iteration 57 : 0.906540032388701
Loss in iteration 58 : 1.3878020288110833
Loss in iteration 59 : 0.8832366956217624
Loss in iteration 60 : 1.3372503528273416
Loss in iteration 61 : 0.8982120763620557
Loss in iteration 62 : 1.413338592741737
Loss in iteration 63 : 0.9167081938387319
Loss in iteration 64 : 1.4503396432125648
Loss in iteration 65 : 0.9029302286714208
Loss in iteration 66 : 1.384913502683191
Loss in iteration 67 : 0.8920129345663359
Loss in iteration 68 : 1.3688108653813076
Loss in iteration 69 : 0.9037913447191283
Loss in iteration 70 : 1.4195658916940503
Loss in iteration 71 : 0.9114903232577882
Loss in iteration 72 : 1.4278443548718938
Loss in iteration 73 : 0.9015006936889598
Loss in iteration 74 : 1.387834621959625
Loss in iteration 75 : 0.897335057319688
Loss in iteration 76 : 1.3877665705985385
Loss in iteration 77 : 0.9052905760347681
Loss in iteration 78 : 1.418028745586072
Loss in iteration 79 : 0.9074865708764919
Loss in iteration 80 : 1.4146765457944277
Loss in iteration 81 : 0.9010354182004205
Loss in iteration 82 : 1.3923860057818969
Loss in iteration 83 : 0.9001251986785804
Loss in iteration 84 : 1.397957558232565
Loss in iteration 85 : 0.9048254595699046
Loss in iteration 86 : 1.4139559273557951
Loss in iteration 87 : 0.9046326808310233
Loss in iteration 88 : 1.407591239787192
Loss in iteration 89 : 0.9008676828189444
Loss in iteration 90 : 1.3964956351322613
Loss in iteration 91 : 0.9012084752573223
Loss in iteration 92 : 1.4025589601649704
Loss in iteration 93 : 0.9036091348163289
Loss in iteration 94 : 1.4098351059873175
Loss in iteration 95 : 0.9026660200014194
Loss in iteration 96 : 1.4041247993810784
Loss in iteration 97 : 0.9006712765809779
Loss in iteration 98 : 1.3993828837136129
Loss in iteration 99 : 0.9012643129552963
Loss in iteration 100 : 1.4039805014927684
Loss in iteration 101 : 0.9022610985322156
Loss in iteration 102 : 1.4065735592310324
Loss in iteration 103 : 0.9013009547182869
Loss in iteration 104 : 1.4025720148952552
Loss in iteration 105 : 0.9003363333730977
Loss in iteration 106 : 1.4009998544315774
Loss in iteration 107 : 0.9007894192065133
Loss in iteration 108 : 1.403840633722156
Loss in iteration 109 : 0.9010443727622203
Loss in iteration 110 : 1.4042837699099526
Loss in iteration 111 : 0.9003090457966587
Loss in iteration 112 : 1.4018799203255203
Loss in iteration 113 : 0.89987199186817
Loss in iteration 114 : 1.4016286702565979
Loss in iteration 115 : 0.9001052977434469
Loss in iteration 116 : 1.40309383627137
Loss in iteration 117 : 0.9000330732330558
Loss in iteration 118 : 1.4027636167471345
Loss in iteration 119 : 0.8995385209385774
Loss in iteration 120 : 1.4014823678185495
Loss in iteration 121 : 0.8993349256800027
Loss in iteration 122 : 1.401624491127112
Loss in iteration 123 : 0.8993942269514305
Loss in iteration 124 : 1.4022240841161524
Loss in iteration 125 : 0.89921758497329
Loss in iteration 126 : 1.4017519252781314
Loss in iteration 127 : 0.8989032671453568
Loss in iteration 128 : 1.40112919840436
Loss in iteration 129 : 0.8987858260903492
Loss in iteration 130 : 1.4012854674774406
Loss in iteration 131 : 0.8987433404774064
Loss in iteration 132 : 1.4014290333827253
Loss in iteration 133 : 0.8985622038549056
Loss in iteration 134 : 1.4010372528550108
Loss in iteration 135 : 0.8983615131441474
Loss in iteration 136 : 1.4007452042853985
Loss in iteration 137 : 0.8982696372334625
Loss in iteration 138 : 1.4008125522313324
Loss in iteration 139 : 0.8981834016431811
Loss in iteration 140 : 1.400758721392834
Loss in iteration 141 : 0.8980313326155435
Loss in iteration 142 : 1.400483416348639
Loss in iteration 143 : 0.8978957421896113
Loss in iteration 144 : 1.400332954462308
Loss in iteration 145 : 0.897811543030307
Loss in iteration 146 : 1.4003186386386417
Loss in iteration 147 : 0.8977167122288932
Loss in iteration 148 : 1.4002026644300865
Loss in iteration 149 : 0.8975977613081435
Loss in iteration 150 : 1.4000170464900856
Loss in iteration 151 : 0.8974986437122863
Loss in iteration 152 : 1.3999172944317015
Loss in iteration 153 : 0.8974207449453911
Loss in iteration 154 : 1.3998554933773588
Loss in iteration 155 : 0.8973340916359573
Loss in iteration 156 : 1.3997351984117565
Loss in iteration 157 : 0.89724264699105
Loss in iteration 158 : 1.3996048800136691
Loss in iteration 159 : 0.8971654601865734
Loss in iteration 160 : 1.3995205167374287
Loss in iteration 161 : 0.8970963547566699
Loss in iteration 162 : 1.3994401218062464
Loss in iteration 163 : 0.8970235466722334
Loss in iteration 164 : 1.399333749466215
Loss in iteration 165 : 0.8969529271105708
Loss in iteration 166 : 1.399234252526955
Loss in iteration 167 : 0.8968909720733281
Loss in iteration 168 : 1.3991555026791975
Loss in iteration 169 : 0.8968324411394253
Loss in iteration 170 : 1.3990733877555723
Loss in iteration 171 : 0.89677379825544
Loss in iteration 172 : 1.3989829726761593
Loss in iteration 173 : 0.896718857629672
Loss in iteration 174 : 1.3989007441377042
Loss in iteration 175 : 0.8966690107526007
Loss in iteration 176 : 1.3988268629057035
Loss in iteration 177 : 0.8966213223977338
Loss in iteration 178 : 1.3987505043354833
Loss in iteration 179 : 0.8965752117422798
Loss in iteration 180 : 1.3986733941033194
Loss in iteration 181 : 0.8965324650925616
Loss in iteration 182 : 1.3986020785819957
Loss in iteration 183 : 0.8964929301303706
Loss in iteration 184 : 1.3985341551287351
Loss in iteration 185 : 0.8964552986881407
Loss in iteration 186 : 1.398465762249256
Loss in iteration 187 : 0.8964197185944581
Loss in iteration 188 : 1.398399028569755
Loss in iteration 189 : 0.8963868109280745
Loss in iteration 190 : 1.39833591394575
Loss in iteration 191 : 0.8963561909082531
Loss in iteration 192 : 1.3982746049657426
Loss in iteration 193 : 0.8963273667391051
Loss in iteration 194 : 1.3982140952726243
Loss in iteration 195 : 0.8963005067683976
Loss in iteration 196 : 1.3981556139059799
Loss in iteration 197 : 0.8962757288396598
Loss in iteration 198 : 1.3980994673864384
Loss in iteration 199 : 0.8962527517368192
Loss in iteration 200 : 1.398044733430822
Testing accuracy  of updater 5 on alg 0 with rate 0.07 = 0.78375, training accuracy 0.7779216574943347, time elapsed: 3305 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 4.160113113303172
Loss in iteration 3 : 1.3270919982519522
Loss in iteration 4 : 0.7720558389304116
Loss in iteration 5 : 1.3848363113036932
Loss in iteration 6 : 0.5488820617944418
Loss in iteration 7 : 0.6721756443770882
Loss in iteration 8 : 0.5769520492666127
Loss in iteration 9 : 0.8107227427877047
Loss in iteration 10 : 0.6704872631188725
Loss in iteration 11 : 1.0617249150457184
Loss in iteration 12 : 0.6604331935529234
Loss in iteration 13 : 0.9461928178453296
Loss in iteration 14 : 0.6253960493689196
Loss in iteration 15 : 0.8542036875845311
Loss in iteration 16 : 0.630590677069122
Loss in iteration 17 : 0.89905314083646
Loss in iteration 18 : 0.6613972357042126
Loss in iteration 19 : 0.9864505715698928
Loss in iteration 20 : 0.6676080109497877
Loss in iteration 21 : 0.9747636635415834
Loss in iteration 22 : 0.6554894127055904
Loss in iteration 23 : 0.9346416214367961
Loss in iteration 24 : 0.6577048388493282
Loss in iteration 25 : 0.9525464604850198
Loss in iteration 26 : 0.6707111554412036
Loss in iteration 27 : 0.9893206085031079
Loss in iteration 28 : 0.6743915307389371
Loss in iteration 29 : 0.9872541543470901
Loss in iteration 30 : 0.6707782993700623
Loss in iteration 31 : 0.9729824776328249
Loss in iteration 32 : 0.6726530748289047
Loss in iteration 33 : 0.9819704566146962
Loss in iteration 34 : 0.6783201047353086
Loss in iteration 35 : 0.9969197310069564
Loss in iteration 36 : 0.6800297713565112
Loss in iteration 37 : 0.9961640752448848
Loss in iteration 38 : 0.679186111447775
Loss in iteration 39 : 0.9917963394907129
Loss in iteration 40 : 0.6805363323673079
Loss in iteration 41 : 0.9965527748569598
Loss in iteration 42 : 0.6829847335100495
Loss in iteration 43 : 1.002430482465308
Loss in iteration 44 : 0.6837247336085989
Loss in iteration 45 : 1.0020926238233874
Loss in iteration 46 : 0.6836424902247329
Loss in iteration 47 : 1.0010547901275821
Loss in iteration 48 : 0.6844141380908982
Loss in iteration 49 : 1.0034489809760219
Loss in iteration 50 : 0.6854103389000519
Loss in iteration 51 : 1.0056894271962977
Loss in iteration 52 : 0.6856813900115322
Loss in iteration 53 : 1.005552991092689
Loss in iteration 54 : 0.6857050220150933
Loss in iteration 55 : 1.0054413391596808
Loss in iteration 56 : 0.6860383359139981
Loss in iteration 57 : 1.0065447623447479
Loss in iteration 58 : 0.686366936082531
Loss in iteration 59 : 1.0073470558596003
Loss in iteration 60 : 0.6863955191870325
Loss in iteration 61 : 1.0072735636900227
Loss in iteration 62 : 0.686353273336046
Loss in iteration 63 : 1.0073130984168817
Loss in iteration 64 : 0.6864119515308019
Loss in iteration 65 : 1.0077486526528017
Loss in iteration 66 : 0.6864309223648097
Loss in iteration 67 : 1.0079777286695837
Loss in iteration 68 : 0.6863293917228818
Loss in iteration 69 : 1.007895920829241
Loss in iteration 70 : 0.6862082264435359
Loss in iteration 71 : 1.0078878794356043
Loss in iteration 72 : 0.6861152547524673
Loss in iteration 73 : 1.007993707030114
Loss in iteration 74 : 0.6859931552516458
Loss in iteration 75 : 1.0079889005017597
Loss in iteration 76 : 0.6858248970299087
Loss in iteration 77 : 1.0078778524794523
Loss in iteration 78 : 0.6856519563063169
Loss in iteration 79 : 1.0078018511692568
Loss in iteration 80 : 0.6854857051941483
Loss in iteration 81 : 1.007751805311389
Loss in iteration 82 : 0.6853047772893023
Loss in iteration 83 : 1.0076497959189539
Loss in iteration 84 : 0.6851082145534362
Loss in iteration 85 : 1.0075132107614015
Loss in iteration 86 : 0.684912133838195
Loss in iteration 87 : 1.0073909417660813
Loss in iteration 88 : 0.6847184911471539
Loss in iteration 89 : 1.0072717010224879
Loss in iteration 90 : 0.6845200764381422
Loss in iteration 91 : 1.007130693829047
Loss in iteration 92 : 0.6843185919803894
Loss in iteration 93 : 1.0069796578335133
Loss in iteration 94 : 0.6841199965379543
Loss in iteration 95 : 1.006834465233655
Loss in iteration 96 : 0.6839243122683992
Loss in iteration 97 : 1.006688691710379
Loss in iteration 98 : 0.6837293962082325
Loss in iteration 99 : 1.0065353565079838
Loss in iteration 100 : 0.6835365985058063
Loss in iteration 101 : 1.0063804156174982
Loss in iteration 102 : 0.6833480122347604
Loss in iteration 103 : 1.0062286723496878
Loss in iteration 104 : 0.6831634498999051
Loss in iteration 105 : 1.0060772718043967
Loss in iteration 106 : 0.6829823756840222
Loss in iteration 107 : 1.0059246326035995
Loss in iteration 108 : 0.6828054957116566
Loss in iteration 109 : 1.005773380910643
Loss in iteration 110 : 0.6826334856482454
Loss in iteration 111 : 1.005624855695964
Loss in iteration 112 : 0.682466207926937
Loss in iteration 113 : 1.0054779697934058
Loss in iteration 114 : 0.6823035433337684
Loss in iteration 115 : 1.0053325367370278
Loss in iteration 116 : 0.6821457703252735
Loss in iteration 117 : 1.0051896039251609
Loss in iteration 118 : 0.6819930539983244
Loss in iteration 119 : 1.0050495005489364
Loss in iteration 120 : 0.6818452883927224
Loss in iteration 121 : 1.0049118557001369
Loss in iteration 122 : 0.6817024149838439
Loss in iteration 123 : 1.0047767314283262
Loss in iteration 124 : 0.681564486564524
Loss in iteration 125 : 1.004644501108927
Loss in iteration 126 : 0.6814314833637507
Loss in iteration 127 : 1.0045152168594662
Loss in iteration 128 : 0.6813033009395377
Loss in iteration 129 : 1.0043887513293444
Loss in iteration 130 : 0.6811798610695828
Loss in iteration 131 : 1.0042651496446737
Loss in iteration 132 : 0.6810611107191348
Loss in iteration 133 : 1.00414451511298
Loss in iteration 134 : 0.6809469624023211
Loss in iteration 135 : 1.0040268243339086
Loss in iteration 136 : 0.6808373041782649
Loss in iteration 137 : 1.0039120152272982
Loss in iteration 138 : 0.680732035014379
Loss in iteration 139 : 1.0038000853330447
Loss in iteration 140 : 0.6806310575705551
Loss in iteration 141 : 1.003691033727391
Loss in iteration 142 : 0.6805342609962112
Loss in iteration 143 : 1.0035848138561303
Loss in iteration 144 : 0.6804415286099038
Loss in iteration 145 : 1.0034813733125354
Loss in iteration 146 : 0.6803527482357092
Loss in iteration 147 : 1.0033806776774956
Loss in iteration 148 : 0.6802678080522314
Loss in iteration 149 : 1.003282686965647
Loss in iteration 150 : 0.6801865923832832
Loss in iteration 151 : 1.0031873460886251
Loss in iteration 152 : 0.6801089854525662
Loss in iteration 153 : 1.0030946001738115
Loss in iteration 154 : 0.6800348741429727
Loss in iteration 155 : 1.0030043991332187
Loss in iteration 156 : 0.6799641463052124
Loss in iteration 157 : 1.0029166894355854
Loss in iteration 158 : 0.6798966900157193
Loss in iteration 159 : 1.0028314131962375
Loss in iteration 160 : 0.6798323951063042
Loss in iteration 161 : 1.00274851346411
Loss in iteration 162 : 0.679771153828284
Loss in iteration 163 : 1.0026679346659468
Loss in iteration 164 : 0.67971286028667
Loss in iteration 165 : 1.0025896200948976
Loss in iteration 166 : 0.6796574104297566
Loss in iteration 167 : 1.0025135123473943
Loss in iteration 168 : 0.679604702597869
Loss in iteration 169 : 1.0024395549976832
Loss in iteration 170 : 0.679554637654624
Loss in iteration 171 : 1.0023676924591929
Loss in iteration 172 : 0.6795071188175335
Loss in iteration 173 : 1.0022978693305982
Loss in iteration 174 : 0.6794620517155576
Loss in iteration 175 : 1.0022300307573728
Loss in iteration 176 : 0.6794193445530662
Loss in iteration 177 : 1.0021641229247187
Loss in iteration 178 : 0.6793789081102234
Loss in iteration 179 : 1.0021000929518706
Loss in iteration 180 : 0.679340655680718
Loss in iteration 181 : 1.002037888758577
Loss in iteration 182 : 0.6793045030852904
Loss in iteration 183 : 1.0019774592382338
Loss in iteration 184 : 0.679270368692738
Loss in iteration 185 : 1.0019187543906918
Loss in iteration 186 : 0.6792381733840569
Loss in iteration 187 : 1.0018617252767879
Loss in iteration 188 : 0.6792078405063503
Loss in iteration 189 : 1.0018063240005406
Loss in iteration 190 : 0.6791792958478385
Loss in iteration 191 : 1.0017525037715436
Loss in iteration 192 : 0.6791524676067404
Loss in iteration 193 : 1.0017002189316992
Loss in iteration 194 : 0.6791272863419372
Loss in iteration 195 : 1.0016494249332293
Loss in iteration 196 : 0.6791036849228665
Loss in iteration 197 : 1.001600078333376
Loss in iteration 198 : 0.6790815984840832
Loss in iteration 199 : 1.0015521368056126
Loss in iteration 200 : 0.6790609643756573
Testing accuracy  of updater 5 on alg 0 with rate 0.049 = 0.6635, training accuracy 0.7562317902233733, time elapsed: 3122 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 2.4325013453160254
Loss in iteration 3 : 0.8223465267664666
Loss in iteration 4 : 0.4769769499965345
Loss in iteration 5 : 0.6719044872669698
Loss in iteration 6 : 0.45485318407919356
Loss in iteration 7 : 0.5698871220531775
Loss in iteration 8 : 0.44284372793343296
Loss in iteration 9 : 0.5375501129965213
Loss in iteration 10 : 0.44383980310333737
Loss in iteration 11 : 0.5446588907194909
Loss in iteration 12 : 0.45356388968001726
Loss in iteration 13 : 0.5697478615504413
Loss in iteration 14 : 0.46218512072366946
Loss in iteration 15 : 0.5865181260093485
Loss in iteration 16 : 0.4648245343555385
Loss in iteration 17 : 0.5878167456240551
Loss in iteration 18 : 0.46587914052683055
Loss in iteration 19 : 0.588405078168984
Loss in iteration 20 : 0.4687600801855396
Loss in iteration 21 : 0.5950010266295995
Loss in iteration 22 : 0.4725063655373276
Loss in iteration 23 : 0.6029050308018007
Loss in iteration 24 : 0.4752319757496061
Loss in iteration 25 : 0.6074579617423064
Loss in iteration 26 : 0.4769333264050037
Loss in iteration 27 : 0.6098496387712442
Loss in iteration 28 : 0.4785564240912393
Loss in iteration 29 : 0.612708594121609
Loss in iteration 30 : 0.4803204130347173
Loss in iteration 31 : 0.6160470820921243
Loss in iteration 32 : 0.48186020789479855
Loss in iteration 33 : 0.6187069922196696
Loss in iteration 34 : 0.4830355269582365
Loss in iteration 35 : 0.6205383836874863
Loss in iteration 36 : 0.48401982100902324
Loss in iteration 37 : 0.6221222447987736
Loss in iteration 38 : 0.4849294228878608
Loss in iteration 39 : 0.6236721012365227
Loss in iteration 40 : 0.4857252952921996
Loss in iteration 41 : 0.6250016956031281
Loss in iteration 42 : 0.48636990698773164
Loss in iteration 43 : 0.6260310805520994
Loss in iteration 44 : 0.48689519324816644
Loss in iteration 45 : 0.6268737986405898
Loss in iteration 46 : 0.487339777615874
Loss in iteration 47 : 0.6276149570174437
Loss in iteration 48 : 0.4877097979004403
Loss in iteration 49 : 0.6282431635546345
Loss in iteration 50 : 0.4880027399434452
Loss in iteration 51 : 0.6287441621307647
Loss in iteration 52 : 0.4882283169253636
Loss in iteration 53 : 0.6291437773759295
Loss in iteration 54 : 0.4884002643233669
Loss in iteration 55 : 0.6294708001888973
Loss in iteration 56 : 0.48852602641791404
Loss in iteration 57 : 0.6297335511149
Loss in iteration 58 : 0.4886094045621322
Loss in iteration 59 : 0.6299344161908723
Loss in iteration 60 : 0.4886556262192034
Loss in iteration 61 : 0.6300824096741058
Loss in iteration 62 : 0.48867080626344644
Loss in iteration 63 : 0.6301882469361206
Loss in iteration 64 : 0.4886596868244537
Loss in iteration 65 : 0.6302583541377096
Loss in iteration 66 : 0.48862571955385636
Loss in iteration 67 : 0.6302966539855221
Loss in iteration 68 : 0.4885721714456846
Loss in iteration 69 : 0.6303076400171416
Loss in iteration 70 : 0.4885022747880693
Loss in iteration 71 : 0.6302960457595541
Loss in iteration 72 : 0.48841880750797684
Loss in iteration 73 : 0.6302655506946384
Loss in iteration 74 : 0.48832403960077136
Loss in iteration 75 : 0.6302189067569146
Loss in iteration 76 : 0.4882199693615658
Loss in iteration 77 : 0.6301586325515748
Loss in iteration 78 : 0.4881084192355155
Loss in iteration 79 : 0.6300871110585585
Loss in iteration 80 : 0.48799098166375804
Loss in iteration 81 : 0.6300063578756574
Loss in iteration 82 : 0.48786900853293863
Loss in iteration 83 : 0.6299180152330311
Loss in iteration 84 : 0.4877436687510477
Loss in iteration 85 : 0.629823509357686
Loss in iteration 86 : 0.48761598942565365
Loss in iteration 87 : 0.6297241151984547
Loss in iteration 88 : 0.4874868613352058
Loss in iteration 89 : 0.6296209317162689
Loss in iteration 90 : 0.48735704519088313
Loss in iteration 91 : 0.6295148845438314
Loss in iteration 92 : 0.4872271913770334
Loss in iteration 93 : 0.6294067671210317
Loss in iteration 94 : 0.48709785825985114
Loss in iteration 95 : 0.6292972693452601
Loss in iteration 96 : 0.4869695220979253
Loss in iteration 97 : 0.6291869841561756
Loss in iteration 98 : 0.4868425848097729
Loss in iteration 99 : 0.6290764145659666
Loss in iteration 100 : 0.4867173835735422
Loss in iteration 101 : 0.6289659881198234
Loss in iteration 102 : 0.4865942000263133
Loss in iteration 103 : 0.6288560696812363
Loss in iteration 104 : 0.48647326722074946
Loss in iteration 105 : 0.6287469687848164
Loss in iteration 106 : 0.4863547753158153
Loss in iteration 107 : 0.6286389455467284
Loss in iteration 108 : 0.48623887706692265
Loss in iteration 109 : 0.6285322174467575
Loss in iteration 110 : 0.4861256929051787
Loss in iteration 111 : 0.6284269656057595
Loss in iteration 112 : 0.48601531521587216
Loss in iteration 113 : 0.6283233395448163
Loss in iteration 114 : 0.4859078119707704
Loss in iteration 115 : 0.6282214610880423
Loss in iteration 116 : 0.48580322999262177
Loss in iteration 117 : 0.6281214280545919
Loss in iteration 118 : 0.4857015978878039
Loss in iteration 119 : 0.628023317612385
Loss in iteration 120 : 0.4856029285944653
Loss in iteration 121 : 0.627927189082336
Loss in iteration 122 : 0.48550722158597753
Loss in iteration 123 : 0.6278330863092785
Loss in iteration 124 : 0.4854144648115028
Loss in iteration 125 : 0.627741039775233
Loss in iteration 126 : 0.485324636411668
Loss in iteration 127 : 0.6276510684798089
Loss in iteration 128 : 0.4852377062187131
Loss in iteration 129 : 0.6275631815633002
Loss in iteration 130 : 0.4851536370621944
Loss in iteration 131 : 0.6274773797037391
Loss in iteration 132 : 0.4850723859111676
Loss in iteration 133 : 0.6273936563412774
Loss in iteration 134 : 0.4849939048753618
Loss in iteration 135 : 0.6273119987550179
Loss in iteration 136 : 0.4849181420793539
Loss in iteration 137 : 0.6272323890003805
Loss in iteration 138 : 0.4848450424235029
Loss in iteration 139 : 0.6271548047222603
Loss in iteration 140 : 0.4847745482465919
Loss in iteration 141 : 0.6270792198642386
Loss in iteration 142 : 0.48470659990291837
Loss in iteration 143 : 0.6270056052884493
Loss in iteration 144 : 0.4846411362637932
Loss in iteration 145 : 0.6269339293155543
Loss in iteration 146 : 0.4845780951521939
Loss in iteration 147 : 0.6268641581939045
Loss in iteration 148 : 0.48451741371891444
Loss in iteration 149 : 0.6267962565075748
Loss in iteration 150 : 0.4844590287675208
Loss in iteration 151 : 0.6267301875313365
Loss in iteration 152 : 0.4844028770343551
Loss in iteration 153 : 0.6266659135389884
Loss in iteration 154 : 0.4843488954289669
Loss in iteration 155 : 0.6266033960705588
Loss in iteration 156 : 0.4842970212398467
Loss in iteration 157 : 0.6265425961636046
Loss in iteration 158 : 0.4842471923097939
Loss in iteration 159 : 0.6264834745532977
Loss in iteration 160 : 0.48419934718462493
Loss in iteration 161 : 0.6264259918449796
Loss in iteration 162 : 0.48415342523852645
Loss in iteration 163 : 0.6263701086627076
Loss in iteration 164 : 0.48410936677896754
Loss in iteration 165 : 0.6263157857767322
Loss in iteration 166 : 0.4840671131336642
Loss in iteration 167 : 0.6262629842124798
Loss in iteration 168 : 0.4840266067219116
Loss in iteration 169 : 0.6262116653434694
Loss in iteration 170 : 0.48398779111221507
Loss in iteration 171 : 0.6261617909700641
Loss in iteration 172 : 0.4839506110679614
Loss in iteration 173 : 0.6261133233858472
Loss in iteration 174 : 0.48391501258264796
Loss in iteration 175 : 0.6260662254331506
Loss in iteration 176 : 0.4838809429060462
Loss in iteration 177 : 0.626020460549187
Loss in iteration 178 : 0.48384835056242476
Loss in iteration 179 : 0.6259759928038066
Loss in iteration 180 : 0.4838171853619316
Loss in iteration 181 : 0.6259327869301067
Loss in iteration 182 : 0.4837873984060058
Loss in iteration 183 : 0.6258908083486625
Loss in iteration 184 : 0.4837589420876514
Loss in iteration 185 : 0.6258500231862902
Loss in iteration 186 : 0.48373177008725243
Loss in iteration 187 : 0.6258103982899615
Loss in iteration 188 : 0.483705837364591
Loss in iteration 189 : 0.6257719012365877
Loss in iteration 190 : 0.48368110014758353
Loss in iteration 191 : 0.6257345003391391
Loss in iteration 192 : 0.4836575159182324
Loss in iteration 193 : 0.6256981646496114
Loss in iteration 194 : 0.4836350433962122
Loss in iteration 195 : 0.6256628639592747
Loss in iteration 196 : 0.48361364252047134
Loss in iteration 197 : 0.6256285687965502
Loss in iteration 198 : 0.48359327442917754
Loss in iteration 199 : 0.6255952504228842
Loss in iteration 200 : 0.4835739014382655
Testing accuracy  of updater 5 on alg 0 with rate 0.028 = 0.68825, training accuracy 0.7730657170605374, time elapsed: 3058 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.718403633367889
Loss in iteration 3 : 0.45673087727976264
Loss in iteration 4 : 0.40073631515864844
Loss in iteration 5 : 0.38905890148407735
Loss in iteration 6 : 0.37977580370168174
Loss in iteration 7 : 0.3721689503197209
Loss in iteration 8 : 0.3658427237651173
Loss in iteration 9 : 0.36053116232748883
Loss in iteration 10 : 0.35604124352325944
Loss in iteration 11 : 0.352224797463148
Loss in iteration 12 : 0.34896690064368335
Loss in iteration 13 : 0.346176391100809
Loss in iteration 14 : 0.34378022196449765
Loss in iteration 15 : 0.34171896920480943
Loss in iteration 16 : 0.3399438126161257
Loss in iteration 17 : 0.3384141672401782
Loss in iteration 18 : 0.3370959989483129
Loss in iteration 19 : 0.33596049274957507
Loss in iteration 20 : 0.334983074771783
Loss in iteration 21 : 0.3341426254694186
Loss in iteration 22 : 0.3334208919227254
Loss in iteration 23 : 0.33280200291068235
Loss in iteration 24 : 0.3322721066746616
Loss in iteration 25 : 0.331819059278454
Loss in iteration 26 : 0.33143219966686954
Loss in iteration 27 : 0.3311021388327618
Loss in iteration 28 : 0.33082063277855117
Loss in iteration 29 : 0.33058045205571224
Loss in iteration 30 : 0.3303754971986489
Loss in iteration 31 : 0.33020157560493535
Loss in iteration 32 : 0.3300623624456975
Loss in iteration 33 : 0.33001000126632407
Loss in iteration 34 : 0.33045926009680476
Loss in iteration 35 : 0.3348586116501157
Loss in iteration 36 : 0.36925190162141985
Loss in iteration 37 : 0.4862274241897854
Loss in iteration 38 : 0.3828160451584535
Loss in iteration 39 : 0.38982753987771845
Loss in iteration 40 : 0.3428282385320164
Loss in iteration 41 : 0.3403793184026704
Loss in iteration 42 : 0.33514620274762547
Loss in iteration 43 : 0.3350885689184503
Loss in iteration 44 : 0.3340788069564143
Loss in iteration 45 : 0.3356666048321699
Loss in iteration 46 : 0.3365734619539801
Loss in iteration 47 : 0.34203416042458407
Loss in iteration 48 : 0.34570758297502147
Loss in iteration 49 : 0.3606406067374648
Loss in iteration 50 : 0.3585383404468562
Loss in iteration 51 : 0.3747030341773777
Loss in iteration 52 : 0.3529367246155267
Loss in iteration 53 : 0.3579727787773636
Loss in iteration 54 : 0.34439646747485786
Loss in iteration 55 : 0.34688663180244916
Loss in iteration 56 : 0.341036200818202
Loss in iteration 57 : 0.34444187170543267
Loss in iteration 58 : 0.34164150510569785
Loss in iteration 59 : 0.3473581818367688
Loss in iteration 60 : 0.34518545161838465
Loss in iteration 61 : 0.3538709134504676
Loss in iteration 62 : 0.34897715988570205
Loss in iteration 63 : 0.35834905630140274
Loss in iteration 64 : 0.3488722565292127
Loss in iteration 65 : 0.3558647658272666
Loss in iteration 66 : 0.34611064783387246
Loss in iteration 67 : 0.3513063378065188
Loss in iteration 68 : 0.34408342575200407
Loss in iteration 69 : 0.34916343901472086
Loss in iteration 70 : 0.34383498727854994
Loss in iteration 71 : 0.3498086407694372
Loss in iteration 72 : 0.3449536940609977
Loss in iteration 73 : 0.3520226047126128
Loss in iteration 74 : 0.3462354299754617
Loss in iteration 75 : 0.3536748587982324
Loss in iteration 76 : 0.3464476914216098
Loss in iteration 77 : 0.35331807684298006
Loss in iteration 78 : 0.34562920453515716
Loss in iteration 79 : 0.351799301114377
Loss in iteration 80 : 0.3447435454085808
Loss in iteration 81 : 0.35070635862098903
Loss in iteration 82 : 0.3444355024793565
Loss in iteration 83 : 0.3506623682478741
Loss in iteration 84 : 0.3446974622100781
Loss in iteration 85 : 0.3513264726247532
Loss in iteration 86 : 0.3451008552127519
Loss in iteration 87 : 0.35191651324910644
Loss in iteration 88 : 0.34520355005764636
Loss in iteration 89 : 0.3518879944893258
Loss in iteration 90 : 0.3449335517679727
Loss in iteration 91 : 0.3513747438032215
Loss in iteration 92 : 0.3445584554230012
Loss in iteration 93 : 0.35088183980611864
Loss in iteration 94 : 0.344344753108807
Loss in iteration 95 : 0.35073040472295974
Loss in iteration 96 : 0.3443467061891493
Loss in iteration 97 : 0.3508731729861492
Loss in iteration 98 : 0.34443363945740624
Loss in iteration 99 : 0.3510459088891862
Loss in iteration 100 : 0.3444433206807034
Loss in iteration 101 : 0.351032846181379
Loss in iteration 102 : 0.3443232173661682
Loss in iteration 103 : 0.35083335500184193
Loss in iteration 104 : 0.3441443033213556
Loss in iteration 105 : 0.35060195034250724
Loss in iteration 106 : 0.34400461396035953
Loss in iteration 107 : 0.35047221878719786
Loss in iteration 108 : 0.3439413410147948
Loss in iteration 109 : 0.35045604282800225
Loss in iteration 110 : 0.34392002869975763
Loss in iteration 111 : 0.3504714517982139
Loss in iteration 112 : 0.34388312381935254
Loss in iteration 113 : 0.35043534346537814
Loss in iteration 114 : 0.3438032824180481
Loss in iteration 115 : 0.35033155394083476
Loss in iteration 116 : 0.3436971254530386
Loss in iteration 117 : 0.35020483299712
Loss in iteration 118 : 0.3435985635073008
Loss in iteration 119 : 0.35010596585627984
Loss in iteration 120 : 0.34352600792762406
Loss in iteration 121 : 0.3500487587969488
Loss in iteration 122 : 0.3434721896093627
Loss in iteration 123 : 0.35001004358970944
Loss in iteration 124 : 0.343417641922879
Loss in iteration 125 : 0.349959284677004
Loss in iteration 126 : 0.34335006067801505
Loss in iteration 127 : 0.34988552625362035
Loss in iteration 128 : 0.34327222795698153
Loss in iteration 129 : 0.3498005884357824
Loss in iteration 130 : 0.3431952787652737
Loss in iteration 131 : 0.3497226791145825
Loss in iteration 132 : 0.3431271569945562
Loss in iteration 133 : 0.3496596952837926
Loss in iteration 134 : 0.3430671271842218
Loss in iteration 135 : 0.34960589985459156
Loss in iteration 136 : 0.34300894565892226
Loss in iteration 137 : 0.3495506077041727
Loss in iteration 138 : 0.3429475510771733
Loss in iteration 139 : 0.3494883152647351
Loss in iteration 140 : 0.3428828250941437
Loss in iteration 141 : 0.3494216428346099
Loss in iteration 142 : 0.34281822061417266
Loss in iteration 143 : 0.3493567937527026
Loss in iteration 144 : 0.3427568911936842
Loss in iteration 145 : 0.34929744685575664
Loss in iteration 146 : 0.3426992160859602
Loss in iteration 147 : 0.3492425174600581
Loss in iteration 148 : 0.3426433115713297
Loss in iteration 149 : 0.3491884459026077
Loss in iteration 150 : 0.34258722553660453
Loss in iteration 151 : 0.3491328151594853
Loss in iteration 152 : 0.34253053772078634
Loss in iteration 153 : 0.3490759708279155
Loss in iteration 154 : 0.3424742412930431
Loss in iteration 155 : 0.3490199163012468
Loss in iteration 156 : 0.3424195108190775
Loss in iteration 157 : 0.3489661882756042
Loss in iteration 158 : 0.34236668875063836
Loss in iteration 159 : 0.3489147420699471
Loss in iteration 160 : 0.34231524829530935
Loss in iteration 161 : 0.34886445122472065
Loss in iteration 162 : 0.34226447260929505
Loss in iteration 163 : 0.3488143367357353
Loss in iteration 164 : 0.34221408667808223
Loss in iteration 165 : 0.3487643121328273
Loss in iteration 166 : 0.3421643428450054
Loss in iteration 167 : 0.3487149826719555
Loss in iteration 168 : 0.3421156525127876
Loss in iteration 169 : 0.3486669421200326
Loss in iteration 170 : 0.34206819790326864
Loss in iteration 171 : 0.34862028744607715
Loss in iteration 172 : 0.3420218450531137
Loss in iteration 173 : 0.34857468076275405
Loss in iteration 174 : 0.3419763392032874
Loss in iteration 175 : 0.3485297457971643
Loss in iteration 176 : 0.3419315401516632
Loss in iteration 177 : 0.34848537781329997
Loss in iteration 178 : 0.3418874946314318
Loss in iteration 179 : 0.348441741005685
Loss in iteration 180 : 0.34184433530872227
Loss in iteration 181 : 0.34839904818106004
Loss in iteration 182 : 0.341802139895362
Loss in iteration 183 : 0.34835736625330893
Loss in iteration 184 : 0.3417608766217698
Loss in iteration 185 : 0.34831659782669694
Loss in iteration 186 : 0.3417204546974479
Loss in iteration 187 : 0.34827660155831025
Loss in iteration 188 : 0.34168080762400954
Loss in iteration 189 : 0.34823731240116257
Loss in iteration 190 : 0.3416419319704129
Loss in iteration 191 : 0.3481987641379617
Loss in iteration 192 : 0.3416038633535285
Loss in iteration 193 : 0.3481610249058703
Loss in iteration 194 : 0.34156662777592117
Loss in iteration 195 : 0.3481241238919899
Loss in iteration 196 : 0.34153021515340964
Loss in iteration 197 : 0.34808803134409494
Loss in iteration 198 : 0.34149458993999526
Loss in iteration 199 : 0.34805269230491875
Loss in iteration 200 : 0.34145971921203183
Testing accuracy  of updater 5 on alg 0 with rate 0.007 = 0.761, training accuracy 0.8268047911945613, time elapsed: 2865 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6929903271671058
Loss in iteration 3 : 0.46948993060676164
Loss in iteration 4 : 0.42218483833597903
Loss in iteration 5 : 0.40799001023565884
Loss in iteration 6 : 0.3964929581606653
Loss in iteration 7 : 0.3869814980757354
Loss in iteration 8 : 0.37901756827092825
Loss in iteration 9 : 0.37228766882689296
Loss in iteration 10 : 0.36655749907109914
Loss in iteration 11 : 0.3616478029810991
Loss in iteration 12 : 0.35741936559999965
Loss in iteration 13 : 0.35376239979282786
Loss in iteration 14 : 0.350589167824069
Loss in iteration 15 : 0.3478286421557561
Loss in iteration 16 : 0.3454226470902964
Loss in iteration 17 : 0.34332300383453973
Loss in iteration 18 : 0.3414894061888542
Loss in iteration 19 : 0.3398878126327812
Loss in iteration 20 : 0.3384892207026241
Loss in iteration 21 : 0.33726872009194947
Loss in iteration 22 : 0.33620475639136405
Loss in iteration 23 : 0.33527855220064534
Loss in iteration 24 : 0.3344736497372087
Loss in iteration 25 : 0.3337755459259945
Loss in iteration 26 : 0.3331714004527209
Loss in iteration 27 : 0.33264979996648014
Loss in iteration 28 : 0.33220056767588607
Loss in iteration 29 : 0.33181460771775684
Loss in iteration 30 : 0.3314837787265741
Loss in iteration 31 : 0.33120078871142494
Loss in iteration 32 : 0.3309591097485994
Loss in iteration 33 : 0.3307529043446285
Loss in iteration 34 : 0.3305769691567513
Loss in iteration 35 : 0.330426688529516
Loss in iteration 36 : 0.3302980819274411
Loss in iteration 37 : 0.3301884099946453
Loss in iteration 38 : 0.33010230295439974
Loss in iteration 39 : 0.33011426386737064
Loss in iteration 40 : 0.33108290796400713
Loss in iteration 41 : 0.3426504074865319
Loss in iteration 42 : 0.4088682444489616
Loss in iteration 43 : 0.410418504497837
Loss in iteration 44 : 0.33352890613680397
Loss in iteration 45 : 0.33120835326355597
Loss in iteration 46 : 0.3302259067049758
Loss in iteration 47 : 0.32994541689030954
Loss in iteration 48 : 0.32978764803286925
Loss in iteration 49 : 0.3297135513602193
Loss in iteration 50 : 0.3296691286512734
Loss in iteration 51 : 0.32968058067203615
Loss in iteration 52 : 0.32976301662172824
Loss in iteration 53 : 0.3300707964216325
Loss in iteration 54 : 0.3308662313173457
Loss in iteration 55 : 0.3334598934579441
Loss in iteration 56 : 0.3397532661218344
Loss in iteration 57 : 0.3570385305455611
Loss in iteration 58 : 0.360291994282676
Loss in iteration 59 : 0.36759601308524004
Loss in iteration 60 : 0.34073764784842236
Loss in iteration 61 : 0.33825427848914863
Loss in iteration 62 : 0.3335514791211325
Loss in iteration 63 : 0.3330043758507425
Loss in iteration 64 : 0.331978317762209
Loss in iteration 65 : 0.33240807707069614
Loss in iteration 66 : 0.33253750763278994
Loss in iteration 67 : 0.3343419959774252
Loss in iteration 68 : 0.33578342880011247
Loss in iteration 69 : 0.34085151313783596
Loss in iteration 70 : 0.34249175130579584
Loss in iteration 71 : 0.34974883269748575
Loss in iteration 72 : 0.34373954864506157
Loss in iteration 73 : 0.34631037907924495
Loss in iteration 74 : 0.3387123699400429
Loss in iteration 75 : 0.33912983410604686
Loss in iteration 76 : 0.33547407514485644
Loss in iteration 77 : 0.3362176462426491
Loss in iteration 78 : 0.3346914356702163
Loss in iteration 79 : 0.3363199959349771
Loss in iteration 80 : 0.33576337401026773
Loss in iteration 81 : 0.3386997090192949
Loss in iteration 82 : 0.3381363022513688
Loss in iteration 83 : 0.34212542876875107
Loss in iteration 84 : 0.33977202603713585
Loss in iteration 85 : 0.3431255778173987
Loss in iteration 86 : 0.3388944141901583
Loss in iteration 87 : 0.3409007197496888
Loss in iteration 88 : 0.33708910252083396
Loss in iteration 89 : 0.3386217981615895
Loss in iteration 90 : 0.3360390735789335
Loss in iteration 91 : 0.33782139834238883
Loss in iteration 92 : 0.3360687365380059
Loss in iteration 93 : 0.33842565611857284
Loss in iteration 94 : 0.33687670979452405
Loss in iteration 95 : 0.33976568484291464
Loss in iteration 96 : 0.33771735479046144
Loss in iteration 97 : 0.3406489440229477
Loss in iteration 98 : 0.33779767357675894
Loss in iteration 99 : 0.340293722459048
Loss in iteration 100 : 0.3371653000914617
Loss in iteration 101 : 0.3392749250160021
Loss in iteration 102 : 0.33649128395895245
Loss in iteration 103 : 0.33854485712172344
Loss in iteration 104 : 0.33622625442404297
Loss in iteration 105 : 0.3384768416517657
Loss in iteration 106 : 0.3363968355118102
Loss in iteration 107 : 0.3389049078541926
Loss in iteration 108 : 0.336735308904298
Loss in iteration 109 : 0.3393606439786955
Loss in iteration 110 : 0.3368888759930045
Loss in iteration 111 : 0.3394181195827977
Loss in iteration 112 : 0.3367185891028452
Loss in iteration 113 : 0.3390681709533916
Loss in iteration 114 : 0.3363908509538604
Loss in iteration 115 : 0.33864486133330257
Loss in iteration 116 : 0.3361486033732577
Loss in iteration 117 : 0.33843754096190654
Loss in iteration 118 : 0.33609927650270555
Loss in iteration 119 : 0.3384937652598589
Loss in iteration 120 : 0.3361835241143911
Loss in iteration 121 : 0.33866058105534436
Loss in iteration 122 : 0.33625656000078397
Loss in iteration 123 : 0.3387345324605173
Loss in iteration 124 : 0.3362131937144082
Loss in iteration 125 : 0.3386286757304409
Loss in iteration 126 : 0.3360657172930782
Loss in iteration 127 : 0.3384212020087868
Loss in iteration 128 : 0.33590583394816476
Loss in iteration 129 : 0.3382497192698703
Loss in iteration 130 : 0.3358104600567495
Loss in iteration 131 : 0.33818757594470017
Loss in iteration 132 : 0.33578839498550445
Loss in iteration 133 : 0.33820802127520017
Loss in iteration 134 : 0.3357916656801605
Loss in iteration 135 : 0.33822894390575564
Loss in iteration 136 : 0.3357645834377834
Loss in iteration 137 : 0.3381872818636402
Loss in iteration 138 : 0.3356886370600282
Loss in iteration 139 : 0.33808423530727527
Loss in iteration 140 : 0.3355884077481533
Loss in iteration 141 : 0.3379689585736629
Loss in iteration 142 : 0.3355014162201585
Loss in iteration 143 : 0.3378878401692794
Loss in iteration 144 : 0.33544595128096577
Loss in iteration 145 : 0.3378499134938793
Loss in iteration 146 : 0.3354122376711393
Loss in iteration 147 : 0.3378296241293281
Loss in iteration 148 : 0.33537674332997
Loss in iteration 149 : 0.3377945638211062
Loss in iteration 150 : 0.3353234656035823
Loss in iteration 151 : 0.33773200592773905
Loss in iteration 152 : 0.3352545236487217
Loss in iteration 153 : 0.33765406840794704
Loss in iteration 154 : 0.335184054348447
Loss in iteration 155 : 0.3375821445834285
Loss in iteration 156 : 0.33512428429335644
Loss in iteration 157 : 0.3375281053004721
Loss in iteration 158 : 0.33507666912178974
Loss in iteration 159 : 0.33748765728733215
Loss in iteration 160 : 0.3350334728548173
Loss in iteration 161 : 0.33744762251556465
Loss in iteration 162 : 0.33498593407433364
Loss in iteration 163 : 0.33739819207082267
Loss in iteration 164 : 0.3349312493858398
Loss in iteration 165 : 0.3373396580867964
Loss in iteration 166 : 0.3348732710086951
Loss in iteration 167 : 0.3372797010939329
Loss in iteration 168 : 0.33481788581065436
Loss in iteration 169 : 0.3372256379284266
Loss in iteration 170 : 0.334767988301698
Loss in iteration 171 : 0.337178903524475
Loss in iteration 172 : 0.33472201009011854
Loss in iteration 173 : 0.33713541562598176
Loss in iteration 174 : 0.334676223820834
Loss in iteration 175 : 0.337090061081215
Loss in iteration 176 : 0.3346281325745463
Loss in iteration 177 : 0.33704087304743774
Loss in iteration 178 : 0.3345780843380485
Loss in iteration 179 : 0.3369897668667054
Loss in iteration 180 : 0.3345282991428942
Loss in iteration 181 : 0.33694010475693253
Loss in iteration 182 : 0.33448069773601824
Loss in iteration 183 : 0.33689375392485527
Loss in iteration 184 : 0.3344355014873997
Loss in iteration 185 : 0.33685002510690465
Loss in iteration 186 : 0.3343914826472254
Loss in iteration 187 : 0.3368068325487877
Loss in iteration 188 : 0.334347262749816
Loss in iteration 189 : 0.3367626374141318
Loss in iteration 190 : 0.3343024063205337
Loss in iteration 191 : 0.3367174926719913
Loss in iteration 192 : 0.3342575203458643
Loss in iteration 193 : 0.33667260699557033
Loss in iteration 194 : 0.3342135281478427
Loss in iteration 195 : 0.3366291287140339
Loss in iteration 196 : 0.33417088063755057
Loss in iteration 197 : 0.33658727804761557
Loss in iteration 198 : 0.3341293258266342
Loss in iteration 199 : 0.3365464064796655
Loss in iteration 200 : 0.334088274424314
Testing accuracy  of updater 5 on alg 0 with rate 0.0049 = 0.76675, training accuracy 0.8319844609906119, time elapsed: 2853 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5371717367481265
Loss in iteration 3 : 0.4740187623116656
Loss in iteration 4 : 0.4532923619804825
Loss in iteration 5 : 0.43927908039489183
Loss in iteration 6 : 0.4271869496444536
Loss in iteration 7 : 0.4166416151491782
Loss in iteration 8 : 0.4073968296423508
Loss in iteration 9 : 0.39925279410958153
Loss in iteration 10 : 0.39204664726179483
Loss in iteration 11 : 0.3856450357539111
Loss in iteration 12 : 0.37993815684825233
Loss in iteration 13 : 0.3748350334471499
Loss in iteration 14 : 0.3702597871298418
Loss in iteration 15 : 0.366148702886637
Loss in iteration 16 : 0.3624479147626361
Loss in iteration 17 : 0.3591115757941105
Loss in iteration 18 : 0.35610040506714163
Loss in iteration 19 : 0.3533805287252221
Loss in iteration 20 : 0.35092255071255707
Loss in iteration 21 : 0.34870080375917134
Loss in iteration 22 : 0.34669274243179155
Loss in iteration 23 : 0.3448784487365284
Loss in iteration 24 : 0.3432402273794835
Loss in iteration 25 : 0.3417622728547884
Loss in iteration 26 : 0.3404303944106653
Loss in iteration 27 : 0.33923178792830383
Loss in iteration 28 : 0.33815484605143237
Loss in iteration 29 : 0.3371889996889704
Loss in iteration 30 : 0.3363245854014667
Loss in iteration 31 : 0.3355527342663969
Loss in iteration 32 : 0.3348652786671502
Loss in iteration 33 : 0.3342546741185168
Loss in iteration 34 : 0.3337139337674055
Loss in iteration 35 : 0.3332365736219237
Loss in iteration 36 : 0.3328165668880967
Loss in iteration 37 : 0.3324483060493709
Loss in iteration 38 : 0.3321265715237864
Loss in iteration 39 : 0.3318465058882527
Loss in iteration 40 : 0.33160359277793106
Loss in iteration 41 : 0.33139363965872665
Loss in iteration 42 : 0.3312127637388757
Loss in iteration 43 : 0.33105738033750276
Loss in iteration 44 : 0.3309241930692484
Loss in iteration 45 : 0.3308101852400127
Loss in iteration 46 : 0.3307126118845482
Loss in iteration 47 : 0.33062899191695105
Loss in iteration 48 : 0.3305570999144938
Loss in iteration 49 : 0.33049495711729276
Loss in iteration 50 : 0.3304408213036801
Loss in iteration 51 : 0.3303931752946227
Loss in iteration 52 : 0.3303507139491445
Loss in iteration 53 : 0.33031232963283486
Loss in iteration 54 : 0.3302770962676528
Loss in iteration 55 : 0.3302442521934891
Loss in iteration 56 : 0.3302131821886131
Loss in iteration 57 : 0.33018339909352623
Loss in iteration 58 : 0.33015452667692
Loss in iteration 59 : 0.3301263240438401
Loss in iteration 60 : 0.3301004656353521
Loss in iteration 61 : 0.33016541496848856
Loss in iteration 62 : 0.3347619169167433
Loss in iteration 63 : 0.37916224994096903
Loss in iteration 64 : 0.3426403428633806
Loss in iteration 65 : 0.3365957304617323
Loss in iteration 66 : 0.33159004626719685
Loss in iteration 67 : 0.33070485895500473
Loss in iteration 68 : 0.3302612132666858
Loss in iteration 69 : 0.3301172277165232
Loss in iteration 70 : 0.33003256693773353
Loss in iteration 71 : 0.3300124167291436
Loss in iteration 72 : 0.33002242496024303
Loss in iteration 73 : 0.33011732886439743
Loss in iteration 74 : 0.33033009035173055
Loss in iteration 75 : 0.33093218997851614
Loss in iteration 76 : 0.3321767388535759
Loss in iteration 77 : 0.33527558920690803
Loss in iteration 78 : 0.3380319149360933
Loss in iteration 79 : 0.34096265339415904
Loss in iteration 80 : 0.33598111579359285
Loss in iteration 81 : 0.334702547328934
Loss in iteration 82 : 0.33215780621274854
Loss in iteration 83 : 0.3315781185338322
Loss in iteration 84 : 0.3309024517794587
Loss in iteration 85 : 0.330839045623405
Loss in iteration 86 : 0.33072281562590117
Loss in iteration 87 : 0.33101713426450213
Loss in iteration 88 : 0.33128351291911673
Loss in iteration 89 : 0.3321974249046138
Loss in iteration 90 : 0.3329030864735363
Loss in iteration 91 : 0.3346053300390612
Loss in iteration 92 : 0.3345954612837555
Loss in iteration 93 : 0.33556706721784174
Loss in iteration 94 : 0.3337958856825159
Loss in iteration 95 : 0.3336615506454031
Loss in iteration 96 : 0.3322225755236513
Loss in iteration 97 : 0.3320900643346634
Loss in iteration 98 : 0.33142179966274377
Loss in iteration 99 : 0.33156623839481336
Loss in iteration 100 : 0.3313473983690075
Loss in iteration 101 : 0.331818766393811
Loss in iteration 102 : 0.33185807689495267
Loss in iteration 103 : 0.33269187337828193
Loss in iteration 104 : 0.3326928471563344
Loss in iteration 105 : 0.33361728990812445
Loss in iteration 106 : 0.33306099113388027
Loss in iteration 107 : 0.3335848518453879
Loss in iteration 108 : 0.3325787387268414
Loss in iteration 109 : 0.3327661616530406
Loss in iteration 110 : 0.3319034957273861
Loss in iteration 111 : 0.33208316296443946
Loss in iteration 112 : 0.33154301813562687
Loss in iteration 113 : 0.33187501137505165
Loss in iteration 114 : 0.33157165044149917
Loss in iteration 115 : 0.33209863694822644
Loss in iteration 116 : 0.3318787715549987
Loss in iteration 117 : 0.3325395825677658
Loss in iteration 118 : 0.33219198044243436
Loss in iteration 119 : 0.33280461110647425
Loss in iteration 120 : 0.3322040328432425
Loss in iteration 121 : 0.3326397654214064
Loss in iteration 122 : 0.33192185903727733
Loss in iteration 123 : 0.3322447601336408
Loss in iteration 124 : 0.33161105583375705
Loss in iteration 125 : 0.3319470486505846
Loss in iteration 126 : 0.3314617219312099
Loss in iteration 127 : 0.3318840754191259
Loss in iteration 128 : 0.33150042583884265
Loss in iteration 129 : 0.33201491300872193
Loss in iteration 130 : 0.33163568943096444
Loss in iteration 131 : 0.33218470326707145
Loss in iteration 132 : 0.3317170998133604
Loss in iteration 133 : 0.3322199328892571
Loss in iteration 134 : 0.3316517319232394
Loss in iteration 135 : 0.3320796222077665
Loss in iteration 136 : 0.3314858828283748
Loss in iteration 137 : 0.33187721470431697
Loss in iteration 138 : 0.3313331522506177
Loss in iteration 139 : 0.3317416568402186
Loss in iteration 140 : 0.33126528975224057
Loss in iteration 141 : 0.3317190884706091
Loss in iteration 142 : 0.33127892015164445
Loss in iteration 143 : 0.3317699681262037
Loss in iteration 144 : 0.33131574625996507
Loss in iteration 145 : 0.33181022512177877
Loss in iteration 146 : 0.33130954372172094
Loss in iteration 147 : 0.33177649680675
Loss in iteration 148 : 0.33123862369668206
Loss in iteration 149 : 0.3316749425190045
Loss in iteration 150 : 0.3311362916068687
Loss in iteration 151 : 0.3315628492030659
Loss in iteration 152 : 0.3310515894700905
Loss in iteration 153 : 0.3314912538894374
Loss in iteration 154 : 0.3310090859186133
Loss in iteration 155 : 0.33147042980734864
Loss in iteration 156 : 0.3309982717678332
Loss in iteration 157 : 0.3314728004690113
Loss in iteration 158 : 0.3309880880483956
Loss in iteration 159 : 0.3314592401610661
Loss in iteration 160 : 0.3309528972274671
Loss in iteration 161 : 0.3314099661173427
Loss in iteration 162 : 0.33089112807030396
Loss in iteration 163 : 0.3313364208156137
Loss in iteration 164 : 0.33082170816894946
Loss in iteration 165 : 0.33126584530973524
Loss in iteration 166 : 0.3307650995137756
Loss in iteration 167 : 0.331217268817041
Loss in iteration 168 : 0.3307280630131721
Loss in iteration 169 : 0.33118981476542725
Loss in iteration 170 : 0.33070194284515064
Loss in iteration 171 : 0.3311676492179206
Loss in iteration 172 : 0.33067200565455945
Loss in iteration 173 : 0.3311343497964262
Loss in iteration 174 : 0.33062944307952
Loss in iteration 175 : 0.3310852322483565
Loss in iteration 176 : 0.33057683287359657
Loss in iteration 177 : 0.3310285456591987
Loss in iteration 178 : 0.33052381931073394
Loss in iteration 179 : 0.3309765135918496
Loss in iteration 180 : 0.3304784081989622
Loss in iteration 181 : 0.33093545448818457
Loss in iteration 182 : 0.3304414975567574
Loss in iteration 183 : 0.33090250230286006
Loss in iteration 184 : 0.33040774938272527
Loss in iteration 185 : 0.3308696130948508
Loss in iteration 186 : 0.3303707597456654
Loss in iteration 187 : 0.33083050389502555
Loss in iteration 188 : 0.330328001677689
Loss in iteration 189 : 0.33078495748476694
Loss in iteration 190 : 0.3302819101124697
Loss in iteration 191 : 0.3307377276208963
Loss in iteration 192 : 0.33023702557793966
Loss in iteration 193 : 0.3306939567082396
Loss in iteration 194 : 0.33019619663995264
Loss in iteration 195 : 0.3306553172226875
Loss in iteration 196 : 0.3301588594605919
Loss in iteration 197 : 0.3306195310061074
Loss in iteration 198 : 0.33012216834047997
Loss in iteration 199 : 0.33058286030978135
Loss in iteration 200 : 0.3300835565941137
Testing accuracy  of updater 5 on alg 0 with rate 0.0028000000000000004 = 0.77425, training accuracy 0.8345742958886371, time elapsed: 3185 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5588005867911673
Loss in iteration 3 : 0.526330366017406
Loss in iteration 4 : 0.5116045639623718
Loss in iteration 5 : 0.5023539361651732
Loss in iteration 6 : 0.49519251886560156
Loss in iteration 7 : 0.48893103583425424
Loss in iteration 8 : 0.48310842960717976
Loss in iteration 9 : 0.4775399463377142
Loss in iteration 10 : 0.4721518346487787
Loss in iteration 11 : 0.46691551286026073
Loss in iteration 12 : 0.46182045153721407
Loss in iteration 13 : 0.45686297795706665
Loss in iteration 14 : 0.4520417443043455
Loss in iteration 15 : 0.44735596767881397
Loss in iteration 16 : 0.44280479700492753
Loss in iteration 17 : 0.4383871207711721
Loss in iteration 18 : 0.43410153703215093
Testing accuracy  of updater 5 on alg 0 with rate 7.000000000000001E-4 = 0.5585, training accuracy 0.6846876011654257, time elapsed: 284 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 4.7570979993793845
Loss in iteration 3 : 5.104775211539485
Loss in iteration 4 : 3.6347286219652664
Loss in iteration 5 : 1.3452313304376515
Loss in iteration 6 : 0.812117598085153
Loss in iteration 7 : 2.384329286161013
Loss in iteration 8 : 2.401559281943636
Loss in iteration 9 : 1.3913622281442068
Loss in iteration 10 : 1.0020913629587636
Loss in iteration 11 : 1.2596835537515523
Loss in iteration 12 : 1.6412555940312263
Loss in iteration 13 : 1.8739891490010616
Loss in iteration 14 : 1.8668399244707412
Loss in iteration 15 : 1.6689935110109844
Loss in iteration 16 : 1.4062218536611921
Loss in iteration 17 : 1.2159118388735495
Loss in iteration 18 : 1.2155392023869112
Loss in iteration 19 : 1.398143812492763
Loss in iteration 20 : 1.5423359829794379
Loss in iteration 21 : 1.480347394270131
Loss in iteration 22 : 1.274675277582184
Loss in iteration 23 : 1.1220466720743612
Loss in iteration 24 : 1.1171693284781483
Loss in iteration 25 : 1.1867287325696523
Loss in iteration 26 : 1.2309066658872279
Loss in iteration 27 : 1.1918719210593358
Loss in iteration 28 : 1.072907771836553
Loss in iteration 29 : 0.934735267990362
Loss in iteration 30 : 0.8664637597126182
Loss in iteration 31 : 0.9019238924998979
Loss in iteration 32 : 0.9187936797849592
Loss in iteration 33 : 0.8051816159963904
Loss in iteration 34 : 0.6747982982664057
Loss in iteration 35 : 0.6554384913524144
Loss in iteration 36 : 0.6768667532191336
Loss in iteration 37 : 0.6234530570309461
Loss in iteration 38 : 0.4972345084178901
Loss in iteration 39 : 0.4573951144416863
Loss in iteration 40 : 0.5140284968483521
Loss in iteration 41 : 0.3689870993375253
Loss in iteration 42 : 0.41857629953018466
Loss in iteration 43 : 0.42261796275729463
Loss in iteration 44 : 0.386695237770276
Loss in iteration 45 : 0.3916936968079283
Loss in iteration 46 : 0.5019822842893799
Loss in iteration 47 : 0.3809637231604097
Loss in iteration 48 : 0.5904686384198105
Loss in iteration 49 : 0.5549306770568416
Loss in iteration 50 : 0.6226579664805731
Loss in iteration 51 : 0.3706749534238877
Loss in iteration 52 : 0.5560654235580146
Loss in iteration 53 : 0.4967250065153971
Loss in iteration 54 : 0.42624984801392984
Loss in iteration 55 : 0.536149447481269
Loss in iteration 56 : 0.5400783918740223
Loss in iteration 57 : 0.45680464688885913
Loss in iteration 58 : 0.4753205516388485
Loss in iteration 59 : 0.5292901467655136
Loss in iteration 60 : 0.4470156349947959
Loss in iteration 61 : 0.44323992845411997
Loss in iteration 62 : 0.47691563424462424
Loss in iteration 63 : 0.4278941388464369
Loss in iteration 64 : 0.3868475349186568
Loss in iteration 65 : 0.43081133384694964
Loss in iteration 66 : 0.3589472753639629
Loss in iteration 67 : 0.38086238309552123
Loss in iteration 68 : 0.373812013427929
Loss in iteration 69 : 0.3458819335080099
Loss in iteration 70 : 0.3749258033352114
Loss in iteration 71 : 0.38079664731504825
Loss in iteration 72 : 0.3669525331651653
Loss in iteration 73 : 0.39568959426536754
Loss in iteration 74 : 0.33717225842251497
Loss in iteration 75 : 0.3572443286839013
Loss in iteration 76 : 0.3341682771225676
Loss in iteration 77 : 0.35155018230350155
Loss in iteration 78 : 0.3388454483429141
Loss in iteration 79 : 0.3501327479864032
Loss in iteration 80 : 0.33372182245081033
Loss in iteration 81 : 0.34889391149900406
Loss in iteration 82 : 0.3315898709583255
Loss in iteration 83 : 0.3428501762700328
Loss in iteration 84 : 0.32935229398855426
Loss in iteration 85 : 0.3394296989498771
Loss in iteration 86 : 0.32714184125138573
Loss in iteration 87 : 0.33577166436661265
Loss in iteration 88 : 0.32664134966895475
Loss in iteration 89 : 0.33609767745426167
Loss in iteration 90 : 0.3269732068296869
Loss in iteration 91 : 0.3316111175171887
Loss in iteration 92 : 0.32746358749822635
Loss in iteration 93 : 0.32600926902203237
Loss in iteration 94 : 0.32826336052667165
Loss in iteration 95 : 0.3236483708796702
Loss in iteration 96 : 0.3286530015738948
Loss in iteration 97 : 0.3246707567798609
Loss in iteration 98 : 0.327062444982567
Loss in iteration 99 : 0.3258074590884276
Loss in iteration 100 : 0.32523244178702665
Loss in iteration 101 : 0.32592045459809404
Loss in iteration 102 : 0.3237255017836538
Loss in iteration 103 : 0.3255144660233578
Loss in iteration 104 : 0.32317368844088284
Loss in iteration 105 : 0.3247034135353797
Loss in iteration 106 : 0.32383299448179903
Loss in iteration 107 : 0.3240468782758282
Loss in iteration 108 : 0.32472688076454503
Loss in iteration 109 : 0.32354752684468224
Loss in iteration 110 : 0.3246049339016675
Loss in iteration 111 : 0.3235732165700658
Loss in iteration 112 : 0.3234805138382028
Loss in iteration 113 : 0.3238113351194047
Loss in iteration 114 : 0.32298754487749104
Loss in iteration 115 : 0.32367185458485415
Loss in iteration 116 : 0.3233716681754786
Testing accuracy  of updater 6 on alg 0 with rate 0.14 = 0.79, training accuracy 0.8433149886694723, time elapsed: 1625 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.5674749599525128
Loss in iteration 3 : 1.9009656176751761
Loss in iteration 4 : 1.2262450932967324
Loss in iteration 5 : 0.47669704548853786
Loss in iteration 6 : 0.9611321053041113
Loss in iteration 7 : 1.3044648365606926
Loss in iteration 8 : 0.8388557649094069
Loss in iteration 9 : 0.6980064103255034
Loss in iteration 10 : 0.9084333729945738
Loss in iteration 11 : 1.1058079500643578
Loss in iteration 12 : 1.135550220062241
Loss in iteration 13 : 1.0178446047225944
Loss in iteration 14 : 0.8731065276281508
Loss in iteration 15 : 0.8361754409489108
Loss in iteration 16 : 0.9449148958776464
Loss in iteration 17 : 1.0264515193017174
Loss in iteration 18 : 0.9474260126916164
Loss in iteration 19 : 0.817874876841884
Loss in iteration 20 : 0.787292641530558
Loss in iteration 21 : 0.8305116189200586
Loss in iteration 22 : 0.8519604847695083
Loss in iteration 23 : 0.8002809342304203
Loss in iteration 24 : 0.7000054815456324
Loss in iteration 25 : 0.6326615777300159
Loss in iteration 26 : 0.6499653098318482
Loss in iteration 27 : 0.6531613996259377
Loss in iteration 28 : 0.5565053377870736
Loss in iteration 29 : 0.489889428393969
Loss in iteration 30 : 0.5047495129179957
Loss in iteration 31 : 0.4899381535674905
Loss in iteration 32 : 0.40384892939953404
Loss in iteration 33 : 0.3838753829654173
Loss in iteration 34 : 0.41894841000711036
Loss in iteration 35 : 0.3346947742079905
Loss in iteration 36 : 0.40852175531835483
Loss in iteration 37 : 0.3533576011351951
Loss in iteration 38 : 0.48921465783313084
Loss in iteration 39 : 0.479826723850831
Loss in iteration 40 : 0.4649052023246356
Loss in iteration 41 : 0.3618127250314049
Loss in iteration 42 : 0.42981128400778884
Loss in iteration 43 : 0.35716689498133525
Loss in iteration 44 : 0.43410763690368015
Loss in iteration 45 : 0.36820124258843884
Loss in iteration 46 : 0.3773897339766329
Loss in iteration 47 : 0.41614622907733895
Loss in iteration 48 : 0.3600595809614853
Loss in iteration 49 : 0.3906948884525007
Loss in iteration 50 : 0.39748438162210736
Loss in iteration 51 : 0.3582797407287959
Loss in iteration 52 : 0.3756322579526782
Loss in iteration 53 : 0.3733107409889122
Loss in iteration 54 : 0.34407591090560197
Loss in iteration 55 : 0.3651278739525661
Loss in iteration 56 : 0.3467838858184497
Loss in iteration 57 : 0.3378051376110804
Loss in iteration 58 : 0.35244734732149335
Loss in iteration 59 : 0.3302285677451768
Loss in iteration 60 : 0.35074362895408195
Loss in iteration 61 : 0.3287912636182216
Loss in iteration 62 : 0.3512831063205748
Loss in iteration 63 : 0.32777179665339273
Loss in iteration 64 : 0.3425759544074976
Loss in iteration 65 : 0.3259984887305784
Loss in iteration 66 : 0.3394192692718536
Loss in iteration 67 : 0.3250802264998849
Loss in iteration 68 : 0.3346969388291053
Loss in iteration 69 : 0.3268927095665135
Loss in iteration 70 : 0.3320377370827474
Loss in iteration 71 : 0.32796029277756283
Loss in iteration 72 : 0.32879470030763347
Loss in iteration 73 : 0.32914465973492396
Loss in iteration 74 : 0.3255462280638957
Loss in iteration 75 : 0.3288720904684929
Loss in iteration 76 : 0.32424628142791057
Loss in iteration 77 : 0.32759993761673123
Loss in iteration 78 : 0.3241154610121807
Loss in iteration 79 : 0.3266340174382073
Loss in iteration 80 : 0.32437119833940575
Loss in iteration 81 : 0.32582749231063823
Loss in iteration 82 : 0.3247596275753117
Loss in iteration 83 : 0.3251032149617659
Loss in iteration 84 : 0.32446603934222307
Loss in iteration 85 : 0.32441337513654867
Loss in iteration 86 : 0.32408880549620894
Loss in iteration 87 : 0.32376316875640304
Loss in iteration 88 : 0.3237894995991162
Loss in iteration 89 : 0.3235532734119921
Loss in iteration 90 : 0.32383004361259105
Loss in iteration 91 : 0.32350841653830303
Loss in iteration 92 : 0.3238997250298207
Loss in iteration 93 : 0.3234761870589432
Loss in iteration 94 : 0.32380703151209284
Loss in iteration 95 : 0.32326334300287923
Loss in iteration 96 : 0.3235907886885955
Loss in iteration 97 : 0.3230739814910175
Loss in iteration 98 : 0.3234008169217967
Loss in iteration 99 : 0.32298912106362826
Loss in iteration 100 : 0.32334846058745487
Loss in iteration 101 : 0.323029795877338
Loss in iteration 102 : 0.3233441394739375
Loss in iteration 103 : 0.32307373491714225
Loss in iteration 104 : 0.32331791783586944
Loss in iteration 105 : 0.3230609462455844
Loss in iteration 106 : 0.3232272511713926
Loss in iteration 107 : 0.32300212354754115
Testing accuracy  of updater 6 on alg 0 with rate 0.098 = 0.79175, training accuracy 0.8449336354807381, time elapsed: 1237 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.0030817488738881
Loss in iteration 3 : 1.235299390408287
Loss in iteration 4 : 0.8710020565457106
Loss in iteration 5 : 0.3956477042231907
Loss in iteration 6 : 0.5855646363532727
Loss in iteration 7 : 0.8683079628232709
Loss in iteration 8 : 0.63611802577422
Loss in iteration 9 : 0.48989018761345665
Loss in iteration 10 : 0.5851839997439983
Loss in iteration 11 : 0.7153810239034861
Loss in iteration 12 : 0.7579166181298012
Loss in iteration 13 : 0.7036563315185722
Loss in iteration 14 : 0.6159464539599611
Loss in iteration 15 : 0.5766227257691434
Loss in iteration 16 : 0.6233032920199785
Loss in iteration 17 : 0.6852949535070902
Loss in iteration 18 : 0.6675164877954513
Loss in iteration 19 : 0.593407451037131
Loss in iteration 20 : 0.5529589000388652
Loss in iteration 21 : 0.5668425242364442
Loss in iteration 22 : 0.5894302293569473
Loss in iteration 23 : 0.5788249126570666
Loss in iteration 24 : 0.5311578880746112
Loss in iteration 25 : 0.48087213612193347
Loss in iteration 26 : 0.46907922755777537
Loss in iteration 27 : 0.4845660075547174
Loss in iteration 28 : 0.4642646470195545
Loss in iteration 29 : 0.4115898326429991
Loss in iteration 30 : 0.39233407367103723
Loss in iteration 31 : 0.4027260740570825
Loss in iteration 32 : 0.39049649733620967
Loss in iteration 33 : 0.3508310000064983
Loss in iteration 34 : 0.344072297642598
Loss in iteration 35 : 0.3649378762427934
Loss in iteration 36 : 0.3320087480453611
Loss in iteration 37 : 0.341171470550533
Loss in iteration 38 : 0.3590563215039649
Loss in iteration 39 : 0.33651420462100573
Loss in iteration 40 : 0.3633252483722054
Loss in iteration 41 : 0.3442747376313702
Loss in iteration 42 : 0.34876020916577316
Loss in iteration 43 : 0.34943334798343745
Loss in iteration 44 : 0.3314266260414037
Loss in iteration 45 : 0.3429028314335637
Loss in iteration 46 : 0.32976740431507245
Loss in iteration 47 : 0.3296403942444975
Loss in iteration 48 : 0.3345389368452626
Loss in iteration 49 : 0.3271555658762931
Loss in iteration 50 : 0.3291225182039884
Loss in iteration 51 : 0.3329454516015914
Loss in iteration 52 : 0.32786433141541466
Loss in iteration 53 : 0.3294520421408301
Loss in iteration 54 : 0.3318330454114205
Loss in iteration 55 : 0.328229403712753
Loss in iteration 56 : 0.32792470076935176
Loss in iteration 57 : 0.32982665272393574
Loss in iteration 58 : 0.326720851774653
Loss in iteration 59 : 0.3261203270231215
Loss in iteration 60 : 0.32736305184900083
Loss in iteration 61 : 0.32511240405545455
Loss in iteration 62 : 0.32471946078681424
Loss in iteration 63 : 0.3258712356461987
Loss in iteration 64 : 0.324081324327855
Loss in iteration 65 : 0.32457236053959604
Loss in iteration 66 : 0.3251142700273971
Loss in iteration 67 : 0.3238264540340775
Loss in iteration 68 : 0.3246479561611054
Loss in iteration 69 : 0.32437948176660947
Loss in iteration 70 : 0.3236574791863483
Loss in iteration 71 : 0.3243288097861043
Loss in iteration 72 : 0.323626139681897
Loss in iteration 73 : 0.323527474603965
Testing accuracy  of updater 6 on alg 0 with rate 0.056 = 0.793, training accuracy 0.8449336354807381, time elapsed: 978 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4780691935314909
Loss in iteration 3 : 0.5460326923535352
Loss in iteration 4 : 0.5702652429909472
Loss in iteration 5 : 0.5109434972260618
Loss in iteration 6 : 0.40841909612717037
Loss in iteration 7 : 0.34201942656927864
Loss in iteration 8 : 0.3616808904298087
Loss in iteration 9 : 0.41223690176817296
Loss in iteration 10 : 0.4117452207070634
Loss in iteration 11 : 0.36957161894160445
Loss in iteration 12 : 0.33882348229574444
Loss in iteration 13 : 0.3394362559256777
Loss in iteration 14 : 0.35818304549549224
Loss in iteration 15 : 0.3758531285306502
Loss in iteration 16 : 0.3815642491443656
Loss in iteration 17 : 0.37479307645954646
Loss in iteration 18 : 0.36216480744054624
Loss in iteration 19 : 0.3524267527624712
Loss in iteration 20 : 0.3513211331160844
Loss in iteration 21 : 0.35816492497689745
Loss in iteration 22 : 0.3665252260601394
Loss in iteration 23 : 0.3694655619477797
Loss in iteration 24 : 0.3651400922212878
Loss in iteration 25 : 0.3574510246634332
Loss in iteration 26 : 0.3517978863815237
Loss in iteration 27 : 0.3507158759685422
Loss in iteration 28 : 0.3529261682074166
Loss in iteration 29 : 0.3552537995537158
Loss in iteration 30 : 0.3550943383477568
Loss in iteration 31 : 0.35186405447758745
Loss in iteration 32 : 0.34701947567423663
Loss in iteration 33 : 0.3429135754390194
Loss in iteration 34 : 0.34116058146708056
Loss in iteration 35 : 0.3414939212326382
Loss in iteration 36 : 0.3420395780123535
Loss in iteration 37 : 0.3409729567570834
Loss in iteration 38 : 0.338122750684297
Loss in iteration 39 : 0.3349477587099708
Loss in iteration 40 : 0.3330197381878927
Loss in iteration 41 : 0.332659471915401
Loss in iteration 42 : 0.3329151973394167
Loss in iteration 43 : 0.3325886285597691
Loss in iteration 44 : 0.3312555687724738
Loss in iteration 45 : 0.32950742469336775
Loss in iteration 46 : 0.32833292830362415
Loss in iteration 47 : 0.32818883620412137
Loss in iteration 48 : 0.3285889285775233
Loss in iteration 49 : 0.32864529958705346
Loss in iteration 50 : 0.32801507132115765
Loss in iteration 51 : 0.3271865019076719
Loss in iteration 52 : 0.3268295330371262
Loss in iteration 53 : 0.32705880716793634
Loss in iteration 54 : 0.3274129536388236
Loss in iteration 55 : 0.3274177655448699
Loss in iteration 56 : 0.3270530168868494
Loss in iteration 57 : 0.32669137749739274
Loss in iteration 58 : 0.3266507439408214
Loss in iteration 59 : 0.3268583962982864
Loss in iteration 60 : 0.32697786246447824
Loss in iteration 61 : 0.3268179811679988
Loss in iteration 62 : 0.32652155231132
Loss in iteration 63 : 0.32634528299132337
Loss in iteration 64 : 0.3263569325858917
Loss in iteration 65 : 0.3264039202271365
Loss in iteration 66 : 0.3263232673808462
Loss in iteration 67 : 0.3261165302282848
Loss in iteration 68 : 0.3259179587281545
Loss in iteration 69 : 0.3258298436677508
Loss in iteration 70 : 0.32582014330354814
Loss in iteration 71 : 0.32578181167977627
Loss in iteration 72 : 0.3256635955680893
Loss in iteration 73 : 0.3255157019550942
Loss in iteration 74 : 0.3254156353052714
Loss in iteration 75 : 0.32537988608240603
Loss in iteration 76 : 0.32535934060835014
Loss in iteration 77 : 0.32530408563957397
Loss in iteration 78 : 0.3252149990760492
Loss in iteration 79 : 0.3251323376565305
Loss in iteration 80 : 0.32508580222581007
Loss in iteration 81 : 0.3250648331674314
Loss in iteration 82 : 0.32503661248473065
Loss in iteration 83 : 0.32498445138023907
Loss in iteration 84 : 0.32492201065772786
Loss in iteration 85 : 0.3248726504492201
Loss in iteration 86 : 0.32484267208779716
Loss in iteration 87 : 0.3248178851386047
Loss in iteration 88 : 0.3247822579860492
Loss in iteration 89 : 0.324734825443591
Loss in iteration 90 : 0.3246877675093681
Loss in iteration 91 : 0.3246512129054702
Loss in iteration 92 : 0.3246229585424171
Loss in iteration 93 : 0.32459302427370346
Loss in iteration 94 : 0.3245556622920446
Loss in iteration 95 : 0.32451472200371784
Loss in iteration 96 : 0.3244778129819508
Loss in iteration 97 : 0.3244475533763982
Loss in iteration 98 : 0.3244197673323498
Loss in iteration 99 : 0.3243892147839475
Loss in iteration 100 : 0.3243553620375558
Loss in iteration 101 : 0.324322088554452
Loss in iteration 102 : 0.3242927464414918
Loss in iteration 103 : 0.32426665702058177
Loss in iteration 104 : 0.3242405479607341
Loss in iteration 105 : 0.32421252787753335
Loss in iteration 106 : 0.3241838524765031
Loss in iteration 107 : 0.3241569575826718
Loss in iteration 108 : 0.32413257293344266
Loss in iteration 109 : 0.32410921883495664
Loss in iteration 110 : 0.32408518884852
Loss in iteration 111 : 0.32406041173421984
Loss in iteration 112 : 0.3240362003381003
Loss in iteration 113 : 0.3240135449075299
Loss in iteration 114 : 0.3239920697289073
Loss in iteration 115 : 0.3239706748815515
Loss in iteration 116 : 0.32394885933170464
Loss in iteration 117 : 0.3239271478397149
Loss in iteration 118 : 0.3239063132468909
Loss in iteration 119 : 0.3238864715650277
Loss in iteration 120 : 0.3238670696773885
Loss in iteration 121 : 0.32384761260829575
Loss in iteration 122 : 0.32382819213673886
Loss in iteration 123 : 0.3238092712544424
Loss in iteration 124 : 0.3237910986212641
Loss in iteration 125 : 0.3237734673966907
Loss in iteration 126 : 0.32375602288443117
Loss in iteration 127 : 0.3237386775020352
Loss in iteration 128 : 0.3237216518260971
Loss in iteration 129 : 0.32370516470236443
Loss in iteration 130 : 0.3236891825460315
Loss in iteration 131 : 0.3236734961351204
Loss in iteration 132 : 0.3236579784656413
Loss in iteration 133 : 0.3236427028162451
Loss in iteration 134 : 0.3236278153768563
Loss in iteration 135 : 0.32361335001351854
Loss in iteration 136 : 0.3235992030775636
Loss in iteration 137 : 0.3235852672381051
Loss in iteration 138 : 0.32357154501394736
Loss in iteration 139 : 0.3235581181566179
Loss in iteration 140 : 0.32354503392579514
Loss in iteration 141 : 0.32353225165528615
Loss in iteration 142 : 0.32351969860328056
Loss in iteration 143 : 0.32350735187653756
Loss in iteration 144 : 0.32349524919382017
Loss in iteration 145 : 0.3234834294412197
Loss in iteration 146 : 0.32347188310690206
Loss in iteration 147 : 0.32346056686793373
Loss in iteration 148 : 0.3234494539867298
Loss in iteration 149 : 0.3234385566821808
Loss in iteration 150 : 0.3234279001652216
Loss in iteration 151 : 0.32341748677045506
Loss in iteration 152 : 0.32340729284774056
Loss in iteration 153 : 0.32339729589350985
Loss in iteration 154 : 0.3233874953769106
Loss in iteration 155 : 0.3233779046352621
Loss in iteration 156 : 0.3233685284327169
Loss in iteration 157 : 0.3233593545175181
Loss in iteration 158 : 0.32335036629530417
Loss in iteration 159 : 0.32334155819812643
Loss in iteration 160 : 0.32333293559313625
Loss in iteration 161 : 0.32332450218769865
Loss in iteration 162 : 0.3233162517506949
Loss in iteration 163 : 0.3233081729062406
Loss in iteration 164 : 0.3233002590607008
Loss in iteration 165 : 0.32329251105074003
Loss in iteration 166 : 0.3232849307926906
Loss in iteration 167 : 0.3232775149451281
Loss in iteration 168 : 0.323270255930931
Loss in iteration 169 : 0.32326314777818355
Loss in iteration 170 : 0.32325618910209797
Loss in iteration 171 : 0.3232493802809282
Loss in iteration 172 : 0.3232427191813502
Loss in iteration 173 : 0.32323620068098174
Loss in iteration 174 : 0.323229819840805
Loss in iteration 175 : 0.32322357434358356
Loss in iteration 176 : 0.323217463477762
Loss in iteration 177 : 0.323211485489937
Loss in iteration 178 : 0.3232056367073868
Loss in iteration 179 : 0.32319991313241037
Loss in iteration 180 : 0.32319431217805716
Loss in iteration 181 : 0.3231888324758481
Loss in iteration 182 : 0.3231834723359118
Loss in iteration 183 : 0.3231782289221454
Loss in iteration 184 : 0.32317309898298113
Loss in iteration 185 : 0.3231680799870814
Loss in iteration 186 : 0.3231631702460493
Loss in iteration 187 : 0.32315836806551296
Loss in iteration 188 : 0.32315367110959126
Loss in iteration 189 : 0.32314907669331766
Loss in iteration 190 : 0.323144582484636
Loss in iteration 191 : 0.3231401867056025
Loss in iteration 192 : 0.3231358876852557
Loss in iteration 193 : 0.3231316834158867
Loss in iteration 194 : 0.32312757164061684
Loss in iteration 195 : 0.3231235502689282
Loss in iteration 196 : 0.3231196175621956
Loss in iteration 197 : 0.323115771908817
Loss in iteration 198 : 0.32311201153306884
Loss in iteration 199 : 0.32310833449812876
Loss in iteration 200 : 0.3231047389445054
Testing accuracy  of updater 6 on alg 0 with rate 0.014 = 0.789, training accuracy 0.8429912593072192, time elapsed: 2501 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.47811885089470807
Loss in iteration 3 : 0.5280923188068137
Loss in iteration 4 : 0.5607806348622463
Loss in iteration 5 : 0.5228549622850955
Loss in iteration 6 : 0.4361702758250345
Loss in iteration 7 : 0.35860728384212315
Loss in iteration 8 : 0.3463552546469011
Loss in iteration 9 : 0.3875660623507778
Loss in iteration 10 : 0.41070025508791125
Loss in iteration 11 : 0.3865708906424597
Loss in iteration 12 : 0.34954403666635725
Loss in iteration 13 : 0.33324777818373313
Loss in iteration 14 : 0.34048259776847656
Loss in iteration 15 : 0.3570974631990524
Loss in iteration 16 : 0.36944537406939904
Loss in iteration 17 : 0.37133524259115863
Loss in iteration 18 : 0.36404381814609976
Loss in iteration 19 : 0.3534254451861256
Loss in iteration 20 : 0.3460841571018134
Loss in iteration 21 : 0.34579917999697674
Loss in iteration 22 : 0.3515209782897194
Loss in iteration 23 : 0.35831733154156864
Loss in iteration 24 : 0.361101244894013
Loss in iteration 25 : 0.35831586457517184
Loss in iteration 26 : 0.3524692051972399
Loss in iteration 27 : 0.347505478391065
Loss in iteration 28 : 0.345785385134065
Loss in iteration 29 : 0.34700649130248884
Loss in iteration 30 : 0.34912712047364214
Loss in iteration 31 : 0.3500046862867646
Loss in iteration 32 : 0.3486178242468884
Loss in iteration 33 : 0.34541739187677023
Loss in iteration 34 : 0.3418557150310979
Loss in iteration 35 : 0.3394284745299539
Loss in iteration 36 : 0.3387421827754085
Loss in iteration 37 : 0.33916805389204974
Loss in iteration 38 : 0.3393713139036646
Loss in iteration 39 : 0.33836899961201455
Loss in iteration 40 : 0.33625524754292285
Loss in iteration 41 : 0.33399086428133645
Loss in iteration 42 : 0.3325299290980451
Loss in iteration 43 : 0.33210497090313723
Loss in iteration 44 : 0.3321979690484703
Loss in iteration 45 : 0.33206208749843913
Loss in iteration 46 : 0.3313005818241499
Loss in iteration 47 : 0.3300983700654863
Loss in iteration 48 : 0.3290063421882816
Loss in iteration 49 : 0.3284757780889342
Loss in iteration 50 : 0.3285026192048286
Loss in iteration 51 : 0.32866793878325135
Loss in iteration 52 : 0.32853718670058524
Loss in iteration 53 : 0.3280420884735868
Loss in iteration 54 : 0.3274852982003375
Loss in iteration 55 : 0.3272049379242748
Loss in iteration 56 : 0.32726457910748935
Loss in iteration 57 : 0.32744494038853245
Loss in iteration 58 : 0.32748186261214923
Loss in iteration 59 : 0.3272966986974185
Loss in iteration 60 : 0.32702839176559056
Loss in iteration 61 : 0.3268741068435187
Loss in iteration 62 : 0.32690165981075514
Loss in iteration 63 : 0.3270038167181457
Loss in iteration 64 : 0.327021866028823
Loss in iteration 65 : 0.32690156033435064
Loss in iteration 66 : 0.32672551248455145
Loss in iteration 67 : 0.3266098722002902
Loss in iteration 68 : 0.3265897631993286
Loss in iteration 69 : 0.3266037486521686
Loss in iteration 70 : 0.32656976653121206
Loss in iteration 71 : 0.3264637982828168
Loss in iteration 72 : 0.3263308246217522
Loss in iteration 73 : 0.3262318756612726
Loss in iteration 74 : 0.3261860897881272
Loss in iteration 75 : 0.3261610941261128
Loss in iteration 76 : 0.32611262985442807
Loss in iteration 77 : 0.32602817715257526
Loss in iteration 78 : 0.325932795716393
Loss in iteration 79 : 0.32585879144492996
Loss in iteration 80 : 0.32581477578444923
Loss in iteration 81 : 0.325782725941789
Loss in iteration 82 : 0.32573965208568406
Loss in iteration 83 : 0.32567916544128883
Loss in iteration 84 : 0.3256138314315523
Loss in iteration 85 : 0.32556010992376055
Loss in iteration 86 : 0.3255226300221437
Loss in iteration 87 : 0.3254920291723207
Loss in iteration 88 : 0.32545588396404995
Loss in iteration 89 : 0.32541042369235174
Loss in iteration 90 : 0.3253621634169258
Loss in iteration 91 : 0.32531986696327114
Loss in iteration 92 : 0.32528605661274074
Loss in iteration 93 : 0.3252558571078385
Loss in iteration 94 : 0.32522273539125646
Loss in iteration 95 : 0.3251846139208221
Loss in iteration 96 : 0.32514483020717
Loss in iteration 97 : 0.32510807586663254
Loss in iteration 98 : 0.3250758836179507
Loss in iteration 99 : 0.3250458016483766
Loss in iteration 100 : 0.32501433923746537
Loss in iteration 101 : 0.3249803273433228
Loss in iteration 102 : 0.32494554955814453
Loss in iteration 103 : 0.324912566203361
Loss in iteration 104 : 0.32488224213108063
Loss in iteration 105 : 0.3248532865185596
Loss in iteration 106 : 0.32482383841226503
Loss in iteration 107 : 0.32479326943696796
Loss in iteration 108 : 0.3247625279829306
Loss in iteration 109 : 0.3247329834483937
Loss in iteration 110 : 0.32470509652539586
Loss in iteration 111 : 0.324678161259263
Loss in iteration 112 : 0.3246511631536834
Loss in iteration 113 : 0.3246237635301594
Loss in iteration 114 : 0.32459647926546636
Loss in iteration 115 : 0.32457003880598817
Loss in iteration 116 : 0.32454466177288965
Loss in iteration 117 : 0.32451994207659113
Loss in iteration 118 : 0.3244953298988353
Loss in iteration 119 : 0.32447065625008104
Loss in iteration 120 : 0.3244462084075734
Loss in iteration 121 : 0.3244223697182887
Loss in iteration 122 : 0.32439923712070495
Loss in iteration 123 : 0.32437657534859843
Loss in iteration 124 : 0.3243540892444542
Loss in iteration 125 : 0.32433170246012527
Loss in iteration 126 : 0.32430958113252517
Loss in iteration 127 : 0.3242879272846057
Loss in iteration 128 : 0.32426677771172113
Loss in iteration 129 : 0.3242459953142219
Loss in iteration 130 : 0.32422542500693374
Loss in iteration 131 : 0.32420503767311054
Loss in iteration 132 : 0.3241849296837968
Loss in iteration 133 : 0.3241652051604382
Loss in iteration 134 : 0.32414587295891933
Loss in iteration 135 : 0.32412685233604466
Loss in iteration 136 : 0.32410806192268865
Loss in iteration 137 : 0.3240894928252099
Loss in iteration 138 : 0.3240711998357762
Loss in iteration 139 : 0.3240532342047753
Loss in iteration 140 : 0.3240355922916711
Loss in iteration 141 : 0.3240182256112798
Loss in iteration 142 : 0.3240010913281244
Loss in iteration 143 : 0.32398418795269546
Loss in iteration 144 : 0.32396754526017396
Loss in iteration 145 : 0.32395118648891136
Loss in iteration 146 : 0.32393510377658224
Loss in iteration 147 : 0.32391926769028634
Loss in iteration 148 : 0.32390365550454336
Loss in iteration 149 : 0.32388826788824826
Loss in iteration 150 : 0.32387312028382637
Loss in iteration 151 : 0.32385822189406477
Loss in iteration 152 : 0.3238435646034846
Loss in iteration 153 : 0.32382913054502654
Loss in iteration 154 : 0.323814907624836
Loss in iteration 155 : 0.32380089671129036
Loss in iteration 156 : 0.3237871051733315
Loss in iteration 157 : 0.32377353547682247
Loss in iteration 158 : 0.32376018066247714
Loss in iteration 159 : 0.3237470297576621
Loss in iteration 160 : 0.3237340760899735
Loss in iteration 161 : 0.3237213199835949
Loss in iteration 162 : 0.32370876428701345
Loss in iteration 163 : 0.32369640835737723
Loss in iteration 164 : 0.3236842465697411
Loss in iteration 165 : 0.3236722719407788
Loss in iteration 166 : 0.3236604804230091
Loss in iteration 167 : 0.3236488716035336
Loss in iteration 168 : 0.32363744579900156
Loss in iteration 169 : 0.32362620101895356
Loss in iteration 170 : 0.323615132755865
Loss in iteration 171 : 0.32360423628068524
Loss in iteration 172 : 0.3235935087566438
Loss in iteration 173 : 0.32358294915655544
Loss in iteration 174 : 0.32357255647136457
Loss in iteration 175 : 0.3235623282660963
Loss in iteration 176 : 0.323552260921433
Loss in iteration 177 : 0.3235423510130456
Loss in iteration 178 : 0.32353259627554964
Loss in iteration 179 : 0.3235229952898236
Loss in iteration 180 : 0.3235135464356055
Loss in iteration 181 : 0.3235042472675552
Loss in iteration 182 : 0.3234950948455527
Loss in iteration 183 : 0.3234860865195797
Loss in iteration 184 : 0.323477220317171
Loss in iteration 185 : 0.32346849462767047
Loss in iteration 186 : 0.32345990762270704
Loss in iteration 187 : 0.323451457029475
Loss in iteration 188 : 0.3234431404131288
Loss in iteration 189 : 0.323434955595797
Loss in iteration 190 : 0.3234269007744617
Loss in iteration 191 : 0.32341897427707833
Loss in iteration 192 : 0.32341117426363336
Loss in iteration 193 : 0.3234034986763674
Loss in iteration 194 : 0.3233959454415182
Loss in iteration 195 : 0.3233885126763391
Loss in iteration 196 : 0.32338119869543397
Loss in iteration 197 : 0.3233740018469084
Loss in iteration 198 : 0.323366920371543
Loss in iteration 199 : 0.3233599524196377
Loss in iteration 200 : 0.32335309617910796
Testing accuracy  of updater 6 on alg 0 with rate 0.0098 = 0.788, training accuracy 0.8436387180317255, time elapsed: 2090 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4978271348672146
Loss in iteration 3 : 0.48687725422421685
Loss in iteration 4 : 0.5225437797581719
Loss in iteration 5 : 0.5304924991998184
Loss in iteration 6 : 0.4984889165546909
Loss in iteration 7 : 0.43953352380953137
Loss in iteration 8 : 0.380402434470656
Loss in iteration 9 : 0.34972927169629553
Loss in iteration 10 : 0.35698954829759666
Loss in iteration 11 : 0.38088635177421204
Loss in iteration 12 : 0.39048610091864366
Loss in iteration 13 : 0.37648585007758767
Loss in iteration 14 : 0.3526319362731899
Loss in iteration 15 : 0.33546903945773887
Loss in iteration 16 : 0.3312804713002863
Loss in iteration 17 : 0.3368845116506893
Loss in iteration 18 : 0.34565428287946376
Loss in iteration 19 : 0.3520633886039569
Loss in iteration 20 : 0.35349148042098133
Loss in iteration 21 : 0.3501965497227962
Loss in iteration 22 : 0.34437542283117556
Loss in iteration 23 : 0.33887995084743106
Loss in iteration 24 : 0.335966346740442
Loss in iteration 25 : 0.33641418515344756
Loss in iteration 26 : 0.3393263211023606
Loss in iteration 27 : 0.34273345833835267
Loss in iteration 28 : 0.34471715044861445
Loss in iteration 29 : 0.34440082645885756
Loss in iteration 30 : 0.34223022170875816
Loss in iteration 31 : 0.3394768309171959
Loss in iteration 32 : 0.3374059884150962
Loss in iteration 33 : 0.33665464091801633
Loss in iteration 34 : 0.3370868314354418
Loss in iteration 35 : 0.33805088435383257
Loss in iteration 36 : 0.33879350924112145
Loss in iteration 37 : 0.3388079050808848
Loss in iteration 38 : 0.33799857200011646
Loss in iteration 39 : 0.3366470542338666
Loss in iteration 40 : 0.335231085208955
Loss in iteration 41 : 0.3341870318145224
Loss in iteration 42 : 0.33371665882819224
Loss in iteration 43 : 0.3337198945700966
Loss in iteration 44 : 0.3338805152844664
Loss in iteration 45 : 0.33385496131403786
Loss in iteration 46 : 0.33345666104924243
Loss in iteration 47 : 0.3327331096331579
Loss in iteration 48 : 0.331903850129555
Loss in iteration 49 : 0.3312144653699403
Loss in iteration 50 : 0.3308027381634602
Loss in iteration 51 : 0.3306477624504122
Loss in iteration 52 : 0.33061159258885114
Loss in iteration 53 : 0.330532060638125
Loss in iteration 54 : 0.33030956006329076
Loss in iteration 55 : 0.32994654576314236
Loss in iteration 56 : 0.32952911242048616
Loss in iteration 57 : 0.32916881576614504
Loss in iteration 58 : 0.3289392864674033
Loss in iteration 59 : 0.32884117841725036
Loss in iteration 60 : 0.32881148159889484
Loss in iteration 61 : 0.3287675421704469
Loss in iteration 62 : 0.3286566276868463
Loss in iteration 63 : 0.3284810165065511
Loss in iteration 64 : 0.328286854485956
Loss in iteration 65 : 0.32812901513395853
Loss in iteration 66 : 0.32803706490285917
Loss in iteration 67 : 0.32800227613202615
Loss in iteration 68 : 0.3279892162864964
Loss in iteration 69 : 0.32796043814261455
Loss in iteration 70 : 0.32789771902800835
Loss in iteration 71 : 0.3278085478948191
Loss in iteration 72 : 0.32771686364140623
Loss in iteration 73 : 0.32764595536443764
Loss in iteration 74 : 0.3276045814119975
Loss in iteration 75 : 0.32758390715275976
Loss in iteration 76 : 0.3275654582457263
Loss in iteration 77 : 0.3275336324512173
Loss in iteration 78 : 0.3274844222136932
Loss in iteration 79 : 0.3274256001791886
Loss in iteration 80 : 0.3273697192229825
Loss in iteration 81 : 0.3273255280005301
Loss in iteration 82 : 0.3272933447413898
Loss in iteration 83 : 0.3272664230808876
Loss in iteration 84 : 0.32723630579406465
Loss in iteration 85 : 0.3271981823130006
Loss in iteration 86 : 0.327153063840843
Loss in iteration 87 : 0.32710608931134033
Loss in iteration 88 : 0.32706268371902925
Loss in iteration 89 : 0.3270252473550789
Loss in iteration 90 : 0.3269922680970492
Loss in iteration 91 : 0.32695996654921006
Loss in iteration 92 : 0.32692502589369743
Loss in iteration 93 : 0.32688654204681483
Loss in iteration 94 : 0.32684613727982387
Loss in iteration 95 : 0.32680650633020975
Loss in iteration 96 : 0.32676958225609726
Loss in iteration 97 : 0.3267355164534805
Loss in iteration 98 : 0.3267029357656546
Loss in iteration 99 : 0.32667007131000003
Loss in iteration 100 : 0.3266359069601783
Loss in iteration 101 : 0.3266006515460856
Loss in iteration 102 : 0.32656538011670355
Loss in iteration 103 : 0.32653121771372234
Loss in iteration 104 : 0.32649864408587587
Loss in iteration 105 : 0.326467318211826
Loss in iteration 106 : 0.32643643390320465
Loss in iteration 107 : 0.3264052924600958
Loss in iteration 108 : 0.32637370271830063
Loss in iteration 109 : 0.3263419958835648
Loss in iteration 110 : 0.3263107182628487
Loss in iteration 111 : 0.32628025080088696
Loss in iteration 112 : 0.3262506019793559
Loss in iteration 113 : 0.326221467471018
Loss in iteration 114 : 0.32619246928677925
Loss in iteration 115 : 0.3261633946828461
Loss in iteration 116 : 0.326134289618874
Loss in iteration 117 : 0.3261053774174601
Loss in iteration 118 : 0.32607688492456516
Loss in iteration 119 : 0.3260488993531681
Loss in iteration 120 : 0.32602133750857903
Loss in iteration 121 : 0.3259940254201997
Loss in iteration 122 : 0.3259668191474097
Loss in iteration 123 : 0.3259396849513866
Loss in iteration 124 : 0.32591269706632914
Loss in iteration 125 : 0.32588596975498324
Loss in iteration 126 : 0.3258595777045359
Loss in iteration 127 : 0.325833515895866
Loss in iteration 128 : 0.3258077161902368
Loss in iteration 129 : 0.32578209974898126
Loss in iteration 130 : 0.3257566262977173
Loss in iteration 131 : 0.3257313104999348
Loss in iteration 132 : 0.325706201486159
Loss in iteration 133 : 0.3256813449936363
Loss in iteration 134 : 0.32565675468343774
Loss in iteration 135 : 0.3256324086400822
Loss in iteration 136 : 0.3256082685520933
Loss in iteration 137 : 0.32558430549297734
Loss in iteration 138 : 0.3255605150895344
Loss in iteration 139 : 0.32553691462602874
Loss in iteration 140 : 0.32551352723533594
Loss in iteration 141 : 0.3254903654042149
Loss in iteration 142 : 0.32546742417669294
Loss in iteration 143 : 0.32544468647442903
Loss in iteration 144 : 0.3254221349627011
Loss in iteration 145 : 0.3253997618540462
Loss in iteration 146 : 0.3253775709272951
Loss in iteration 147 : 0.32535557195319703
Loss in iteration 148 : 0.32533377241807876
Loss in iteration 149 : 0.3253121722190343
Loss in iteration 150 : 0.3252907640991497
Loss in iteration 151 : 0.3252695384884689
Loss in iteration 152 : 0.32524848890217445
Loss in iteration 153 : 0.3252276144080159
Loss in iteration 154 : 0.32520691815907526
Loss in iteration 155 : 0.3251864036455223
Loss in iteration 156 : 0.32516607144644866
Loss in iteration 157 : 0.3251459184320779
Loss in iteration 158 : 0.32512593946209123
Loss in iteration 159 : 0.3251061300453474
Loss in iteration 160 : 0.3250864881056087
Loss in iteration 161 : 0.3250670139058928
Loss in iteration 162 : 0.32504770852157616
Loss in iteration 163 : 0.3250285720963464
Loss in iteration 164 : 0.32500960302379406
Loss in iteration 165 : 0.3249907984003405
Loss in iteration 166 : 0.3249721552262718
Loss in iteration 167 : 0.32495367145429077
Loss in iteration 168 : 0.32493534624840403
Loss in iteration 169 : 0.3249171794351819
Loss in iteration 170 : 0.3248991706457849
Loss in iteration 171 : 0.3248813187512333
Loss in iteration 172 : 0.32486362189506074
Loss in iteration 173 : 0.32484607799082316
Loss in iteration 174 : 0.32482868528056075
Loss in iteration 175 : 0.32481144258654243
Loss in iteration 176 : 0.32479434915256966
Loss in iteration 177 : 0.32477740425236395
Loss in iteration 178 : 0.3247606068584649
Loss in iteration 179 : 0.3247439555720142
Loss in iteration 180 : 0.3247274488098008
Loss in iteration 181 : 0.32471108508089236
Loss in iteration 182 : 0.32469486315888263
Loss in iteration 183 : 0.3246787820586563
Loss in iteration 184 : 0.3246628408688898
Loss in iteration 185 : 0.3246470385746455
Loss in iteration 186 : 0.3246313739858125
Loss in iteration 187 : 0.32461584579761754
Loss in iteration 188 : 0.32460045271955595
Loss in iteration 189 : 0.32458519357692583
Loss in iteration 190 : 0.3245700673245546
Loss in iteration 191 : 0.32455507298021563
Loss in iteration 192 : 0.3245402095356383
Loss in iteration 193 : 0.3245254759067809
Loss in iteration 194 : 0.3245108709481917
Loss in iteration 195 : 0.32449639351025666
Loss in iteration 196 : 0.324482042494273
Loss in iteration 197 : 0.3244678168700523
Loss in iteration 198 : 0.3244537156515766
Loss in iteration 199 : 0.32443973785425484
Loss in iteration 200 : 0.3244258824648869
Testing accuracy  of updater 6 on alg 0 with rate 0.005600000000000001 = 0.78725, training accuracy 0.8429912593072192, time elapsed: 2151 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6118311697388887
Loss in iteration 3 : 0.5291062150791107
Loss in iteration 4 : 0.48640885052993194
Loss in iteration 5 : 0.4770383273572222
Loss in iteration 6 : 0.48305277464207597
Loss in iteration 7 : 0.4915710760604377
Loss in iteration 8 : 0.4957639312015591
Loss in iteration 9 : 0.4928799315059901
Loss in iteration 10 : 0.4826401569216916
Loss in iteration 11 : 0.4662620061882552
Loss in iteration 12 : 0.44587287439747214
Loss in iteration 13 : 0.424092992154866
Loss in iteration 14 : 0.4036396268568312
Loss in iteration 15 : 0.38688127572810405
Loss in iteration 16 : 0.37536494341232984
Loss in iteration 17 : 0.3694425089949213
Loss in iteration 18 : 0.3681784028229354
Loss in iteration 19 : 0.3696601635040531
Loss in iteration 20 : 0.37164842475980053
Loss in iteration 21 : 0.3723042701053759
Loss in iteration 22 : 0.37068011710786053
Loss in iteration 23 : 0.3668050744109054
Loss in iteration 24 : 0.36141916756034875
Loss in iteration 25 : 0.3555527850925191
Loss in iteration 26 : 0.35014875879625806
Loss in iteration 27 : 0.34583610965698847
Loss in iteration 28 : 0.34286756630824883
Loss in iteration 29 : 0.3411745564735037
Loss in iteration 30 : 0.34047844423291507
Loss in iteration 31 : 0.3404087504196669
Loss in iteration 32 : 0.34059962063934757
Loss in iteration 33 : 0.340753674095196
Loss in iteration 34 : 0.34067376733852794
Loss in iteration 35 : 0.34026870504312495
Loss in iteration 36 : 0.33954056066148136
Loss in iteration 37 : 0.33856092581120767
Loss in iteration 38 : 0.3374423172142107
Loss in iteration 39 : 0.336309738371096
Loss in iteration 40 : 0.33527621084211995
Loss in iteration 41 : 0.33442494072803314
Loss in iteration 42 : 0.3337995995297108
Loss in iteration 43 : 0.3334029503968072
Loss in iteration 44 : 0.33320281134025204
Loss in iteration 45 : 0.333143280305771
Loss in iteration 46 : 0.3331584594528636
Loss in iteration 47 : 0.33318577249893366
Loss in iteration 48 : 0.33317641240204443
Loss in iteration 49 : 0.3331013704306606
Loss in iteration 50 : 0.33295263408185005
Loss in iteration 51 : 0.332740204039629
Loss in iteration 52 : 0.33248632316896837
Loss in iteration 53 : 0.3322186086608777
Loss in iteration 54 : 0.3319636473054952
Loss in iteration 55 : 0.33174217708552817
Loss in iteration 56 : 0.3315664077235558
Loss in iteration 57 : 0.3314394889555066
Loss in iteration 58 : 0.3313567279281615
Loss in iteration 59 : 0.3313079328056589
Loss in iteration 60 : 0.33128021084279785
Loss in iteration 61 : 0.33126063423277097
Loss in iteration 62 : 0.3312383514760912
Loss in iteration 63 : 0.33120591444096076
Loss in iteration 64 : 0.3311597721646728
Loss in iteration 65 : 0.3311000267418298
Loss in iteration 66 : 0.3310296424959413
Loss in iteration 67 : 0.3309533453198345
Loss in iteration 68 : 0.3308764498050879
Loss in iteration 69 : 0.3308038170153984
Loss in iteration 70 : 0.3307390871954853
Loss in iteration 71 : 0.33068426179048466
Loss in iteration 72 : 0.3306396398008266
Loss in iteration 73 : 0.33060405492474443
Loss in iteration 74 : 0.33057531955854597
Loss in iteration 75 : 0.3305507633273338
Loss in iteration 76 : 0.33052775724482913
Loss in iteration 77 : 0.330504136053082
Loss in iteration 78 : 0.3304784643163222
Loss in iteration 79 : 0.33045012877860086
Loss in iteration 80 : 0.33041927307288343
Loss in iteration 81 : 0.3303866155715213
Loss in iteration 82 : 0.33035320405690644
Loss in iteration 83 : 0.3303201617755562
Loss in iteration 84 : 0.33028847043130277
Loss in iteration 85 : 0.3302588203405993
Loss in iteration 86 : 0.3302315403489213
Loss in iteration 87 : 0.33020660381534006
Loss in iteration 88 : 0.3301836946251586
Loss in iteration 89 : 0.3301623101275356
Loss in iteration 90 : 0.33014187622856933
Loss in iteration 91 : 0.3301218527496172
Loss in iteration 92 : 0.3301018131447645
Loss in iteration 93 : 0.3300814901119892
Loss in iteration 94 : 0.330060785996004
Loss in iteration 95 : 0.33003975297130506
Loss in iteration 96 : 0.3300185520702635
Loss in iteration 97 : 0.32999740191496557
Loss in iteration 98 : 0.3299765276720212
Loss in iteration 99 : 0.32995611873668845
Loss in iteration 100 : 0.3299363006072462
Loss in iteration 101 : 0.3299171230213262
Loss in iteration 102 : 0.3298985633133449
Loss in iteration 103 : 0.3298805415820035
Loss in iteration 104 : 0.3298629428912193
Loss in iteration 105 : 0.3298456414167497
Loss in iteration 106 : 0.32982852205859564
Loss in iteration 107 : 0.32981149629852163
Loss in iteration 108 : 0.32979451066087856
Loss in iteration 109 : 0.3297775477049018
Loss in iteration 110 : 0.32976062076936996
Loss in iteration 111 : 0.3297437645337794
Loss in iteration 112 : 0.329727023791344
Loss in iteration 113 : 0.3297104426861751
Loss in iteration 114 : 0.3296940561635118
Loss in iteration 115 : 0.32967788467274284
Loss in iteration 116 : 0.3296619324095163
Loss in iteration 117 : 0.3296461887236547
Loss in iteration 118 : 0.3296306318492887
Loss in iteration 119 : 0.3296152338770424
Loss in iteration 120 : 0.32959996588101903
Loss in iteration 121 : 0.3295848022930542
Loss in iteration 122 : 0.3295697239169325
Loss in iteration 123 : 0.32955471932203073
Loss in iteration 124 : 0.32953978468140915
Loss in iteration 125 : 0.3295249223724564
Loss in iteration 126 : 0.3295101388096588
Loss in iteration 127 : 0.3294954420221504
Loss in iteration 128 : 0.32948083943652184
Loss in iteration 129 : 0.3294663362043691
Loss in iteration 130 : 0.3294519342577564
Loss in iteration 131 : 0.32943763211712357
Loss in iteration 132 : 0.3294234253428124
Loss in iteration 133 : 0.32940930743131974
Loss in iteration 134 : 0.32939527091811394
Loss in iteration 135 : 0.32938130845770053
Loss in iteration 136 : 0.3293674136982737
Loss in iteration 137 : 0.3293535818377025
Loss in iteration 138 : 0.32933980982367766
Loss in iteration 139 : 0.3293260962291716
Loss in iteration 140 : 0.32931244088468403
Loss in iteration 141 : 0.32929884437554935
Loss in iteration 142 : 0.3292853075157861
Loss in iteration 143 : 0.32927183089318623
Loss in iteration 144 : 0.3292584145504085
Loss in iteration 145 : 0.329245057831205
Loss in iteration 146 : 0.3292317593869386
Loss in iteration 147 : 0.3292185173117185
Loss in iteration 148 : 0.3292053293581705
Loss in iteration 149 : 0.32919219318088294
Loss in iteration 150 : 0.3291791065597127
Loss in iteration 151 : 0.32916606756757394
Loss in iteration 152 : 0.3291530746636038
Loss in iteration 153 : 0.3291401267091387
Loss in iteration 154 : 0.32912722291782764
Loss in iteration 155 : 0.32911436276055617
Loss in iteration 156 : 0.32910154584987117
Loss in iteration 157 : 0.32908877182758095
Loss in iteration 158 : 0.32907604027423887
Loss in iteration 159 : 0.3290633506519618
Loss in iteration 160 : 0.3290507022840765
Loss in iteration 161 : 0.3290380943680625
Loss in iteration 162 : 0.3290255260130775
Loss in iteration 163 : 0.32901299629072284
Loss in iteration 164 : 0.3290005042874896
Loss in iteration 165 : 0.32898804914921503
Loss in iteration 166 : 0.32897563011104386
Loss in iteration 167 : 0.3289632465102034
Loss in iteration 168 : 0.3289508977823632
Loss in iteration 169 : 0.32893858344514093
Loss in iteration 170 : 0.32892630307387083
Loss in iteration 171 : 0.3289140562752125
Loss in iteration 172 : 0.32890184266347183
Loss in iteration 173 : 0.32888966184318524
Loss in iteration 174 : 0.3288775133996972
Loss in iteration 175 : 0.3288653968978055
Loss in iteration 176 : 0.3288533118870632
Loss in iteration 177 : 0.3288412579114529
Loss in iteration 178 : 0.3288292345208046
Loss in iteration 179 : 0.3288172412814985
Loss in iteration 180 : 0.32880527778460883
Loss in iteration 181 : 0.328793343650426
Loss in iteration 182 : 0.3287814385291383
Loss in iteration 183 : 0.3287695620981919
Loss in iteration 184 : 0.32875771405732607
Loss in iteration 185 : 0.3287458941225252
Loss in iteration 186 : 0.3287341020200744
Loss in iteration 187 : 0.32872233748170615
Loss in iteration 188 : 0.3287106002413988
Loss in iteration 189 : 0.328698890034042
Loss in iteration 190 : 0.32868720659578504
Loss in iteration 191 : 0.32867554966563495
Loss in iteration 192 : 0.3286639189877258
Loss in iteration 193 : 0.32865231431367165
Loss in iteration 194 : 0.32864073540451844
Loss in iteration 195 : 0.3286291820319611
Loss in iteration 196 : 0.3286176539787007
Loss in iteration 197 : 0.328606151037989
Loss in iteration 198 : 0.32859467301254
Loss in iteration 199 : 0.32858321971308235
Loss in iteration 200 : 0.3285717909568338
Testing accuracy  of updater 6 on alg 0 with rate 0.0014000000000000002 = 0.78425, training accuracy 0.8387827775979282, time elapsed: 2113 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 4.806587259903958
Loss in iteration 3 : 4.991969479442522
Loss in iteration 4 : 3.0984730392456847
Loss in iteration 5 : 0.7682986467837825
Loss in iteration 6 : 1.4426781655094987
Loss in iteration 7 : 2.5824259728989523
Loss in iteration 8 : 1.9202999038395139
Loss in iteration 9 : 1.0392282186858466
Loss in iteration 10 : 0.9344697202585254
Loss in iteration 11 : 1.253119538415422
Loss in iteration 12 : 1.5656581093950317
Loss in iteration 13 : 1.6818145466843029
Loss in iteration 14 : 1.583322409595971
Loss in iteration 15 : 1.361894527949007
Loss in iteration 16 : 1.1420692455481056
Loss in iteration 17 : 1.0220625402047332
Loss in iteration 18 : 1.0516337664841022
Loss in iteration 19 : 1.1856676073088133
Loss in iteration 20 : 1.2899635367051245
Loss in iteration 21 : 1.2747232772268704
Loss in iteration 22 : 1.1576265993236907
Loss in iteration 23 : 1.0287854175873838
Loss in iteration 24 : 0.9665119132196904
Loss in iteration 25 : 0.9772148033740691
Loss in iteration 26 : 1.0189790225310402
Loss in iteration 27 : 1.0486119208431732
Loss in iteration 28 : 1.040594951853552
Loss in iteration 29 : 0.9924994866906895
Loss in iteration 30 : 0.9221035644720619
Loss in iteration 31 : 0.8575983956490406
Loss in iteration 32 : 0.8238492120007189
Loss in iteration 33 : 0.8263197465009258
Loss in iteration 34 : 0.8401787648562322
Loss in iteration 35 : 0.8302201648793669
Loss in iteration 36 : 0.7855182391462915
Loss in iteration 37 : 0.7283595784642081
Loss in iteration 38 : 0.6890994153230017
Loss in iteration 39 : 0.6759465334978614
Loss in iteration 40 : 0.673357815540104
Loss in iteration 41 : 0.6611023305163174
Loss in iteration 42 : 0.6297981069617454
Loss in iteration 43 : 0.5866564424983951
Loss in iteration 44 : 0.5498818274606125
Loss in iteration 45 : 0.5327571401978695
Loss in iteration 46 : 0.5267986244190881
Loss in iteration 47 : 0.5087478862997518
Loss in iteration 48 : 0.4729981591931643
Loss in iteration 49 : 0.44088000091832774
Loss in iteration 50 : 0.42743110049806676
Loss in iteration 51 : 0.42070906945887315
Loss in iteration 52 : 0.40202175093048936
Loss in iteration 53 : 0.3735975508050556
Loss in iteration 54 : 0.35764487142078877
Loss in iteration 55 : 0.35839626293143956
Loss in iteration 56 : 0.347280734866949
Loss in iteration 57 : 0.32903239440431575
Loss in iteration 58 : 0.33293991896342356
Loss in iteration 59 : 0.33723685402893705
Loss in iteration 60 : 0.32810527871540635
Loss in iteration 61 : 0.3389867650671854
Loss in iteration 62 : 0.3455290592636015
Loss in iteration 63 : 0.3417406931800927
Loss in iteration 64 : 0.3539045898486216
Loss in iteration 65 : 0.34689370313831247
Loss in iteration 66 : 0.3465492579583023
Loss in iteration 67 : 0.34517172493954795
Loss in iteration 68 : 0.33535027358944924
Loss in iteration 69 : 0.3362204811898132
Loss in iteration 70 : 0.33018917003055137
Loss in iteration 71 : 0.32655495432111314
Loss in iteration 72 : 0.3283106965576378
Loss in iteration 73 : 0.3250878323394733
Loss in iteration 74 : 0.3247895813345185
Loss in iteration 75 : 0.32702228609543377
Loss in iteration 76 : 0.3261240711570346
Loss in iteration 77 : 0.32605545482677556
Loss in iteration 78 : 0.3280270166583139
Loss in iteration 79 : 0.3278100851456286
Loss in iteration 80 : 0.3271795085595701
Loss in iteration 81 : 0.3281440706399798
Loss in iteration 82 : 0.32803684842316566
Loss in iteration 83 : 0.32689387792732566
Loss in iteration 84 : 0.326910752479506
Loss in iteration 85 : 0.32677251638037885
Loss in iteration 86 : 0.3255711812413969
Loss in iteration 87 : 0.32516265659230265
Loss in iteration 88 : 0.325068737826277
Loss in iteration 89 : 0.3241891301871666
Loss in iteration 90 : 0.3238160424817589
Loss in iteration 91 : 0.32393621175353704
Loss in iteration 92 : 0.32345623593542444
Loss in iteration 93 : 0.32335754986096577
Loss in iteration 94 : 0.32363770649768253
Loss in iteration 95 : 0.323433750231103
Loss in iteration 96 : 0.3235060045155882
Loss in iteration 97 : 0.32377051734504986
Loss in iteration 98 : 0.32360775054486923
Loss in iteration 99 : 0.32368397635515395
Loss in iteration 100 : 0.3237785179422918
Loss in iteration 101 : 0.32356878515028825
Loss in iteration 102 : 0.3235757257421569
Loss in iteration 103 : 0.3235343662508978
Loss in iteration 104 : 0.32332602186618575
Loss in iteration 105 : 0.32331724268133827
Loss in iteration 106 : 0.32324696122739377
Loss in iteration 107 : 0.3231105776233775
Loss in iteration 108 : 0.32313311523084043
Loss in iteration 109 : 0.3230963644280551
Loss in iteration 110 : 0.3230361117534491
Loss in iteration 111 : 0.3230848186601885
Loss in iteration 112 : 0.3230753703221335
Loss in iteration 113 : 0.32305294902801857
Loss in iteration 114 : 0.3230983497781935
Loss in iteration 115 : 0.3230892858329492
Loss in iteration 116 : 0.3230711795382523
Loss in iteration 117 : 0.32309547630299273
Loss in iteration 118 : 0.3230751674041544
Loss in iteration 119 : 0.3230516973581501
Loss in iteration 120 : 0.32305850552946785
Loss in iteration 121 : 0.3230327740788868
Loss in iteration 122 : 0.3230113482511247
Loss in iteration 123 : 0.3230130775219222
Loss in iteration 124 : 0.32299248893802235
Loss in iteration 125 : 0.3229807989588263
Loss in iteration 126 : 0.3229855802562606
Loss in iteration 127 : 0.3229738823141075
Loss in iteration 128 : 0.3229720101288775
Loss in iteration 129 : 0.3229790612296398
Loss in iteration 130 : 0.32297291689586216
Loss in iteration 131 : 0.32297515818327277
Loss in iteration 132 : 0.3229801255306563
Loss in iteration 133 : 0.32297475512999035
Loss in iteration 134 : 0.3229763470994642
Loss in iteration 135 : 0.32297710809545604
Loss in iteration 136 : 0.32297109789051126
Loss in iteration 137 : 0.32297116247894203
Loss in iteration 138 : 0.3229692528723561
Loss in iteration 139 : 0.32296401014692994
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999998 = 0.7895, training accuracy 0.8449336354807381, time elapsed: 1458 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6761591185004534
Loss in iteration 3 : 0.786723946339388
Loss in iteration 4 : 0.5421460804800143
Loss in iteration 5 : 0.33759082176415905
Loss in iteration 6 : 0.5506820599528993
Loss in iteration 7 : 0.5002949372848793
Loss in iteration 8 : 0.349718404130148
Loss in iteration 9 : 0.3929796220960851
Loss in iteration 10 : 0.4767062341012954
Loss in iteration 11 : 0.47923640760230013
Loss in iteration 12 : 0.4222923988324405
Loss in iteration 13 : 0.38968657571690896
Loss in iteration 14 : 0.4213129436681173
Loss in iteration 15 : 0.4649712294722369
Loss in iteration 16 : 0.45740844849048
Loss in iteration 17 : 0.4203389885187268
Loss in iteration 18 : 0.4085130753978096
Loss in iteration 19 : 0.42712190365372976
Loss in iteration 20 : 0.4441009762750715
Loss in iteration 21 : 0.4371070120181809
Loss in iteration 22 : 0.4134914697081381
Loss in iteration 23 : 0.3980451227480094
Loss in iteration 24 : 0.4033313450596634
Loss in iteration 25 : 0.41226477256977956
Loss in iteration 26 : 0.4027414429803385
Loss in iteration 27 : 0.38246249371776475
Loss in iteration 28 : 0.37363475611915387
Loss in iteration 29 : 0.37747384408905754
Loss in iteration 30 : 0.3771902647992138
Loss in iteration 31 : 0.36455971678525356
Loss in iteration 32 : 0.35074429294908
Loss in iteration 33 : 0.34896404176889406
Loss in iteration 34 : 0.3517356867277313
Loss in iteration 35 : 0.3438876981510786
Loss in iteration 36 : 0.3331858104276753
Loss in iteration 37 : 0.3332224918943634
Loss in iteration 38 : 0.3358603196558247
Loss in iteration 39 : 0.3302442456104535
Loss in iteration 40 : 0.32490612103002314
Loss in iteration 41 : 0.32885708036293115
Loss in iteration 42 : 0.33008327859045616
Loss in iteration 43 : 0.32535434492557364
Loss in iteration 44 : 0.3274545815155179
Loss in iteration 45 : 0.3309442924015854
Loss in iteration 46 : 0.3281957409170807
Loss in iteration 47 : 0.3280792669057348
Loss in iteration 48 : 0.33123022115570816
Loss in iteration 49 : 0.3290321355279144
Loss in iteration 50 : 0.3278279081278214
Loss in iteration 51 : 0.3295169750882561
Loss in iteration 52 : 0.32791523927163274
Loss in iteration 53 : 0.3260078340046366
Loss in iteration 54 : 0.3268489473212892
Loss in iteration 55 : 0.32605506112203425
Loss in iteration 56 : 0.32435163944449624
Loss in iteration 57 : 0.3247131126398075
Loss in iteration 58 : 0.324777425273307
Loss in iteration 59 : 0.323676226724136
Loss in iteration 60 : 0.3236364676263196
Loss in iteration 61 : 0.3241909866027155
Loss in iteration 62 : 0.32375463576964747
Loss in iteration 63 : 0.323454844343018
Loss in iteration 64 : 0.3239361160548491
Loss in iteration 65 : 0.324015166539369
Loss in iteration 66 : 0.32365104447895265
Loss in iteration 67 : 0.32377264603205186
Loss in iteration 68 : 0.32403514520614835
Loss in iteration 69 : 0.32380875554985755
Loss in iteration 70 : 0.3236337759593579
Loss in iteration 71 : 0.32379394642021386
Loss in iteration 72 : 0.3237493300307704
Loss in iteration 73 : 0.3234891132758674
Loss in iteration 74 : 0.3234667473010194
Loss in iteration 75 : 0.32351784422796837
Loss in iteration 76 : 0.3233419664570226
Loss in iteration 77 : 0.3232162233065499
Loss in iteration 78 : 0.3232682648611226
Loss in iteration 79 : 0.32321903812684266
Loss in iteration 80 : 0.32309238964859727
Loss in iteration 81 : 0.3231083471940845
Loss in iteration 82 : 0.32314142195473944
Loss in iteration 83 : 0.32306781855242256
Loss in iteration 84 : 0.32305135402199087
Loss in iteration 85 : 0.323104914700424
Loss in iteration 86 : 0.32308438242406023
Loss in iteration 87 : 0.32305007948606823
Loss in iteration 88 : 0.3230852994322449
Loss in iteration 89 : 0.3230930928893634
Loss in iteration 90 : 0.32305613892767115
Loss in iteration 91 : 0.32306214419832296
Loss in iteration 92 : 0.3230763895712376
Loss in iteration 93 : 0.3230473903682969
Loss in iteration 94 : 0.3230321250242923
Loss in iteration 95 : 0.32304230745722734
Loss in iteration 96 : 0.3230262607771696
Loss in iteration 97 : 0.32300433699581166
Loss in iteration 98 : 0.323008015897722
Loss in iteration 99 : 0.32300437893570644
Loss in iteration 100 : 0.322986572328094
Loss in iteration 101 : 0.32298514767850794
Loss in iteration 102 : 0.3229891197373549
Loss in iteration 103 : 0.3229795143227537
Loss in iteration 104 : 0.3229752384712013
Loss in iteration 105 : 0.32298091902279447
Loss in iteration 106 : 0.32297835430419314
Loss in iteration 107 : 0.3229731082794497
Loss in iteration 108 : 0.322976515448941
Loss in iteration 109 : 0.322977614825471
Loss in iteration 110 : 0.3229728472504562
Loss in iteration 111 : 0.32297285232254735
Loss in iteration 112 : 0.32297465760412186
Loss in iteration 113 : 0.3229712332050821
Loss in iteration 114 : 0.32296883260981524
Loss in iteration 115 : 0.3229698651628505
Loss in iteration 116 : 0.3229680362155034
Loss in iteration 117 : 0.32296496727934776
Loss in iteration 118 : 0.322965000720524
Loss in iteration 119 : 0.3229645181392494
Testing accuracy  of updater 7 on alg 0 with rate 0.13999999999999999 = 0.79, training accuracy 0.8442861767562317, time elapsed: 1292 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5367744424872787
Loss in iteration 3 : 0.6294789126264102
Loss in iteration 4 : 0.5597343272612697
Loss in iteration 5 : 0.39289060726982994
Loss in iteration 6 : 0.37435336922127443
Loss in iteration 7 : 0.46301714509836844
Loss in iteration 8 : 0.41103387696561067
Loss in iteration 9 : 0.3356635769082291
Loss in iteration 10 : 0.34637833509462745
Loss in iteration 11 : 0.3874723517545432
Loss in iteration 12 : 0.3977658306074135
Loss in iteration 13 : 0.37389726395187284
Loss in iteration 14 : 0.3493321148508145
Loss in iteration 15 : 0.35145699881112513
Loss in iteration 16 : 0.3734723425152435
Loss in iteration 17 : 0.386459114358329
Loss in iteration 18 : 0.3777418366019231
Loss in iteration 19 : 0.3625513926609484
Loss in iteration 20 : 0.3589070860996152
Loss in iteration 21 : 0.3672986745478455
Loss in iteration 22 : 0.3759886073040168
Loss in iteration 23 : 0.3757120270797134
Loss in iteration 24 : 0.36732272324876375
Loss in iteration 25 : 0.3590526636103887
Loss in iteration 26 : 0.3577313929162282
Loss in iteration 27 : 0.36189385119689016
Loss in iteration 28 : 0.36392671128409715
Loss in iteration 29 : 0.3594530310245282
Loss in iteration 30 : 0.3521848400838375
Loss in iteration 31 : 0.34829666584112623
Loss in iteration 32 : 0.3488956939745806
Loss in iteration 33 : 0.3498842014908311
Loss in iteration 34 : 0.3474905300387106
Loss in iteration 35 : 0.3423575664160789
Loss in iteration 36 : 0.33830862281153873
Loss in iteration 37 : 0.33758292294059644
Loss in iteration 38 : 0.33810830233491934
Loss in iteration 39 : 0.3365150803324568
Loss in iteration 40 : 0.33291283722545334
Loss in iteration 41 : 0.3303674890966163
Loss in iteration 42 : 0.33022030093461485
Loss in iteration 43 : 0.3305332906406991
Loss in iteration 44 : 0.32927586547808546
Loss in iteration 45 : 0.3271381474875112
Loss in iteration 46 : 0.32623305019675686
Loss in iteration 47 : 0.3268158457029075
Loss in iteration 48 : 0.3270088946712391
Loss in iteration 49 : 0.3259841097118524
Loss in iteration 50 : 0.3251600117907806
Loss in iteration 51 : 0.3255349589460909
Loss in iteration 52 : 0.3261277845254661
Loss in iteration 53 : 0.3258646050827962
Loss in iteration 54 : 0.325283545072398
Loss in iteration 55 : 0.3253942561388095
Loss in iteration 56 : 0.3259068342380564
Loss in iteration 57 : 0.3258828012291094
Loss in iteration 58 : 0.3254394663824823
Loss in iteration 59 : 0.32535730622941766
Loss in iteration 60 : 0.32561941984705417
Loss in iteration 61 : 0.32560261912897875
Loss in iteration 62 : 0.32523856228571857
Loss in iteration 63 : 0.325025158199504
Loss in iteration 64 : 0.32509536007686335
Loss in iteration 65 : 0.3250632647937131
Loss in iteration 66 : 0.32478216579033353
Loss in iteration 67 : 0.3245535108221232
Loss in iteration 68 : 0.3245340955264548
Loss in iteration 69 : 0.3245125118316084
Loss in iteration 70 : 0.3243378559301245
Loss in iteration 71 : 0.32415485030824254
Loss in iteration 72 : 0.32411050600932223
Loss in iteration 73 : 0.32411376004252485
Loss in iteration 74 : 0.3240299625094633
Loss in iteration 75 : 0.3239089218816494
Loss in iteration 76 : 0.32386501247846755
Loss in iteration 77 : 0.3238774742349294
Loss in iteration 78 : 0.3238501515188892
Loss in iteration 79 : 0.3237782653369029
Loss in iteration 80 : 0.32373602655810135
Loss in iteration 81 : 0.32374146275878535
Loss in iteration 82 : 0.3237367440669594
Loss in iteration 83 : 0.32369402888353294
Loss in iteration 84 : 0.32365334715973537
Loss in iteration 85 : 0.3236445965627285
Loss in iteration 86 : 0.3236413883187077
Loss in iteration 87 : 0.323613610392825
Loss in iteration 88 : 0.3235756880186661
Loss in iteration 89 : 0.32355519702788293
Loss in iteration 90 : 0.3235464970027881
Loss in iteration 91 : 0.3235259697903388
Loss in iteration 92 : 0.3234934219495293
Loss in iteration 93 : 0.3234682546963591
Loss in iteration 94 : 0.3234548607991682
Loss in iteration 95 : 0.3234386467827568
Loss in iteration 96 : 0.3234132152648456
Loss in iteration 97 : 0.3233892921866629
Loss in iteration 98 : 0.3233744548774405
Loss in iteration 99 : 0.32336155975181047
Loss in iteration 100 : 0.3233430645586794
Loss in iteration 101 : 0.32332338576065184
Loss in iteration 102 : 0.3233094992172489
Loss in iteration 103 : 0.3232990806377279
Loss in iteration 104 : 0.32328596175657626
Loss in iteration 105 : 0.32327080554002136
Loss in iteration 106 : 0.32325866730917246
Loss in iteration 107 : 0.32324981956110577
Loss in iteration 108 : 0.32324007476206473
Loss in iteration 109 : 0.32322837858257075
Loss in iteration 110 : 0.3232178655305383
Loss in iteration 111 : 0.3232098326407452
Loss in iteration 112 : 0.32320187117778315
Loss in iteration 113 : 0.32319245618834513
Loss in iteration 114 : 0.3231832356276005
Loss in iteration 115 : 0.32317567743509984
Loss in iteration 116 : 0.3231686441547102
Loss in iteration 117 : 0.32316073326190203
Loss in iteration 118 : 0.32315263153907214
Loss in iteration 119 : 0.3231455757845256
Loss in iteration 120 : 0.32313919584108497
Loss in iteration 121 : 0.3231324287929239
Loss in iteration 122 : 0.32312541035229414
Loss in iteration 123 : 0.32311902579221236
Loss in iteration 124 : 0.32311330153838214
Loss in iteration 125 : 0.32310752216089655
Loss in iteration 126 : 0.32310155741174457
Loss in iteration 127 : 0.32309596578192845
Loss in iteration 128 : 0.3230909307447939
Loss in iteration 129 : 0.32308601947927784
Loss in iteration 130 : 0.32308101068207806
Loss in iteration 131 : 0.32307621295281613
Loss in iteration 132 : 0.3230718396766051
Loss in iteration 133 : 0.32306765872256216
Loss in iteration 134 : 0.32306345044134116
Loss in iteration 135 : 0.32305935889398063
Loss in iteration 136 : 0.3230555696101313
Loss in iteration 137 : 0.32305197884866665
Loss in iteration 138 : 0.3230484090959118
Loss in iteration 139 : 0.3230449081258467
Loss in iteration 140 : 0.32304161576520307
Loss in iteration 141 : 0.32303850222237807
Loss in iteration 142 : 0.3230354405038512
Loss in iteration 143 : 0.32303242926986075
Loss in iteration 144 : 0.3230295638336912
Loss in iteration 145 : 0.32302685089669997
Loss in iteration 146 : 0.32302420757825656
Loss in iteration 147 : 0.3230216118699107
Loss in iteration 148 : 0.3230191229523093
Loss in iteration 149 : 0.3230167612999341
Loss in iteration 150 : 0.32301447669905914
Loss in iteration 151 : 0.3230122417666276
Loss in iteration 152 : 0.323010089621537
Loss in iteration 153 : 0.32300804233954894
Loss in iteration 154 : 0.3230060715178664
Loss in iteration 155 : 0.3230041517095085
Loss in iteration 156 : 0.3230022989050249
Loss in iteration 157 : 0.3230005315949855
Loss in iteration 158 : 0.32299883487137343
Loss in iteration 159 : 0.32299718799048166
Loss in iteration 160 : 0.3229955967726341
Loss in iteration 161 : 0.32299407482160536
Loss in iteration 162 : 0.32299261507018523
Loss in iteration 163 : 0.32299120195448794
Loss in iteration 164 : 0.3229898359127707
Loss in iteration 165 : 0.32298852611954926
Loss in iteration 166 : 0.3229872697766475
Loss in iteration 167 : 0.32298605586649437
Loss in iteration 168 : 0.32298488240710804
Loss in iteration 169 : 0.3229837551527314
Loss in iteration 170 : 0.32298267341292614
Loss in iteration 171 : 0.3229816297019176
Loss in iteration 172 : 0.3229806212416255
Loss in iteration 173 : 0.3229796513503147
Loss in iteration 174 : 0.32297872020309515
Loss in iteration 175 : 0.32297782282633936
Loss in iteration 176 : 0.322976956427036
Loss in iteration 177 : 0.32297612268508624
Testing accuracy  of updater 7 on alg 0 with rate 0.08 = 0.78975, training accuracy 0.8439624473939786, time elapsed: 2068 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5663395200846489
Loss in iteration 3 : 0.5040880148577208
Loss in iteration 4 : 0.5133490845819867
Loss in iteration 5 : 0.5321059771556826
Loss in iteration 6 : 0.5298074391112354
Loss in iteration 7 : 0.5008697417313344
Loss in iteration 8 : 0.45398659929881596
Loss in iteration 9 : 0.40675933500234374
Loss in iteration 10 : 0.3782344526981591
Loss in iteration 11 : 0.37597357017950067
Loss in iteration 12 : 0.3885323954849082
Loss in iteration 13 : 0.39602530206892445
Loss in iteration 14 : 0.388044635443281
Loss in iteration 15 : 0.36909647973754994
Loss in iteration 16 : 0.3502822187704092
Loss in iteration 17 : 0.3394224216465347
Loss in iteration 18 : 0.33759374249019114
Loss in iteration 19 : 0.34123236924292927
Loss in iteration 20 : 0.34568411960528095
Loss in iteration 21 : 0.3476330054380035
Loss in iteration 22 : 0.34596264600386295
Loss in iteration 23 : 0.34149947717378765
Loss in iteration 24 : 0.33616307936251816
Loss in iteration 25 : 0.3319442803134756
Loss in iteration 26 : 0.33006130516108506
Loss in iteration 27 : 0.3305648259306308
Loss in iteration 28 : 0.3324997204354907
Loss in iteration 29 : 0.3344948075356975
Loss in iteration 30 : 0.33545411231332356
Loss in iteration 31 : 0.33499940838476755
Loss in iteration 32 : 0.3334892146488574
Loss in iteration 33 : 0.3316903560117093
Loss in iteration 34 : 0.33033944704857154
Loss in iteration 35 : 0.329827205850979
Loss in iteration 36 : 0.33011471203631354
Loss in iteration 37 : 0.3308517890786736
Loss in iteration 38 : 0.33159022013062345
Loss in iteration 39 : 0.33198121733740005
Loss in iteration 40 : 0.33188759810710095
Loss in iteration 41 : 0.33139173969955155
Loss in iteration 42 : 0.3307202948212619
Loss in iteration 43 : 0.33012922994251764
Loss in iteration 44 : 0.3297978444097023
Loss in iteration 45 : 0.3297699816230345
Loss in iteration 46 : 0.3299585332724715
Loss in iteration 47 : 0.3302028387567765
Loss in iteration 48 : 0.3303479036235873
Loss in iteration 49 : 0.3303085520205878
Loss in iteration 50 : 0.3300930076329673
Loss in iteration 51 : 0.32978228599023623
Loss in iteration 52 : 0.3294822460008648
Loss in iteration 53 : 0.3292743061818384
Loss in iteration 54 : 0.3291862611082153
Loss in iteration 55 : 0.32919139205901493
Loss in iteration 56 : 0.3292302757939806
Loss in iteration 57 : 0.3292415663716302
Loss in iteration 58 : 0.3291874557273249
Loss in iteration 59 : 0.329064708901702
Loss in iteration 60 : 0.3288996675438512
Loss in iteration 61 : 0.3287321518464155
Loss in iteration 62 : 0.32859665152094897
Loss in iteration 63 : 0.32850889392767835
Loss in iteration 64 : 0.3284624681679925
Loss in iteration 65 : 0.3284353477642284
Loss in iteration 66 : 0.3284019962452902
Loss in iteration 67 : 0.32834495784240053
Loss in iteration 68 : 0.32826096929547016
Loss in iteration 69 : 0.3281598266114515
Loss in iteration 70 : 0.32805775442136487
Loss in iteration 71 : 0.32796917329789677
Loss in iteration 72 : 0.32790077020040725
Loss in iteration 73 : 0.327850032100659
Loss in iteration 74 : 0.3278080361304071
Loss in iteration 75 : 0.3277644826047512
Loss in iteration 76 : 0.32771238146594844
Loss in iteration 77 : 0.3276504457971606
Loss in iteration 78 : 0.327582586991654
Loss in iteration 79 : 0.3275152505080337
Loss in iteration 80 : 0.3274541230408071
Loss in iteration 81 : 0.32740174953723017
Loss in iteration 82 : 0.3273569463366767
Loss in iteration 83 : 0.32731596406007074
Loss in iteration 84 : 0.32727458748819965
Loss in iteration 85 : 0.3272300750236126
Loss in iteration 86 : 0.3271820950955932
Loss in iteration 87 : 0.3271324132267641
Loss in iteration 88 : 0.3270836928627419
Loss in iteration 89 : 0.3270381113305966
Loss in iteration 90 : 0.32699644970548647
Loss in iteration 91 : 0.32695798002184456
Loss in iteration 92 : 0.32692105319506426
Loss in iteration 93 : 0.3268839933581109
Loss in iteration 94 : 0.3268458399346887
Loss in iteration 95 : 0.32680663189691633
Loss in iteration 96 : 0.3267671899750214
Loss in iteration 97 : 0.32672858600787524
Loss in iteration 98 : 0.32669159553757254
Loss in iteration 99 : 0.3266563848172457
Loss in iteration 100 : 0.326622532864467
Loss in iteration 101 : 0.3265893188425354
Loss in iteration 102 : 0.3265560957553141
Loss in iteration 103 : 0.3265225631946336
Loss in iteration 104 : 0.32648883155195696
Loss in iteration 105 : 0.32645528573093047
Loss in iteration 106 : 0.3264223478264824
Loss in iteration 107 : 0.3263902670154283
Loss in iteration 108 : 0.32635902883263473
Loss in iteration 109 : 0.3263284037348902
Loss in iteration 110 : 0.3262980866883628
Loss in iteration 111 : 0.32626784631793143
Loss in iteration 112 : 0.3262376127771887
Loss in iteration 113 : 0.3262074748971078
Loss in iteration 114 : 0.326177604579637
Loss in iteration 115 : 0.326148156856376
Loss in iteration 116 : 0.3261191965348365
Loss in iteration 117 : 0.3260906807536748
Loss in iteration 118 : 0.3260624953799912
Loss in iteration 119 : 0.32603451837134295
Loss in iteration 120 : 0.326006675445138
Loss in iteration 121 : 0.3259789632948331
Loss in iteration 122 : 0.3259514353663823
Loss in iteration 123 : 0.32592416375826366
Loss in iteration 124 : 0.32589719958248703
Loss in iteration 125 : 0.32587055074379384
Loss in iteration 126 : 0.32584418437962065
Loss in iteration 127 : 0.32581804822564714
Loss in iteration 128 : 0.32579209729745273
Loss in iteration 129 : 0.3257663122845963
Loss in iteration 130 : 0.32574070248279435
Loss in iteration 131 : 0.3257152947255155
Loss in iteration 132 : 0.3256901161355931
Loss in iteration 133 : 0.32566518001042727
Loss in iteration 134 : 0.32564048095949166
Loss in iteration 135 : 0.3256159998949159
Loss in iteration 136 : 0.3255917146767519
Loss in iteration 137 : 0.3255676102891523
Loss in iteration 138 : 0.32554368377126613
Loss in iteration 139 : 0.32551994251068145
Loss in iteration 140 : 0.3254963979386747
Loss in iteration 141 : 0.32547305849256425
Loss in iteration 142 : 0.32544992534887585
Loss in iteration 143 : 0.32542699245590334
Loss in iteration 144 : 0.32540425004973766
Loss in iteration 145 : 0.32538168931718775
Loss in iteration 146 : 0.3253593057505641
Loss in iteration 147 : 0.32533709982201997
Loss in iteration 148 : 0.3253150751480652
Loss in iteration 149 : 0.3252932354917713
Loss in iteration 150 : 0.32527158226195924
Loss in iteration 151 : 0.32525011362327205
Loss in iteration 152 : 0.32522882534807523
Loss in iteration 153 : 0.32520771267512
Loss in iteration 154 : 0.3251867720881493
Loss in iteration 155 : 0.3251660021668109
Loss in iteration 156 : 0.3251454032684385
Loss in iteration 157 : 0.32512497641132837
Loss in iteration 158 : 0.3251047220496912
Loss in iteration 159 : 0.32508463935568516
Loss in iteration 160 : 0.3250647262635343
Loss in iteration 161 : 0.3250449801126039
Loss in iteration 162 : 0.32502539846463574
Loss in iteration 163 : 0.3250059796640189
Loss in iteration 164 : 0.32498672291506797
Loss in iteration 165 : 0.324967627928015
Loss in iteration 166 : 0.3249486943867137
Loss in iteration 167 : 0.3249299215316139
Loss in iteration 168 : 0.3249113080409465
Loss in iteration 169 : 0.3248928522131041
Loss in iteration 170 : 0.32487455230488316
Loss in iteration 171 : 0.3248564068304995
Loss in iteration 172 : 0.32483841468160063
Loss in iteration 173 : 0.3248205750423742
Loss in iteration 174 : 0.32480288717954037
Loss in iteration 175 : 0.3247853502339675
Loss in iteration 176 : 0.32476796311642114
Loss in iteration 177 : 0.32475072453959714
Loss in iteration 178 : 0.3247336331453244
Loss in iteration 179 : 0.32471668764632783
Loss in iteration 180 : 0.32469988690967044
Loss in iteration 181 : 0.3246832299513077
Loss in iteration 182 : 0.32466671586096707
Loss in iteration 183 : 0.32465034370754065
Loss in iteration 184 : 0.32463411247562557
Loss in iteration 185 : 0.3246180210590029
Loss in iteration 186 : 0.3246020683037493
Loss in iteration 187 : 0.3245862530703158
Loss in iteration 188 : 0.3245705742800854
Loss in iteration 189 : 0.3245550309260678
Loss in iteration 190 : 0.3245396220490136
Loss in iteration 191 : 0.32452434669730124
Loss in iteration 192 : 0.3245092038936595
Loss in iteration 193 : 0.3244941926241105
Loss in iteration 194 : 0.3244793118505362
Loss in iteration 195 : 0.32446456053617073
Loss in iteration 196 : 0.3244499376688048
Loss in iteration 197 : 0.3244354422704435
Loss in iteration 198 : 0.3244210733910699
Loss in iteration 199 : 0.3244068300925749
Loss in iteration 200 : 0.32439271143277837
Testing accuracy  of updater 7 on alg 0 with rate 0.02 = 0.78725, training accuracy 0.8429912593072192, time elapsed: 2153 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5956662228278868
Loss in iteration 3 : 0.5184441929590073
Loss in iteration 4 : 0.5019715629803556
Loss in iteration 5 : 0.5141553241058939
Loss in iteration 6 : 0.5252826427644856
Loss in iteration 7 : 0.5218629226830122
Loss in iteration 8 : 0.5013955855557668
Loss in iteration 9 : 0.468106507534794
Loss in iteration 10 : 0.4305428897226751
Loss in iteration 11 : 0.39905626176135833
Loss in iteration 12 : 0.38169350217443987
Loss in iteration 13 : 0.37963662205842913
Loss in iteration 14 : 0.38621030103810366
Loss in iteration 15 : 0.39152465742291126
Loss in iteration 16 : 0.38902639946207485
Loss in iteration 17 : 0.3785351713523202
Loss in iteration 18 : 0.3644656988319368
Loss in iteration 19 : 0.3519851535815986
Loss in iteration 20 : 0.3441959217588081
Loss in iteration 21 : 0.34145606751921764
Loss in iteration 22 : 0.3422141976621363
Loss in iteration 23 : 0.3442697373435121
Loss in iteration 24 : 0.3457518506685675
Loss in iteration 25 : 0.345609740294673
Loss in iteration 26 : 0.34368439776277476
Loss in iteration 27 : 0.34050795560130903
Loss in iteration 28 : 0.3369673638097514
Loss in iteration 29 : 0.333946028344361
Loss in iteration 30 : 0.33203717909855124
Loss in iteration 31 : 0.33139671430806694
Loss in iteration 32 : 0.33176169277002976
Loss in iteration 33 : 0.3326076870744093
Loss in iteration 34 : 0.33337218248469236
Loss in iteration 35 : 0.3336538062231945
Loss in iteration 36 : 0.33331780379024395
Loss in iteration 37 : 0.33248565258765417
Loss in iteration 38 : 0.33143605847319896
Loss in iteration 39 : 0.33047306942603744
Loss in iteration 40 : 0.3298164118040415
Loss in iteration 41 : 0.3295468826005055
Loss in iteration 42 : 0.3296110553398721
Loss in iteration 43 : 0.32986803238146983
Loss in iteration 44 : 0.3301522603485895
Loss in iteration 45 : 0.33032900529155435
Loss in iteration 46 : 0.3303279197772728
Loss in iteration 47 : 0.3301502798580021
Loss in iteration 48 : 0.3298537753406168
Loss in iteration 49 : 0.3295238576822548
Loss in iteration 50 : 0.32924239200558925
Loss in iteration 51 : 0.3290631187542226
Loss in iteration 52 : 0.3289999582991938
Loss in iteration 53 : 0.3290295610334486
Loss in iteration 54 : 0.32910503322558643
Loss in iteration 55 : 0.32917474646832673
Loss in iteration 56 : 0.3291994093936603
Loss in iteration 57 : 0.3291621852572637
Loss in iteration 58 : 0.32906977319434105
Loss in iteration 59 : 0.3289457302817885
Loss in iteration 60 : 0.3288196634706847
Loss in iteration 61 : 0.32871659859276625
Loss in iteration 62 : 0.328649917454854
Loss in iteration 63 : 0.32861939438421633
Loss in iteration 64 : 0.32861390467089857
Loss in iteration 65 : 0.32861699319827503
Loss in iteration 66 : 0.3286129901075989
Loss in iteration 67 : 0.3285916834734775
Loss in iteration 68 : 0.32855041036992216
Loss in iteration 69 : 0.32849343396939734
Loss in iteration 70 : 0.3284293124305669
Loss in iteration 71 : 0.32836743336732555
Loss in iteration 72 : 0.32831492501918963
Loss in iteration 73 : 0.32827482767852884
Loss in iteration 74 : 0.3282458712140684
Loss in iteration 75 : 0.3282236485055002
Loss in iteration 76 : 0.32820257226225313
Loss in iteration 77 : 0.32817785732448185
Loss in iteration 78 : 0.32814689086075394
Loss in iteration 79 : 0.32810965972242956
Loss in iteration 80 : 0.3280682672855253
Loss in iteration 81 : 0.32802586164539355
Loss in iteration 82 : 0.32798542923348534
Loss in iteration 83 : 0.32794886708056414
Loss in iteration 84 : 0.3279165789372405
Loss in iteration 85 : 0.3278876251043678
Loss in iteration 86 : 0.3278602727029521
Loss in iteration 87 : 0.3278326948804503
Loss in iteration 88 : 0.3278035684025788
Loss in iteration 89 : 0.3277723990455029
Loss in iteration 90 : 0.3277395224762118
Loss in iteration 91 : 0.3277058405991977
Loss in iteration 92 : 0.3276724259151349
Loss in iteration 93 : 0.3276401440823838
Loss in iteration 94 : 0.3276094119809231
Loss in iteration 95 : 0.3275801443391349
Loss in iteration 96 : 0.3275518720878339
Loss in iteration 97 : 0.3275239632833202
Loss in iteration 98 : 0.32749585653828495
Loss in iteration 99 : 0.3274672288749311
Loss in iteration 100 : 0.32743805525901015
Loss in iteration 101 : 0.32740856015772196
Loss in iteration 102 : 0.32737909673534965
Loss in iteration 103 : 0.3273500064940255
Loss in iteration 104 : 0.3273215088602535
Loss in iteration 105 : 0.32729365125406323
Loss in iteration 106 : 0.32726632470895706
Loss in iteration 107 : 0.32723932772818254
Loss in iteration 108 : 0.3272124485257506
Loss in iteration 109 : 0.3271855353547059
Loss in iteration 110 : 0.3271585341442621
Loss in iteration 111 : 0.32713148719069196
Loss in iteration 112 : 0.3271045005251133
Loss in iteration 113 : 0.32707769642425155
Loss in iteration 114 : 0.3270511693817455
Loss in iteration 115 : 0.32702495938009407
Loss in iteration 116 : 0.3269990480965802
Loss in iteration 117 : 0.3269733750959924
Loss in iteration 118 : 0.3269478649973104
Loss in iteration 119 : 0.32692245460631425
Loss in iteration 120 : 0.3268971110189502
Loss in iteration 121 : 0.3268718363385257
Loss in iteration 122 : 0.3268466598989505
Loss in iteration 123 : 0.326821622884555
Loss in iteration 124 : 0.32679676187443174
Loss in iteration 125 : 0.3267720969971025
Loss in iteration 126 : 0.3267476277943454
Loss in iteration 127 : 0.3267233367465583
Loss in iteration 128 : 0.3266991978614087
Loss in iteration 129 : 0.3266751865155001
Loss in iteration 130 : 0.326651287023698
Loss in iteration 131 : 0.32662749582170725
Loss in iteration 132 : 0.3266038200059812
Loss in iteration 133 : 0.3265802725778948
Loss in iteration 134 : 0.3265568665969749
Loss in iteration 135 : 0.3265336104094778
Loss in iteration 136 : 0.3265105053595784
Loss in iteration 137 : 0.3264875462965525
Loss in iteration 138 : 0.3264647241911103
Loss in iteration 139 : 0.3264420295877655
Loss in iteration 140 : 0.3264194555707838
Loss in iteration 141 : 0.32639699932849087
Loss in iteration 142 : 0.3263746620437908
Loss in iteration 143 : 0.32635244745670533
Loss in iteration 144 : 0.32633035983223035
Loss in iteration 145 : 0.32630840213402335
Loss in iteration 146 : 0.3262865749869744
Loss in iteration 147 : 0.3262648766356424
Loss in iteration 148 : 0.32624330372802507
Loss in iteration 149 : 0.32622185250415364
Loss in iteration 150 : 0.32620051990841903
Loss in iteration 151 : 0.32617930425931924
Loss in iteration 152 : 0.3261582053291688
Loss in iteration 153 : 0.32613722391536476
Loss in iteration 154 : 0.3261163611439494
Loss in iteration 155 : 0.3260956177936516
Loss in iteration 156 : 0.3260749938685579
Loss in iteration 157 : 0.3260544885201781
Loss in iteration 158 : 0.32603410028098107
Loss in iteration 159 : 0.32601382747123164
Loss in iteration 160 : 0.3259936686066476
Loss in iteration 161 : 0.3259736226658524
Loss in iteration 162 : 0.3259536891509843
Loss in iteration 163 : 0.32593386795861307
Loss in iteration 164 : 0.325914159140601
Loss in iteration 165 : 0.325894562657992
Loss in iteration 166 : 0.32587507821447115
Loss in iteration 167 : 0.32585570521233
Loss in iteration 168 : 0.32583644282346147
Loss in iteration 169 : 0.32581729012928645
Loss in iteration 170 : 0.32579824626804094
Loss in iteration 171 : 0.3257793105366609
Loss in iteration 172 : 0.32576048242005645
Loss in iteration 173 : 0.32574176155091034
Loss in iteration 174 : 0.32572314762684373
Loss in iteration 175 : 0.32570464032173035
Loss in iteration 176 : 0.3256862392232253
Loss in iteration 177 : 0.325667943813489
Loss in iteration 178 : 0.32564975349182873
Loss in iteration 179 : 0.3256316676234884
Loss in iteration 180 : 0.32561368559258225
Loss in iteration 181 : 0.32559580683972356
Loss in iteration 182 : 0.32557803087390225
Loss in iteration 183 : 0.32556035725914956
Loss in iteration 184 : 0.3255427855853193
Loss in iteration 185 : 0.32552531543620017
Loss in iteration 186 : 0.325507946366688
Loss in iteration 187 : 0.32549067789537406
Loss in iteration 188 : 0.32547350951228854
Loss in iteration 189 : 0.3254564406962173
Loss in iteration 190 : 0.3254394709336483
Loss in iteration 191 : 0.32542259973229953
Loss in iteration 192 : 0.32540582662539197
Loss in iteration 193 : 0.3253891511668501
Loss in iteration 194 : 0.32537257292079164
Loss in iteration 195 : 0.32535609145008926
Loss in iteration 196 : 0.3253397063082492
Loss in iteration 197 : 0.32532341703685713
Loss in iteration 198 : 0.32530722316847965
Loss in iteration 199 : 0.32529112423295053
Loss in iteration 200 : 0.32527511976414547
Testing accuracy  of updater 7 on alg 0 with rate 0.014 = 0.7855, training accuracy 0.8400776950469407, time elapsed: 2688 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6322867784213156
Loss in iteration 3 : 0.55964495903368
Loss in iteration 4 : 0.51471797433318
Loss in iteration 5 : 0.5010010641281456
Loss in iteration 6 : 0.5045144738260927
Loss in iteration 7 : 0.5119870605367175
Loss in iteration 8 : 0.5153861469894309
Loss in iteration 9 : 0.5111318809539391
Loss in iteration 10 : 0.49867508159239077
Loss in iteration 11 : 0.4794773337903896
Loss in iteration 12 : 0.456322318917444
Loss in iteration 13 : 0.43272405880493586
Loss in iteration 14 : 0.41222291571260583
Loss in iteration 15 : 0.39751739754311943
Loss in iteration 16 : 0.3896422598546926
Loss in iteration 17 : 0.3876148433799716
Loss in iteration 18 : 0.3888680548168266
Loss in iteration 19 : 0.3903413958882392
Loss in iteration 20 : 0.3896736089854077
Loss in iteration 21 : 0.3859121122199244
Loss in iteration 22 : 0.37950926742509095
Loss in iteration 23 : 0.3717863631908041
Loss in iteration 24 : 0.36424015158440975
Loss in iteration 25 : 0.3580090312180006
Loss in iteration 26 : 0.3536342075516362
Loss in iteration 27 : 0.3510857119883827
Loss in iteration 28 : 0.34994216521053195
Loss in iteration 29 : 0.3496113431255007
Loss in iteration 30 : 0.3495169704676238
Loss in iteration 31 : 0.34922000084439314
Loss in iteration 32 : 0.3484724954261003
Loss in iteration 33 : 0.34721703830106837
Loss in iteration 34 : 0.345549179745227
Loss in iteration 35 : 0.3436599213355237
Loss in iteration 36 : 0.3417730145489925
Loss in iteration 37 : 0.34008908649111125
Loss in iteration 38 : 0.33874541217856263
Loss in iteration 39 : 0.33779632019281175
Loss in iteration 40 : 0.33721485198144907
Loss in iteration 41 : 0.3369119954792796
Loss in iteration 42 : 0.33676650329017654
Loss in iteration 43 : 0.33665682814439385
Loss in iteration 44 : 0.3364874212539841
Loss in iteration 45 : 0.3362042136694218
Loss in iteration 46 : 0.33579763356892495
Loss in iteration 47 : 0.3352948849686781
Loss in iteration 48 : 0.3347455091713295
Loss in iteration 49 : 0.334205038342498
Loss in iteration 50 : 0.33372093786259427
Loss in iteration 51 : 0.33332351339522975
Loss in iteration 52 : 0.33302265857892327
Loss in iteration 53 : 0.33280977503235415
Loss in iteration 54 : 0.3326632184197102
Loss in iteration 55 : 0.3325552808808973
Loss in iteration 56 : 0.33245890044672327
Loss in iteration 57 : 0.3323527952664682
Loss in iteration 58 : 0.3322243521179103
Loss in iteration 59 : 0.33207019512603536
Loss in iteration 60 : 0.3318948217342818
Loss in iteration 61 : 0.33170797377232086
Loss in iteration 62 : 0.33152150901020727
Loss in iteration 63 : 0.3313464782237946
Loss in iteration 64 : 0.3311909370744109
Loss in iteration 65 : 0.33105878231927544
Loss in iteration 66 : 0.3309496507809134
Loss in iteration 67 : 0.33085970427213085
Loss in iteration 68 : 0.3307829791307855
Loss in iteration 69 : 0.33071292253694284
Loss in iteration 70 : 0.3306437672770837
Loss in iteration 71 : 0.3305714926518388
Loss in iteration 72 : 0.33049425096438173
Loss in iteration 73 : 0.3304122724800274
Loss in iteration 74 : 0.33032736795131895
Loss in iteration 75 : 0.3302422087796073
Loss in iteration 76 : 0.3301595759987674
Loss in iteration 77 : 0.3300817378844422
Loss in iteration 78 : 0.33001005742765704
Loss in iteration 79 : 0.3299448632634769
Loss in iteration 80 : 0.32988555708910977
Loss in iteration 81 : 0.3298308882283455
Loss in iteration 82 : 0.3297793065718654
Loss in iteration 83 : 0.3297293075948621
Loss in iteration 84 : 0.32967970227864757
Loss in iteration 85 : 0.3296297731775056
Loss in iteration 86 : 0.32957930805113034
Loss in iteration 87 : 0.32952852822017775
Loss in iteration 88 : 0.3294779460673782
Loss in iteration 89 : 0.32942819330261325
Loss in iteration 90 : 0.32937985934550845
Loss in iteration 91 : 0.3293333697098796
Loss in iteration 92 : 0.3292889207777823
Loss in iteration 93 : 0.32924647317623634
Loss in iteration 94 : 0.32920579398951244
Loss in iteration 95 : 0.32916653019420655
Loss in iteration 96 : 0.32912829278809247
Loss in iteration 97 : 0.32909073278418716
Loss in iteration 98 : 0.32905359538454654
Loss in iteration 99 : 0.3290167455783067
Loss in iteration 100 : 0.32898016540456865
Loss in iteration 101 : 0.3289439287593047
Loss in iteration 102 : 0.32890816301950143
Loss in iteration 103 : 0.32887300764250604
Loss in iteration 104 : 0.32883857857091203
Loss in iteration 105 : 0.32880494442465696
Loss in iteration 106 : 0.3287721169597438
Loss in iteration 107 : 0.328740054948583
Loss in iteration 108 : 0.3287086781229647
Loss in iteration 109 : 0.3286778864622173
Loss in iteration 110 : 0.3286475799520939
Loss in iteration 111 : 0.3286176747815901
Loss in iteration 112 : 0.3285881134245559
Loss in iteration 113 : 0.3285588677543881
Loss in iteration 114 : 0.328529935884752
Loss in iteration 115 : 0.32850133454255637
Loss in iteration 116 : 0.32847308932424457
Loss in iteration 117 : 0.3284452251631976
Loss in iteration 118 : 0.3284177588564045
Loss in iteration 119 : 0.32839069474277194
Loss in iteration 120 : 0.3283640237944788
Loss in iteration 121 : 0.3283377256550832
Loss in iteration 122 : 0.3283117726587546
Loss in iteration 123 : 0.3282861346509988
Loss in iteration 124 : 0.3282607834922293
Loss in iteration 125 : 0.3282356963984987
Loss in iteration 126 : 0.32821085766480185
Loss in iteration 127 : 0.3281862587254098
Loss in iteration 128 : 0.32816189684748237
Loss in iteration 129 : 0.32813777297268903
Loss in iteration 130 : 0.3281138892959591
Loss in iteration 131 : 0.3280902471127453
Loss in iteration 132 : 0.32806684531275476
Loss in iteration 133 : 0.328043679697572
Loss in iteration 134 : 0.32802074310123835
Loss in iteration 135 : 0.32799802613587803
Loss in iteration 136 : 0.32797551829271676
Loss in iteration 137 : 0.3279532091083614
Loss in iteration 138 : 0.3279310891477335
Loss in iteration 139 : 0.32790915063866244
Loss in iteration 140 : 0.32788738769410064
Loss in iteration 141 : 0.32786579615284817
Loss in iteration 142 : 0.3278443731406277
Loss in iteration 143 : 0.32782311649055906
Loss in iteration 144 : 0.32780202416448095
Loss in iteration 145 : 0.32778109379000847
Loss in iteration 146 : 0.32776032238358627
Loss in iteration 147 : 0.32773970627923626
Loss in iteration 148 : 0.3277192412376674
Loss in iteration 149 : 0.3276989226787798
Loss in iteration 150 : 0.32767874596637
Loss in iteration 151 : 0.32765870667661773
Loss in iteration 152 : 0.32763880079800706
Loss in iteration 153 : 0.3276190248340179
Loss in iteration 154 : 0.3275993758049876
Loss in iteration 155 : 0.3275798511666876
Loss in iteration 156 : 0.3275604486768193
Loss in iteration 157 : 0.3275411662454157
Loss in iteration 158 : 0.32752200180174024
Loss in iteration 159 : 0.3275029532008743
Loss in iteration 160 : 0.3274840181808487
Loss in iteration 161 : 0.3274651943688903
Loss in iteration 162 : 0.3274464793256793
Loss in iteration 163 : 0.3274278706108543
Loss in iteration 164 : 0.3274093658518513
Loss in iteration 165 : 0.3273909628008182
Loss in iteration 166 : 0.32737265936966603
Loss in iteration 167 : 0.32735445363964355
Loss in iteration 168 : 0.32733634384770655
Loss in iteration 169 : 0.32731832835632485
Loss in iteration 170 : 0.3273004056154882
Loss in iteration 171 : 0.32728257412572037
Loss in iteration 172 : 0.3272648324090314
Loss in iteration 173 : 0.32724717899187095
Loss in iteration 174 : 0.32722961240094994
Loss in iteration 175 : 0.3272121311699876
Loss in iteration 176 : 0.32719473385360653
Loss in iteration 177 : 0.3271774190438166
Loss in iteration 178 : 0.32716018538487857
Loss in iteration 179 : 0.3271430315834516
Loss in iteration 180 : 0.32712595641249365
Loss in iteration 181 : 0.32710895870897133
Loss in iteration 182 : 0.3270920373666953
Loss in iteration 183 : 0.32707519132640406
Loss in iteration 184 : 0.32705841956536286
Loss in iteration 185 : 0.3270417210884684
Loss in iteration 186 : 0.3270250949221615
Loss in iteration 187 : 0.3270085401116401
Loss in iteration 188 : 0.32699205572110324
Loss in iteration 189 : 0.3269756408361805
Loss in iteration 190 : 0.32695929456742456
Loss in iteration 191 : 0.32694301605372167
Loss in iteration 192 : 0.32692680446472877
Loss in iteration 193 : 0.3269106590018265
Loss in iteration 194 : 0.3268945788974485
Loss in iteration 195 : 0.3268785634130783
Loss in iteration 196 : 0.32686261183638915
Loss in iteration 197 : 0.3268467234781066
Loss in iteration 198 : 0.326830897669151
Loss in iteration 199 : 0.32681513375844534
Loss in iteration 200 : 0.32679943111156406
Testing accuracy  of updater 7 on alg 0 with rate 0.008 = 0.78525, training accuracy 0.8381353188734219, time elapsed: 3010 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.676563395880249
Testing accuracy  of updater 7 on alg 0 with rate 0.0019999999999999983 = 0.5, training accuracy 0.6474587245063127, time elapsed: 53 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 4.43367777299195
Loss in iteration 3 : 3.1867748329994554
Loss in iteration 4 : 1.5341342904508808
Loss in iteration 5 : 0.43024170991574134
Loss in iteration 6 : 0.7707109350394268
Loss in iteration 7 : 0.6508448898475265
Loss in iteration 8 : 0.5560562283419753
Loss in iteration 9 : 0.5442188727550202
Loss in iteration 10 : 0.560021659167801
Loss in iteration 11 : 0.5741289357093321
Loss in iteration 12 : 0.5796729979717384
Loss in iteration 13 : 0.5789949537976432
Loss in iteration 14 : 0.5754622665749077
Loss in iteration 15 : 0.5700892520706229
Loss in iteration 16 : 0.562155343462374
Loss in iteration 17 : 0.5511517143191627
Loss in iteration 18 : 0.5374443223843289
Loss in iteration 19 : 0.5216551539107356
Loss in iteration 20 : 0.5041586804992235
Loss in iteration 21 : 0.48517261348315743
Loss in iteration 22 : 0.4649972960252462
Loss in iteration 23 : 0.4440568258854123
Loss in iteration 24 : 0.4228268451276932
Loss in iteration 25 : 0.4018336945716414
Loss in iteration 26 : 0.38171698495616774
Loss in iteration 27 : 0.36326399679624294
Loss in iteration 28 : 0.3474161952891476
Loss in iteration 29 : 0.33525078857210616
Loss in iteration 30 : 0.3278543109065284
Loss in iteration 31 : 0.3259668539857496
Loss in iteration 32 : 0.3292969740537456
Loss in iteration 33 : 0.3357638260307986
Loss in iteration 34 : 0.3417262689075233
Loss in iteration 35 : 0.3440276415443259
Loss in iteration 36 : 0.34212103284677364
Loss in iteration 37 : 0.3377170175540596
Loss in iteration 38 : 0.3328895193779605
Loss in iteration 39 : 0.32891196755499946
Loss in iteration 40 : 0.3262310780203025
Loss in iteration 41 : 0.3247535841491648
Loss in iteration 42 : 0.3242046668176899
Loss in iteration 43 : 0.32422183949492617
Loss in iteration 44 : 0.3245244181183133
Loss in iteration 45 : 0.32487257678574516
Loss in iteration 46 : 0.3251323037791782
Loss in iteration 47 : 0.3252263335616403
Loss in iteration 48 : 0.3251459696135722
Loss in iteration 49 : 0.324920503977146
Loss in iteration 50 : 0.3246102437567069
Loss in iteration 51 : 0.3242931571667878
Loss in iteration 52 : 0.32405200169948045
Loss in iteration 53 : 0.3240196741240175
Loss in iteration 54 : 0.3243623238422506
Loss in iteration 55 : 0.3257946910251193
Loss in iteration 56 : 0.3293815535560803
Loss in iteration 57 : 0.3425815806035328
Loss in iteration 58 : 0.3682931348082212
Loss in iteration 59 : 0.48353029798961217
Loss in iteration 60 : 0.4587100412601427
Loss in iteration 61 : 0.6495930874601255
Loss in iteration 62 : 0.35420062369534205
Loss in iteration 63 : 0.33702315974406594
Loss in iteration 64 : 0.34109930289876134
Loss in iteration 65 : 0.3390577296217694
Loss in iteration 66 : 0.33797915044096083
Loss in iteration 67 : 0.33605880672990823
Loss in iteration 68 : 0.3335889627939295
Loss in iteration 69 : 0.3307763315626453
Loss in iteration 70 : 0.3280256281534137
Loss in iteration 71 : 0.3257026053738525
Loss in iteration 72 : 0.3241153783539851
Loss in iteration 73 : 0.32340315600898983
Loss in iteration 74 : 0.32362387568704926
Loss in iteration 75 : 0.32495160797542816
Loss in iteration 76 : 0.3288295790489498
Loss in iteration 77 : 0.34498382237183844
Loss in iteration 78 : 0.39688803766439545
Loss in iteration 79 : 0.6708677382739501
Loss in iteration 80 : 0.45534054597661944
Loss in iteration 81 : 0.5991426123502894
Loss in iteration 82 : 0.36548036787037286
Loss in iteration 83 : 0.3456105552292662
Loss in iteration 84 : 0.3485780011152807
Loss in iteration 85 : 0.34640133585257454
Loss in iteration 86 : 0.34543987902163265
Loss in iteration 87 : 0.34317710131122603
Loss in iteration 88 : 0.3398851642612333
Loss in iteration 89 : 0.3359265768887372
Loss in iteration 90 : 0.3318548572963744
Loss in iteration 91 : 0.32821370206459655
Loss in iteration 92 : 0.32547271832133085
Loss in iteration 93 : 0.3239091361596492
Loss in iteration 94 : 0.3235427294328246
Loss in iteration 95 : 0.3240761088168601
Loss in iteration 96 : 0.325080836848837
Loss in iteration 97 : 0.32636541393384233
Loss in iteration 98 : 0.32938175198790826
Loss in iteration 99 : 0.34422454928772
Loss in iteration 100 : 0.40036825181489577
Loss in iteration 101 : 0.7037282506317272
Loss in iteration 102 : 0.443354135908126
Loss in iteration 103 : 0.5529187439072015
Loss in iteration 104 : 0.37464723362249436
Loss in iteration 105 : 0.35476346582907875
Loss in iteration 106 : 0.3534532524883437
Loss in iteration 107 : 0.3500408295528403
Loss in iteration 108 : 0.348428963730954
Loss in iteration 109 : 0.34548350536605316
Loss in iteration 110 : 0.34152223626522904
Loss in iteration 111 : 0.3369287453302037
Loss in iteration 112 : 0.33233514441645073
Loss in iteration 113 : 0.3283467775282135
Loss in iteration 114 : 0.3254763413238154
Loss in iteration 115 : 0.32398783226820216
Loss in iteration 116 : 0.3238456856282362
Loss in iteration 117 : 0.32466719111297543
Loss in iteration 118 : 0.3261909683005768
Loss in iteration 119 : 0.3297463663722501
Loss in iteration 120 : 0.34442852956103887
Loss in iteration 121 : 0.4320454012079021
Loss in iteration 122 : 0.5800169253063913
Loss in iteration 123 : 1.34414476419081
Loss in iteration 124 : 0.4535472444356451
Loss in iteration 125 : 0.49464286210899966
Loss in iteration 126 : 0.3983845578386568
Loss in iteration 127 : 0.37842348689989685
Loss in iteration 128 : 0.3819236221889952
Loss in iteration 129 : 0.38185551159848735
Loss in iteration 130 : 0.37927582847611613
Loss in iteration 131 : 0.3739063858464966
Loss in iteration 132 : 0.3664221034073651
Loss in iteration 133 : 0.35760276557322346
Loss in iteration 134 : 0.34832426895295504
Loss in iteration 135 : 0.3395326179618601
Loss in iteration 136 : 0.3321826590807862
Loss in iteration 137 : 0.3271063605782103
Loss in iteration 138 : 0.3247830507154456
Loss in iteration 139 : 0.3250500367911061
Loss in iteration 140 : 0.3269488222516409
Loss in iteration 141 : 0.3290161397994779
Loss in iteration 142 : 0.3300375139803716
Loss in iteration 143 : 0.329668482936751
Loss in iteration 144 : 0.3283501138128148
Loss in iteration 145 : 0.32676053215082856
Loss in iteration 146 : 0.3253907893355733
Loss in iteration 147 : 0.32444955917646895
Loss in iteration 148 : 0.32393543967024463
Loss in iteration 149 : 0.3237659889165928
Loss in iteration 150 : 0.32385076370086585
Loss in iteration 151 : 0.3242040198880867
Loss in iteration 152 : 0.3251087880350708
Loss in iteration 153 : 0.3272042470733563
Loss in iteration 154 : 0.33387803871150085
Loss in iteration 155 : 0.3469862529358779
Loss in iteration 156 : 0.40147731760158806
Loss in iteration 157 : 0.4325387015909602
Loss in iteration 158 : 0.6721725089412909
Loss in iteration 159 : 0.3899984994297612
Loss in iteration 160 : 0.39172868343827866
Loss in iteration 161 : 0.3717276641201677
Loss in iteration 162 : 0.3679719086025671
Loss in iteration 163 : 0.35264568251345246
Loss in iteration 164 : 0.3474656050624456
Loss in iteration 165 : 0.34181820952701153
Loss in iteration 166 : 0.3385264435540629
Loss in iteration 167 : 0.3353283671177638
Loss in iteration 168 : 0.33639060801712306
Loss in iteration 169 : 0.3402627948128612
Loss in iteration 170 : 0.36882971036552453
Loss in iteration 171 : 0.4100635079640662
Loss in iteration 172 : 0.6746848512858163
Loss in iteration 173 : 0.4423157883373787
Loss in iteration 174 : 0.5906722352277475
Loss in iteration 175 : 0.3895409033115044
Loss in iteration 176 : 0.3786386228116541
Loss in iteration 177 : 0.36120179748549025
Loss in iteration 178 : 0.352886061605393
Loss in iteration 179 : 0.34626889637803854
Loss in iteration 180 : 0.3415419821406086
Loss in iteration 181 : 0.33730804953066396
Loss in iteration 182 : 0.33335834643461965
Loss in iteration 183 : 0.32998999058476913
Loss in iteration 184 : 0.3280584882809289
Loss in iteration 185 : 0.3283178155842874
Loss in iteration 186 : 0.33542858785945023
Loss in iteration 187 : 0.35618863105667686
Loss in iteration 188 : 0.4693422395534795
Loss in iteration 189 : 0.5402048511676915
Loss in iteration 190 : 1.1714114054912632
Loss in iteration 191 : 0.36542969375243983
Loss in iteration 192 : 0.47454685165167976
Loss in iteration 193 : 0.4362272402112218
Loss in iteration 194 : 0.3645574793386202
Loss in iteration 195 : 0.364099339553838
Loss in iteration 196 : 0.3628945714503566
Loss in iteration 197 : 0.359780381991829
Loss in iteration 198 : 0.3546567773466336
Loss in iteration 199 : 0.34823654681752764
Loss in iteration 200 : 0.34132498526300203
Testing accuracy  of updater 8 on alg 0 with rate 0.08000000000000002 = 0.79275, training accuracy 0.8449336354807381, time elapsed: 2486 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 2.062875617644325
Loss in iteration 3 : 1.599630701503695
Loss in iteration 4 : 0.679758266935935
Loss in iteration 5 : 0.4198513575339323
Loss in iteration 6 : 0.5530628677808501
Loss in iteration 7 : 0.4831171422987258
Loss in iteration 8 : 0.4720010423312547
Loss in iteration 9 : 0.48491045004558053
Loss in iteration 10 : 0.49679907582471894
Loss in iteration 11 : 0.5021748343687119
Loss in iteration 12 : 0.502703638983875
Loss in iteration 13 : 0.5001465541895039
Loss in iteration 14 : 0.4947575025777058
Loss in iteration 15 : 0.4864196570211819
Loss in iteration 16 : 0.47545752949927556
Loss in iteration 17 : 0.46242001105993946
Loss in iteration 18 : 0.4477726837287535
Loss in iteration 19 : 0.4319252885867405
Loss in iteration 20 : 0.4153391755032977
Loss in iteration 21 : 0.39853799259927986
Loss in iteration 22 : 0.3820876583755493
Loss in iteration 23 : 0.3666077811200146
Loss in iteration 24 : 0.35278172118557366
Loss in iteration 25 : 0.34133270619523604
Loss in iteration 26 : 0.332954567820728
Loss in iteration 27 : 0.32816415680563826
Loss in iteration 28 : 0.32704954805083153
Loss in iteration 29 : 0.32897218227792935
Loss in iteration 30 : 0.33245866169855093
Loss in iteration 31 : 0.33561501966173196
Loss in iteration 32 : 0.33701077430204485
Loss in iteration 33 : 0.3363275870296765
Loss in iteration 34 : 0.334193757599
Loss in iteration 35 : 0.3315237221231459
Loss in iteration 36 : 0.329017278913277
Loss in iteration 37 : 0.3270287479702874
Loss in iteration 38 : 0.325645034442782
Loss in iteration 39 : 0.32480196822586105
Loss in iteration 40 : 0.324371977799844
Loss in iteration 41 : 0.3242166729480019
Loss in iteration 42 : 0.3242139235133027
Loss in iteration 43 : 0.3242692201079006
Loss in iteration 44 : 0.324318075461542
Loss in iteration 45 : 0.3243236643802459
Loss in iteration 46 : 0.32427206931785457
Loss in iteration 47 : 0.32416644424977975
Loss in iteration 48 : 0.32402088919699784
Loss in iteration 49 : 0.32385463259950553
Loss in iteration 50 : 0.3236870423854087
Loss in iteration 51 : 0.32353390713540153
Loss in iteration 52 : 0.32340528437309785
Loss in iteration 53 : 0.3233049884279062
Loss in iteration 54 : 0.32323150876821116
Loss in iteration 55 : 0.3231798770932026
Loss in iteration 56 : 0.3231438323745237
Loss in iteration 57 : 0.3231176480101944
Loss in iteration 58 : 0.3230971972001522
Loss in iteration 59 : 0.32308016240061094
Loss in iteration 60 : 0.32306560337534873
Loss in iteration 61 : 0.32305326222086006
Loss in iteration 62 : 0.3230429610616573
Loss in iteration 63 : 0.3230342919380159
Loss in iteration 64 : 0.32302661179288383
Loss in iteration 65 : 0.3230192270001357
Loss in iteration 66 : 0.3230116157793164
Loss in iteration 67 : 0.3230035744832608
Loss in iteration 68 : 0.3229952419299233
Loss in iteration 69 : 0.32298701623076836
Loss in iteration 70 : 0.32297941177560907
Loss in iteration 71 : 0.3229729095008656
Loss in iteration 72 : 0.32296784079409585
Loss in iteration 73 : 0.3229643256793916
Loss in iteration 74 : 0.3229622675776048
Loss in iteration 75 : 0.3229613940083855
Loss in iteration 76 : 0.3229613258638994
Loss in iteration 77 : 0.322961656348927
Loss in iteration 78 : 0.322962022842523
Testing accuracy  of updater 8 on alg 0 with rate 0.056 = 0.7925, training accuracy 0.8449336354807381, time elapsed: 939 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.2579354576592092
Loss in iteration 3 : 0.989100555188195
Loss in iteration 4 : 0.5048410488873581
Loss in iteration 5 : 0.34657990090330715
Loss in iteration 6 : 0.4200506614684029
Loss in iteration 7 : 0.39776343621867144
Loss in iteration 8 : 0.3830091586045545
Loss in iteration 9 : 0.3873156460521637
Loss in iteration 10 : 0.3962341960308788
Loss in iteration 11 : 0.4026534506948477
Loss in iteration 12 : 0.40581216499495926
Loss in iteration 13 : 0.40695238455277655
Loss in iteration 14 : 0.4068391049710916
Loss in iteration 15 : 0.40545598844837155
Loss in iteration 16 : 0.40266052733654456
Loss in iteration 17 : 0.398575248010284
Loss in iteration 18 : 0.39348351233417006
Loss in iteration 19 : 0.38763665048161555
Loss in iteration 20 : 0.3812210855300712
Loss in iteration 21 : 0.37441697207141533
Loss in iteration 22 : 0.3674283861428036
Loss in iteration 23 : 0.3604684422723663
Loss in iteration 24 : 0.3537415329280621
Loss in iteration 25 : 0.3474418228355843
Loss in iteration 26 : 0.34175377162369397
Loss in iteration 27 : 0.33684314102003393
Loss in iteration 28 : 0.3328425491587358
Loss in iteration 29 : 0.32983625971612857
Loss in iteration 30 : 0.32784415353469626
Loss in iteration 31 : 0.3268075201409383
Loss in iteration 32 : 0.32658329388685925
Loss in iteration 33 : 0.32695329763682346
Loss in iteration 34 : 0.32765222702274
Loss in iteration 35 : 0.3284116914476618
Loss in iteration 36 : 0.3290088629229169
Loss in iteration 37 : 0.32930339407021975
Loss in iteration 38 : 0.32925006106663407
Loss in iteration 39 : 0.32888540305579
Loss in iteration 40 : 0.3282974333652446
Loss in iteration 41 : 0.327591894843614
Loss in iteration 42 : 0.32686567656306
Loss in iteration 43 : 0.3261917763470348
Loss in iteration 44 : 0.3256148681451451
Loss in iteration 45 : 0.32515397857364975
Loss in iteration 46 : 0.3248085947842529
Loss in iteration 47 : 0.3245655065475294
Loss in iteration 48 : 0.3244048578582901
Loss in iteration 49 : 0.3243047891833965
Loss in iteration 50 : 0.3242446012194556
Loss in iteration 51 : 0.32420663772887887
Loss in iteration 52 : 0.3241771739784263
Loss in iteration 53 : 0.32414659333427764
Loss in iteration 54 : 0.32410909200237764
Loss in iteration 55 : 0.3240621004076542
Loss in iteration 56 : 0.32400556284657933
Loss in iteration 57 : 0.32394117919359994
Loss in iteration 58 : 0.32387168335734495
Loss in iteration 59 : 0.3238002107901725
Loss in iteration 60 : 0.3237297892800397
Loss in iteration 61 : 0.32366297161078084
Loss in iteration 62 : 0.3236016143975933
Loss in iteration 63 : 0.3235467942873633
Loss in iteration 64 : 0.3234988412835217
Loss in iteration 65 : 0.3234574601844013
Loss in iteration 66 : 0.3234219060555434
Loss in iteration 67 : 0.32339117900459297
Loss in iteration 68 : 0.32336420735228394
Loss in iteration 69 : 0.3233399958516443
Loss in iteration 70 : 0.32331772544363463
Loss in iteration 71 : 0.32329680129489574
Loss in iteration 72 : 0.3232768547275424
Loss in iteration 73 : 0.3232577107733638
Loss in iteration 74 : 0.3232393358543653
Loss in iteration 75 : 0.3232217796814094
Loss in iteration 76 : 0.3232051226621283
Loss in iteration 77 : 0.3231894360244849
Loss in iteration 78 : 0.3231747575970137
Loss in iteration 79 : 0.32316108259558296
Loss in iteration 80 : 0.3231483663336303
Loss in iteration 81 : 0.3231365346117761
Loss in iteration 82 : 0.3231254974839076
Loss in iteration 83 : 0.3231151628116207
Loss in iteration 84 : 0.3231054471381949
Loss in iteration 85 : 0.32309628261416656
Loss in iteration 86 : 0.3230876197578622
Loss in iteration 87 : 0.32307942660773925
Loss in iteration 88 : 0.32307168527973995
Loss in iteration 89 : 0.3230643871071096
Loss in iteration 90 : 0.3230575274744279
Loss in iteration 91 : 0.32305110123869346
Loss in iteration 92 : 0.32304509933301634
Loss in iteration 93 : 0.3230395068360055
Loss in iteration 94 : 0.32303430250967424
Loss in iteration 95 : 0.3230294595907715
Loss in iteration 96 : 0.32302494747988125
Loss in iteration 97 : 0.3230207339109781
Loss in iteration 98 : 0.32301678719314836
Loss in iteration 99 : 0.3230130781804134
Loss in iteration 100 : 0.3230095817258311
Loss in iteration 101 : 0.3230062774920295
Loss in iteration 102 : 0.32300315010339586
Loss in iteration 103 : 0.32300018872032454
Loss in iteration 104 : 0.3229973861831285
Loss in iteration 105 : 0.3229947379080659
Loss in iteration 106 : 0.3229922407210628
Loss in iteration 107 : 0.3229898917913266
Loss in iteration 108 : 0.322987687785093
Loss in iteration 109 : 0.3229856243084846
Loss in iteration 110 : 0.3229836956567976
Loss in iteration 111 : 0.32298189484296524
Loss in iteration 112 : 0.3229802138453532
Loss in iteration 113 : 0.3229786439968787
Loss in iteration 114 : 0.3229771764334837
Loss in iteration 115 : 0.32297580252806507
Loss in iteration 116 : 0.32297451425256113
Loss in iteration 117 : 0.32297330443205957
Loss in iteration 118 : 0.32297216687657476
Loss in iteration 119 : 0.3229710963955542
Loss in iteration 120 : 0.32297008871484406
Loss in iteration 121 : 0.3229691403249012
Loss in iteration 122 : 0.32296824829231324
Loss in iteration 123 : 0.3229674100649913
Loss in iteration 124 : 0.32296662329600134
Loss in iteration 125 : 0.32296588570339557
Loss in iteration 126 : 0.3229651949751336
Loss in iteration 127 : 0.3229645487204166
Loss in iteration 128 : 0.32296394446249305
Loss in iteration 129 : 0.32296337966374183
Loss in iteration 130 : 0.32296285177163153
Loss in iteration 131 : 0.3229623582740207
Testing accuracy  of updater 8 on alg 0 with rate 0.032 = 0.79, training accuracy 0.8442861767562317, time elapsed: 1441 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5125851428322934
Loss in iteration 3 : 0.5144055468543502
Loss in iteration 4 : 0.47087367207696623
Loss in iteration 5 : 0.4104150551942342
Loss in iteration 6 : 0.3675458591286114
Loss in iteration 7 : 0.3534914241701167
Loss in iteration 8 : 0.3515168918765299
Loss in iteration 9 : 0.34728037326957706
Loss in iteration 10 : 0.34055820257846353
Loss in iteration 11 : 0.3349838780357259
Loss in iteration 12 : 0.332007987212644
Loss in iteration 13 : 0.3311156539109653
Loss in iteration 14 : 0.33126356773877763
Loss in iteration 15 : 0.3316880265735447
Loss in iteration 16 : 0.33206937256252195
Loss in iteration 17 : 0.33238716036062116
Loss in iteration 18 : 0.3327202130270465
Loss in iteration 19 : 0.3331160916491767
Loss in iteration 20 : 0.3335551995441998
Loss in iteration 21 : 0.33397773314608475
Loss in iteration 22 : 0.3343262094645668
Loss in iteration 23 : 0.33457103241395375
Loss in iteration 24 : 0.33471158325232075
Loss in iteration 25 : 0.33476284211226226
Loss in iteration 26 : 0.3347411889141998
Loss in iteration 27 : 0.33465721244660807
Loss in iteration 28 : 0.33451589116259367
Loss in iteration 29 : 0.33432036049221814
Loss in iteration 30 : 0.33407532447290866
Loss in iteration 31 : 0.33378819635771234
Loss in iteration 32 : 0.3334681839862722
Loss in iteration 33 : 0.333124607458044
Loss in iteration 34 : 0.33276564389296054
Loss in iteration 35 : 0.332397975147115
Loss in iteration 36 : 0.33202711853704236
Loss in iteration 37 : 0.3316579298064868
Loss in iteration 38 : 0.33129488944076485
Loss in iteration 39 : 0.33094208410741166
Loss in iteration 40 : 0.33060302441797784
Loss in iteration 41 : 0.33028048837168006
Loss in iteration 42 : 0.3299764909082075
Loss in iteration 43 : 0.3296923650958136
Loss in iteration 44 : 0.32942888190869785
Loss in iteration 45 : 0.32918634669566826
Loss in iteration 46 : 0.3289646552059822
Loss in iteration 47 : 0.328763327805481
Loss in iteration 48 : 0.32858154803493733
Loss in iteration 49 : 0.32841821795346343
Loss in iteration 50 : 0.32827202579330506
Loss in iteration 51 : 0.3281415139741022
Loss in iteration 52 : 0.3280251385286377
Loss in iteration 53 : 0.3279213180234178
Loss in iteration 54 : 0.32782847469685494
Loss in iteration 55 : 0.32774507079229864
Loss in iteration 56 : 0.3276696408407595
Loss in iteration 57 : 0.327600818734254
Loss in iteration 58 : 0.32753735815452706
Loss in iteration 59 : 0.327478145869085
Loss in iteration 60 : 0.3274222084871482
Loss in iteration 61 : 0.32736871378333693
Loss in iteration 62 : 0.32731696764532087
Loss in iteration 63 : 0.327266407452834
Loss in iteration 64 : 0.32721659255279484
Loss in iteration 65 : 0.32716719251348036
Loss in iteration 66 : 0.32711797390836717
Loss in iteration 67 : 0.3270687863842934
Loss in iteration 68 : 0.3270195486808618
Loss in iteration 69 : 0.32697023512947915
Loss in iteration 70 : 0.3269208630228969
Loss in iteration 71 : 0.32687148113397196
Loss in iteration 72 : 0.32682215957235117
Loss in iteration 73 : 0.326772981087963
Loss in iteration 74 : 0.3267240338550677
Loss in iteration 75 : 0.3266754057029874
Loss in iteration 76 : 0.3266271797055231
Loss in iteration 77 : 0.32657943100306475
Loss in iteration 78 : 0.32653222470875315
Loss in iteration 79 : 0.3264856147392731
Loss in iteration 80 : 0.32643964340884823
Loss in iteration 81 : 0.3263943416296833
Loss in iteration 82 : 0.32634972957196434
Loss in iteration 83 : 0.3263058176504799
Loss in iteration 84 : 0.3262626077215083
Loss in iteration 85 : 0.32622009439146926
Loss in iteration 86 : 0.326178266356778
Loss in iteration 87 : 0.32613710771146004
Loss in iteration 88 : 0.3260965991748475
Loss in iteration 89 : 0.3260567192058477
Loss in iteration 90 : 0.3260174449824832
Loss in iteration 91 : 0.32597875323579134
Loss in iteration 92 : 0.3259406209355862
Loss in iteration 93 : 0.32590302583215874
Loss in iteration 94 : 0.3258659468629296
Loss in iteration 95 : 0.32582936443650906
Loss in iteration 96 : 0.3257932606087846
Loss in iteration 97 : 0.32575761916675045
Loss in iteration 98 : 0.3257224256360368
Loss in iteration 99 : 0.32568766722766446
Loss in iteration 100 : 0.32565333273857366
Loss in iteration 101 : 0.32561941241917775
Loss in iteration 102 : 0.325585897819672
Loss in iteration 103 : 0.32555278162512746
Loss in iteration 104 : 0.32552005748776397
Loss in iteration 105 : 0.3254877198631168
Loss in iteration 106 : 0.32545576385527203
Loss in iteration 107 : 0.32542418507492504
Loss in iteration 108 : 0.32539297951275664
Loss in iteration 109 : 0.32536214342952496
Loss in iteration 110 : 0.3253316732633442
Loss in iteration 111 : 0.3253015655538995
Loss in iteration 112 : 0.32527181688274454
Loss in iteration 113 : 0.32524242382841234
Loss in iteration 114 : 0.3252133829347707
Loss in iteration 115 : 0.3251846906908856
Loss in iteration 116 : 0.3251563435205856
Loss in iteration 117 : 0.32512833777992795
Loss in iteration 118 : 0.325100669760836
Loss in iteration 119 : 0.3250733356993393
Loss in iteration 120 : 0.3250463317869577
Loss in iteration 121 : 0.3250196541840012
Loss in iteration 122 : 0.3249932990337216
Loss in iteration 123 : 0.3249672624764676
Loss in iteration 124 : 0.32494154066315695
Loss in iteration 125 : 0.3249161297675802
Loss in iteration 126 : 0.3248910259971995
Loss in iteration 127 : 0.32486622560222556
Loss in iteration 128 : 0.32484172488291246
Loss in iteration 129 : 0.32481752019507143
Loss in iteration 130 : 0.32479360795388235
Loss in iteration 131 : 0.32476998463617923
Loss in iteration 132 : 0.32474664678134973
Loss in iteration 133 : 0.32472359099109616
Loss in iteration 134 : 0.3247008139282308
Loss in iteration 135 : 0.3246783123147564
Loss in iteration 136 : 0.3246560829293994
Loss in iteration 137 : 0.3246341226048041
Loss in iteration 138 : 0.32461242822453545
Loss in iteration 139 : 0.3245909967200392
Loss in iteration 140 : 0.3245698250676645
Loss in iteration 141 : 0.3245489102858472
Loss in iteration 142 : 0.3245282494325044
Loss in iteration 143 : 0.32450783960270296
Loss in iteration 144 : 0.3244876779266102
Loss in iteration 145 : 0.324467761567741
Loss in iteration 146 : 0.3244480877214943
Loss in iteration 147 : 0.32442865361396733
Loss in iteration 148 : 0.3244094565010139
Loss in iteration 149 : 0.3243904936675268
Loss in iteration 150 : 0.32437176242690646
Loss in iteration 151 : 0.3243532601206854
Loss in iteration 152 : 0.32433498411827355
Loss in iteration 153 : 0.3243169318167939
Loss in iteration 154 : 0.32429910064098577
Loss in iteration 155 : 0.32428148804313556
Loss in iteration 156 : 0.32426409150303337
Loss in iteration 157 : 0.3242469085279297
Loss in iteration 158 : 0.32422993665246486
Loss in iteration 159 : 0.32421317343859124
Loss in iteration 160 : 0.3241966164754562
Loss in iteration 161 : 0.3241802633792482
Loss in iteration 162 : 0.3241641117930174
Loss in iteration 163 : 0.32414815938645203
Loss in iteration 164 : 0.32413240385563147
Loss in iteration 165 : 0.3241168429227442
Loss in iteration 166 : 0.32410147433578934
Loss in iteration 167 : 0.32408629586825444
Loss in iteration 168 : 0.32407130531877915
Loss in iteration 169 : 0.3240565005108144
Loss in iteration 170 : 0.32404187929227646
Loss in iteration 171 : 0.3240274395351969
Loss in iteration 172 : 0.324013179135376
Loss in iteration 173 : 0.32399909601205445
Loss in iteration 174 : 0.3239851881075692
Loss in iteration 175 : 0.3239714533870487
Loss in iteration 176 : 0.3239578898380943
Loss in iteration 177 : 0.3239444954704908
Loss in iteration 178 : 0.3239312683159188
Loss in iteration 179 : 0.3239182064276835
Loss in iteration 180 : 0.32390530788045707
Loss in iteration 181 : 0.32389257077002187
Loss in iteration 182 : 0.32387999321303745
Loss in iteration 183 : 0.32386757334680033
Loss in iteration 184 : 0.32385530932902945
Loss in iteration 185 : 0.32384319933763783
Loss in iteration 186 : 0.3238312415705292
Loss in iteration 187 : 0.32381943424538534
Loss in iteration 188 : 0.32380777559946156
Loss in iteration 189 : 0.32379626388938815
Loss in iteration 190 : 0.32378489739097194
Loss in iteration 191 : 0.323773674398994
Loss in iteration 192 : 0.3237625932270221
Loss in iteration 193 : 0.323751652207211
Loss in iteration 194 : 0.3237408496901116
Loss in iteration 195 : 0.3237301840444772
Loss in iteration 196 : 0.32371965365707506
Loss in iteration 197 : 0.32370925693249225
Loss in iteration 198 : 0.32369899229294846
Loss in iteration 199 : 0.32368885817810883
Loss in iteration 200 : 0.32367885304489435
Testing accuracy  of updater 8 on alg 0 with rate 0.008 = 0.788, training accuracy 0.8429912593072192, time elapsed: 2191 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4907380224656143
Loss in iteration 3 : 0.49763239222492117
Loss in iteration 4 : 0.47335013801861986
Loss in iteration 5 : 0.42918063917104854
Loss in iteration 6 : 0.3875458800792636
Loss in iteration 7 : 0.36397957593912633
Loss in iteration 8 : 0.35615694822900235
Loss in iteration 9 : 0.352650939782235
Loss in iteration 10 : 0.34748540167979075
Loss in iteration 11 : 0.34134604114410716
Loss in iteration 12 : 0.3363129335804997
Loss in iteration 13 : 0.3332367345147323
Loss in iteration 14 : 0.33181929864422893
Loss in iteration 15 : 0.3313670500598289
Loss in iteration 16 : 0.331301077993816
Loss in iteration 17 : 0.33131857915698204
Loss in iteration 18 : 0.3313434216542681
Loss in iteration 19 : 0.33140797828444296
Loss in iteration 20 : 0.3315538320093966
Loss in iteration 21 : 0.3317845544823743
Loss in iteration 22 : 0.3320654881546707
Loss in iteration 23 : 0.3323472751226968
Loss in iteration 24 : 0.3325897047396865
Loss in iteration 25 : 0.3327730326272918
Loss in iteration 26 : 0.33289602439775273
Loss in iteration 27 : 0.3329670902651603
Loss in iteration 28 : 0.3329956108877329
Loss in iteration 29 : 0.3329873831390601
Loss in iteration 30 : 0.33294440231677963
Loss in iteration 31 : 0.33286703054408284
Loss in iteration 32 : 0.33275632938557537
Loss in iteration 33 : 0.3326152359852163
Loss in iteration 34 : 0.33244839817787075
Loss in iteration 35 : 0.33226122775896116
Loss in iteration 36 : 0.3320589012284046
Loss in iteration 37 : 0.33184578596688236
Loss in iteration 38 : 0.33162538172891504
Loss in iteration 39 : 0.3314005828068141
Loss in iteration 40 : 0.33117398666470704
Loss in iteration 41 : 0.33094806362463397
Loss in iteration 42 : 0.3307251511527997
Loss in iteration 43 : 0.3305073459636797
Loss in iteration 44 : 0.33029639529878374
Loss in iteration 45 : 0.3300936518281731
Loss in iteration 46 : 0.3299000994021976
Loss in iteration 47 : 0.32971641780990585
Loss in iteration 48 : 0.32954304770896287
Loss in iteration 49 : 0.32938023326379046
Loss in iteration 50 : 0.3292280417915114
Loss in iteration 51 : 0.32908637292455567
Loss in iteration 52 : 0.32895497058563994
Loss in iteration 53 : 0.3288334439081128
Loss in iteration 54 : 0.32872129527559885
Loss in iteration 55 : 0.3286179497167863
Loss in iteration 56 : 0.32852278063950335
Loss in iteration 57 : 0.3284351300602389
Loss in iteration 58 : 0.32835432432350337
Loss in iteration 59 : 0.3282796873417557
Loss in iteration 60 : 0.3282105527840403
Loss in iteration 61 : 0.32814627544789327
Loss in iteration 62 : 0.32808624122827273
Loss in iteration 63 : 0.32802987499678077
Loss in iteration 64 : 0.3279766461034731
Loss in iteration 65 : 0.3279260716868289
Loss in iteration 66 : 0.327877718231154
Loss in iteration 67 : 0.3278312018011287
Loss in iteration 68 : 0.32778618723706426
Loss in iteration 69 : 0.32774238646119075
Loss in iteration 70 : 0.3276995559985825
Loss in iteration 71 : 0.32765749384252674
Loss in iteration 72 : 0.3276160358390715
Loss in iteration 73 : 0.3275750517849644
Loss in iteration 74 : 0.32753444141615046
Loss in iteration 75 : 0.3274941304251209
Loss in iteration 76 : 0.3274540666057304
Loss in iteration 77 : 0.32741421619565814
Loss in iteration 78 : 0.32737456047002195
Loss in iteration 79 : 0.32733509262835925
Loss in iteration 80 : 0.3272958150057237
Loss in iteration 81 : 0.3272567366249924
Loss in iteration 82 : 0.32721787109312667
Loss in iteration 83 : 0.32717923483168676
Loss in iteration 84 : 0.3271408456228199
Loss in iteration 85 : 0.3271027214462688
Loss in iteration 86 : 0.3270648795797679
Loss in iteration 87 : 0.32702733593347594
Loss in iteration 88 : 0.3269901045883069
Loss in iteration 89 : 0.3269531975079958
Loss in iteration 90 : 0.32691662439556207
Loss in iteration 91 : 0.3268803926664349
Loss in iteration 92 : 0.32684450751281463
Loss in iteration 93 : 0.32680897203652504
Loss in iteration 94 : 0.3267737874304331
Loss in iteration 95 : 0.3267389531913807
Loss in iteration 96 : 0.3267044673502682
Loss in iteration 97 : 0.3266703267075388
Loss in iteration 98 : 0.3266365270647074
Loss in iteration 99 : 0.3266030634447519
Loss in iteration 100 : 0.32656993029617193
Loss in iteration 101 : 0.32653712167718874
Loss in iteration 102 : 0.3265046314180113
Loss in iteration 103 : 0.32647245326026986
Loss in iteration 104 : 0.3264405809736749
Loss in iteration 105 : 0.3264090084506898
Loss in iteration 106 : 0.3263777297805869
Loss in iteration 107 : 0.32634673930461544
Loss in iteration 108 : 0.3263160316543004
Loss in iteration 109 : 0.32628560177497573
Loss in iteration 110 : 0.3262554449367493
Loss in iteration 111 : 0.32622555673499304
Loss in iteration 112 : 0.32619593308240136
Loss in iteration 113 : 0.32616657019447964
Loss in iteration 114 : 0.3261374645701741
Loss in iteration 115 : 0.3261086129691504
Loss in iteration 116 : 0.3260800123870278
Loss in iteration 117 : 0.3260516600296911
Loss in iteration 118 : 0.32602355328758603
Loss in iteration 119 : 0.32599568971074344
Loss in iteration 120 : 0.32596806698509534
Loss in iteration 121 : 0.3259406829104959
Loss in iteration 122 : 0.3259135353807385
Loss in iteration 123 : 0.32588662236572163
Loss in iteration 124 : 0.3258599418958527
Loss in iteration 125 : 0.3258334920486566
Loss in iteration 126 : 0.3258072709375333
Loss in iteration 127 : 0.32578127670253665
Loss in iteration 128 : 0.3257555075030101
Loss in iteration 129 : 0.3257299615119066
Loss in iteration 130 : 0.32570463691159496
Loss in iteration 131 : 0.3256795318909549
Loss in iteration 132 : 0.32565464464356
Loss in iteration 133 : 0.3256299733667652
Loss in iteration 134 : 0.32560551626151457
Loss in iteration 135 : 0.32558127153271826
Loss in iteration 136 : 0.3255572373900357
Loss in iteration 137 : 0.3255334120489523
Loss in iteration 138 : 0.32550979373203814
Loss in iteration 139 : 0.3254863806702847
Loss in iteration 140 : 0.3254631711044573
Loss in iteration 141 : 0.32544016328640224
Loss in iteration 142 : 0.3254173554802574
Loss in iteration 143 : 0.3253947459635391
Loss in iteration 144 : 0.32537233302808605
Loss in iteration 145 : 0.32535011498084604
Loss in iteration 146 : 0.3253280901445001
Loss in iteration 147 : 0.32530625685793796
Loss in iteration 148 : 0.3252846134765759
Loss in iteration 149 : 0.32526315837254444
Loss in iteration 150 : 0.3252418899347514
Loss in iteration 151 : 0.32522080656883934
Loss in iteration 152 : 0.32519990669704907
Loss in iteration 153 : 0.3251791887580141
Loss in iteration 154 : 0.32515865120649184
Loss in iteration 155 : 0.32513829251305343
Loss in iteration 156 : 0.32511811116374456
Loss in iteration 157 : 0.3250981056597195
Loss in iteration 158 : 0.3250782745168828
Loss in iteration 159 : 0.32505861626551225
Loss in iteration 160 : 0.3250391294499037
Loss in iteration 161 : 0.3250198126280232
Loss in iteration 162 : 0.32500066437117775
Loss in iteration 163 : 0.3249816832637029
Loss in iteration 164 : 0.32496286790267703
Loss in iteration 165 : 0.32494421689765396
Loss in iteration 166 : 0.3249257288704225
Loss in iteration 167 : 0.32490740245478356
Loss in iteration 168 : 0.3248892362963488
Loss in iteration 169 : 0.3248712290523613
Loss in iteration 170 : 0.3248533793915336
Loss in iteration 171 : 0.3248356859938929
Loss in iteration 172 : 0.3248181475506538
Loss in iteration 173 : 0.32480076276408953
Loss in iteration 174 : 0.32478353034741925
Loss in iteration 175 : 0.32476644902470475
Loss in iteration 176 : 0.3247495175307464
Loss in iteration 177 : 0.32473273461099184
Loss in iteration 178 : 0.32471609902144244
Loss in iteration 179 : 0.3246996095285643
Loss in iteration 180 : 0.32468326490920063
Loss in iteration 181 : 0.32466706395048617
Loss in iteration 182 : 0.32465100544975833
Loss in iteration 183 : 0.3246350882144734
Loss in iteration 184 : 0.32461931106211755
Loss in iteration 185 : 0.3246036728201204
Loss in iteration 186 : 0.3245881723257662
Loss in iteration 187 : 0.32457280842610614
Loss in iteration 188 : 0.32455757997786966
Loss in iteration 189 : 0.32454248584737183
Loss in iteration 190 : 0.3245275249104289
Loss in iteration 191 : 0.32451269605226507
Loss in iteration 192 : 0.324497998167423
Loss in iteration 193 : 0.32448343015967795
Loss in iteration 194 : 0.32446899094194404
Loss in iteration 195 : 0.324454679436193
Loss in iteration 196 : 0.3244404945733588
Loss in iteration 197 : 0.3244264352932612
Loss in iteration 198 : 0.3244125005445111
Loss in iteration 199 : 0.32439868928443266
Loss in iteration 200 : 0.3243850004789796
Testing accuracy  of updater 8 on alg 0 with rate 0.005600000000000001 = 0.78725, training accuracy 0.8429912593072192, time elapsed: 2644 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.48469985902079477
Loss in iteration 3 : 0.47847387423813914
Loss in iteration 4 : 0.4765298774546494
Loss in iteration 5 : 0.46171325988122663
Loss in iteration 6 : 0.43619350183452504
Loss in iteration 7 : 0.40782979303254535
Loss in iteration 8 : 0.38447030447791686
Loss in iteration 9 : 0.36985133547807564
Loss in iteration 10 : 0.3624743015733214
Loss in iteration 11 : 0.3582846794630633
Loss in iteration 12 : 0.354213602676776
Loss in iteration 13 : 0.3494397608696963
Loss in iteration 14 : 0.3445462877522145
Loss in iteration 15 : 0.34033055679668417
Loss in iteration 16 : 0.337203471154891
Loss in iteration 17 : 0.335142230462215
Loss in iteration 18 : 0.33387815487910855
Loss in iteration 19 : 0.333093583807252
Loss in iteration 20 : 0.33254120181140484
Loss in iteration 21 : 0.33208213932701997
Loss in iteration 22 : 0.3316703661482184
Loss in iteration 23 : 0.33131421160121016
Loss in iteration 24 : 0.33103805190775376
Loss in iteration 25 : 0.3308567125829355
Loss in iteration 26 : 0.33076576179481115
Loss in iteration 27 : 0.3307443879557804
Loss in iteration 28 : 0.3307645924461864
Loss in iteration 29 : 0.33080064152073374
Loss in iteration 30 : 0.3308349630498263
Loss in iteration 31 : 0.3308594641896735
Loss in iteration 32 : 0.3308733716231773
Loss in iteration 33 : 0.3308796022767009
Loss in iteration 34 : 0.3308814746775154
Loss in iteration 35 : 0.3308807701210074
Loss in iteration 36 : 0.3308772862903037
Loss in iteration 37 : 0.330869441023234
Loss in iteration 38 : 0.3308552820945551
Loss in iteration 39 : 0.3308333616500737
Loss in iteration 40 : 0.3308031870922227
Loss in iteration 41 : 0.33076521910142553
Loss in iteration 42 : 0.33072056155764734
Loss in iteration 43 : 0.33067054912143345
Loss in iteration 44 : 0.3306164055866409
Loss in iteration 45 : 0.33055906356317527
Loss in iteration 46 : 0.3304991491918612
Loss in iteration 47 : 0.33043707636070757
Loss in iteration 48 : 0.3303731751989739
Loss in iteration 49 : 0.3303077935632536
Loss in iteration 50 : 0.3302413417086792
Loss in iteration 51 : 0.3301742820721621
Loss in iteration 52 : 0.33010708631549823
Loss in iteration 53 : 0.33004018684060854
Loss in iteration 54 : 0.32997394310872874
Loss in iteration 55 : 0.3299086308793639
Loss in iteration 56 : 0.3298444513217124
Loss in iteration 57 : 0.3297815506776152
Loss in iteration 58 : 0.32972004047320524
Loss in iteration 59 : 0.3296600115513964
Loss in iteration 60 : 0.32960153986174906
Loss in iteration 61 : 0.32954468577541235
Loss in iteration 62 : 0.32948949053060184
Loss in iteration 63 : 0.32943597324030477
Loss in iteration 64 : 0.3293841304576764
Loss in iteration 65 : 0.3293339385736345
Loss in iteration 66 : 0.32928535807389675
Loss in iteration 67 : 0.32923833823122006
Loss in iteration 68 : 0.32919282105973974
Loss in iteration 69 : 0.3291487439778599
Loss in iteration 70 : 0.3291060412490475
Loss in iteration 71 : 0.32906464465301605
Loss in iteration 72 : 0.329024483916299
Loss in iteration 73 : 0.32898548727686705
Loss in iteration 74 : 0.32894758231302346
Loss in iteration 75 : 0.32891069695980385
Loss in iteration 76 : 0.3288747605354611
Loss in iteration 77 : 0.32883970460925743
Loss in iteration 78 : 0.3288054636182776
Loss in iteration 79 : 0.3287719752314236
Loss in iteration 80 : 0.3287391805222429
Loss in iteration 81 : 0.3287070240329419
Loss in iteration 82 : 0.32867545379648644
Loss in iteration 83 : 0.3286444213507666
Loss in iteration 84 : 0.32861388174757644
Loss in iteration 85 : 0.3285837935412887
Loss in iteration 86 : 0.328554118739812
Loss in iteration 87 : 0.3285248227088523
Loss in iteration 88 : 0.3284958740321667
Loss in iteration 89 : 0.3284672443393418
Loss in iteration 90 : 0.3284389081157436
Loss in iteration 91 : 0.32841084250724195
Loss in iteration 92 : 0.32838302712757483
Loss in iteration 93 : 0.32835544387130633
Loss in iteration 94 : 0.32832807673228454
Loss in iteration 95 : 0.3283009116264871
Loss in iteration 96 : 0.32827393621881945
Loss in iteration 97 : 0.3282471397546711
Loss in iteration 98 : 0.32822051289797766
Loss in iteration 99 : 0.3281940475778573
Loss in iteration 100 : 0.3281677368454447
Loss in iteration 101 : 0.32814157474180056
Loss in iteration 102 : 0.3281155561770095
Loss in iteration 103 : 0.3280896768200166
Loss in iteration 104 : 0.32806393299855097
Loss in iteration 105 : 0.32803832160849006
Loss in iteration 106 : 0.32801284003215103
Loss in iteration 107 : 0.32798748606508354
Loss in iteration 108 : 0.32796225785100064
Loss in iteration 109 : 0.3279371538244295
Loss in iteration 110 : 0.32791217266057116
Loss in iteration 111 : 0.32788731323179954
Loss in iteration 112 : 0.32786257457016327
Loss in iteration 113 : 0.32783795583528674
Loss in iteration 114 : 0.3278134562870645
Loss in iteration 115 : 0.327789075262628
Loss in iteration 116 : 0.32776481215709896
Loss in iteration 117 : 0.32774066640769617
Loss in iteration 118 : 0.3277166374807809
Loss in iteration 119 : 0.32769272486146456
Loss in iteration 120 : 0.3276689280454308
Loss in iteration 121 : 0.32764524653262567
Loss in iteration 122 : 0.3276216798225375
Loss in iteration 123 : 0.3275982274107934
Loss in iteration 124 : 0.32757488878683666
Loss in iteration 125 : 0.327551663432497
Loss in iteration 126 : 0.32752855082126936
Loss in iteration 127 : 0.32750555041816515
Loss in iteration 128 : 0.3274826616799865
Loss in iteration 129 : 0.32745988405593607
Loss in iteration 130 : 0.3274372169884507
Loss in iteration 131 : 0.32741465991419416
Loss in iteration 132 : 0.32739221226513276
Loss in iteration 133 : 0.3273698734696537
Loss in iteration 134 : 0.3273476429536786
Loss in iteration 135 : 0.32732552014174005
Loss in iteration 136 : 0.32730350445800915
Loss in iteration 137 : 0.3272815953272423
Loss in iteration 138 : 0.3272597921756464
Loss in iteration 139 : 0.32723809443165364
Loss in iteration 140 : 0.32721650152659726
Loss in iteration 141 : 0.32719501289530134
Loss in iteration 142 : 0.32717362797656785
Loss in iteration 143 : 0.3271523462135871
Loss in iteration 144 : 0.3271311670542586
Loss in iteration 145 : 0.32711008995143714
Loss in iteration 146 : 0.3270891143631129
Loss in iteration 147 : 0.3270682397525321
Loss in iteration 148 : 0.3270474655882527
Loss in iteration 149 : 0.3270267913441709
Loss in iteration 150 : 0.3270062164994878
Loss in iteration 151 : 0.32698574053865753
Loss in iteration 152 : 0.32696536295130096
Loss in iteration 153 : 0.32694508323209426
Loss in iteration 154 : 0.32692490088065507
Loss in iteration 155 : 0.32690481540138866
Loss in iteration 156 : 0.3268848263033593
Loss in iteration 157 : 0.3268649331001266
Loss in iteration 158 : 0.32684513530959763
Loss in iteration 159 : 0.3268254324538752
Loss in iteration 160 : 0.32680582405910125
Loss in iteration 161 : 0.3267863096553151
Loss in iteration 162 : 0.3267668887763096
Loss in iteration 163 : 0.3267475609594956
Loss in iteration 164 : 0.3267283257457753
Loss in iteration 165 : 0.32670918267942095
Loss in iteration 166 : 0.32669013130796043
Loss in iteration 167 : 0.3266711711820733
Loss in iteration 168 : 0.32665230185549593
Loss in iteration 169 : 0.32663352288492853
Loss in iteration 170 : 0.3266148338299586
Loss in iteration 171 : 0.32659623425298007
Loss in iteration 172 : 0.32657772371912863
Loss in iteration 173 : 0.32655930179622183
Loss in iteration 174 : 0.3265409680546995
Loss in iteration 175 : 0.3265227220675775
Loss in iteration 176 : 0.3265045634103996
Loss in iteration 177 : 0.3264864916611957
Loss in iteration 178 : 0.326468506400448
Loss in iteration 179 : 0.32645060721105584
Loss in iteration 180 : 0.326432793678305
Loss in iteration 181 : 0.3264150653898412
Loss in iteration 182 : 0.3263974219356428
Loss in iteration 183 : 0.3263798629080016
Loss in iteration 184 : 0.3263623879014984
Loss in iteration 185 : 0.3263449965129846
Loss in iteration 186 : 0.32632768834156173
Loss in iteration 187 : 0.3263104629885678
Loss in iteration 188 : 0.32629332005755607
Loss in iteration 189 : 0.32627625915428443
Loss in iteration 190 : 0.3262592798866949
Loss in iteration 191 : 0.32624238186490145
Loss in iteration 192 : 0.32622556470117553
Loss in iteration 193 : 0.3262088280099294
Loss in iteration 194 : 0.32619217140770573
Loss in iteration 195 : 0.3261755945131596
Loss in iteration 196 : 0.3261590969470468
Loss in iteration 197 : 0.3261426783322106
Loss in iteration 198 : 0.32612633829356474
Loss in iteration 199 : 0.3261100764580806
Loss in iteration 200 : 0.3260938924547776
Testing accuracy  of updater 8 on alg 0 with rate 0.0032000000000000006 = 0.785, training accuracy 0.8391065069601813, time elapsed: 2275 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.594530515579869
Loss in iteration 3 : 0.5288819368240988
Loss in iteration 4 : 0.4947369526134954
Loss in iteration 5 : 0.48008233833732333
Loss in iteration 6 : 0.4750555076653054
Loss in iteration 7 : 0.4734168388293115
Loss in iteration 8 : 0.4716644635911978
Loss in iteration 9 : 0.4681011545059028
Loss in iteration 10 : 0.4621702632078504
Loss in iteration 11 : 0.4540166309441874
Loss in iteration 12 : 0.44419337208129184
Loss in iteration 13 : 0.43345515854859795
Loss in iteration 14 : 0.42260175286201956
Loss in iteration 15 : 0.41235436635007816
Loss in iteration 16 : 0.4032616298478967
Loss in iteration 17 : 0.39564088516169066
Loss in iteration 18 : 0.3895625731012577
Loss in iteration 19 : 0.38488009588929917
Loss in iteration 20 : 0.3812970000927102
Loss in iteration 21 : 0.3784529937983175
Loss in iteration 22 : 0.37600587455872453
Loss in iteration 23 : 0.3736903808400082
Loss in iteration 24 : 0.3713450421061371
Loss in iteration 25 : 0.3689091644449023
Loss in iteration 26 : 0.3663995948283528
Loss in iteration 27 : 0.36387913895129415
Loss in iteration 28 : 0.3614264997643753
Loss in iteration 29 : 0.359113622232362
Loss in iteration 30 : 0.35699239309480235
Loss in iteration 31 : 0.35508985133287035
Loss in iteration 32 : 0.3534096462561193
Loss in iteration 33 : 0.35193716014068
Loss in iteration 34 : 0.35064605768597756
Loss in iteration 35 : 0.3495046519308889
Loss in iteration 36 : 0.3484811286403835
Loss in iteration 37 : 0.3475472144615984
Loss in iteration 38 : 0.3466802626758221
Loss in iteration 39 : 0.3458639695474947
Loss in iteration 40 : 0.34508805229301964
Loss in iteration 41 : 0.34434725022200624
Loss in iteration 42 : 0.3436399841533782
Loss in iteration 43 : 0.34296695009919426
Loss in iteration 44 : 0.34232984944500133
Loss in iteration 45 : 0.34173038215415624
Loss in iteration 46 : 0.34116956051049724
Loss in iteration 47 : 0.34064734407444885
Loss in iteration 48 : 0.3401625548351759
Loss in iteration 49 : 0.33971300585903924
Loss in iteration 50 : 0.3392957661366806
Loss in iteration 51 : 0.3389074864632258
Loss in iteration 52 : 0.3385447227187324
Loss in iteration 53 : 0.33820421007719215
Loss in iteration 54 : 0.3378830607906761
Loss in iteration 55 : 0.33757887616805826
Loss in iteration 56 : 0.33728977797742915
Loss in iteration 57 : 0.3370143745438743
Loss in iteration 58 : 0.3367516820445577
Loss in iteration 59 : 0.33650102243982727
Loss in iteration 60 : 0.3362619171343224
Loss in iteration 61 : 0.3360339910577308
Loss in iteration 62 : 0.3358168965808712
Loss in iteration 63 : 0.33561026152601214
Loss in iteration 64 : 0.33541366118412147
Loss in iteration 65 : 0.33522661109729385
Loss in iteration 66 : 0.33504857550604633
Loss in iteration 67 : 0.33487898569171565
Loss in iteration 68 : 0.33471726272366414
Loss in iteration 69 : 0.3345628400484229
Testing accuracy  of updater 8 on alg 0 with rate 7.999999999999995E-4 = 0.77775, training accuracy 0.8381353188734219, time elapsed: 829 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 20.21503118918446
Loss in iteration 3 : 21.93482906700504
Loss in iteration 4 : 13.930637741498709
Loss in iteration 5 : 3.4845146685226895
Loss in iteration 6 : 8.291591407718522
Loss in iteration 7 : 12.094897452916426
Loss in iteration 8 : 6.889789936505664
Loss in iteration 9 : 4.977001380463292
Loss in iteration 10 : 6.828018904173862
Loss in iteration 11 : 8.868389583010917
Loss in iteration 12 : 9.589647960012211
Loss in iteration 13 : 8.853575485536275
Loss in iteration 14 : 7.446681831499979
Loss in iteration 15 : 6.359314720895459
Loss in iteration 16 : 6.4627639275324436
Loss in iteration 17 : 7.598953644143644
Loss in iteration 18 : 8.104863745967767
Loss in iteration 19 : 7.341528498825917
Loss in iteration 20 : 6.2161915618679755
Loss in iteration 21 : 5.893104037686494
Loss in iteration 22 : 6.218121364664631
Loss in iteration 23 : 6.5269030257412695
Loss in iteration 24 : 6.392136525605005
Loss in iteration 25 : 5.7860037036110015
Loss in iteration 26 : 5.010002315948704
Loss in iteration 27 : 4.57006399900443
Loss in iteration 28 : 4.724527475123661
Loss in iteration 29 : 4.78993502584735
Loss in iteration 30 : 4.122932222292375
Loss in iteration 31 : 3.428018657733133
Loss in iteration 32 : 3.331684225453491
Loss in iteration 33 : 3.358207966389452
Loss in iteration 34 : 2.994183417663665
Loss in iteration 35 : 2.292376590140154
Loss in iteration 36 : 2.004677016791191
Loss in iteration 37 : 2.178427531194283
Loss in iteration 38 : 1.297724104684721
Loss in iteration 39 : 1.4347907609264048
Loss in iteration 40 : 1.2238379668248671
Loss in iteration 41 : 1.487933682486073
Loss in iteration 42 : 3.3034729879595006
Loss in iteration 43 : 2.580981062979764
Loss in iteration 44 : 3.9134992898241463
Loss in iteration 45 : 5.162281100417463
Loss in iteration 46 : 8.435963743914394
Loss in iteration 47 : 7.410458557156138
Loss in iteration 48 : 4.002060440879874
Loss in iteration 49 : 2.383215119637904
Loss in iteration 50 : 3.9177935561438737
Loss in iteration 51 : 5.889412907462553
Loss in iteration 52 : 5.336691173307329
Loss in iteration 53 : 3.875846616428294
Loss in iteration 54 : 3.521215774542719
Loss in iteration 55 : 4.151319039456129
Loss in iteration 56 : 4.849735078219702
Loss in iteration 57 : 5.15590470889713
Loss in iteration 58 : 4.970457857418037
Loss in iteration 59 : 4.467688744063749
Loss in iteration 60 : 3.9498596779097976
Loss in iteration 61 : 3.74463634193529
Loss in iteration 62 : 4.016139679074471
Loss in iteration 63 : 4.3272420040380775
Loss in iteration 64 : 4.198647855568778
Loss in iteration 65 : 3.7057503460425116
Loss in iteration 66 : 3.3158427152523435
Loss in iteration 67 : 3.2923454511565216
Loss in iteration 68 : 3.38530991974363
Loss in iteration 69 : 3.3685617966325627
Loss in iteration 70 : 3.146367154895789
Loss in iteration 71 : 2.780975694704588
Loss in iteration 72 : 2.4609072545525765
Loss in iteration 73 : 2.3954299800427767
Loss in iteration 74 : 2.431389417226016
Loss in iteration 75 : 2.195519369985163
Loss in iteration 76 : 1.815929671270241
Loss in iteration 77 : 1.6791290617506773
Loss in iteration 78 : 1.6833565006153546
Loss in iteration 79 : 1.5148476627376257
Loss in iteration 80 : 1.1805377694306718
Loss in iteration 81 : 1.111959997007074
Loss in iteration 82 : 1.1404198009475568
Loss in iteration 83 : 0.8385200275911109
Loss in iteration 84 : 1.0438462748563666
Loss in iteration 85 : 0.8235179571802784
Loss in iteration 86 : 1.8835625371346203
Loss in iteration 87 : 4.4026310699006
Loss in iteration 88 : 5.911786607472264
Loss in iteration 89 : 3.8876752105342818
Loss in iteration 90 : 1.0776398779945318
Loss in iteration 91 : 3.606094498156073
Loss in iteration 92 : 3.373295845052201
Loss in iteration 93 : 1.592110143887112
Loss in iteration 94 : 2.1898760410868445
Loss in iteration 95 : 3.123741194662577
Loss in iteration 96 : 3.2876206598880913
Loss in iteration 97 : 2.743789200085467
Loss in iteration 98 : 2.1488144737733457
Loss in iteration 99 : 2.141886358433389
Loss in iteration 100 : 2.7078849278804107
Loss in iteration 101 : 2.8016829977696744
Loss in iteration 102 : 2.280173973166461
Loss in iteration 103 : 1.995318539198361
Loss in iteration 104 : 2.1666160279095394
Loss in iteration 105 : 2.3419340949390226
Loss in iteration 106 : 2.255396955980115
Loss in iteration 107 : 1.9307883662136591
Loss in iteration 108 : 1.6454475328817628
Loss in iteration 109 : 1.7030546952108825
Loss in iteration 110 : 1.8101440899556016
Loss in iteration 111 : 1.4879727773647937
Loss in iteration 112 : 1.2475164725699122
Loss in iteration 113 : 1.322422239753869
Loss in iteration 114 : 1.3038765469103752
Loss in iteration 115 : 1.0144446473630333
Loss in iteration 116 : 0.8732081253152603
Loss in iteration 117 : 1.0629658147572463
Loss in iteration 118 : 0.6887975807016077
Loss in iteration 119 : 0.910635708626842
Loss in iteration 120 : 0.7137156592064844
Loss in iteration 121 : 1.2232858346426323
Loss in iteration 122 : 1.9683218926765391
Loss in iteration 123 : 1.9470499846707066
Loss in iteration 124 : 0.6139066253600464
Loss in iteration 125 : 2.4061027655207625
Loss in iteration 126 : 0.7827821346606796
Loss in iteration 127 : 1.6067535857832325
Loss in iteration 128 : 1.7448958432592105
Loss in iteration 129 : 1.1949156024124257
Loss in iteration 130 : 1.1269094481798905
Loss in iteration 131 : 1.6398021003460246
Loss in iteration 132 : 1.3619203651553575
Loss in iteration 133 : 1.1149006405744772
Loss in iteration 134 : 1.3382060871577404
Loss in iteration 135 : 1.4256959613374813
Loss in iteration 136 : 1.2023238336756474
Loss in iteration 137 : 0.9984672030039567
Loss in iteration 138 : 1.1678596681584277
Loss in iteration 139 : 1.0892665825776378
Loss in iteration 140 : 0.8317932480805393
Loss in iteration 141 : 0.9296728039534345
Loss in iteration 142 : 0.9034156781240639
Loss in iteration 143 : 0.647622858826104
Loss in iteration 144 : 0.7945575755152764
Loss in iteration 145 : 0.5436619877304898
Loss in iteration 146 : 0.6614630712722914
Loss in iteration 147 : 0.547523496225388
Loss in iteration 148 : 0.987416163496513
Loss in iteration 149 : 2.2032250340660333
Loss in iteration 150 : 2.3339800630634855
Loss in iteration 151 : 0.5793227959479451
Loss in iteration 152 : 2.5153667070733654
Loss in iteration 153 : 0.7037300924896488
Loss in iteration 154 : 1.3427526406147723
Loss in iteration 155 : 1.7026483139328659
Loss in iteration 156 : 1.3158720883737005
Loss in iteration 157 : 1.026300265727297
Loss in iteration 158 : 1.4450073084786972
Loss in iteration 159 : 1.5263534210313117
Loss in iteration 160 : 1.1251664948549265
Loss in iteration 161 : 1.1884330681953852
Loss in iteration 162 : 1.3852815566058403
Loss in iteration 163 : 1.3130134727618428
Loss in iteration 164 : 1.0455187152702121
Loss in iteration 165 : 0.9919335996070412
Loss in iteration 166 : 1.1535828702841562
Loss in iteration 167 : 0.8915186554216721
Loss in iteration 168 : 0.7923743396201441
Loss in iteration 169 : 0.894466265340651
Loss in iteration 170 : 0.7387521096839509
Loss in iteration 171 : 0.5642325221045023
Loss in iteration 172 : 0.7771388950171938
Loss in iteration 173 : 0.5464144639730537
Loss in iteration 174 : 0.7290918660074672
Loss in iteration 175 : 0.47559333422824845
Loss in iteration 176 : 0.4571307863140769
Loss in iteration 177 : 0.7892846670221517
Loss in iteration 178 : 0.3892689496432872
Loss in iteration 179 : 0.9094752424431499
Loss in iteration 180 : 2.039987283950866
Loss in iteration 181 : 2.2777391287117923
Loss in iteration 182 : 0.7324821457831634
Loss in iteration 183 : 1.5609779553451335
Loss in iteration 184 : 1.3153408861596985
Loss in iteration 185 : 0.8623440931449647
Loss in iteration 186 : 1.3676716075272364
Loss in iteration 187 : 1.5303547319291086
Loss in iteration 188 : 1.2261711242850317
Loss in iteration 189 : 1.0029874285911924
Loss in iteration 190 : 1.3041212822224995
Loss in iteration 191 : 1.3551217746641044
Loss in iteration 192 : 1.011692717637905
Loss in iteration 193 : 1.0501749266704348
Loss in iteration 194 : 1.194672356674729
Loss in iteration 195 : 1.0883397701023552
Loss in iteration 196 : 0.8367929637579838
Loss in iteration 197 : 0.8740134023402185
Loss in iteration 198 : 0.9185690643076135
Loss in iteration 199 : 0.6289000246306169
Loss in iteration 200 : 0.7275998856798181
Testing accuracy  of updater 9 on alg 0 with rate 0.14 = 0.74875, training accuracy 0.8235674975720297, time elapsed: 2031 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 2.6190776451803446
Loss in iteration 3 : 3.2400179049966615
Loss in iteration 4 : 2.100390342740317
Loss in iteration 5 : 0.5439186106304396
Loss in iteration 6 : 2.037780939054052
Loss in iteration 7 : 1.6985192685675168
Loss in iteration 8 : 0.8392663100746991
Loss in iteration 9 : 1.2320274427109557
Loss in iteration 10 : 1.6966937135428986
Loss in iteration 11 : 1.7516570662226936
Loss in iteration 12 : 1.466973563695169
Loss in iteration 13 : 1.1756430864403304
Loss in iteration 14 : 1.2032503298180641
Loss in iteration 15 : 1.50422369751257
Loss in iteration 16 : 1.5463640254501663
Loss in iteration 17 : 1.2742856692236932
Loss in iteration 18 : 1.1261878014998339
Loss in iteration 19 : 1.2135448703326768
Loss in iteration 20 : 1.3122198247599752
Loss in iteration 21 : 1.27013171870504
Loss in iteration 22 : 1.095633088719645
Loss in iteration 23 : 0.9380152188320311
Loss in iteration 24 : 0.9613019997181851
Loss in iteration 25 : 1.0219377495792688
Loss in iteration 26 : 0.85719756760835
Loss in iteration 27 : 0.709094070564268
Loss in iteration 28 : 0.7422890262669836
Loss in iteration 29 : 0.7377554268238299
Loss in iteration 30 : 0.5776670630564787
Loss in iteration 31 : 0.48874563629978046
Loss in iteration 32 : 0.5899538955191794
Loss in iteration 33 : 0.3760007767953167
Loss in iteration 34 : 0.5096054931082381
Loss in iteration 35 : 0.367323476960402
Loss in iteration 36 : 0.9501457509757044
Loss in iteration 37 : 2.1474202516353764
Loss in iteration 38 : 3.0852982842711367
Loss in iteration 39 : 2.3496348089614285
Loss in iteration 40 : 0.6886256630784303
Loss in iteration 41 : 1.3799895430807718
Loss in iteration 42 : 1.920892307158725
Loss in iteration 43 : 0.8423531578450503
Loss in iteration 44 : 0.979744419974697
Loss in iteration 45 : 1.4662522877661446
Loss in iteration 46 : 1.6135249320425114
Loss in iteration 47 : 1.3736277483217352
Loss in iteration 48 : 1.056556612069343
Loss in iteration 49 : 1.0187866595035553
Loss in iteration 50 : 1.308994066185008
Loss in iteration 51 : 1.3786502431422203
Loss in iteration 52 : 1.106429861882723
Loss in iteration 53 : 0.9615122391978399
Loss in iteration 54 : 1.0598983959255257
Loss in iteration 55 : 1.1546528399757194
Loss in iteration 56 : 1.099227810360175
Loss in iteration 57 : 0.9212703262144785
Loss in iteration 58 : 0.7925404344764002
Loss in iteration 59 : 0.8605001990498808
Loss in iteration 60 : 0.8876368750703503
Loss in iteration 61 : 0.6917630724832435
Loss in iteration 62 : 0.6250387256381115
Loss in iteration 63 : 0.6875553020877292
Loss in iteration 64 : 0.6357785682595261
Loss in iteration 65 : 0.47336591589754246
Loss in iteration 66 : 0.5267444032668194
Loss in iteration 67 : 0.47256952256370066
Loss in iteration 68 : 0.42996175931220276
Loss in iteration 69 : 0.5154553334467785
Loss in iteration 70 : 0.3737703103117006
Loss in iteration 71 : 0.611188697645894
Loss in iteration 72 : 1.0379062776177044
Loss in iteration 73 : 1.0528049869336744
Loss in iteration 74 : 0.36735536121663
Loss in iteration 75 : 1.213851546636401
Loss in iteration 76 : 0.4386113147772513
Loss in iteration 77 : 0.8211791423610848
Loss in iteration 78 : 0.8762260510467642
Loss in iteration 79 : 0.6192653055723361
Loss in iteration 80 : 0.61629921310634
Loss in iteration 81 : 0.8493761354317712
Loss in iteration 82 : 0.6927277708539402
Loss in iteration 83 : 0.6062294100785647
Loss in iteration 84 : 0.7210451922710426
Loss in iteration 85 : 0.7469247517948973
Loss in iteration 86 : 0.6281035602145832
Loss in iteration 87 : 0.5562111524118624
Loss in iteration 88 : 0.6483810549797405
Loss in iteration 89 : 0.5763999005414592
Loss in iteration 90 : 0.4810300541848732
Loss in iteration 91 : 0.5374123565108461
Loss in iteration 92 : 0.5068391011149914
Loss in iteration 93 : 0.3985340130680535
Loss in iteration 94 : 0.4875737244817251
Loss in iteration 95 : 0.36694158631921386
Loss in iteration 96 : 0.4305639627450241
Loss in iteration 97 : 0.38958298477240805
Loss in iteration 98 : 0.46076393049287656
Loss in iteration 99 : 0.3974146413112439
Loss in iteration 100 : 0.3918792720514949
Loss in iteration 101 : 0.4087806853176156
Loss in iteration 102 : 0.343472879917505
Loss in iteration 103 : 0.3726878364446757
Loss in iteration 104 : 0.34471992806155977
Loss in iteration 105 : 0.3708487022441732
Loss in iteration 106 : 0.3465498564876079
Loss in iteration 107 : 0.3625840427829074
Loss in iteration 108 : 0.34140795133995105
Loss in iteration 109 : 0.3564643230671184
Loss in iteration 110 : 0.337098463244432
Loss in iteration 111 : 0.3469704536426602
Loss in iteration 112 : 0.33497972195735154
Loss in iteration 113 : 0.3427579621932346
Loss in iteration 114 : 0.33105570285714053
Loss in iteration 115 : 0.33973695252993874
Loss in iteration 116 : 0.3296511278391203
Loss in iteration 117 : 0.3392468971022106
Loss in iteration 118 : 0.3310489899589322
Loss in iteration 119 : 0.33061721100111824
Loss in iteration 120 : 0.3319043804660331
Loss in iteration 121 : 0.32434808130314224
Loss in iteration 122 : 0.3292963987036524
Loss in iteration 123 : 0.32518666054242884
Loss in iteration 124 : 0.3281810445213058
Loss in iteration 125 : 0.3273739942100678
Loss in iteration 126 : 0.32643782313225866
Loss in iteration 127 : 0.32722212720220106
Loss in iteration 128 : 0.32423565383840297
Loss in iteration 129 : 0.3258748030520662
Loss in iteration 130 : 0.32327603835701985
Loss in iteration 131 : 0.32524792387741763
Loss in iteration 132 : 0.3237464611045179
Loss in iteration 133 : 0.3246576949421829
Loss in iteration 134 : 0.3246711999240615
Loss in iteration 135 : 0.32403381408305754
Loss in iteration 136 : 0.32492143805721146
Loss in iteration 137 : 0.3235332269653486
Loss in iteration 138 : 0.32427460189118645
Loss in iteration 139 : 0.32343210702922776
Loss in iteration 140 : 0.32338619185570205
Loss in iteration 141 : 0.3236719040609314
Loss in iteration 142 : 0.3231572715732835
Loss in iteration 143 : 0.32385027711863246
Loss in iteration 144 : 0.32337666224074296
Loss in iteration 145 : 0.3237205984537926
Loss in iteration 146 : 0.32350688245558623
Testing accuracy  of updater 9 on alg 0 with rate 0.098 = 0.7875, training accuracy 0.8433149886694723, time elapsed: 1371 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.278146986923631
Loss in iteration 3 : 1.5765487614117382
Loss in iteration 4 : 1.0636967951834466
Loss in iteration 5 : 0.43930589482388893
Loss in iteration 6 : 0.7657358055212489
Loss in iteration 7 : 1.1040881188098537
Loss in iteration 8 : 0.7493683247275585
Loss in iteration 9 : 0.5990054147293543
Loss in iteration 10 : 0.7556760849372457
Loss in iteration 11 : 0.9225599264753843
Loss in iteration 12 : 0.9588394978598239
Loss in iteration 13 : 0.8710094341848883
Loss in iteration 14 : 0.7532076974072389
Loss in iteration 15 : 0.7152209880558165
Loss in iteration 16 : 0.7940568310519719
Loss in iteration 17 : 0.8664218630161025
Loss in iteration 18 : 0.8160607713735741
Loss in iteration 19 : 0.7117939309175578
Loss in iteration 20 : 0.6756593390012681
Loss in iteration 21 : 0.7049557644131935
Loss in iteration 22 : 0.7273043077304763
Loss in iteration 23 : 0.6956888896038151
Loss in iteration 24 : 0.6204418183701474
Loss in iteration 25 : 0.5592084057399535
Loss in iteration 26 : 0.5604662226664144
Loss in iteration 27 : 0.5743659872816501
Loss in iteration 28 : 0.517018449662511
Loss in iteration 29 : 0.44918045722200634
Loss in iteration 30 : 0.44623519485326546
Loss in iteration 31 : 0.4522238144660615
Loss in iteration 32 : 0.40515003899328167
Loss in iteration 33 : 0.3549711406143376
Loss in iteration 34 : 0.38713277841639193
Loss in iteration 35 : 0.35464571851208115
Loss in iteration 36 : 0.34136778886993446
Loss in iteration 37 : 0.3808892821444237
Loss in iteration 38 : 0.34095147576382406
Loss in iteration 39 : 0.41017652345902184
Loss in iteration 40 : 0.3550721710801093
Loss in iteration 41 : 0.3839426844824932
Loss in iteration 42 : 0.33659416019592225
Loss in iteration 43 : 0.37022863398483763
Loss in iteration 44 : 0.3292850339088718
Loss in iteration 45 : 0.34772807117112564
Loss in iteration 46 : 0.3397995012751073
Loss in iteration 47 : 0.3312828295624183
Loss in iteration 48 : 0.34660428914106783
Loss in iteration 49 : 0.3344953908826477
Loss in iteration 50 : 0.3371838829539294
Loss in iteration 51 : 0.34342472403069374
Loss in iteration 52 : 0.334476966222465
Loss in iteration 53 : 0.33577230881862113
Loss in iteration 54 : 0.338849495623994
Loss in iteration 55 : 0.330367937960986
Loss in iteration 56 : 0.332886353837511
Loss in iteration 57 : 0.3324632295888602
Loss in iteration 58 : 0.3264904844573149
Loss in iteration 59 : 0.33030899180179374
Loss in iteration 60 : 0.32705422318853894
Loss in iteration 61 : 0.32619396966794983
Loss in iteration 62 : 0.3286889993020803
Loss in iteration 63 : 0.32488615807595583
Loss in iteration 64 : 0.3278899671813593
Loss in iteration 65 : 0.32557554114058046
Loss in iteration 66 : 0.32576838262061464
Loss in iteration 67 : 0.3262198623929202
Loss in iteration 68 : 0.3240800815871118
Loss in iteration 69 : 0.3259017370773097
Loss in iteration 70 : 0.3239156133340664
Loss in iteration 71 : 0.3248515556793354
Loss in iteration 72 : 0.3246436428599902
Loss in iteration 73 : 0.32391890706702614
Loss in iteration 74 : 0.32488461950501646
Loss in iteration 75 : 0.3237607528712691
Loss in iteration 76 : 0.3242189720303522
Loss in iteration 77 : 0.3240403785525056
Loss in iteration 78 : 0.3234548649605886
Loss in iteration 79 : 0.3239698342597763
Loss in iteration 80 : 0.32332923680981485
Loss in iteration 81 : 0.3234979670367249
Loss in iteration 82 : 0.32355349697246233
Loss in iteration 83 : 0.32319683986972747
Loss in iteration 84 : 0.3235831554962716
Loss in iteration 85 : 0.3232803641480822
Loss in iteration 86 : 0.3233801557949013
Loss in iteration 87 : 0.3234393028824215
Loss in iteration 88 : 0.3231951734767995
Loss in iteration 89 : 0.3233880186809916
Loss in iteration 90 : 0.32314426457862644
Loss in iteration 91 : 0.32317604594041077
Loss in iteration 92 : 0.32315154362911064
Loss in iteration 93 : 0.3230102537524869
Loss in iteration 94 : 0.32312838125283583
Loss in iteration 95 : 0.3229989085420479
Loss in iteration 96 : 0.3230720012328055
Loss in iteration 97 : 0.32307413005572216
Loss in iteration 98 : 0.32303323910940107
Loss in iteration 99 : 0.3231123119684088
Loss in iteration 100 : 0.3230341266737948
Loss in iteration 101 : 0.3230719411722981
Loss in iteration 102 : 0.323044563310197
Loss in iteration 103 : 0.3230044236000696
Loss in iteration 104 : 0.3230291096257337
Loss in iteration 105 : 0.32297052265154236
Loss in iteration 106 : 0.3229924232674118
Loss in iteration 107 : 0.3229765889468032
Loss in iteration 108 : 0.3229667225017941
Loss in iteration 109 : 0.3229915475481289
Loss in iteration 110 : 0.3229686212955109
Testing accuracy  of updater 9 on alg 0 with rate 0.056 = 0.7905, training accuracy 0.8436387180317255, time elapsed: 1329 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.49647009569146544
Loss in iteration 3 : 0.599698769489144
Loss in iteration 4 : 0.602473522763701
Loss in iteration 5 : 0.49541017172697965
Loss in iteration 6 : 0.36728506371744457
Loss in iteration 7 : 0.3429777100524182
Loss in iteration 8 : 0.4186642348304371
Loss in iteration 9 : 0.44607123471433946
Loss in iteration 10 : 0.39227859835209117
Loss in iteration 11 : 0.34561595355214897
Loss in iteration 12 : 0.3471237057594797
Loss in iteration 13 : 0.3752603047419944
Loss in iteration 14 : 0.3993584860281949
Loss in iteration 15 : 0.40422086910104055
Loss in iteration 16 : 0.39148638206304515
Loss in iteration 17 : 0.37315599974289126
Loss in iteration 18 : 0.362758875905065
Loss in iteration 19 : 0.36669424759231806
Loss in iteration 20 : 0.3794993684413987
Loss in iteration 21 : 0.38841649584949517
Loss in iteration 22 : 0.38541208670392785
Loss in iteration 23 : 0.37406985890376476
Loss in iteration 24 : 0.36409133029560464
Loss in iteration 25 : 0.36142597798151577
Loss in iteration 26 : 0.36466022817916866
Loss in iteration 27 : 0.36833624551357474
Loss in iteration 28 : 0.36785348797989825
Loss in iteration 29 : 0.3623170055584636
Loss in iteration 30 : 0.3545388438803699
Loss in iteration 31 : 0.3487384522348171
Loss in iteration 32 : 0.3472674241869513
Loss in iteration 33 : 0.34858343667407776
Loss in iteration 34 : 0.3486672827468825
Loss in iteration 35 : 0.3451447963712708
Loss in iteration 36 : 0.33967604157751485
Loss in iteration 37 : 0.33576313288704857
Loss in iteration 38 : 0.33491352504998817
Loss in iteration 39 : 0.33557780111448654
Loss in iteration 40 : 0.33523391555257237
Loss in iteration 41 : 0.3329176144161023
Loss in iteration 42 : 0.3299370362177925
Loss in iteration 43 : 0.32838137642657994
Loss in iteration 44 : 0.32883351203947225
Loss in iteration 45 : 0.32969927808548155
Loss in iteration 46 : 0.32921467305950375
Loss in iteration 47 : 0.32767573509479164
Loss in iteration 48 : 0.3267831760009223
Loss in iteration 49 : 0.3272505550407274
Loss in iteration 50 : 0.32810419424071685
Loss in iteration 51 : 0.32815420886210983
Loss in iteration 52 : 0.3274100209118287
Loss in iteration 53 : 0.3268600715469739
Loss in iteration 54 : 0.3271104238976885
Loss in iteration 55 : 0.32763492411410905
Loss in iteration 56 : 0.3275985256512714
Loss in iteration 57 : 0.327023010964094
Loss in iteration 58 : 0.3266058937957576
Loss in iteration 59 : 0.32666883746983943
Loss in iteration 60 : 0.3268401158640344
Loss in iteration 61 : 0.32668285086800375
Loss in iteration 62 : 0.32624889423336395
Loss in iteration 63 : 0.32592191658260294
Loss in iteration 64 : 0.3258873829266548
Loss in iteration 65 : 0.3259386976293043
Loss in iteration 66 : 0.3258117716058925
Loss in iteration 67 : 0.3255335313080614
Loss in iteration 68 : 0.3253289363192795
Loss in iteration 69 : 0.3252987889261379
Loss in iteration 70 : 0.32532536789697863
Loss in iteration 71 : 0.32526158744329525
Loss in iteration 72 : 0.3251093828279199
Loss in iteration 73 : 0.32498490213917236
Loss in iteration 74 : 0.32495730169427994
Loss in iteration 75 : 0.3249734642682757
Loss in iteration 76 : 0.3249429140068158
Loss in iteration 77 : 0.32485259489223905
Loss in iteration 78 : 0.3247663173279261
Loss in iteration 79 : 0.32473218404694976
Loss in iteration 80 : 0.3247284287542958
Loss in iteration 81 : 0.3247029836536873
Loss in iteration 82 : 0.3246399887243674
Loss in iteration 83 : 0.3245710326327336
Loss in iteration 84 : 0.3245293132225741
Loss in iteration 85 : 0.32451033755863345
Loss in iteration 86 : 0.3244840978525861
Loss in iteration 87 : 0.32443554587478945
Loss in iteration 88 : 0.32437994222587346
Loss in iteration 89 : 0.3243390954214304
Loss in iteration 90 : 0.3243143364795285
Loss in iteration 91 : 0.3242890167345163
Loss in iteration 92 : 0.3242516889842613
Loss in iteration 93 : 0.3242086794470781
Loss in iteration 94 : 0.3241733211694687
Loss in iteration 95 : 0.32414879175163813
Loss in iteration 96 : 0.3241260795934026
Loss in iteration 97 : 0.3240969383868202
Loss in iteration 98 : 0.3240636056168312
Loss in iteration 99 : 0.3240340758648036
Loss in iteration 100 : 0.3240113039156463
Loss in iteration 101 : 0.32399052703285836
Loss in iteration 102 : 0.323966367411572
Loss in iteration 103 : 0.3239393279212927
Loss in iteration 104 : 0.3239140274152692
Loss in iteration 105 : 0.32389282428702065
Loss in iteration 106 : 0.32387333998756845
Loss in iteration 107 : 0.32385217954882123
Loss in iteration 108 : 0.3238292156350977
Loss in iteration 109 : 0.3238071142782274
Loss in iteration 110 : 0.32378756259364855
Loss in iteration 111 : 0.32376940652094033
Loss in iteration 112 : 0.3237505852547364
Loss in iteration 113 : 0.3237307908297373
Loss in iteration 114 : 0.32371150449833747
Loss in iteration 115 : 0.32369386264390787
Loss in iteration 116 : 0.3236773377164583
Loss in iteration 117 : 0.32366068170575873
Loss in iteration 118 : 0.32364356689819856
Loss in iteration 119 : 0.3236267977670968
Loss in iteration 120 : 0.3236111055780551
Loss in iteration 121 : 0.3235962609735136
Loss in iteration 122 : 0.3235815201554999
Loss in iteration 123 : 0.32356660793871217
Loss in iteration 124 : 0.32355194614157623
Loss in iteration 125 : 0.3235379952945172
Loss in iteration 126 : 0.32352467428474313
Loss in iteration 127 : 0.32351154714402497
Loss in iteration 128 : 0.32349840678561664
Loss in iteration 129 : 0.3234854717459063
Loss in iteration 130 : 0.32347302960589064
Loss in iteration 131 : 0.3234610653427391
Loss in iteration 132 : 0.3234493286878632
Loss in iteration 133 : 0.3234376748542628
Loss in iteration 134 : 0.32342621324604814
Loss in iteration 135 : 0.3234151200600983
Loss in iteration 136 : 0.3234044054191658
Loss in iteration 137 : 0.3233939263552428
Loss in iteration 138 : 0.32338358372387394
Loss in iteration 139 : 0.32337342877059755
Loss in iteration 140 : 0.3233635664395906
Loss in iteration 141 : 0.32335401148336057
Loss in iteration 142 : 0.32334468152179163
Loss in iteration 143 : 0.3233355095736054
Loss in iteration 144 : 0.3233265164818955
Loss in iteration 145 : 0.3233177626435432
Loss in iteration 146 : 0.32330926059284126
Loss in iteration 147 : 0.3233009624150986
Loss in iteration 148 : 0.32329282338039816
Loss in iteration 149 : 0.3232848498384546
Loss in iteration 150 : 0.3232770758314851
Loss in iteration 151 : 0.32326951044252966
Loss in iteration 152 : 0.3232621257734554
Loss in iteration 153 : 0.3232548925937825
Loss in iteration 154 : 0.32324781109398776
Loss in iteration 155 : 0.32324090006251227
Loss in iteration 156 : 0.32323416554118006
Loss in iteration 157 : 0.3232275911697135
Loss in iteration 158 : 0.3232211579632063
Loss in iteration 159 : 0.3232148637455986
Loss in iteration 160 : 0.3232087184589811
Loss in iteration 161 : 0.32320272562244934
Loss in iteration 162 : 0.3231968752774488
Loss in iteration 163 : 0.3231911548944797
Loss in iteration 164 : 0.3231855614596567
Loss in iteration 165 : 0.32318009967468914
Loss in iteration 166 : 0.323174771087006
Loss in iteration 167 : 0.323169569203165
Loss in iteration 168 : 0.32316448546425014
Loss in iteration 169 : 0.32315951669368853
Loss in iteration 170 : 0.3231546645791563
Loss in iteration 171 : 0.32314992932606496
Loss in iteration 172 : 0.3231453063932283
Loss in iteration 173 : 0.32314078975059196
Loss in iteration 174 : 0.32313637642083204
Loss in iteration 175 : 0.3231320664711908
Loss in iteration 176 : 0.32312785932385174
Loss in iteration 177 : 0.32312375162687224
Loss in iteration 178 : 0.3231197390187244
Loss in iteration 179 : 0.32311581887763824
Loss in iteration 180 : 0.3231119904766787
Loss in iteration 181 : 0.32310825284000527
Loss in iteration 182 : 0.32310460337889635
Loss in iteration 183 : 0.32310103884368957
Loss in iteration 184 : 0.323097556976916
Loss in iteration 185 : 0.32309415668967245
Loss in iteration 186 : 0.3230908368185476
Loss in iteration 187 : 0.3230875952655111
Loss in iteration 188 : 0.3230844295110212
Loss in iteration 189 : 0.3230813376047777
Loss in iteration 190 : 0.32307831831515404
Loss in iteration 191 : 0.3230753704085375
Loss in iteration 192 : 0.3230724921138488
Loss in iteration 193 : 0.3230696813998757
Loss in iteration 194 : 0.3230669365672246
Loss in iteration 195 : 0.3230642563601567
Loss in iteration 196 : 0.3230616395488938
Loss in iteration 197 : 0.32305908459986676
Loss in iteration 198 : 0.3230565898263546
Loss in iteration 199 : 0.3230541537413946
Loss in iteration 200 : 0.32305177513561306
Testing accuracy  of updater 9 on alg 0 with rate 0.014 = 0.78975, training accuracy 0.8436387180317255, time elapsed: 1919 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4766249176116999
Loss in iteration 3 : 0.5379531234774159
Loss in iteration 4 : 0.5683749107247198
Loss in iteration 5 : 0.5209311645971206
Loss in iteration 6 : 0.4252158361780152
Loss in iteration 7 : 0.349581839408711
Loss in iteration 8 : 0.35116850116610715
Loss in iteration 9 : 0.40014487126786263
Loss in iteration 10 : 0.41468194904974764
Loss in iteration 11 : 0.38034780328484696
Loss in iteration 12 : 0.3438038791456631
Loss in iteration 13 : 0.33488436721488984
Loss in iteration 14 : 0.34842379711687305
Loss in iteration 15 : 0.3667611675533866
Loss in iteration 16 : 0.37683731898627243
Loss in iteration 17 : 0.37481140554252534
Loss in iteration 18 : 0.36446624157259194
Loss in iteration 19 : 0.3533057081814468
Loss in iteration 20 : 0.34807858610506315
Loss in iteration 21 : 0.35104906397110947
Loss in iteration 22 : 0.358775509734556
Loss in iteration 23 : 0.3647911657858295
Loss in iteration 24 : 0.36466514587709636
Loss in iteration 25 : 0.35908973281632284
Loss in iteration 26 : 0.3524032671279161
Loss in iteration 27 : 0.348608965612693
Loss in iteration 28 : 0.3487280388258175
Loss in iteration 29 : 0.35099389314874313
Loss in iteration 30 : 0.35270653206789787
Loss in iteration 31 : 0.35203944802971304
Loss in iteration 32 : 0.348895804722094
Loss in iteration 33 : 0.344683745534021
Loss in iteration 34 : 0.34130688624097477
Loss in iteration 35 : 0.33992562649397295
Loss in iteration 36 : 0.34021040557524107
Loss in iteration 37 : 0.34066023908651544
Loss in iteration 38 : 0.3398465524176508
Loss in iteration 39 : 0.3375640338067059
Loss in iteration 40 : 0.33485676926198116
Loss in iteration 41 : 0.33298581706789243
Loss in iteration 42 : 0.3323939037008282
Loss in iteration 43 : 0.33252701849727506
Loss in iteration 44 : 0.33245212509726385
Loss in iteration 45 : 0.33162572412911556
Loss in iteration 46 : 0.33023987096822444
Loss in iteration 47 : 0.32897584492863186
Loss in iteration 48 : 0.32840378759152744
Loss in iteration 49 : 0.3285081856030218
Loss in iteration 50 : 0.3287434797966833
Loss in iteration 51 : 0.32857517893473975
Loss in iteration 52 : 0.32797195925490846
Loss in iteration 53 : 0.3273575134565243
Loss in iteration 54 : 0.32713063152405847
Loss in iteration 55 : 0.3272943769589731
Loss in iteration 56 : 0.32752399582519015
Loss in iteration 57 : 0.3275158952892281
Loss in iteration 58 : 0.3272495993485771
Loss in iteration 59 : 0.3269533647022455
Loss in iteration 60 : 0.3268523881839682
Loss in iteration 61 : 0.3269545396733872
Loss in iteration 62 : 0.32707111667782757
Loss in iteration 63 : 0.32702867316141415
Loss in iteration 64 : 0.32683618516805046
Loss in iteration 65 : 0.32664288204698566
Loss in iteration 66 : 0.32656553436669616
Loss in iteration 67 : 0.32658346432971513
Loss in iteration 68 : 0.3265877534579436
Loss in iteration 69 : 0.32650232245665584
Loss in iteration 70 : 0.3263488574689314
Loss in iteration 71 : 0.32620854979720415
Loss in iteration 72 : 0.32613583762765935
Loss in iteration 73 : 0.3261125189730425
Loss in iteration 74 : 0.3260787381224217
Loss in iteration 75 : 0.32599835121818327
Loss in iteration 76 : 0.32588929795118654
Loss in iteration 77 : 0.32579600631734984
Loss in iteration 78 : 0.3257421445723564
Loss in iteration 79 : 0.32571285345630274
Loss in iteration 80 : 0.32567647857424364
Loss in iteration 81 : 0.3256169813981374
Loss in iteration 82 : 0.3255452904034885
Loss in iteration 83 : 0.3254837906899354
Loss in iteration 84 : 0.32544333731519015
Loss in iteration 85 : 0.32541516792109143
Loss in iteration 86 : 0.3253824556552466
Loss in iteration 87 : 0.3253372276010883
Loss in iteration 88 : 0.32528586809005056
Loss in iteration 89 : 0.32524023955730075
Loss in iteration 90 : 0.32520552127322144
Loss in iteration 91 : 0.3251766654615771
Loss in iteration 92 : 0.32514493763799274
Loss in iteration 93 : 0.32510651957222947
Loss in iteration 94 : 0.3250649764922435
Loss in iteration 95 : 0.3250265637977755
Loss in iteration 96 : 0.3249939807708049
Loss in iteration 97 : 0.3249645404387763
Loss in iteration 98 : 0.32493362997882574
Loss in iteration 99 : 0.32489931737802746
Loss in iteration 100 : 0.32486363334572227
Loss in iteration 101 : 0.32482995586250696
Loss in iteration 102 : 0.32479965154466617
Loss in iteration 103 : 0.32477121193751735
Loss in iteration 104 : 0.32474219316597885
Loss in iteration 105 : 0.3247116387949563
Loss in iteration 106 : 0.3246806777783983
Loss in iteration 107 : 0.3246510952961414
Loss in iteration 108 : 0.32462356276372367
Loss in iteration 109 : 0.32459721771923666
Loss in iteration 110 : 0.3245707370880485
Loss in iteration 111 : 0.32454363859722507
Loss in iteration 112 : 0.32451655978530686
Loss in iteration 113 : 0.3244904442591098
Loss in iteration 114 : 0.32446559647107615
Loss in iteration 115 : 0.32444150785458986
Loss in iteration 116 : 0.32441746924020537
Loss in iteration 117 : 0.32439325344783476
Loss in iteration 118 : 0.3243692234163066
Loss in iteration 119 : 0.3243458737403232
Loss in iteration 120 : 0.3243233361857546
Loss in iteration 121 : 0.3243013152815154
Loss in iteration 122 : 0.32427943470818354
Loss in iteration 123 : 0.324257595494541
Loss in iteration 124 : 0.32423600915726547
Loss in iteration 125 : 0.32421493540860524
Loss in iteration 126 : 0.3241944249642235
Loss in iteration 127 : 0.3241743065213486
Loss in iteration 128 : 0.32415438364133936
Loss in iteration 129 : 0.32413461808788846
Loss in iteration 130 : 0.3241151312225129
Loss in iteration 131 : 0.32409605636640865
Loss in iteration 132 : 0.324077407801924
Loss in iteration 133 : 0.32405908564524527
Loss in iteration 134 : 0.32404098662538555
Loss in iteration 135 : 0.3240230972206887
Loss in iteration 136 : 0.3240054851791318
Loss in iteration 137 : 0.32398821648089654
Loss in iteration 138 : 0.3239712896317994
Loss in iteration 139 : 0.32395464555920633
Loss in iteration 140 : 0.3239382295240428
Loss in iteration 141 : 0.32392203725291424
Loss in iteration 142 : 0.32390610510701257
Loss in iteration 143 : 0.32389046406897304
Loss in iteration 144 : 0.32387510748701837
Loss in iteration 145 : 0.323860000253462
Loss in iteration 146 : 0.3238451130393513
Loss in iteration 147 : 0.32383044474261163
Loss in iteration 148 : 0.3238160142846669
Loss in iteration 149 : 0.32380183524266193
Loss in iteration 150 : 0.3237879003951822
Loss in iteration 151 : 0.32377418882805103
Loss in iteration 152 : 0.3237606846566437
Loss in iteration 153 : 0.3237473875741191
Loss in iteration 154 : 0.3237343068491971
Loss in iteration 155 : 0.32372144752916393
Loss in iteration 156 : 0.32370880328772716
Loss in iteration 157 : 0.32369636139558167
Loss in iteration 158 : 0.3236841128496889
Loss in iteration 159 : 0.3236720571419695
Loss in iteration 160 : 0.32366019817535574
Loss in iteration 161 : 0.32364853683990874
Loss in iteration 162 : 0.3236370678756815
Loss in iteration 163 : 0.32362578316987
Loss in iteration 164 : 0.32361467716198283
Loss in iteration 165 : 0.32360374885724835
Loss in iteration 166 : 0.32359299918505813
Loss in iteration 167 : 0.32358242707217333
Loss in iteration 168 : 0.32357202818238884
Loss in iteration 169 : 0.3235617970134112
Loss in iteration 170 : 0.3235517297376152
Loss in iteration 171 : 0.3235418249596209
Loss in iteration 172 : 0.3235320820657948
Loss in iteration 173 : 0.3235224991818024
Loss in iteration 174 : 0.32351307274660784
Loss in iteration 175 : 0.3235037988023197
Loss in iteration 176 : 0.32349467445467367
Loss in iteration 177 : 0.32348569808767025
Loss in iteration 178 : 0.32347686836318257
Loss in iteration 179 : 0.3234681831829526
Loss in iteration 180 : 0.3234596396050034
Loss in iteration 181 : 0.3234512346141725
Loss in iteration 182 : 0.3234429658538814
Loss in iteration 183 : 0.3234348316304733
Loss in iteration 184 : 0.3234268303220635
Loss in iteration 185 : 0.3234189598652368
Loss in iteration 186 : 0.32341121779232057
Loss in iteration 187 : 0.3234036016792861
Loss in iteration 188 : 0.32339610949981906
Loss in iteration 189 : 0.32338873956527936
Loss in iteration 190 : 0.32338149018490575
Loss in iteration 191 : 0.3233743594219857
Loss in iteration 192 : 0.32336734515980237
Loss in iteration 193 : 0.32336044535564595
Loss in iteration 194 : 0.3233536582046662
Loss in iteration 195 : 0.32334698207360163
Loss in iteration 196 : 0.3233404153104222
Loss in iteration 197 : 0.32333395613452565
Loss in iteration 198 : 0.32332760269677024
Loss in iteration 199 : 0.3233213532195653
Loss in iteration 200 : 0.3233152060676235
Testing accuracy  of updater 9 on alg 0 with rate 0.0098 = 0.787, training accuracy 0.8442861767562317, time elapsed: 2089 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4954431158502519
Loss in iteration 3 : 0.4844468834105553
Loss in iteration 4 : 0.5206830361816669
Loss in iteration 5 : 0.5302956593021587
Loss in iteration 6 : 0.500401172580938
Loss in iteration 7 : 0.44277918782745623
Loss in iteration 8 : 0.3830059691981134
Loss in iteration 9 : 0.34960834338480096
Loss in iteration 10 : 0.3541589434849582
Loss in iteration 11 : 0.37816555896712056
Loss in iteration 12 : 0.3905158416380347
Loss in iteration 13 : 0.37897554665774275
Loss in iteration 14 : 0.35545401960108686
Loss in iteration 15 : 0.33699255731305355
Loss in iteration 16 : 0.3312123979129627
Loss in iteration 17 : 0.3358515377043344
Loss in iteration 18 : 0.344542055729295
Loss in iteration 19 : 0.3515649143672109
Loss in iteration 20 : 0.3538971318573861
Loss in iteration 21 : 0.3513696901668799
Loss in iteration 22 : 0.34586761403461597
Loss in iteration 23 : 0.34014058168672695
Loss in iteration 24 : 0.3365836425621792
Loss in iteration 25 : 0.33629939716219626
Loss in iteration 26 : 0.33874738324290854
Loss in iteration 27 : 0.34215970398290474
Loss in iteration 28 : 0.3445527395719594
Loss in iteration 29 : 0.34477254469987606
Loss in iteration 30 : 0.3429588219768983
Loss in iteration 31 : 0.34022412255391815
Loss in iteration 32 : 0.3378774313777941
Loss in iteration 33 : 0.3367331326330454
Loss in iteration 34 : 0.33684890569181575
Loss in iteration 35 : 0.33769218382812266
Loss in iteration 36 : 0.33852144871276185
Loss in iteration 37 : 0.33875228761969484
Loss in iteration 38 : 0.3381691638099949
Loss in iteration 39 : 0.33694537084010007
Loss in iteration 40 : 0.33550564609431743
Loss in iteration 41 : 0.3343074374238367
Loss in iteration 42 : 0.3336342483966369
Loss in iteration 43 : 0.333486438664013
Loss in iteration 44 : 0.33361492573460394
Loss in iteration 45 : 0.33367595844252973
Loss in iteration 46 : 0.33342014420379407
Loss in iteration 47 : 0.3328099835222462
Loss in iteration 48 : 0.3320073448333638
Loss in iteration 49 : 0.3312551298208397
Loss in iteration 50 : 0.3307362269262199
Loss in iteration 51 : 0.33049090573446843
Loss in iteration 52 : 0.3304242576784055
Loss in iteration 53 : 0.3303802794015367
Loss in iteration 54 : 0.3302312979497847
Loss in iteration 55 : 0.3299362686559785
Loss in iteration 56 : 0.3295461442729764
Loss in iteration 57 : 0.3291628012076783
Loss in iteration 58 : 0.32887830947001373
Loss in iteration 59 : 0.3287274103502883
Loss in iteration 60 : 0.3286764409014445
Loss in iteration 61 : 0.32865094200637374
Loss in iteration 62 : 0.3285824733137713
Loss in iteration 63 : 0.3284455581844595
Loss in iteration 64 : 0.3282644354965218
Loss in iteration 65 : 0.32809005476245345
Loss in iteration 66 : 0.32796571257330054
Loss in iteration 67 : 0.327903392937963
Loss in iteration 68 : 0.32788236764097967
Loss in iteration 69 : 0.3278664458934565
Loss in iteration 70 : 0.32782654515057763
Loss in iteration 71 : 0.3277551111193719
Loss in iteration 72 : 0.3276660302230173
Loss in iteration 73 : 0.32758268034443233
Loss in iteration 74 : 0.3275227295584828
Loss in iteration 75 : 0.32748872275753155
Loss in iteration 76 : 0.3274689231016137
Loss in iteration 77 : 0.3274463503139349
Loss in iteration 78 : 0.32740942911941917
Loss in iteration 79 : 0.3273576626792552
Loss in iteration 80 : 0.32729977951770756
Loss in iteration 81 : 0.3272467934378388
Loss in iteration 82 : 0.3272050497140588
Loss in iteration 83 : 0.32717336810684833
Loss in iteration 84 : 0.3271451452287025
Loss in iteration 85 : 0.3271132018692474
Loss in iteration 86 : 0.3270740299566001
Loss in iteration 87 : 0.32702911664682965
Loss in iteration 88 : 0.3269831463949521
Loss in iteration 89 : 0.3269406962604196
Loss in iteration 90 : 0.3269036032378626
Loss in iteration 91 : 0.3268704076216849
Loss in iteration 92 : 0.326837825976216
Loss in iteration 93 : 0.3268030258218354
Loss in iteration 94 : 0.3267652152689669
Loss in iteration 95 : 0.3267257328114844
Loss in iteration 96 : 0.3266868643533338
Loss in iteration 97 : 0.3266503285460321
Loss in iteration 98 : 0.3266163861297278
Loss in iteration 99 : 0.3265839729337019
Loss in iteration 100 : 0.3265515772497104
Loss in iteration 101 : 0.32651820523104785
Loss in iteration 102 : 0.32648385784578865
Loss in iteration 103 : 0.326449338230592
Loss in iteration 104 : 0.32641562923279765
Loss in iteration 105 : 0.3263832831961627
Loss in iteration 106 : 0.3263521761821351
Loss in iteration 107 : 0.3263217020862887
Loss in iteration 108 : 0.32629121179362647
Loss in iteration 109 : 0.32626039494491305
Loss in iteration 110 : 0.32622939144524965
Loss in iteration 111 : 0.3261986183553841
Loss in iteration 112 : 0.3261684677515295
Loss in iteration 113 : 0.3261390771642525
Loss in iteration 114 : 0.3261102928559531
Loss in iteration 115 : 0.32608181023994265
Loss in iteration 116 : 0.32605337557136704
Loss in iteration 117 : 0.32602491984649745
Loss in iteration 118 : 0.32599656004139377
Loss in iteration 119 : 0.3259684921700506
Loss in iteration 120 : 0.3259408587844607
Loss in iteration 121 : 0.32591367286248585
Loss in iteration 122 : 0.32588683190003753
Loss in iteration 123 : 0.32586019710461867
Loss in iteration 124 : 0.3258336791833999
Loss in iteration 125 : 0.32580727923303693
Loss in iteration 126 : 0.3257810691715401
Loss in iteration 127 : 0.32575513453969046
Loss in iteration 128 : 0.325729520277547
Loss in iteration 129 : 0.3257042106572097
Loss in iteration 130 : 0.3256791486264494
Loss in iteration 131 : 0.3256542757508801
Loss in iteration 132 : 0.3256295654670548
Loss in iteration 133 : 0.32560503144197067
Loss in iteration 134 : 0.32558071082884926
Loss in iteration 135 : 0.32555663693424824
Loss in iteration 136 : 0.3255328192923211
Loss in iteration 137 : 0.3255092414420008
Loss in iteration 138 : 0.3254858743982128
Loss in iteration 139 : 0.3254626949632623
Loss in iteration 140 : 0.32543969720044463
Loss in iteration 141 : 0.32541689159638876
Loss in iteration 142 : 0.32539429471432085
Loss in iteration 143 : 0.3253719172190017
Loss in iteration 144 : 0.3253497576340054
Loss in iteration 145 : 0.3253278044297533
Loss in iteration 146 : 0.32530604364324
Loss in iteration 147 : 0.3252844664986436
Loss in iteration 148 : 0.32526307253842945
Loss in iteration 149 : 0.3252418672642147
Loss in iteration 150 : 0.32522085671112155
Loss in iteration 151 : 0.32520004273423636
Loss in iteration 152 : 0.32517942165864056
Loss in iteration 153 : 0.3251589864618744
Loss in iteration 154 : 0.32513873053748255
Loss in iteration 155 : 0.32511865051708216
Loss in iteration 156 : 0.3250987466571292
Loss in iteration 157 : 0.32507902101848507
Loss in iteration 158 : 0.32505947493201437
Loss in iteration 159 : 0.32504010738645067
Loss in iteration 160 : 0.3250209151206626
Loss in iteration 161 : 0.3250018940447111
Loss in iteration 162 : 0.32498304089647645
Loss in iteration 163 : 0.32496435410622176
Loss in iteration 164 : 0.32494583350443856
Loss in iteration 165 : 0.3249274792634754
Loss in iteration 166 : 0.32490929084534004
Loss in iteration 167 : 0.3248912665797099
Loss in iteration 168 : 0.32487340400462905
Loss in iteration 169 : 0.32485570062379604
Loss in iteration 170 : 0.3248381545511825
Loss in iteration 171 : 0.3248207646799056
Loss in iteration 172 : 0.3248035303637796
Loss in iteration 173 : 0.32478645089259306
Loss in iteration 174 : 0.3247695251124346
Loss in iteration 175 : 0.3247527513899567
Loss in iteration 176 : 0.3247361278769415
Loss in iteration 177 : 0.3247196528605435
Loss in iteration 178 : 0.32470332497377763
Loss in iteration 179 : 0.32468714316664105
Loss in iteration 180 : 0.3246711064999912
Loss in iteration 181 : 0.32465521391862906
Loss in iteration 182 : 0.3246394641431474
Loss in iteration 183 : 0.3246238557229241
Loss in iteration 184 : 0.32460838718824125
Loss in iteration 185 : 0.3245930571917931
Loss in iteration 186 : 0.3245778645567743
Loss in iteration 187 : 0.32456280822021427
Loss in iteration 188 : 0.32454788712517885
Loss in iteration 189 : 0.3245331001360199
Loss in iteration 190 : 0.32451844602313107
Loss in iteration 191 : 0.3245039235132176
Loss in iteration 192 : 0.3244895313625116
Loss in iteration 193 : 0.32447526840458324
Loss in iteration 194 : 0.32446113354874734
Loss in iteration 195 : 0.32444712573926454
Loss in iteration 196 : 0.3244332439072037
Loss in iteration 197 : 0.3244194869452846
Loss in iteration 198 : 0.32440585371649
Loss in iteration 199 : 0.3243923430848893
Loss in iteration 200 : 0.3243789539458968
Testing accuracy  of updater 9 on alg 0 with rate 0.005600000000000001 = 0.787, training accuracy 0.8429912593072192, time elapsed: 2078 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6131462584318327
Loss in iteration 3 : 0.5294327592378624
Loss in iteration 4 : 0.4849063769224536
Loss in iteration 5 : 0.4740181656749326
Loss in iteration 6 : 0.47929814179712626
Loss in iteration 7 : 0.48799440977229397
Loss in iteration 8 : 0.4931808471237272
Loss in iteration 9 : 0.4918733207564349
Loss in iteration 10 : 0.48349086687251813
Loss in iteration 11 : 0.4689221233555277
Loss in iteration 12 : 0.4499712664439272
Loss in iteration 13 : 0.4289781977016712
Loss in iteration 14 : 0.40847534671007457
Loss in iteration 15 : 0.3908065261023467
Loss in iteration 16 : 0.37770649154278235
Loss in iteration 17 : 0.36992456547517183
Loss in iteration 18 : 0.3670386013787844
Loss in iteration 19 : 0.3675914489210475
Loss in iteration 20 : 0.36956245226587947
Loss in iteration 21 : 0.3710136519575519
Loss in iteration 22 : 0.37064560044373107
Loss in iteration 23 : 0.36804874999937026
Loss in iteration 24 : 0.36360644704417844
Loss in iteration 25 : 0.3581692541003287
Loss in iteration 26 : 0.3526818861047505
Loss in iteration 27 : 0.34790211731323994
Loss in iteration 28 : 0.34426583556350854
Loss in iteration 29 : 0.34188171346294355
Loss in iteration 30 : 0.34060598294640365
Loss in iteration 31 : 0.34014686733112387
Loss in iteration 32 : 0.34016320326343935
Loss in iteration 33 : 0.34033921065626566
Loss in iteration 34 : 0.3404305357384817
Loss in iteration 35 : 0.3402842528137612
Loss in iteration 36 : 0.33983869899515806
Loss in iteration 37 : 0.3391096112107069
Loss in iteration 38 : 0.3381684289351927
Loss in iteration 39 : 0.3371176355256235
Loss in iteration 40 : 0.33606699549654045
Loss in iteration 41 : 0.3351135908904938
Loss in iteration 42 : 0.3343276317346078
Loss in iteration 43 : 0.33374506058988035
Loss in iteration 44 : 0.3333669818128711
Loss in iteration 45 : 0.333164986559535
Loss in iteration 46 : 0.3330906419558057
Loss in iteration 47 : 0.333086916617744
Loss in iteration 48 : 0.33309923707606814
Loss in iteration 49 : 0.33308423214702765
Loss in iteration 50 : 0.33301493323684145
Loss in iteration 51 : 0.33288207481545556
Loss in iteration 52 : 0.3326919638346618
Loss in iteration 53 : 0.332461977302264
Loss in iteration 54 : 0.33221500653091673
Loss in iteration 55 : 0.3319740985096144
Loss in iteration 56 : 0.33175823401045546
Loss in iteration 57 : 0.3315797529190753
Loss in iteration 58 : 0.33144351118884335
Loss in iteration 59 : 0.3313475186180604
Loss in iteration 60 : 0.33128460487033773
Loss in iteration 61 : 0.33124459420672975
Loss in iteration 62 : 0.3312165108971923
Loss in iteration 63 : 0.3311904483850868
Loss in iteration 64 : 0.3311588770603943
Loss in iteration 65 : 0.3311173064084458
Loss in iteration 66 : 0.33106433604320623
Loss in iteration 67 : 0.3310012153361971
Loss in iteration 68 : 0.33093107964304747
Loss in iteration 69 : 0.33085804480828007
Loss in iteration 70 : 0.33078632659272467
Loss in iteration 71 : 0.3307195158170836
Loss in iteration 72 : 0.33066009207258373
Loss in iteration 73 : 0.33060920743814753
Loss in iteration 74 : 0.3305667244852225
Loss in iteration 75 : 0.3305314560440206
Loss in iteration 76 : 0.33050153162922147
Loss in iteration 77 : 0.33047480839553295
Loss in iteration 78 : 0.3304492517774452
Loss in iteration 79 : 0.3304232291925498
Loss in iteration 80 : 0.3303956846239632
Loss in iteration 81 : 0.33036618743120366
Loss in iteration 82 : 0.33033487084001883
Loss in iteration 83 : 0.33030229103220354
Loss in iteration 84 : 0.3302692451202828
Loss in iteration 85 : 0.3302365857841828
Loss in iteration 86 : 0.3302050635640806
Loss in iteration 87 : 0.3301752171469157
Loss in iteration 88 : 0.3301473200548469
Loss in iteration 89 : 0.33012138122635787
Loss in iteration 90 : 0.3300971886989758
Loss in iteration 91 : 0.3300743807808585
Loss in iteration 92 : 0.3300525278002175
Loss in iteration 93 : 0.3300312092137475
Loss in iteration 94 : 0.3300100746362174
Loss in iteration 95 : 0.32998888218426725
Loss in iteration 96 : 0.3299675124291033
Loss in iteration 97 : 0.3299459604443094
Loss in iteration 98 : 0.3299243114087711
Loss in iteration 99 : 0.32990270676353506
Loss in iteration 100 : 0.32988130806936866
Loss in iteration 101 : 0.3298602647148573
Loss in iteration 102 : 0.32983968985482665
Loss in iteration 103 : 0.3298196468349668
Loss in iteration 104 : 0.3298001462763859
Loss in iteration 105 : 0.32978115227070864
Loss in iteration 106 : 0.3297625949770642
Loss in iteration 107 : 0.3297443863971251
Loss in iteration 108 : 0.3297264361979667
Loss in iteration 109 : 0.32970866502945534
Loss in iteration 110 : 0.32969101366251674
Loss in iteration 111 : 0.3296734472602225
Loss in iteration 112 : 0.32965595500655537
Loss in iteration 113 : 0.32963854602356
Loss in iteration 114 : 0.3296212429305928
Loss in iteration 115 : 0.3296040745236819
Loss in iteration 116 : 0.3295870689148878
Loss in iteration 117 : 0.32957024814308633
Loss in iteration 118 : 0.3295536248384007
Loss in iteration 119 : 0.32953720108138207
Loss in iteration 120 : 0.32952096921859686
Loss in iteration 121 : 0.32950491412789007
Loss in iteration 122 : 0.32948901629107236
Loss in iteration 123 : 0.32947325502521535
Loss in iteration 124 : 0.3294576113223811
Loss in iteration 125 : 0.3294420699153657
Loss in iteration 126 : 0.32942662038380094
Loss in iteration 127 : 0.3294112573038752
Loss in iteration 128 : 0.3293959795973176
Loss in iteration 129 : 0.32938078933388637
Loss in iteration 130 : 0.32936569028098195
Loss in iteration 131 : 0.3293506864790557
Loss in iteration 132 : 0.32933578106486094
Loss in iteration 133 : 0.3293209754831969
Loss in iteration 134 : 0.32930626913917965
Loss in iteration 135 : 0.3292916594627762
Loss in iteration 136 : 0.32927714229663146
Loss in iteration 137 : 0.32926271248309963
Loss in iteration 138 : 0.32924836451780065
Loss in iteration 139 : 0.3292340931512824
Loss in iteration 140 : 0.32921989385102435
Loss in iteration 141 : 0.3292057630750046
Loss in iteration 142 : 0.32919169834761
Loss in iteration 143 : 0.32917769816224157
Loss in iteration 144 : 0.3291637617583639
Loss in iteration 145 : 0.3291498888319951
Loss in iteration 146 : 0.3291360792383095
Loss in iteration 147 : 0.32912233273524455
Loss in iteration 148 : 0.3291086488012239
Loss in iteration 149 : 0.3290950265419396
Loss in iteration 150 : 0.3290814646839653
Loss in iteration 151 : 0.3290679616394344
Loss in iteration 152 : 0.3290545156175395
Loss in iteration 153 : 0.3290411247556496
Loss in iteration 154 : 0.3290277872448296
Loss in iteration 155 : 0.32901450143022337
Loss in iteration 156 : 0.32900126587451506
Loss in iteration 157 : 0.3289880793808799
Loss in iteration 158 : 0.32897494097906843
Loss in iteration 159 : 0.3289618498835511
Loss in iteration 160 : 0.32894880543549526
Loss in iteration 161 : 0.32893580704075465
Loss in iteration 162 : 0.3289228541144274
Loss in iteration 163 : 0.32890994603949686
Loss in iteration 164 : 0.3288970821433845
Loss in iteration 165 : 0.32888426169264995
Loss in iteration 166 : 0.3288714839030884
Loss in iteration 167 : 0.3288587479605095
Loss in iteration 168 : 0.3288460530466461
Loss in iteration 169 : 0.32883339836485914
Loss in iteration 170 : 0.3288207831613396
Loss in iteration 171 : 0.32880820673906713
Loss in iteration 172 : 0.3287956684634642
Loss in iteration 173 : 0.3287831677602491
Loss in iteration 174 : 0.32877070410717385
Loss in iteration 175 : 0.3287582770219938
Loss in iteration 176 : 0.3287458860492277
Loss in iteration 177 : 0.3287335307479537
Loss in iteration 178 : 0.3287212106823277
Loss in iteration 179 : 0.3287089254157341
Loss in iteration 180 : 0.3286966745087445
Loss in iteration 181 : 0.32868445752039277
Loss in iteration 182 : 0.32867227401183113
Loss in iteration 183 : 0.32866012355125035
Loss in iteration 184 : 0.32864800571890906
Loss in iteration 185 : 0.3286359201113666
Loss in iteration 186 : 0.32862386634428836
Loss in iteration 187 : 0.32861184405355326
Loss in iteration 188 : 0.3285998528947324
Loss in iteration 189 : 0.32858789254125814
Loss in iteration 190 : 0.3285759626817574
Loss in iteration 191 : 0.3285640630170768
Loss in iteration 192 : 0.3285521932574848
Loss in iteration 193 : 0.3285403531204167
Loss in iteration 194 : 0.32852854232895845
Loss in iteration 195 : 0.32851676061114304
Loss in iteration 196 : 0.32850500769995217
Loss in iteration 197 : 0.32849328333384537
Loss in iteration 198 : 0.3284815872575853
Loss in iteration 199 : 0.32846991922310875
Loss in iteration 200 : 0.32845827899025876
Testing accuracy  of updater 9 on alg 0 with rate 0.0014000000000000002 = 0.7845, training accuracy 0.8391065069601813, time elapsed: 2020 millisecond.
